|Source|Title|Summary|
|---|---|---|
|2507.09618v1|[THAI Speech Emotion Recognition (THAI-SER) corpus](http://arxiv.org/abs/2507.09618v1)|总结：  <br/>提出首个泰语音情感识别语料库THAI-SER，包含丰富多场景数据与专业注释，提供高质量的模型评估结果及开放代码。<br/><br/>贡献点：  <br/>1. **构建首个泰语音情识别语料库**：提供41小时36分钟（27,854句）的多环境数据（Zoom与2个工作室），覆盖200名专业演员的剧本与即兴对话。  <br/>2. **明确情绪分类标准**：定义五类基本情绪（中性、愤怒、快乐、悲伤、挫败），由专业人员标注且通过众包+过滤机制确保一致性（多数同意>0.71）。  <br/>3. **提出高质量评估指标**：采用Krippendorff's alpha计算注释者可靠性（α=0.692），并验证人类识别准确率（0.772），推动任务评价基准。  <br/>4. **公开数据与代码**：语料库及实验代码在Creative Commons许可下开放，支持研究复现与跨领域应用。|
|2507.08012v1|[RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](http://arxiv.org/abs/2507.08012v1)|**贡献点总结**  <br/>提出基于主成分分析的新型微调框架，通过挖掘模型潜在特征实现可控性与灵活性的平衡，验证了方法在提升语音合成可控性方面的有效性。<br/><br/>**分点贡献：**  <br/>1. **创新控制机制**：设计了结合提示与潜在特征的双重控制策略，同时解决传统方法在可控性限制和输入变异不可控之间的矛盾。  <br/>2. **潜在特征提取**：利用主成分分析（PCA）识别合成语音样本中对输出变化贡献最大的潜在特征，并将其作为新标签用于二次微调。  <br/>3. **实验验证**：在Icelandic语音语料库上对比评估两种模型（含/不含情感披露），证明该方法显著提升了模型的可控性。  <br/>4. **多类型控制提升**：针对无情感披露模型，方法不仅增强了连续控制（如语速），还实现离散控制（如性别），优化了整体生成稳定性。|
|2507.07806v1|[End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced   by Semi-supervised Learning](http://arxiv.org/abs/2507.07806v1)|**总结（100字以内）：**  <br/>本文提出利用半监督学习结合声学与语言模型及多任务学习，比较fix-match和full-match方法，实验证明在语音情感与意图识别任务中，通过晚融合技术提升性能达12.3%和10.4%。<br/><br/>**贡献点分点总结：**  <br/>1. **提出半监督学习方案**：解决大规模语音数据标注成本高的问题，融合大量未标注数据与少量标注数据提升模型训练效率。  <br/>2. **端到端多模态模型设计**：同时构建声学和语言模型，采用多任务学习统一处理情感和意图识别任务。  <br/>3. **对比两种半监督方法**：系统比较fix-match和full-match方法在语音和文本数据上的效果差异。  <br/>4. **联合识别平衡度量提升**：通过晚融合技术实现情感与意图联合识别的性能优化，分别提升12.3%和10.4%。  <br/>5. **实验验证有效性**：在真实场景下验证半监督学习对语音情感和意图识别的显著改进作用。|
|2507.07046v1|[A Novel Hybrid Deep Learning Technique for Speech Emotion Detection   using Feature Engineering](http://arxiv.org/abs/2507.07046v1)|总结：  <br/>提出DCRF-BiLSTM模型，实现跨五大数据集的七种情感识别，在多个基准数据集上达到超高水平准确率，验证了模型的鲁棒性和泛化能力。<br/><br/>贡献点：  <br/>1. **多情感识别能力**：首次在五大数据集（RAVDESS、TESS、SAVEE、EmoDB、Crema-D）中实现对7种情感（neutral, happy, sad, angry, fear, disgust, surprise）的高精度识别。  <br/>2. **跨数据集泛化性**：突破性地同时评估单一SER模型在全部五大数据集上的性能，整体准确率达93.76%，填补该领域研究空白。  <br/>3. **性能优越性**：在RAVDESS、SAVEE、Crema-D等数据集上分别取得97.83%、97.02%、95.10%等超高水平准确率，部分数据集达100%。  <br/>4. **多数据集融合验证**：对(R+T+S)组合数据集的识别准确率（98.82%）优于现有方法，证明模型在数据融合场景下的有效性。|
|2507.06670v1|[STARS: A Unified Framework for Singing Transcription, Alignment, and   Refined Style Annotation](http://arxiv.org/abs/2507.06670v1)|**贡献点分点总结**：<br/>1. **提出首个统一框架**：STARS是首个同时处理唱歌转录、对齐及风格注释的整合性方案，突破现有方法仅针对单个任务的局限。<br/>2. **多层级综合标注**：涵盖音素-音频对齐、音符转录与时间定位、声乐技巧识别、全局风格特征（情感、节奏）等四个维度的标注。<br/>3. **分层声学特征处理**：设计跨帧、词、音素、音符、句子层级的层次化声学特征提取与分析架构。<br/>4. **新型非自回归编码器**：引入非自回归本地声学编码器，实现结构化的分层表示学习，提升标注效率与准确性。<br/>5. **验证性能优势**：在多个评估维度上显著优于现有标注方法，证实框架的优越性。<br/>6. **提升SVS效果**：应用STARS标注数据训练的语音合成模型，在感知自然性和风格控制方面表现更优。<br/>7. **解决可扩展性问题**：克服数据集创建的规模化挑战，推动可控式歌唱语音合成方法的发展。<br/><br/>**总结**（100字以内）：  <br/>STARS提出首个统一框架，实现多层级唱歌标注，包含音素对齐、音符转录、声乐技巧识别与风格特征分析，结合新型编码器与分层处理，显著提升语音合成效果与数据集可扩展性。|
|2507.05911v1|[Differentiable Reward Optimization for LLM based TTS system](http://arxiv.org/abs/2507.05911v1)|**贡献点分点列表：**  <br/>1. 提出差异可微奖励优化（DiffRO）方法，直接基于神经编码器令牌计算奖励，避免传统RLHF依赖合成音频的瓶颈。  <br/>2. 引入Gumbel-Softmax技术，使奖励函数可微，简化RLHF训练流程并提升效率。  <br/>3. 设计多任务奖励（MTR）模型，从多维度提供反馈，增强系统对指令的遵循能力。  <br/>4. 实验验证DiffRO显著提升TTS发音准确性，达到seed-tts-eval基准的SOTA WER表现，并实现情感与质量属性的零样本控制。  <br/><br/>**总结（100字以内）：**  <br/>本论文提出DiffRO方法及MTR模型，通过直接利用神经编码器令牌计算奖励，结合Gumbel-Softmax技术优化训练流程，显著提升TTS系统的发音准确性和情感控制能力，取得SOTA结果。|
|2507.04598v1|[Multi-Step Prediction and Control of Hierarchical Emotion Distribution   in Text-to-Speech Synthesis](http://arxiv.org/abs/2507.04598v1)|**贡献点：**  <br/>1. 提出分层情感分布（ED）预测模块，实现言语、词语、音素层级的多级情感渲染定量控制。  <br/>2. 引入多步骤情感预测机制，通过全局情感上下文优化局部情感变化，捕捉语音情感的层级结构。  <br/>3. 构建兼容多种TTS系统的模块化设计，包括变体适配器和外部模块，提升方法的通用性。  <br/>4. 通过客观与主观评估验证有效性，显著增强语音情感表达并实现多粒度情感渲染的精准控制。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出分层情感预测模块，结合多步骤机制与系统兼容设计，实现对语音情感的多层级定量控制，显著提升情感表达精度与自然度。|
|2507.04349v1|[TTS-CtrlNet: Time varying emotion aligned text-to-speech generation with   ControlNet](http://arxiv.org/abs/2507.04349v1)|**贡献点总结（100字以内）：**  <br/>该论文提出TTS-CtrlNet，首次将ControlNet应用于TTS情绪控制，通过冻结原模型并引入可训练副本实现高效、时间可变的情绪调控，保留原始模型性能，同时提供三种实用方案并取得情感相似度SOTA结果。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：首次将ControlNet的条件控制机制引入TTS领域，提出TTS-CtrlNet框架，通过冻结预训练模型并使用可训练副本处理额外情绪条件，实现无需全量微调的情绪控制。  <br/>2. **情绪控制能力提升**：支持时间可变、可扩展的细粒度情绪调控，同时保留原模型的零样本语音克隆与语音自然度等关键能力。  <br/>3. **实用方案设计**：提出三个具体可落地的优化策略：基于块分析的架构设计、情绪特定的流步长调整、灵活的控制尺度调节。  <br/>4. **实验验证**：在情感相似度指标（Emo-SIM、Aro-Val SIM）上取得SOTA性能，验证了方法的有效性，并提供项目资源供复现。|
|2507.04048v1|[CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization   via Soft Prompt Tuning](http://arxiv.org/abs/2507.04048v1)|总结：  <br/>本文提出CLEP-DG框架，通过微调CLAP增强情感特征编码，引入文本驱动的ACPT策略优化声学环境建模，并利用跨模态迁移缓解领域偏移，实验表明在情感识别任务中优于现有方法。<br/><br/>贡献点：  <br/>1. **情感对齐增强**：通过大规模情感语音数据集微调CLAP模型，获得CLEP，提升对情绪相关特征的编码能力。  <br/>2. **文本驱动的声学环境建模**：提出Acoustic Context Prompt Tuning（ACPT）策略，在无需标注音频的情况下优化可学习提示向量以适应多样声学条件。  <br/>3. **跨模态领域迁移**：利用文本衍生嵌入训练分类器，并在推理时应用至音频编码器，减少文本监督与音频情感识别之间的领域差异。  <br/>4. **性能提升验证**：在五个基准数据集上验证了CLEP-DG框架，实现监督与领域泛化场景下的SOTA性能。|
|2507.03382v1|[Speaker-agnostic Emotion Vector for Cross-speaker Emotion Intensity   Control](http://arxiv.org/abs/2507.03382v1)|总结：  <br/>提出一种说话人无关的情绪向量，解决跨说话人情绪强度控制中的说话人一致性问题，实现高质量且可控制的情绪语音生成，适用于任意说话人，包括未见过的场景。<br/><br/>贡献点：  <br/>1. **提出说话人无关情绪向量**：设计可跨多个说话人捕捉共享情绪表达的模型，解决传统方法因说话人差异导致的不一致性问题。  <br/>2. **跨说话人情绪强度控制**：仅需目标说话人的中性语音即可生成指定情绪强度的语音，突破原有方法局限于同说话人场景的限制。  <br/>3. **保持语音质量与可控制性**：在确保跨说话人一致性的同时，维持语音自然度和情绪调控的精确性。  <br/>4. **适用性扩展**：方法无需特定说话人训练，可泛化到任意未知说话人，提升实际应用的灵活性。|
|2507.03251v2|[Toward Efficient Speech Emotion Recognition via Spectral Learning and   Attention](http://arxiv.org/abs/2507.03251v2)|总结（100字以内）:  <br/>提出集成数据增强的1D-CNN框架，结合通道与空间注意力机制，使用MFCC特征提升情感识别精度与泛化能力，在多数据集表现优异，验证了实际应用潜力。<br/><br/>贡献点分点:  <br/>1. **提出1D-CNN框架**：引入数据增强技术提升模型鲁棒性和特征多样性。  <br/>2. **双注意力机制**：通过通道与空间注意力突出关键情绪模式，增强对细微情感变化的捕捉能力。  <br/>3. **特征选择优化**：采用MFCC作为频谱特征，实现计算情感处理与人类听觉感知的更紧密衔接。  <br/>4. **多数据集验证**：在SAVEE、RAVDESS、CREMA-D等数据集上达到SOTA精度，建立SER新基准。  <br/>5. **应用潜力**：证明深度学习方法在真实场景（如助人技术、人机交互）中的泛化能力与可行性。|
|2507.03251v1|[Toward Efficient Speech Emotion Recognition via Spectral Learning and   Attention](http://arxiv.org/abs/2507.03251v1)|**贡献点**（分点）:  <br/>1. **提出融合MFCC频谱特征的SER框架**：首次将Mel-Frequency Cepstral Coefficients（MFCCs）作为核心频谱特征，弥合计算情感处理与人类听觉感知之间的差距。  <br/>2. **设计增强鲁棒性的1D-CNN模型**：引入数据增强技术并构建基于1D卷积神经网络的框架，提升模型对复杂语音信号的适应能力。  <br/>3. **集成通道与空间注意力机制**：通过注意力模块优化特征提取，突出关键情感模式，增强对细微情感变化的识别能力。  <br/>4. **多数据集性能验证**：在SAVEE、RAVDESS、CREMA-D、TESS、EMO-DB和EMOVO等多个基准数据集上取得SOTA结果，显著提升准确率。  <br/>5. **推动实际应用潜力**：验证深度学习方法在多样化数据上的泛化能力，证明其在辅助技术与人机交互中的可行性和先进性。  <br/><br/>**总结**（100字以内）:  <br/>本文提出基于MFCC和1D-CNN的SER框架，融合数据增强与注意力机制，显著提升情感识别准确率，验证了其在多数据集上的泛化能力，为实际应用提供新思路。|
|2507.02080v1|[TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation](http://arxiv.org/abs/2507.02080v1)|**总结**  <br/>本研究提出TAGF框架，通过时间门控机制动态融合多模态情感特征，有效解决跨模态对齐和噪声问题，提升valence-arousal估计性能，并在现实场景中展示出强鲁棒性。<br/><br/>**贡献点**  <br/>1. **提出时间感知门控融合框架（TAGF）**  <br/>   - 首次结合时间动态与递归注意力机制，通过BiLSTM实现跨模态特征的时序加权融合。<br/><br/>2. **动态调整多模态特征贡献**  <br/>   - 利用递归步骤的相对重要性判断，适应性优化不同模态信息在情感识别中的权重分配。<br/><br/>3. **强鲁棒性设计**  <br/>   - 有效缓解音频-视觉跨模态对齐偏差，提升模型在噪声和真实场景下的稳定性。<br/><br/>4. **实验证明有效性**  <br/>   - 在Aff-Wild2数据集上验证，TAGF性能优于现有递归注意力模型，且在情感过渡建模上表现可靠。|
|2507.01022v1|[Workflow-Based Evaluation of Music Generation Systems](http://arxiv.org/abs/2507.01022v1)|总结：  <br/>该研究提出了针对音乐生成系统的结构化评估框架，分析其在创作流程中的局限性与协作潜力，为AI在创意领域的应用提供了实证指导。<br/><br/>贡献点：  <br/>1. **构建系统评估框架**：设计结合技术分析与实践实验的评价体系，专门针对音乐生成系统的实际应用及创造性潜力。  <br/>2. **实证分析系统局限性**：揭示当前音乐生成系统在主题连贯性与结构一致性上的不足，强调人类创造力在复杂决策中的关键作用。  <br/>3. **探索非线性创作流程**：首次系统性研究音乐创作的迭代特性，提出适用于非线性工作流的评估方法。  <br/>4. **提出AI协作发展方向**：确定AI在音乐创作中作为协作工具的可行整合领域，促进人机协同的创造性应用。  <br/>5. **方法论改进与实践指导**：为后续全面研究提供方法论优化建议，并为音乐生成技术的实际开发路径提供数据支持。|
|2506.21619v1|[IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](http://arxiv.org/abs/2506.21619v1)|总结:  <br/>IndexTTS2提出了一种自回归模型友好的语音时长控制方法，支持精确和自由生成模式，并实现情感与音色的解耦控制，在零样本场景下表现优异。<br/><br/>贡献点:  <br/>1. **双生成模式设计**：支持显式指定token数量以精确控制语音时长，以及无需手动输入的自由生成模式以保留输入韵律。  <br/>2. **情感与身份解耦**：实现情绪表达和说话人身份的分离，允许对音色和情感进行独立控制。  <br/>3. **零样本情感复现**：模型可准确还原输入提示的情感特征，并支持跨说话人的情感输入以重建目标音色。  <br/>4. **GPT潜表示增强**：引入GPT latent representations提升语音稳定性，尤其在强情感表达时表现更佳。  <br/>5. **软指令机制**：基于文本描述的自然语言输入，通过微调Qwen3实现情绪倾向的灵活引导，降低控制门槛。  <br/>6. **性能超越现有模型**：在单词错误率、说话人相似度和情感保真度等指标上优于当前最先进的零样本TTS模型。|
|2506.19887v1|[MATER: Multi-level Acoustic and Textual Emotion Representation for   Interpretable Speech Emotion Recognition](http://arxiv.org/abs/2506.19887v1)|1. 提出Multi-level Acoustic-Textual Emotion Representation (MATER)框架，首次在语音情感识别任务中融合声学、文本特征于单词、句子和嵌入层级。  <br/>2. 通过整合低级词汇特征与高级语境化表示，有效捕捉语调微变和语义细节，提升自然语音处理能力。  <br/>3. 引入不确定性感知的集成策略，解决标注不一致性问题，增强对模糊情感表达的鲁棒性。  <br/>4. 在SERNC挑战中取得优异成绩：宏F1达41.01%，平均CCC为0.5928，valence预测任务中以CCC 0.6941位列第二。|
|2506.18196v1|[Two Sonification Methods for the MindCube](http://arxiv.org/abs/2506.18196v1)|总结：  <br/>本文提出两种基于MindCube的音乐控制映射方案，探索其在情绪调节中的应用潜力，创新性地引入生成AI在潜在空间中注入意义及导航技术，为情感化音乐交互系统提供新思路。<br/><br/>贡献点：  <br/>1. **重新定位MindCube的音乐控制功能**：首次将该设备作为音乐系统控制器，突出其在情绪调节中的应用价值。  <br/>2. **提出两种映射方法**：分别设计了基于AI与非AI的交互映射方案，对比验证其在音乐生成中的差异。  <br/>3. **生成AI映射的创新技术**：开发了在潜在空间中注入语义的方法，并实现外部控制器对潜在空间的导航控制。  <br/>4. **实验与未来方向**：通过实验证明方案有效性，并为后续研究提供改进方向和应用扩展建议。|
|2506.16381v1|[InstructTTSEval: Benchmarking Complex Natural-Language Instruction   Following in Text-to-Speech Systems](http://arxiv.org/abs/2506.16381v1)|总结（100字以内）：  <br/>本研究提出InstructTTSEval，构建了首个支持中英双语的复杂指令式语音合成评估基准，包含三种任务类型与1k测试案例，并引入Gemini自动评估工具，推动该领域实现更灵活、准确的语音风格控制技术。<br/><br/>贡献点：  <br/>1. **提出首个指令式语音合成评估基准**：针对当前缺乏专用基准的问题，构建InstructTTSEval，涵盖Acoustic-Parameter Specification、Descriptive-Style Directive和Role-Play三个任务。  <br/>2. **设计多维度任务与语言支持**：包含英语和中文子集，共6k测试案例（1k/语言），与参考音频配对，覆盖语音生成的复杂指令场景。  <br/>3. **引入自动化评估工具Gemini**：利用Gemini的指令跟随能力进行模型评估，解决传统人工评估效率低、主观性强的局限。  <br/>4. **揭示指令式TTS的改进空间**：通过实证分析现有系统的性能表现，为后续优化提供量化依据和研究方向。  <br/>5. **推动语音合成技术发展**：指出更灵活、准确的指令式语音合成的潜在价值，促进模型在情感、语调等参数控制上的提升。|
|2506.16310v1|[Optimizing Multilingual Text-To-Speech with Accents & Emotions](http://arxiv.org/abs/2506.16310v1)|**总结（100字以内）:**  <br/>本研究提出了一种面向南亚多语言的TTS架构，通过语言特异性音素对齐、文化敏感情感嵌入及动态语调切换技术，显著提升印地语与印度英语的语调准确率与情感识别效果，并实现跨语言实时混合生成，具备实际应用价值。  <br/><br/>**贡献点分点:**  <br/>1. **多语言语调与情感联合建模**：首次将 accents（语调）与 multi-scale emotion modelling（多尺度情感建模）结合，特别针对印地语和印度英语，解决多语言环境中语调与情感表达不一致的问题。  <br/>2. **语言特异性音素对齐架构**：设计混合编码器-解码器结构，实现音素对齐与音译保留，提升多语言发音准确性（Word Error Rate 降低 23.7%）。  <br/>3. **文化敏感情感嵌入**：引入基于母语者语料的情感嵌入层，增强情感表达的真实性（情感识别准确率达 85.3%）。  <br/>4. **动态语调切换技术**：采用 residual vector quantization（残差向量量化）实现跨语言语调实时切换，支持如 "Namaste, let's talk..." 等混合语句生成。  <br/>5. **显著性能提升**：实验结果超越 METTS 和 VECL-TTS 基线，语调准确率提升 23.7%，情感识别准确率达 85.3%，且用户主观评价 MOS 为 4.2/5。  <br/>6. **实际应用价值**：为南亚教育科技（EdTech）和无障碍软件提供可扩展的跨语言合成解决方案，验证了系统在真实场景中的可行性。|
|2506.15754v1|[Explainable speech emotion recognition through attentive pooling:   insights from attention-based temporal localization](http://arxiv.org/abs/2506.15754v1)|**贡献点**  <br/>1. **提出新型池化方法**：提出 Multi-Query Multi-Head Attentive Statistics Pooling（MMAS Pooling），在平均池化基础上实现3.5%的宏F1提升。  <br/>2. **揭示情感信息的局部性**：通过注意力分析发现，仅15%的帧包含80%的情感线索，表明情感表征具有高度局部化特征。  <br/>3. **发现语音特征优先级与人类策略一致**：高注意力帧中非语言发声和超清晰音素被显著优先处理，模拟了人类对情感信号的感知机制。  <br/>4. **验证注意力池化的有效性与可解释性**：论证注意力池化不仅是高性能的SER技术，还为情感定位提供生物可解释的依据。  <br/>5. **实验性能表现突出**：在Interspeech 2025自然条件下的SER挑战赛中，模型宏F1达0.3649，证明方法的有效性。  <br/><br/>**总结**（100字以内）：  <br/>提出注意力池化新方法，揭示情感信息局部分布规律，模拟人类感知策略，提升模型性能并增强可解释性，于挑战赛中取得显著成果。|
|2506.12573v3|[Video-Guided Text-to-Music Generation Using Public Domain Movie   Collections](http://arxiv.org/abs/2506.12573v3)|总结：  <br/>提出OSSL数据集与视频适配器，提升电影音乐生成模型在情绪、类型和配对保真度上的表现，并开放数据与代码促进研究。<br/><br/>贡献点：  <br/>1. 构建首个整合视觉内容、对话与情绪标注的电影音乐数据集OSSL（36.5小时公有领域影片）。  <br/>2. 设计视频条件视频适配器，增强文本到音乐模型的多模态理解能力。  <br/>3. 验证方法有效性，显著提升MusicGen-Medium在分布保真度、配对保真度及主观情绪匹配上的性能。  <br/>4. 公开数据集、代码与演示工具，推动模型训练与研究可复现性。|
|2506.12573v2|[Video-Guided Text-to-Music Generation Using Public Domain Movie   Collections](http://arxiv.org/abs/2506.12573v2)|**贡献点（分点）：**  <br/>1. **提出OSSL数据集**：首个整合电影片段、高质量配乐及人类标注情绪信息的综合性数据集，包含约36.5小时公共领域影片，填补电影音乐生成领域数据不足的空白。  <br/>2. **设计视频适配器**：开发一种基于视频条件的视频适配器，增强文本到音乐的自回归Transformer模型（如MusicGen-Medium），提升模型对场景内容的理解能力。  <br/>3. **验证有效性**：通过实验表明，该方法在配乐生成任务中显著改善生成音乐的客观保真度（分布与配对）及主观情绪/类型匹配表现。  <br/>4. **开源共享**：提供数据集与代码，方便后续研究复现和扩展，促进电影音乐生成技术的发展（URL见摘要末尾）。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出OSSL数据集和视频适配器方法，显著提升电影音乐生成模型的性能，填补了现有数据不足的空白，并开源以推动领域发展。|
|2506.12573v1|[Video-Guided Text-to-Music Generation Using Public Domain Movie   Collections](http://arxiv.org/abs/2506.12573v1)|总结：本文提出Open Screen Sound Library数据集及视频适配器，通过整合多要素提升电影音乐生成质量，实验证明方法在客观和主观指标上有效，并公开数据和代码。<br/><br/>贡献点：<br/>1. **构建首个综合电影音乐数据集**：OSSL包含36.5小时电影片段、同步音轨及人类标注的情绪信息，填补了电影制作场景与生成系统间的数据鸿沟。<br/>2. **开发视频条件化适配器**：提出基于视频的条件增强模块，改进文本到音乐的Transformer模型，实现视觉-音乐协同生成。<br/>3. **验证生成效果提升**：实验表明该方法在分布一致性、配对保真度、情绪匹配和风格一致性等指标上显著优于基线模型。<br/>4. **开源研究资源**：提供完整数据集、代码及模型，推动电影音乐生成领域的研究与应用。|
|2506.12325v1|[GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum   Perspective for Conversation Emotion Recognition](http://arxiv.org/abs/2506.12325v1)|总结：  <br/>提出GSDNet模型，通过图谱扩散解决多模态情感识别中的模态缺失问题，有效保持语义和拓扑信息，实现SOTA性能。<br/><br/>贡献点：  <br/>1. **提出新型图谱扩散框架**：首次结合图神经网络与扩散模型，创新性地将Gaussian噪声映射至图谱空间以恢复缺失模态。  <br/>2. **解决模态缺失问题**：通过保留邻接矩阵的原始分布，避免直接噪声破坏图结构，从而维持全局拓扑信息和关键谱特征。  <br/>3. **提升多模态融合能力**：改进传统方法对语义信息的处理，实现更鲁棒的模态恢复以增强情感识别效果。  <br/>4. **实验验证优越性**：在多种模态缺失场景下，GSDNet达到SOTA性能，证明其在实际应用中的有效性。|
|2506.10574v1|[DanceChat: Large Language Model-Guided Music-to-Dance Generation](http://arxiv.org/abs/2506.10574v1)|**贡献点：**  <br/>1. **提出LLM作为编舞者的框架**：首次将大型语言模型用于生成显式文本运动指令，弥补音乐与舞蹈动作之间的语义鸿沟。  <br/>2. **三阶段模块化方法**：设计伪指令生成模块、多模态特征融合模块及扩散模型合成模块，实现音乐、节奏与文本的协同引导。  <br/>3. **多模态对齐损失**：引入跨模态对齐机制，确保生成的舞蹈动作与音乐及文本指令在风格和结构上高度一致。  <br/>4. **数据集验证**：在AIST++数据集上验证方法有效性，证明其在多样性和风格匹配上优于现有技术。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出DanceChat框架，利用LLM生成文本指令并结合多模态技术，解决音乐到舞蹈生成中的语义鸿沟与多样性挑战，实验表明其在AIST++数据集上表现优于现有方法。|
|2506.08717v1|[Multi-Teacher Language-Aware Knowledge Distillation for Multilingual   Speech Emotion Recognition](http://arxiv.org/abs/2506.08717v1)|**贡献点分点总结**：<br/>1. 提出语言感知的多教师知识蒸馏方法，首次实现跨语言SER的模型迁移。<br/>2. 构建支持英、芬、法三语的多语言SER系统，突破单语模型的局限性。<br/>3. 以Wav2Vec2.0为基础构建教师模型，通过知识蒸馏提取跨语言特征。<br/>4. 学生模型在英语（加权召回率72.9）和芬兰语（非加权召回率63.4）数据集上达到SOTA性能。<br/>5. 显著提升悲伤与中性情绪的召回率，但存在识别愤怒和快乐情绪的性能瓶颈。<br/>6. 验证了多语言知识蒸馏在语音情感识别中的有效性与挑战性。<br/><br/>**总结（100字以内）**：<br/>提出语言感知多教师知识蒸馏方法，实现跨语言SER；基于Wav2Vec2.0构建多语言系统，超越基线性能；在悲伤和中性情绪识别上效果显著，但愤怒与快乐仍有挑战。|
|2506.08346v1|[SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on   Speech Classification Models](http://arxiv.org/abs/2506.08346v1)|**贡献点总结（100字以内）**  <br/>本论文提出 Speech Prompt Backdoor Attack (SPBA) 攻击方法，利用语音元素（音色、情感等）和 SLLM 生成多样化触发器，显著提升攻击有效性；同时引入 Multiple Gradient Descent Algorithm (MGDA) 作为防御策略，揭示语音分类模型在后门攻击中的安全隐患。<br/><br/>---<br/><br/>**分点贡献列表**  <br/>1. **提出新型语音后门攻击框架**  <br/>   - 首次将语音元素（如音色、情感）作为攻击目标，结合 SLLM 生成多样化的触发器（poisoned speech samples），突破传统触发函数的局限性，提升攻击的泛化性和有效性。  <br/><br/>2. **设计多目标优化防御策略**  <br/>   - 提出基于 Multiple Gradient Descent Algorithm (MGDA) 的缓解方法，通过多梯度下降算法优化攻击与防御的对抗过程，降低攻击成功率并提高模型鲁棒性。  <br/><br/>3. **验证攻击与防御效果**  <br/>   - 在两个典型语音分类任务（关键词检测、说话人验证）中进行全面实验，实验结果表明 SPBA 攻击在触发有效性（trigger effectiveness）和攻击指标（attack metrics）上均表现出色，验证了其威胁性及防御策略的实际意义。|
|2506.06820v1|[Beyond Classification: Towards Speech Emotion Reasoning with Multitask   AudioLLMs](http://arxiv.org/abs/2506.06820v1)|总结：  <br/>提出基于音频大语言模型的情感推理框架，通过生成解释性内容提升情感识别性能，在多任务学习中结合推理增强数据监督、双编码器架构和任务交替训练策略，实验验证了该方法在IEMOCAP和MELD数据集上有效提升情感预测准确性和生成解释的连贯性。  <br/><br/>贡献点：  <br/>1. **情感推理方法创新**：首次将情感理解与音频LLMs的生成能力结合，通过生成语义对齐、证据支持的解释提升情感识别的可解释性与准确性。  <br/>2. **统一框架设计**：提出融合推理增强数据监督、双编码器架构和任务交替训练的多任务学习框架，有效整合情感建模与语音任务的学习目标。  <br/>3. **实验验证有效性**：在IEMOCAP和MELD数据集上验证方法，显著提升情感预测准确率，并增强生成响应的连贯性与证据合理性。|
|2506.04779v1|[MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark](http://arxiv.org/abs/2506.04779v1)|总结：  <br/>本文提出MMSU基准，涵盖多模态语音理解的47任务与5000数据对，整合语音学、韵律等语言现象，评估现有SpeechLLMs并指明改进方向，为复杂人机语音交互系统发展提供资源支撑。<br/><br/>贡献点：  <br/>1. **提出MMSU基准**：首个针对语音理解与推理的综合性评估基准，覆盖47个任务和5,000个音频-question-answer三元组。  <br/>2. **构建多模态数据集**：整合语音的细致声学信息、语义、情感及韵律特征，为语言模型提供更全面的训练与测试数据。  <br/>3. **语言理论驱动设计**：系统性纳入语音学、语义学、修辞学等语言现象，增强基准的学术严谨性与实践指导意义。  <br/>4. **量化评估模型能力**：对14个SpeechLLMs进行严格验证，揭示其在细粒度感知与复杂推理上的不足，为改进方向提供依据。  <br/>5. **推动人机交互发展**：建立新的评估标准，助力开发更智能、更自然的语音交互系统，提升实际应用效果。  <br/>6. **开放数据与代码**：提供基准及评估代码的公开访问链接，促进研究复现与社区协作。|
|2506.04013v1|[Towards Better Disentanglement in Non-Autoregressive Zero-Shot   Expressive Voice Conversion](http://arxiv.org/abs/2506.04013v1)|总结：  <br/>本研究提出一种基于条件变分自编码器的改进框架，通过多语言离散语音单位和对抗性损失减少源音色泄露，并结合局部F0与全局音高能量特征提升表达性迁移，显著优于基线模型。<br/><br/>贡献点：  <br/>1. **提出改进的语音转换框架**：采用条件变分自编码器结合自监督和非自回归机制，优化风格迁移过程。  <br/>2. **内容与风格解耦策略**：利用多语言离散语音单位作为内容表示，通过对抗性相似性损失和混合风格层归一化减少源音色泄露。  <br/>3. **增强表达性迁移能力**：引入局部F0特征（通过交叉注意力）和全局音高能量特征，提升情感与说话人风格的适配效果。|
|2506.02863v1|[CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech](http://arxiv.org/abs/2506.02863v1)|总结（100字以内）:  <br/>本研究提出CapSpeech，首个大规模、多任务基准数据集，包含1.36亿音频-文本对，涵盖风格/口音/情感/聊天场景生成等场景。通过实验验证了模型在多样语境下的高保真合成能力，并为CapTTS系统研究提供了关键洞见。<br/><br/>贡献点分点列出：  <br/>1. **提出CapSpeech基准**：构建首个涵盖风格、口音、情感、聊天场景等多任务的CapTTS综合数据集，填补领域标准化数据空白。  <br/>2. **数据规模与多样性**：包含10.36百万真实/人工标注音频-文本对，覆盖广泛说话风格和场景，是当前最大CapTTS相关数据集。  <br/>3. **专用数据集开发**：为AgentTTS和CapTTS-SE任务设计专业录制数据集，由声优与音频工程师共同完成，提升数据质量。  <br/>4. **多模型实验验证**：对比自回归与非自回归模型性能，证明系统在高保真与高可懂度语音生成上的有效性。  <br/>5. **任务场景扩展**：定义并支持风格-事件、口音、情感、聊天机器人等新型CapTTS任务，推动应用边界拓展。  <br/>6. **标注方法创新**：结合机器标注与人工标注，兼顾效率与准确性，为下游任务提供多维度语料支持。  <br/>7. **系统研究价值**：通过实验分析揭示CapTTS开发中的核心挑战，为后续算法优化提供理论依据与实证参考。|
|2506.02742v1|[Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions](http://arxiv.org/abs/2506.02742v1)|总结：  <br/>提出基于LLM-TTS架构的PUE方法，实现零样本下多样情感语音的生成与量化控制，提升情感表达的自然性和灵活性。<br/><br/>贡献点：  <br/>1. **突破传统情绪分类限制**：首次提出针对未见过情绪（Prompt-Unseen-Emotion）的生成框架，扩展了TTS系统的情感表达范围。  <br/>2. **情感引导的提示学习机制**：设计情感引导的提示学习方法，通过LLM-TTS架构实现情绪相关提示与生成语音之间的情感一致性。  <br/>3. **情绪权重量化控制**：使模型能够量化捕捉每句话中不同情绪的权重，并在推理阶段灵活调整情绪比例以生成混合情感语音。  <br/>4. **零样本情感合成有效性**：验证PUE在零样本场景下的性能，成功生成未见过情绪的语音，证明其无需额外训练数据即可泛化到新情感类别。  <br/>5. **情感风格量化建模**：结合LLM的上下文知识，实现对不同情感风格的量化建模，增强生成语音的多样性与自然交互能力。|
|2506.02258v1|[Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal   Emotion Recognition?](http://arxiv.org/abs/2506.02258v1)|总结：  <br/>本研究首次将mamba-based模型用于非语音发声情绪识别，提出RENO框架并实现SOTA性能，突破传统注意力机制的局限，展现异构模型融合的有效性。<br/><br/>贡献点：  <br/>1. **首次应用MAFMs于NVER**：提出基于mamba的状态空间建模方法，用于非语音发声情绪识别任务，突破传统注意力机制的局限。  <br/>2. **RENO框架设计**：引入Rényi散度损失函数和自注意力机制，实现基础模型的高效对齐与内部表示交互优化。  <br/>3. **性能验证**：通过实验验证MAFMs在NVER中优于AAFMs，证明其捕捉情绪结构的能力。  <br/>4. **模型融合探索**：借鉴语音情感识别、合成语音检测领域，首次尝试将基础模型融合应用于NVER。  <br/>5. **SOTA结果**：通过MAFMs与AAFMs的异构融合，实现当前NVER任务的最佳性能，超越先前方法。|
|2506.02230v1|[Towards Machine Unlearning for Paralinguistic Speech Processing](http://arxiv.org/abs/2506.02230v1)|贡献点总结（100字以内）:  <br/>本研究首次将机器遗忘应用于语音伴随特征处理，提出SISA++方法，在SER和DD任务中验证其优于SOTA方法，并提供可操作的配方指导未来研究，有效缓解模型遗忘后的性能退化问题。<br/><br/>分点贡献：<br/>1. **领域创新**：首次将机器遗忘（MU）系统性引入语音伴随特征处理（PSP）领域，探索其在语音情感识别（SER）和抑郁症检测（DD）等任务中的应用潜力。<br/>2. **方法改进**：提出SISA++，通过分片模型权重平均技术改进原有SISA方法，在保持模型轻量化的同时提升遗忘后的性能稳定性。<br/>3. **实证验证**：在CREMA-D（SER）和E-DAIC（DD）基准数据集上验证SISA++的优越性，证明其遗忘后性能显著优于基线方法。<br/>4. **实践指导**：设计"配方指南"（cookbook recipes），提供特征表示选择和下游架构优化的可操作建议，降低MU在PSP中的应用门槛。|
|2506.02088v1|[Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion   and Prosodic Features for the Speech Emotion Recognition in Naturalistic   Conditions Challenge at Interspeech 2025](http://arxiv.org/abs/2506.02088v1)|总结（100字以内）:  <br/>本文提出一种结合先进音频模型与增强文本特征的语音情感识别系统，通过F0量化、预训练模型和集成方法提升性能，验证了图注意力网络在融合技术中的有效性，并公开代码供复现。<br/><br/>贡献点:  <br/>1. **方法创新**：融合最新音频模型与文本特征（含韵律/频谱线索），提升自然语音情感识别的鲁棒性。  <br/>2. **技术验证**：系统性验证了基频（F0）量化的有效性，并利用预训练音频标签模型增强特征表示。  <br/>3. **模型优化**：引入集成模型（Ensemble）策略，显著提高模型在复杂环境下的稳定性与准确性。  <br/>4. **模型效能**：通过融合技术分析，证实了图注意力网络（GATs）在情感识别中的关键作用。  <br/>5. **开放性**：公开完整源代码，便利学术界复现与进一步研究。|
|2506.02059v1|[Learning More with Less: Self-Supervised Approaches for Low-Resource   Speech Emotion Recognition](http://arxiv.org/abs/2506.02059v1)|总结：  <br/>本文提出利用对比学习与BYOL自监督方法提升低资源语言的语音情感识别性能，通过实验证明其有效性，并分析模型行为揭示关键影响因素与挑战，为构建包容、可解释的系统奠定基础。<br/><br/>贡献点：  <br/>1. **引入自监督方法**：首次将对比学习（CL）与Bootstrap Your Own Latent（BYOL）应用于低资源语言（LRLs）的语音情感识别（SER），解决标注数据不足的瓶颈问题。  <br/>2. **实证性能提升**：在乌尔都语、德语和孟加拉语等低资源语言上实现显著F1分数提升（10.6%、15.2%、13.9%），验证方法的有效性。  <br/>3. **分析模型行为**：系统研究模型在不同语言中的表现，提出影响跨语言泛化的关键因素，并揭示低资源SER的挑战。  <br/>4. **推动系统发展**：为构建更包容（覆盖低资源语言）、可解释（揭示影响因素）和鲁棒（提升泛化能力）的语音情感识别系统提供理论基础与实践指导。|
|2506.01483v3|[Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction](http://arxiv.org/abs/2506.01483v3)|**贡献点总结（100字以内）：**  <br/>提出基于说话人相对特征的新方法，通过整合连续（时间顺序、年龄、音高）与离散（语言、性别、情感）线索提升语音分离性能，尤其在复杂场景和跨语言环境下表现出更强的鲁棒性，并结合预训练模型微调优化效果。<br/><br/>**分点贡献：**  <br/>1. **提出基于说话人间相对特征的语音分离新框架**：利用说话人相对属性（如性别、时间顺序）而非固定分类方法，增强模型对目标说话人识别的灵活性。  <br/>2. **区分连续与离散特征的处理策略**：连续特征通过相对差异建模，离散特征保留类别区分，有效捕捉多维说话人信息。  <br/>3. **实验验证多线索整合的有效性**：结果表明结合所有相对线索优于随机子集，性别和时间顺序对性能提升最显著。  <br/>4. **引入预训练模型微调方案**：基于WavLM Base + CNN编码器的优化方法显著优于传统Conv1d基线，提升整体效果。  <br/>5. **拓展复杂场景下的适用性**：额外线索（如音高、响度、说话时长）在特定复杂条件下（如多语言、回声）表现出额外优势。|
|2506.01157v1|[Source Tracing of Synthetic Speech Systems Through Paralinguistic   Pre-Trained Representations](http://arxiv.org/abs/2506.01157v1)|**贡献点：**  <br/>1. 提出将语音学预训练模型（paralinguistic SPTM）应用于合成语音生成系统（STSGS）的源追踪任务，填补了这一领域的研究空白。  <br/>2. 验证语音学预训练模型在STSGS中优于传统单语、多语和说话人识别模型，证明其对提取源特征的更高效性。  <br/>3. 设计TRIO框架，通过门控机制实现跨模型表示的自适应加权，结合典型相关损失对齐表示并利用自注意力优化特征。  <br/>4. 实现SOTA性能：融合TRILLsson（语音学SPTM）与x-vector（说话人识别SPTM）后，TRIO在STSGS任务中超越单一模型与基线融合方法。  <br/><br/>**总结：**  <br/>该研究提出利用语音学预训练模型提升合成语音源追踪性能，设计TRIO框架融合多源表示，实现SOTA效果。|
|2506.01138v1|[PARROT: Synergizing Mamba and Attention-based SSL Pre-Trained Models via   Parallel Branch Hadamard Optimal Transport for Speech Emotion Recognition](http://arxiv.org/abs/2506.01138v1)|贡献点总结（100字以内）:<br/>提出PARROT框架，融合Mamba与注意力机制，结合最优传输和Hadamard乘积技术，在语音情感识别任务中实现超越同构模型和基线方法的SOTA性能，验证了异构预训练模型融合的有效性。|
|2505.24493v1|[MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging   LLM Embedded Knowledge](http://arxiv.org/abs/2505.24493v1)|总结：  <br/>本研究提出利用GPT-4o对语音情感数据进行自动标注，创建首个完全由大语言模型标注的多模态情感数据集MELT，并验证其在自监督学习中的有效性，显著提升语音情感识别性能。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的无监督语音情感标注方法**：首次探索GPT-4o在无需人工监督的情况下，通过文本线索为多模态语音数据生成情感标签的可行性。  <br/>2. **构建全自动生成的多模态情感数据集MELT**：开发首个完全基于大语言模型标注的公开数据集，突破传统人工标注的效率与一致性瓶颈。  <br/>3. **验证数据集对SSL模型的提升效果**：通过微调四种自监督学习框架，证明MELT在语音情感识别任务中具有更高的性能表现。  <br/>4. **设计结构化文本提示增强模型能力**：通过精细化的文本提示工程，使GPT-4o更高效利用训练知识，生成准确且语境相关的标注。  <br/>5. **提供主观实验支持**：通过主观实验验证模型标注结果的可靠性，补充量化评估之外的可信性证明。|
|2505.23962v1|[Can Emotion Fool Anti-spoofing?](http://arxiv.org/abs/2505.23962v1)|**贡献点：**<br/>1. 提出首个面向情感变化的语音合成对抗样本数据集EmoSpoof-TTS，涵盖多样情感状态。<br/>2. 揭示现有反欺骗模型在情感合成语音上的性能缺陷，指出情感针对性攻击存在的安全风险。<br/>3. 设计GEM（门控情感模型集成）框架，通过情感识别门控网络提升模型对多情感及中性状态的鲁棒性。<br/>4. 公开数据集与模型，推动语音反欺骗领域对情感维度的研究与技术发展。<br/><br/>**总结：**  <br/>该论文提出情感导向语音合成对抗数据集与GEM模型，解决传统反欺骗方法忽视情感变化导致的防御不足问题，提升对情感攻击的检测效果。|
|2505.23236v1|[Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable   Emotion Recognition](http://arxiv.org/abs/2505.23236v1)|**贡献点：**  <br/>1. **端到端框架创新**：提出基于LLM（大语言模型）的可解释语音情感识别（SER）方法，整合SER与语音情感描述（SED）的细粒度特征（如音高、语调、重音）联合预测。  <br/>2. **特征解耦与多任务学习**：通过交替的LLM微调策略，从HuBERT的SSL表示中解耦情感特征，同时优化SER和ASR任务的协同训练。  <br/>3. **特征粒度控制技术**：利用VAE压缩的HuBERT特征结合信息瓶颈（IB）方法，动态调整特征的粒度以提升模型效果。  <br/>4. **性能验证与效果提升**：在IEMOCAP和MELD数据集上验证方法的有效性，显著优于LLaMA-based SER基线，SER准确率提升4.0%（绝对值）和5.4%（相对值）。  <br/>5. **可解释性增强**：通过提取情绪描述符（SED），为SER提供更清晰的可解释性，增强模型决策的透明度。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于LLM的端到端可解释SER框架，通过特征解耦与粒度优化提升性能，并在基准数据集上取得显著效果，同时增强模型的可解释性。|
|2505.23009v1|[EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,   Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](http://arxiv.org/abs/2505.23009v1)|**贡献点：**  <br/>1. 提出新的TTS评估框架**EmergentTTS-Eval**，覆盖6类复杂场景（情感表达、语用特征、外语词汇、语法复杂性、复杂发音及问题回答）。  <br/>2. 采用自动化测试生成与评估机制，支持框架的可扩展性与灵活性。  <br/>3. 基于少量种子提示，利用LLM生成1,645个多样化测试案例，针对性测试结构性、发音及韵律挑战。  <br/>4. 引入**LALM（Large Audio Language Model）**作为评委，从多维度（情感、韵律、语调、发音准确性）评估语音质量。  <br/>5. 对主流TTS系统（如11Labs、Deepgram、OpenAI 4o-mini-TTS）进行系统性评估，揭示其细粒度性能差异。  <br/>6. 开源评估代码与数据集，促进研究复现与社区应用。  <br/><br/>**总结：**  <br/>提出新型TTS基准EmergentTTS-Eval，覆盖6大复杂场景，结合自动化生成评估与模型作为评委方法，揭示性能差异，开源代码与数据集。|
|2505.22133v2|[Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices](http://arxiv.org/abs/2505.22133v2)|总结（100字以内）:  <br/>该论文提出SAILER系统，针对自然情感语音的主观注解和标签不平衡问题，通过简化建模、优化学习目标和数据增强策略，实现了高表现的语音情感识别，单系统超越95%参赛作品，集成系统位列前三，并开源代码便于复现。<br/><br/>贡献点整理:  <br/>1. **解决情感标注难题**：针对自然情感语音中情绪注解的主观性，提出鲁棒的处理方法，提升模型对模糊标注的适应能力。  <br/>2. **应对标签不平衡问题**：利用挑战任务的不平衡数据集，设计有效策略缓解类别分布不均对模型性能的影响。  <br/>3. **提出SAILER系统**：构建了一个简单、可复现、高效的语音情感识别系统，为研究提供建模和工程实践范式。  <br/>4. **性能突破**：单系统在INTERSPEECH 2025任务中取得Macro-F1>0.4，超越95%参赛作品；三系统集成后位列前三，验证方法的有效性。  <br/>5. **开源实现**：公开代码库（GitHub链接），促进研究社区的复现与进一步改进。|
|2505.20794v1|[VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing   Voice Conversion](http://arxiv.org/abs/2505.20794v1)|**贡献点总结（100字以内）：**  <br/>提出VibESVC模型，通过离散小波变换显式提取和操控颤音，克服传统隐式方法的局限，实现精准的歌唱风格转换并保持说话人特征相似性，实验验证其转换效果高质量。  <br/><br/>**详细贡献点分项：**  <br/>1. **提出新的可控声调转换框架**：首次显式建模颤音（vibrato），通过离散小波变换（DWT）提取声调特征，解决其动态控制难题。  <br/>2. **创新性F0分解方法**：将基频轮廓（F0 contour）分解为频率分量，实现对颤音的精确操控与风格迁移。  <br/>3. **保持说话人相似性的转换**：在风格转换过程中有效保留原说话人的语音特征，提升转换结果的自然度与一致性。  <br/>4. **全面验证有效性**：通过主观和客观评价方法，证明模型在歌唱风格转换任务中的高质量表现。|
|2505.20678v1|[PromptEVC: Controllable Emotional Voice Conversion with Natural Language   Prompts](http://arxiv.org/abs/2505.20678v1)|**贡献点分点总结：**  <br/>1. **提出PromptEVC框架**：首次利用自然语言提示实现可控情感语音转换，突破传统依赖预定义标签或参考音频的局限，提升情感表达的灵活性与多样性。  <br/>2. **情感描述符与提示映射器**：设计可生成细粒度情感嵌入的模块，通过联合训练参考嵌入与情感嵌入，精准捕捉个体情感差异。  <br/>3. **韵律建模与控制机制**：根据语言内容和情感线索动态调整语音韵律（如节奏），增强合成语音的自然度与情感表现力。  <br/>4. **说话人编码器集成**：保留说话人身份特征，避免情感转换过程中语音身份信息的丢失。  <br/>5. **实验验证优越性**：在情感转换、强度控制、混合情感合成及韵律调整等任务中均超越现有SOTA方法。  <br/><br/>**总结（100字内）：**  <br/>PromptEVC通过自然语言提示实现灵活情感控制，引入情感描述符和提示映射器生成细粒度嵌入，结合韵律建模与说话人编码器，在情感语音转换任务中显著优于现有方法。|
|2505.20341v1|[Towards Emotionally Consistent Text-Based Speech Editing: Introducing   EmoCorrector and The ECD-TSE Dataset](http://arxiv.org/abs/2505.20341v1)|**总结（100字以内）：**  <br/>本文提出EmoCorrector，通过RAG技术实现语音情感校正，构建首个TSE情感基准数据集ECD-TSE，验证了其在情感一致性与语音质量上的提升，代码与音频资源公开。<br/><br/>**贡献点分点列出：**  <br/>1. **提出EmoCorrector方法**：首次引入检索增强生成（RAG）技术，提取文本情感特征并检索匹配情感的语音样本，确保合成语音情感一致性和保留说话人身份/质量。  <br/>2. **构建ECD-TSE数据集**：创建首个针对TSE情感校正的基准数据集，包含文本与语音配对数据，支持多样化情感表达和文本变化的联合建模。  <br/>3. **验证情感校正有效性**：通过主观/客观实验与全面分析，证明EmoCorrector显著提升情感表达并解决现有TSE方法的情感不一致问题。  <br/>4. **开源实现与资源**：公开代码和音频示例，推动语音情感编辑领域的研究与应用。|
|2505.20007v2|[Improving Speech Emotion Recognition Through Cross Modal Attention   Alignment and Balanced Stacking Model](http://arxiv.org/abs/2505.20007v2)|**贡献点分点总结：**  <br/>1. **提出跨模态融合机制**：采用跨模态注意力技术，有效整合多模态表示信息，提升语音情感识别的鲁棒性。  <br/>2. **创新处理数据不平衡策略**：  <br/>   - 引入加权交叉熵损失（WCE）缓解类别不平衡问题；  <br/>   - 结合中性情感软边距损失与平衡策略，进一步优化模型对稀有类别的识别能力。  <br/>3. **构建多模型集成系统**：设计并训练12个多模态模型，通过平衡堆叠模型进行集成，提升整体性能。  <br/>4. **实验验证有效性**：在8类情感识别任务中，系统在2025自然条件挑战数据集上取得MacroF1（0.4094）和准确率（0.4128）的显著结果。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出基于跨模态注意力的语音情感识别系统，创新性结合加权损失与中性软边距策略应对类别不平衡，通过多模型集成提升性能，在8类任务中实现较高的MacroF1和准确率，为自然条件下的情感识别提供有效解决方案。|
|2505.19978v1|[DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](http://arxiv.org/abs/2505.19978v1)|**贡献点总结：**  <br/>1. 提出首个大规模、多模态的跨轮次对话数据集DeepDialogue，涵盖41领域、20情感及语音合成。  <br/>2. 引入9种不同参数规模的LM生成对话，并结合人机评估筛选确保质量。  <br/>3. 创新语音合成技术，实现情感一致的语音生成，增强多模态情感上下文连贯性。  <br/>4. 揭示关键发现：小模型连贯性下降、具体领域对话更有意义、跨模型交互优于同模型对话。  <br/><br/>**100字内总结：**  <br/>DeepDialogue是首个大规模多模态对话数据集，涵盖41领域及20情感，并通过语音合成强化情感表达，采用多模型生成与人机评估方法，揭示多轮对话连贯性关键影响因素，推动更自然的跨模态交互研究。|
|2505.19937v1|[ALAS: Measuring Latent Speech-Text Alignment For Spoken Language   Understanding In Multimodal LLMs](http://arxiv.org/abs/2505.19937v1)|**贡献点：**  <br/>1. 提出新型跨模态对齐评估指标ALAS，填补LLMs在语音领域缺乏标准化评估的空白。  <br/>2. 深入分析音频与文本表征在Transformer不同层间的相关性，揭示多模态对齐机制的层次特征。  <br/>3. 在两个语音理解任务（语音问答与情感识别）中验证ALAS的有效性，证明其跨任务与跨层的鲁棒性。  <br/><br/>**总结（100字内）：**  <br/>本文提出ALAS指标，用于评估语音-语言模型的跨模态对齐质量，并通过分析Transformer各层的音频-文本相关性，验证其在语音问答与情感识别任务中的适用性与可靠性。|
|2505.19693v1|[EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical   Representation with Auxiliary Classification](http://arxiv.org/abs/2505.19693v1)|总结：  <br/>提出EmoSphere-SER联合模型，融合球形VAD区域分类与回归，结合动态加权、风格池化及多头自注意力机制，提升语音情感预测的结构化与一致性。<br/><br/>贡献点：  <br/>1. **联合建模框架**：首次将球形VAD区域分类任务与VAD回归任务联合优化，通过分类引导回归提升情感预测精度。  <br/>2. **球面坐标转换**：将传统线性VAD值转换为球面坐标，划分多区域以捕捉情感状态的空间分布特征。  <br/>3. **动态加权机制**：引入动态权重调整策略，增强模型对频谱和时间动态的适应能力。  <br/>4. **风格池化与多头注意力**：设计风格池化层结合多头自注意力机制，有效建模语音的时空特性。  <br/>5. **联合训练策略**：通过结构化学习策略促进预测一致性，实验验证优于现有方法。|
|2505.19687v1|[DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised   Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech](http://arxiv.org/abs/2505.19687v1)|**总结（100字以内）:**  <br/>本研究提出DiEmo-TTS，通过自监督蒸馏方法实现跨说话人情感迁移，有效分离情感与语音特质，解决现有方法的speaker leakage问题，并设计双条件Transformer模型提升风格特征整合，实验验证其在保留说话人身份的同时准确建模情感的能力。<br/><br/>---<br/><br/>**贡献点分点列出:**  <br/>1. **提出DiEmo-TTS方法**：首次使用自监督蒸馏框架，旨在在语音合成中实现跨说话人情感迁移，同时最小化情感信息损失并保留说话人身份。  <br/>2. **引入Cluster-Driven Sampling**：通过情感聚类驱动的采样策略，增强情感特征提取的针对性，减少无关因素干扰。  <br/>3. **设计Information Perturbation技术**：通过信息扰动机制，进一步分离情感与说话人特质，避免speaker leakage。  <br/>4. **开发情感聚类与匹配框架**：结合情感属性预测与说话人嵌入，实现对无标签数据的泛化能力，提升情感迁移的鲁棒性。  <br/>5. **提出Dual Conditioning Transformer**：设计双条件Transformer模型，优化风格特征的整合效果，提升合成质量。  <br/>6. **实验验证有效性**：通过实验证明方法能有效学习speaker-irrelevant的emotion embeddings，验证其理论与实践价值。|
|2505.19437v1|[RA-CLAP: Relation-Augmented Emotional Speaking Style Contrastive   Language-Audio Pretraining For Speech Retrieval](http://arxiv.org/abs/2505.19437v1)|总结：  <br/>该研究提出情感语音检索任务（ESSR）及专用模型ESS-CLAP，并创新性地设计关系增强CLAP（RA-CLAP）以提升情感说话风格描述性能，通过自蒸馏学习局部匹配关系，实验证明其有效性。<br/><br/>贡献点：  <br/>1. **提出新型任务**：定义情感说话风格检索（ESSR）任务，拓展语音描述研究领域。  <br/>2. **设计专用模型**：开发ESS-CLAP模型，专门学习语音与自然语言描述的关联关系。  <br/>3. **改进传统方法**：提出关系增强CLAP（RA-CLAP），突破传统二元关系假设，增强模型泛化能力。  <br/>4. **引入自蒸馏技术**：通过自蒸馏学习语音与描述的潜在局部匹配关系，提升任务表现。  <br/>5. **实验证明有效性**：在ESSD领域验证RA-CLAP的优越性，为相关研究提供有效参考。|
|2505.19103v1|[WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](http://arxiv.org/abs/2505.19103v1)|**贡献点**：  <br/>1. 提出WHISTRESS，首个无需对齐的语音重音检测方法，提升转录系统性能；  <br/>2. 构建TINYSTRESS-15K，首个可扩展的合成训练数据集，通过全自动流程生成；  <br/>3. 实验证明WHISTRESS在零样本迁移场景下表现优于现有方法，无需额外输入先验；  <br/>4. 突破合成数据限制，实现跨多样语音任务的强泛化能力。  <br/><br/>**总结**：  <br/>WHISTRESS通过合成数据TINYSTRESS-15K训练，无需对齐和先验信息，显著提升语音重音检测效果，并展现优异的零样本迁移能力。|
|2505.18498v1|[Learning Emotion-Invariant Speaker Representations for Speaker   Verification](http://arxiv.org/abs/2505.18498v1)|**贡献点总结**（100字以内）:  <br/>提出CopyPaste数据增强、余弦相似度损失和情感感知掩码三类改进，提升说话人编码器对情绪的鲁棒性，并通过消融实验验证有效性，使EER下降19.29%。  <br/><br/>**分点贡献**:  <br/>1. **数据增强**：基于CopyPaste方法收集同一说话人不同情绪的平行数据，增强训练多样性。  <br/>2. **损失函数优化**：引入余弦相似度损失，减少说话人表示内的类别差异，降低与情绪信息的关联性。  <br/>3. **情感感知掩码**：利用语音信号能量对输入样本进行情感感知掩码处理，进一步增强说话人表示的鲁棒性。  <br/>4. **消融实验验证**：系统分析各改进组件的影响，证实方法的有效性（EER下降19.29%）。|
|2505.18484v1|[Token-Level Logits Matter: A Closer Look at Speech Foundation Models for   Ambiguous Emotion Recognition](http://arxiv.org/abs/2505.18484v1)|**贡献点分点总结：**  <br/>1. **提出针对模糊情感预测的提示设计**：设计专门用于处理情绪模糊性的提示，提升语音模型在复杂情感识别任务中的适用性。  <br/>2. **创新性方法推断模糊情感分布**：首次引入两种新型方法：通过生成文本响应分析情感，以及通过token级logits研究模型内部情感处理机制。  <br/>3. **揭示SFMs在token级的情感识别能力**：发现尽管语音模型难以生成精准文本响应，但其在token级基于先验知识具备情感识别的鲁棒性，为改进情绪感知模型提供新思路。  <br/>4. **为下一代情绪感知模型提供理论依据**：强调在大型语音基础模型（SFMs）时代，理解其处理模糊情感的能力对后续模型开发具有重要意义。|
|2505.18453v1|[MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal   Prompt](http://arxiv.org/abs/2505.18453v1)|总结：  <br/>本文提出基于多模态提示的定制化情感ZS-TTS系统，通过解耦语音成分与情感一致性损失提升情感控制能力，结合扩散模型实现高质量语音生成，显著优于现有系统。<br/><br/>贡献点：  <br/>1. **多模态情感提示分离**：首次支持通过文本、图像或语音提供情感提示，突破传统单模态限制。  <br/>2. **语音多成分解耦**：将语音分解为内容、音色、情感和语调，实现情感与语调的独立建模。  <br/>3. **多模态情感编码器设计**：创新性构建统一的多模态情感提取模块，适配不同类型的输入提示。  <br/>4. **情感一致性损失提出**：引入损失函数保障情感信息在语调生成中的完整性。  <br/>5. **扩散模型声学生成**：采用高效扩散模型生成目标mel谱图，提升语音自然度和相似性。  <br/>6. **实验验证性能优势**：通过客观与主观实验验证系统在情感表达和语音质量上的优越性。|
|2505.18217v1|[ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic   Conditions Challenge](http://arxiv.org/abs/2505.18217v1)|**总结**：  <br/>该研究提出Abhinaya系统，通过多模态融合与自监督模型优化，有效应对自然语音情感识别中的挑战，实现优异性能。<br/><br/>**贡献点**：  <br/>1. **多模态整合**：开发Abhinaya系统，结合语音、文本及语音-文本联合模型，提升情感识别的鲁棒性。  <br/>2. **SLLM微调策略**：利用自监督和语音大语言模型（SLLM）进行语音表征学习，捕捉细粒度情感线索。  <br/>3. **文本上下文增强**：通过大语言模型（LLM）提取文本语境信息，辅助情感判断。  <br/>4. **类别不平衡解决方案**：设计定制损失函数并采用多数投票生成分类决策，缓解数据分布不均问题。  <br/>5. **实际效果验证**：系统在Interspeech挑战中取得第4名，完成训练后达到当前最佳性能，证明方法有效性。|
|2505.17655v1|[Audio-to-Audio Emotion Conversion With Pitch And Duration Style Transfer](http://arxiv.org/abs/2505.17655v1)|**贡献点分点总结：**  <br/>1. **提出A2A-ZEST框架**：首个实现零样本情感风格迁移（Zero-shot Emotion Style Transfer）的音频到音频方法，无需参考与源语音的对齐数据。  <br/>2. **分析-合成结构设计**：将语音分解为语义标记、说话人表征和情感嵌入，分离内容与风格特征。  <br/>3. **自监督训练策略**：基于自动编码损失的纯自监督训练，无需人工标注或并行语料。  <br/>4. **融合多源表征**：结合参考情感嵌入和源语音的其他表征（语义、说话人）生成目标语音，实现风格迁移。  <br/>5. **实验验证优势**：在内容/说话人保留和情感迁移效果上优于现有方法，且无需平行训练数据。  <br/>6. **应用场景拓展**：用于情感识别任务的数据增强，提升数据多样性和模型泛化能力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出A2A-ZEST框架，实现无需对齐数据的情感风格迁移，通过自监督学习分离并融合语音内容、说话人和情感特征，显著提升迁移效果，并拓展至情感识别的数据增强应用。|
|2505.17589v2|[CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training](http://arxiv.org/abs/2505.17589v2)|总结：  <br/>本文提出CosyVoice 3，通过多任务监督训练的新型语音分词器、可微奖励模型、百万小时多语言数据训练及模型参数扩容，显著提升零样本多语言语音合成的性能和通用性。<br/><br/>贡献点：  <br/>1. **多语言语音分词器**：采用监督多任务训练，整合自动语音识别、情感识别、语言识别、音频事件检测及说话人分析，提升韵律自然度。  <br/>2. **通用奖励模型**：设计可微奖励模型，适用于CosyVoice 3及其它LLM-based语音合成模型，优化后训练效果。  <br/>3. **超大规模数据集**：训练数据从10万小时扩展至100万小时，覆盖9种语言及18种中文方言，提升跨领域与格式的泛化能力。  <br/>4. **更大模型参数**：模型参数从0.5B增至1.5B，增强多语言基准测试表现，提升内容一致性和说话人相似性。|
|2505.16369v2|[X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance](http://arxiv.org/abs/2505.16369v2)|贡献点：  <br/>1. **提出X-ARES框架**：设计了一个全面的开源基准，系统性评估音频编码器的性能。  <br/>2. **多领域覆盖**：涵盖语音、环境音、音乐三大领域，支持跨任务、跨场景的对比研究。  <br/>3. **双重评估方法**：引入线性微调（linear fine-tuning）和无参数评估（unparameterized evaluation）两种互补的评估方式。  <br/>4. **任务多样性与全面性**：包含22个任务，覆盖语音识别、情感检测、声景分类、音乐分类等关键领域。  <br/>5. **揭示性能差异**：通过实验证明不同模型在任务和领域间的显著性能差异，强调通用音频表示学习的挑战。  <br/><br/>总结（100字以内）：  <br/>X-ARES提出一个跨领域、多任务的开放式评估框架，通过两种评估方法系统分析音频编码器性能，揭示其在不同任务与场景中的差异，推动通用音频表示学习的研究与优化。|
|2505.15773v1|[ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic   Utterance Tonality](http://arxiv.org/abs/2505.15773v1)|总结：  <br/>本文提出ToxicTone，首个大规模标注口语中文毒性数据集，涵盖13类主题及多维度标注，结合多模态框架提升毒性检测效果，验证了语音特征对识别潜在危害性表达的重要性。<br/><br/>贡献点：  <br/>1. **构建首个大规模口语中文毒性数据集**：ToxicTone包含真实场景音频，覆盖13个主题，标注毒性类型（如亵渎、欺凌）和来源（如愤怒、讽刺），填补语音领域研究空白。  <br/>2. **多模态检测框架创新**：集成声学、语言和情感特征，利用先进语音情感编码器，提升对复杂毒性表达的识别能力。  <br/>3. **验证语音特征的关键作用**：实验表明该框架在毒性检测任务中显著优于文本-only和基线模型，凸显语音特性对揭示潜在毒性信息的不可替代性。|
|2505.15772v1|[MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech   Paralinguistic and Affect Labeling](http://arxiv.org/abs/2505.15772v1)|总结：  <br/>本研究提出MIKU-PAL全自动多模态系统，实现高一致性情感语音提取，显著提升效率并释放新基准数据集MIKU-EmoBench，支持26种细粒度情感分类。<br/><br/>贡献点：  <br/>1. **提出MIKU-PAL全自动多模态管道**：首次构建无需人工标注的语音情感提取系统，通过视频数据自动获取高质量情感语音。  <br/>2. **多模态大语言模型（MLLM）应用**：结合面部检测与多模态语言模型，提升情感分析的准确性与一致性（Fleiss κ=0.93）。  <br/>3. **高效低成本**：相较人工标注，显著降低标注成本与时间，同时保持接近人类水平的准确率（MELD测试中达68.5%）。  <br/>4. **支持26种细粒度情感分类**：实现更精细的情感标签体系，经人类验证具有83%的合理性。  <br/>5. **发布MIKU-EmoBench数据集**：提供131.2小时情感语音数据，为情感语音合成与视觉语音克隆建模提供新基准。|
|2505.15667v1|[Segmentation-Variant Codebooks for Preservation of Paralinguistic and   Prosodic Information](http://arxiv.org/abs/2505.15667v1)|**贡献点总结（100字以内）：**  <br/>提出Segmentation-Variant Codebooks（SVCs），通过分语言单位量化语音并分解为多流离散特征，有效保留韵律和旁语言学信息。实验表明，池化时机影响信息保留，改进的池化策略提升重合成质量与风格表现，同时保持可理解性。<br/><br/>**分点贡献：**  <br/>1. **提出SVCs方法**：首次设计基于不同语言单位（帧、音素、词、语句）的分段量化机制，将语音分解为段特定的离散特征流，显著提升对韵律和旁语言学信息（如情感、重音）的保留。  <br/>2. **优化池化策略**：发现池化操作应在离散化前执行，而非后，从而更高效保留段级信息，改善模型表现。  <br/>3. **验证效果**：通过多项探针任务和重合成实验，证明SVCs在风格实现（如情感、语调）和语音质量上优于传统量化方法，同时维持语音可理解性。|
|2505.15004v2|[EASY: Emotion-aware Speaker Anonymization via Factorized Distillation](http://arxiv.org/abs/2505.15004v2)|总结：  <br/>本研究提出EASY框架，通过顺序解缠和分节蒸馏方法，首次在说话人匿名化中同时分离身份、语言内容和情感信息，有效保护隐私且保留情感与语言内容，实验验证其优越性。<br/><br/>贡献点：  <br/>1. 提出首个融合情感感知的说话人匿名化框架（EASY），突破传统仅分离身份和语言内容的限制。  <br/>2. 引入顺序解缠机制，实现对说话人身份、语言内容及情感表示的三元分离。  <br/>3. 采用分节蒸馏方法，通过因子化建模将三类语音属性分别映射到独立子空间。  <br/>4. 独立约束说话人身份与情感表示，降低信息泄露风险，兼顾隐私保护与情感保留。  <br/>5. 在VoicePrivacy Challenge数据集上验证方法有效性，显著优于现有基线系统。|
|2505.14648v1|[Vox-Profile: A Speech Foundation Model Benchmark for Characterizing   Diverse Speaker and Speech Traits](http://arxiv.org/abs/2505.14648v1)|**贡献点总结：**  <br/>1. 提出Vox-Profile，首次构建多维（静态+动态）语音/说话人特征基准。  <br/>2. 结合语音科学与语言学，由领域专家参与设计以提升特征索引准确性。  <br/>3. 跨15个公开数据集及多种语音基础模型进行系统性基准实验验证。  <br/>4. 支持下游应用：提高语音识别性能分析、评估语音生成系统、验证自动分析效度。  <br/>5. 公开代码和数据，推动语音领域的研究与应用。|
|2505.14449v3|[Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](http://arxiv.org/abs/2505.14449v3)|总结：  <br/>本文提出一种隐式人口推断模块，结合伪标签与无监督学习缓解语音情感识别中的偏见，有效提升公平性指标且保持模型性能。<br/><br/>贡献点：  <br/>1. **提出隐式人口推断（IDI）框架**：首次在语音情感识别（SER）中引入无显式标签的人口特征推断方法，解决因隐私问题导致的显式人口标签难以获取的难题。  <br/>2. **融合伪标签与聚类技术**：利用预训练模型生成伪标签，并通过k-means无监督学习实现子群差异的自动识别和缓解，兼顾公平性与识别精度。  <br/>3. **验证显著性能提升**：实验证明IDI在保持SER准确率（下降<2%）的同时，提升公平性指标超28%；无监督IDI则实现公平性提升4.6%且性能损失<3.6%。  <br/>4. **证明通用性与鲁棒性**：进一步分析表明，IDI在种族和年龄等敏感属性上具有持续缓解差异的能力，验证了其在无显式人口信息场景下的有效性。|
|2505.14356v1|[PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and   Behavioral Cues in Fully-Duplex Speech Dialogs](http://arxiv.org/abs/2505.14356v1)|总结：  <br/>提出面向个性感知对话系统的预处理管道，结合ASR与大语言模型生成多维度注释数据，引入人工评估提升准确性，验证系统优于现有方法。<br/><br/>贡献点：  <br/>1. **构建多模态标注对话数据集**：首次将时间戳、响应类型和情绪/情感标签整合，为个性化对话研究提供结构化数据支持。  <br/>2. **融合ASR与深度学习技术**：利用自动语音识别系统提取音频信息，并通过大语言模型实现对话级别的个性预测。  <br/>3. **引入人工评估机制**：结合人类评价者对对话特征的标注，提升模型对个性属性的标注质量与准确性。  <br/>4. **验证系统有效性**：通过对比实验表明，所提方法在个性化对齐度上显著优于现有技术，为领域发展提供新范式。|
|2505.13978v1|[Bridging Speech Emotion Recognition and Personality: Dataset and   Temporal Interaction Condition Network](http://arxiv.org/abs/2505.13978v1)|总结（100字以内）:  <br/>该研究提出结合个性特征与声学特征的TICN模型，通过统计分析揭示个性与情感的关联，实现语音情感识别效价识别的显著提升，并开发自动人格识别模块以应对实际场景中数据缺失的问题。<br/><br/>贡献点分点列出:  <br/>1. **数据集扩展与标注**：首次为IEMOCAP数据集添加个性特征标注，为后续研究提供多维语料基础。  <br/>2. **理论关联发现**：通过统计分析验证人格特质与情感表达存在显著相关性，为情感识别模型设计提供依据。  <br/>3. **模型创新**：提出时序交互条件网络（TICN），创新性地融合人格特征与Hubert声学特征用于SER任务。  <br/>4. **性能提升验证**：实验表明，使用真实人格信息可将效价识别CCC提升11.17%（0.698→0.785），证明人格感知SER的有效性。  <br/>5. **实际应用适配**：开发自动人格识别前端模块，解决对话系统中用户人格信息缺失的场景问题，仍实现0.776的CCC提升。  <br/>6. **方法通用性**：构建可迁移的人格感知SER框架，为个性化语音处理应用提供理论和技术支持。|
|2505.13805v1|[ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with   Dual Control from Natural Language and Speech](http://arxiv.org/abs/2505.13805v1)|总结：  <br/>提出ClapFM-EVC框架，结合EVC-CLAP预训练模型、自适应情感融合编码器和流匹配模型，实现基于文本提示或语音参考的高保真情感语音转换，并通过多维度评估验证效果。<br/><br/>贡献点：  <br/>1. **提出ClapFM-EVC整体框架**：首次设计结合自然语言提示和参考语音的高保真情感语音转换系统，支持可调节情感强度的语音生成。  <br/>2. **开发EVC-CLAP对比预训练模型**：通过语言提示与类别标签联合训练，实现跨模态（语音-文本）情感元素的细粒度提取与对齐。  <br/>3. **创新FuEncoder融合机制**：引入自适应强度门，将情感特征与预训练ASR模型的Phonetic PosteriorGrams无缝融合。  <br/>4. **设计流匹配模型重构策略**：基于融合后的情感特征重建Mel谱图，增强情感表达与语音自然度。  <br/>5. **完整评估体系验证有效性**：通过主观听觉评价与客观指标（如MOS、BLEU）全面验证模型性能。|
|2505.13082v1|[MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and   Voices of Multiple Speakers](http://arxiv.org/abs/2505.13082v1)|贡献点总结（100字以内）:  <br/>提出MultiActor-Audiobook框架，通过MSP和LSI实现零样本生成，解决手动配置、单调语调及训练成本高的问题，经评估与商业产品竞争，消融实验证明其有效性。<br/><br/>分点贡献:  <br/>1. **提出零样本生成方法**：无需用户手动配置或额外训练，直接生成符合说话者特征的音频内容。  <br/>2. **创新双模块技术**：  <br/>   - (1) **MSP（多模态说话者人格生成）**：自动建模说话者特质以生成一致语调。  <br/>   - (2) **LSI（基于LLM的脚本指令生成）**：利用语言模型生成富含情感的语句指令。  <br/>3. **对比实验验证**：通过人类评估与MLLM对比，证明系统性能可竞争商业产品。  <br/>4. **消融研究支持**：验证MSP和LSI对生成质量的关键作用。|
|2505.12597v1|[Chain-Talker: Chain Understanding and Rendering for Empathetic   Conversational Speech Synthesis](http://arxiv.org/abs/2505.12597v1)|贡献点总结（100字以内）:<br/>提出Chain-Talker三阶段框架（情感理解-语义理解-同理心渲染），开发LLM驱动的CSS-EmCap情感生成管道，实现更自然的情感映射与语音表达，在三个基准数据集上验证了方法的有效性，并开源代码和演示。|
|2505.04203v2|[ELGAR: Expressive Cello Performance Motion Generation for Audio   Rendition](http://arxiv.org/abs/2505.04203v2)|总结：  <br/>本文提出ELGAR框架，通过扩散模型从音频生成完整身体的精细乐器演奏动作，并创新性地引入互动损失函数和专用评估指标，构建了SPD-GEN数据集，推动动画与音乐教育等领域的应用。<br/><br/>贡献点：  <br/>1. 提出首个基于扩散模型（Diffusion-based）的音频驱动全身体精细乐器演奏动作生成框架ELGAR。  <br/>2. 引入**手-乐器接触损失（HICL）**和**弓-弦接触损失（BICL）**，强化演奏动作与乐器的交互真实性。  <br/>3. 设计针对弦乐演奏动作的三项新评估指标：手指接触距离、弓弦距离、弓ing得分。  <br/>4. 构建标准化数据集SPD-GEN，基于MoCap数据集SPD进行整合与归一化处理。  <br/>5. 验证了ELGAR在复杂快速演奏动作生成中的有效性，推动动画、音乐教育及互动艺术等领域的应用。|
|2504.12339v2|[GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch   LLM](http://arxiv.org/abs/2504.12339v2)|总结（100字以内）:  <br/>提出GOAT-TTS框架，通过双分支架构解决文本-语音合成中的音色丢失、对齐依赖与知识遗忘问题，实现高质量实时流式TTS，并验证合成方言语音数据的有效性。<br/><br/>贡献点分点列出:  <br/>1. **提出解决三重矛盾的框架**：针对LLM在TTS领域的三个核心问题（音色信息丢失、对齐依赖、语言理解遗忘）设计了优化架构GOAT-TTS。  <br/>2. **模态对齐分支创新**：结合语音编码器与投影器，生成连续音色嵌入，实现语言、音色、情感等语用特征与语义文本的双向关联，无需依赖转录。  <br/>3. **分层模块微调策略**：对LLM的top-k层进行语音token预测微调，同时冻结bottom-n层以保留基础语言知识，提升模型稳定性与泛化能力。  <br/>4. **支持实时流式TTS**：引入多token预测机制，实现端到端的实时语音生成。  <br/>5. **验证合成方言有效性**：通过实验证明生成的方言语音数据在性能上与SOTA模型相当，验证了方法的实际应用价值。|