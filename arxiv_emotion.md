|Source|Title|Summary|
|---|---|---|
|2512.00120v1|[Art2Music: Generating Music for Art Images with Multi-modal Feeling Alignment](https://arxiv.org/abs/2512.00120v1)||
|2511.17926v1|[Three-Class Emotion Classification for Audiovisual Scenes Based on Ensemble Learning Scheme](https://arxiv.org/abs/2511.17926v1)||
|2511.13936v1|[Preference-Based Learning in Audio Applications: A Systematic Analysis](https://arxiv.org/abs/2511.13936v1)|**分点贡献：**  <br/>1. **揭示研究空白**：系统综述显示音视频领域偏好学习应用不足（仅6%），指出其在音频生成任务中的未被充分探索潜力。  <br/>2. **时间趋势分析**：区分2021年前后研究范式，指出传统排序方法（rankSVM）向现代RLHF生成框架的转变。  <br/>3. **关键模式识别**：提出三类核心趋势：多维度评估策略整合、传统指标与人类判断的不一致性、多阶段训练流程收敛。  <br/>4. **实践建议**：呼吁建立标准化基准、高质量音频数据集，并系统研究时间因素对偏好学习框架的影响。  <br/><br/>**总结（100字内）**：论文通过系统综述揭示音频领域偏好学习研究不足，指出范式转变与核心趋势，提出标准化评估与时间因素研究需求，为音频生成模型评价体系提供方向。|
|2511.12074v2|[MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement](https://arxiv.org/abs/2511.12074v2)|**贡献点：**  <br/>1. **提出MF-Speech框架**：解决语音生成中因素纠缠与控制粗粒度的挑战，通过双重核心组件实现高效分解与控制。  <br/>2. **多目标优化分解**：MF-SpeechEncoder采用多目标优化策略，分离出内容、音色、情感等独立且纯净的语音因素。  <br/>3. **动态控制机制**：MF-SpeechGenerator通过动态融合与Hierarchical Style Adaptive Normalization（HSAN）实现可组合、细粒度的风格控制。  <br/>4. **实验性能领先**：在多因素生成任务中，指标优于现有方法（WER=4.67%，SECS=0.5685，nMOS=3.96等）。  <br/>5. **通用表示潜力**：学习到的离散因素具备强迁移性，可作为通用的语音表示范式。  <br/><br/>**总结（100字以内）**：  <br/>MF-Speech通过多目标优化与动态控制机制，解决语音生成中因素纠缠与控制粗粒度问题，在多因素任务中显著提升性能，并展示出通用表示的迁移潜力。|
|2511.12074v1|[MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement](https://arxiv.org/abs/2511.12074v1)|总结：  <br/>本文提出MF-Speech框架，通过多目标优化分解语音因素并结合HSAN技术实现细粒度控制，在多因素合成任务中取得SOTA性能，同时验证了离散因素的可迁移性。<br/><br/>贡献点：<br/>1. **提出MF-Speech框架**：首次构建多因素分离与控制的生成框架，包含MF-SpeechEncoder和MF-SpeechGenerator双核心模块。  <br/>2. **内容-音色-情感因素纯化**：通过多目标优化策略，将语音信号解耦为高度纯净且独立的内容、音色与情感表征。  <br/>3. **HSAN与动态融合技术**：引入分层风格自适应归一化（HSAN）和动态融合机制，实现因素的精确、可组合控制。  <br/>4. **多因素合成任务SOTA表现**：在复杂任务中取得更低WER（4.67%）、更优风格控制（SECS=0.5685, Corr=0.68）及更高主观评分（nMOS=3.96, sMOS=3.78-3.86）。  <br/>5. **离散因素的可迁移性**：验证提取的离散语音因素具有跨任务泛化能力，展现作为通用语音表示的潜力。|
|2511.11006v1|[MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification](https://arxiv.org/abs/2511.11006v1)|总结（100字以内）:  <br/>提出MSMT-FN多段多任务融合网络，针对营销电话音频分类任务优化，表现优于现有方法，并开源数据集与代码，推动领域研究。<br/><br/>贡献点分点:  <br/>1. **提出新型网络架构**：设计MSMT-FN，专门解决营销电话中客户购买倾向分类的挑战，支持多段音频与多任务融合。  <br/>2. **构建专用数据集**：创建MarketCalls数据集，填补营销场景音频分类的空白，用于验证模型有效性。  <br/>3. **验证方法优越性**：在MarketCalls及多个基准数据集（CMU-MOSI、CMU-MOSEI、MELD）上实现性能领先或持平，证明模型泛化能力。  <br/>4. **开源促进研究**：提供代码库与数据集访问，便于学术界与工业界复现与拓展，加速音频分类技术发展。|
|2511.08642v1|[Robust Multi-modal Task-oriented Communications with Redundancy-aware Representations](https://arxiv.org/abs/2511.08642v1)|**贡献点总结：**  <br/>1. 提出融合两阶段变分信息瓶颈（VIB）与互信息（MI）冗余最小化的多模态语义通信框架，实现模态特异性压缩与跨模态冗余同步优化。  <br/>2. 引入对抗训练机制，通过抑制跨模态依赖提升效率，同时增强模态间互补性以提高语义可靠性。  <br/>3. 设计多模态融合后的二次VIB压缩，增强对通道扰动的鲁棒性，尤其在低信噪比场景下表现突出。  <br/>4. 验证框架在多模态情感识别任务中的有效性，显著优于现有基线方法，证明其在噪声和带宽受限环境下的优势。  <br/>5. 构建理论统一的联合优化模型，兼顾模态压缩、冗余消除与通信可靠性的协同提升。  <br/><br/>**摘要总结（100字内）：**  <br/>提出融合两阶段VIB与MI冗余最小化的多模态语义通信框架，通过对抗训练抑制跨模态冗余并增强互补性，显著提升情感识别任务的准确性和可靠性，尤其在低信噪比下表现优异。|
|2511.07955v1|[Speech Emotion Recognition with Phonation Excitation Information and Articulatory Kinematics](https://arxiv.org/abs/2511.07955v1)|总结（100字以内）:  <br/>本研究提出STEM-E2VA数据集，融合语音与生理信号，探索声门激励和发音运动学在情感识别中的潜力，创新性地通过逆向方法估计生理数据，并验证其在实际场景中的有效性。<br/><br/>贡献点：  <br/>1. **提出STEM-E2VA数据集**：首个融合语音、音频与生理信号（EGG/EMA）的情感识别数据集，填补生理信息研究空白。  <br/>2. **探索生理信息潜力**：首次系统研究声门激励（EGG）与发音运动学（EMA）在SER中的作用，揭示其对情感识别的增强效果。  <br/>3. **生理数据估计方法**：开发基于逆向技术的语音衍生生理信号估计方案，降低实际应用中数据采集的难度与成本。  <br/>4. **实证有效性验证**：实验结果证明生理信息在提升SER性能和拓展实际应用场景中的关键价值。|
|2511.07493v1|[Enabling Automatic Self-Talk Detection via Earables](https://arxiv.org/abs/2511.07493v1)|总结：本文提出MutterMeter系统，通过创新的分层分类架构实现高效自我对话检测，构建首个专属数据集并在实际场景中取得优于传统方法的F1分数，推动语音领域对隐性自我对话的研究。<br/><br/>贡献点：<br/>1. **首个自适应自我对话检测系统**：开发MutterMeter，首次在移动设备上实现对日常生活中无声或发声自我对话的自动检测。<br/>2. **跨模态分层分类架构**：设计融合声学特征、语言模式和上下文信息的分层处理框架，突破传统语音模型假设局限。<br/>3. **专用数据集构建**：建立首个聚焦自我对话的31.1小时真实场景音频数据集（25名参与者），填补研究空白。<br/>4. **性能突破**：在复杂场景下实现宏观F1值0.84，显著优于基于LLM和语音情感识别的现有方法。|
|2511.06458v1|[EchoMark: Perceptual Acoustic Environment Transfer with Watermark-Embedded Room Impulse Response](https://arxiv.org/abs/2511.06458v1)|总结：  <br/>EchoMark首次提出深度学习驱动的声学环境匹配框架，通过潜域操作实现高质量RIR转换与水印嵌入，在保持感知质量的同时显著提升安全性，实验结果验证其性能与FiNS相当，具备广泛应用潜力。<br/><br/>贡献点：  <br/>1. **首个水印嵌入的AEM框架**：EchoMark是首个将水印机制引入声学环境匹配的深度学习方法，解决音频迁移可能被恶意利用的问题。  <br/>2. **潜域操作应对RIR变异性**：通过在潜域中处理RIR的持续时间、能量衰减等动态特性，提升模型对复杂声学环境的适应能力。  <br/>3. **联合优化损失函数**：结合感知损失（RIR重建质量）与水印检测损失，实现环境迁移的高质量与水印嵌入的鲁棒性。  <br/>4. **卓越性能验证**：在多样化数据集上表现优于现有方法，MOS达4.22，水印检测准确率超99%，BER低于0.3%。  <br/>5. **应用拓展价值**：为音频认证、安全应用（如反语音欺骗）提供可靠技术支撑，增强音频内容的可信度与防篡改能力。|
|2511.06288v1|[ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2511.06288v1)|总结：提出ELEGANCE框架，通过三种语言学指导策略融合LLM知识提升AV-TSE性能，验证其在复杂场景下的有效性，并提供演示页面。<br/><br/>贡献点：<br/>1. 提出ELEGANCE框架，首次将大语言模型的语言知识引入音频-视觉目标说话人提取（AV-TSE）任务。<br/>2. 设计三种创新性引导策略：输出层语言约束、中间层语言预测、输入层对话先验，构建多阶段语言知识融合机制。<br/>3. 在RoBERTa、Qwen3-0.6B和Qwen3-4B等多款LLM上验证框架有效性，证明其模型通用性和知识迁移能力。<br/>4. 在视觉缺失、未知语言、说话人切换等挑战性场景中实现显著性能提升，增强模型鲁棒性。<br/>5. 构建首个基于语言知识的AV-TSE框架Demo，直观展示技术应用效果。|
|2511.06246v1|[IDMap: A Pseudo-Speaker Generator Framework Based on Speaker Identity Index to Vector Mapping](https://arxiv.org/abs/2511.06246v1)|**贡献点总结：**  <br/>1. 提出IDMap框架，通过将说话人身份索引映射到说话人向量，解决伪说话人生成的唯一性问题；  <br/>2. 设计两种模型（IDMap-MLP和IDMap-Diff），兼顾生成效率与隐私保护效果；  <br/>3. 在小规模（LibriSpeech）和大规模（MLS、Common Voice）数据集验证框架有效性，证明其在高并发场景下的稳定性；  <br/>4. 提供开源代码和音频样本，支持研究复现与应用。  <br/><br/>（100字以内）  <br/>该论文提出IDMap框架，通过身份索引到说话人向量的映射解决伪说话人生成问题，设计MLP和Diff模型兼顾效率与隐私，验证了其在不同规模数据集中的鲁棒性，并开源实现。|
|2510.10078v2|[Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model](https://arxiv.org/abs/2510.10078v2)|总结：  <br/>提出结合跨模态信息传递与互信息正则化的数据增强框架，有效提升语音情感识别性能，支持多模态输入扩展，且可生成无需跨模态信息的高质量输入。<br/><br/>贡献点：  <br/>1. **提出新型数据增强框架**：融合跨模态信息传输与互信息正则化技术，解决SER中依赖高质量标注数据的问题。  <br/>2. **互信息作为质量指标**：利用互信息度量生成数据的质量，增强模型鲁棒性和泛化能力。  <br/>3. **支持多模态扩展**：通过确保模态间依赖性，扩展框架至多模态输入（如语音+文本/视觉）。  <br/>4. **生成独立高质量输入**：实验表明框架可生成无需跨模态信息的高质量输入，提升数据多样性与实用性。|