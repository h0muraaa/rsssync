|Source|Title|Summary|
|---|---|---|
|2508.20796v1|[Speech Emotion Recognition via Entropy-Aware Score Selection](http://arxiv.org/abs/2508.20796v1)|**贡献点：**  <br/>1. 提出多模态框架，结合语音与文本预测（基于wav2vec2.0和RoBERTa-XLM），并通过Whisper-large-v3生成转录。  <br/>2. 创新性地采用熵与变熵阈值的晚期评分融合方法，解决单一模态预测的置信度限制。  <br/>3. 设计情感映射策略，将3类情感分类扩展至4类目标情绪类别，提升多模态预测的一致性。  <br/>4. 实验验证在IEMOCAP和MSP-IMPROV数据集上的有效性，证明方法较传统单模态系统更可靠且实用。  <br/><br/>**总结：**  <br/>该研究提出多模态语音情感识别框架，结合熵感知评分融合与情感映射策略，显著提升传统单模态系统的性能。|
|2508.19251v1|[MuSpike: A Benchmark and Evaluation Framework for Symbolic Music   Generation with Spiking Neural Networks](http://arxiv.org/abs/2508.19251v1)|**贡献点：**  <br/>1. 提出首个统一的SNN音乐生成基准框架MuSpike，系统评估五种SNN架构（SNN-CNN、SNN-RNN、SNN-LSTM、SNN-GAN、SNN-Transformer）及跨五类数据集。  <br/>2. 结合传统客观指标与大规模主观听觉实验，提出针对音乐感知的三类新型主观评估维度（音乐印象、自传联想、个人偏好）。  <br/>3. 揭示了不同SNN模型在评估维度的优势差异、用户音乐背景对感知模式的影响，以及客观与主观评价的显著不一致。  <br/>4. 强调生物可实现模型的评估需结合人类感知判断，凸显纯统计指标的局限性，推动认知导向的音乐生成研究。  <br/><br/>**总结（100字以内）：**  <br/>本文构建MuSpike框架，系统评估SNN在符号音乐生成中的表现，结合客观与主观指标揭示模型优势及评估偏差，强调人类感知的重要性，为生物合理音乐生成研究奠定基础。|
|2508.18653v1|[The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for   Forecasting Market Volatility and Enhancing Market Interpretability](http://arxiv.org/abs/2508.18653v1)|总结：  <br/>该研究提出了一种结合文本情感与高管声学特征的多模态框架，通过物理信息声学模型（PIAM）处理信号干扰，构建三维情感空间，并揭示高管情感动态对波动率预测的关键作用，为市场风险分析提供新工具。<br/><br/>贡献点：  <br/>1. **提出多模态风险评估框架**：首次整合文本情感分析与语音中的非语言线索（如高管声学动态），突破传统文本分析的局限。  <br/>2. **开发物理信息声学模型（PIAM）**：利用非线性声学技术，从受干扰的电话会议音频中稳健提取情绪特征（如信号剪切处理）。  <br/>3. **构建三维情感空间（ASL）**：将情感状态映射至可解释的Tension（紧张）、Stability（稳定）、Arousal（唤醒）三维度。  <br/>4. **发现情感动态与波动率关联性**：揭示高管从脚本到自发问答的语音转换中，情绪变化（如CFO的声学不稳定性、CEO的唤醒波动）对30天波动率的43.8%解释力。  <br/>5. **验证多模态优势**：通过消融实验证明，多模态方法显著优于仅依赖财务数据的基线模型，凸显声学与文本模态的互补性。  <br/>6. **提供风险识别工具**：基于可验证的生物特征信号，为投资者和监管者提供增强市场透明度、识别隐藏公司不确定性的新手段。|
|2508.17878v1|[Enhancing Speech Emotion Recognition with Multi-Task Learning and   Dynamic Feature Fusion](http://arxiv.org/abs/2508.17878v1)|贡献点总结（100字以内）:<br/>该研究提出基于多任务学习的SSL模型微调框架，创新性地引入共注意力机制和样本加权焦点对比损失函数，有效解决类别不平衡与语义混淆问题，并在自然条件语音情感识别任务中实现性能显著提升。<br/><br/>分点贡献：<br/>1. 提出多任务学习与自监督学习结合的框架，同时优化情感识别、性别识别、说话人验证和自动语音识别四项任务<br/>2. 设计创新的共注意力模块，实现跨任务特征动态交互与上下文感知融合<br/>3. 开发样本加权聚焦对比（SWFC）损失函数，针对性解决困难样本与少数类不平衡问题<br/>4. 在Speech Emotion Recognition in Naturalistic Conditions Challenge的分类任务中验证方法有效性，取得显著性能提升|
|2508.16188v2|[Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for   Expressive Speech Generation](http://arxiv.org/abs/2508.16188v2)|**贡献点：**  <br/>1. 提出Audio-Visual Language Model (AVLM)，首次将全脸视觉信息整合进表达性语音生成模型。  <br/>2. 探索多种视觉编码器与多模态融合策略，确定最优的视觉-语音整合方法。  <br/>3. 通过情感识别和表达性对话任务的微调，在语音仅基线模型上实现显著性能提升（如+5 F1）。  <br/>4. 验证了表达性视觉信息对语音生成的指导作用，为构建端到端多模态对话系统提供基础。  <br/><br/>**总结：**  <br/>该研究提出AVLM，融合全脸视觉信息提升表达性语音生成效果，为多模态对话系统奠定基础。|
|2508.16188v1|[Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for   Expressive Speech Generation](http://arxiv.org/abs/2508.16188v1)|总结：  <br/>本文提出AVLM模型，通过融合全面部视觉信息提升表达性语音生成效果，在情感识别任务中取得5%F1提升，并为多模态对话系统提供基础。<br/><br/>贡献点：  <br/>1. **提出AVLM框架**：首次将全面部视觉线索整合进预训练表达性语音模型，实现语音与视觉的跨模态融合。  <br/>2. **多模态方法优化**：系统探索多种视觉编码器和融合策略，确定最优的视觉信息整合方案。  <br/>3. **任务表现提升**：在情感识别与表达性对话任务中，通过微调显著超越语音-only基线（如+5 F1）。  <br/>4. **多模态应用基础**：为端到端多模态对话系统提供关键模型支持，凸显视觉信息对语音生成的指导价值。|
|2508.14920v1|[Human Feedback Driven Dynamic Speech Emotion Recognition](http://arxiv.org/abs/2508.14920v1)|**贡献点：**  <br/>1. **提出动态语音情绪识别新领域**：认为语音中存在随时间变化的多情绪序列，突破传统单一时点情绪识别的局限性。  <br/>2. **构建多阶段方法**：  <br/>   - 集成经典语音情绪识别模型的训练  <br/>   - 设计情绪序列的合成生成技术  <br/>   - 引入以人类反馈驱动的模型优化机制  <br/>3. **创新情绪混合建模方法**：基于狄利克雷分布构建情绪混合模型，提升对复杂情绪状态的表征能力。  <br/>4. **真实数据评估体系**：利用3D面部动画数据集提取的标注情绪进行模型验证，增强结果可靠性。  <br/>5. **对比实验验证有效性**：通过与滑动窗口方法的对比，证明狄利克雷分布和人类反馈整合方案的优越性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出动态语音情绪识别的新范式，设计多阶段方法结合狄利克雷分布建模与人类反馈优化，并在3D面部动画数据集上验证其有效性，显著提升情绪序列建模与识别性能。|
|2508.14548v1|[EmoTale: An Enacted Speech-emotion Dataset in Danish](http://arxiv.org/abs/2508.14548v1)|**贡献点总结：**  <br/>1. 提出首个包含丹麦语和英语的多语言情感语音语料库EmoTale，填补小语言情感数据集空白。  <br/>2. 采用自监督模型嵌入与openSMILE特征提取器提升语音情感识别性能。  <br/>3. 验证EmoTale数据集的有效性，证明其预测能力与现有基准（DES）相当。  <br/>4. 展示自监督嵌入优于传统手工特征，为语音情感研究提供新方法参考。  <br/><br/>（99字）|
|2508.12368v1|[CEM-Net: Cross-Emotion Memory Network for Emotional Talking Face   Generation](http://arxiv.org/abs/2508.12368v1)|贡献点（分点）：<br/>1. 提出跨情绪记忆网络（CEM-Net），解决参考图像与音频情绪冲突导致的生成失真问题<br/>2. 设计音频情绪增强模块（AEE），通过跨重建训练策略提升音频情绪表达的鲁棒性<br/>3. 引入情绪桥梁记忆模块（EBM），利用参考图像情绪信息补偿音频驱动的面部运动不足<br/>4. 构建表达位移记忆机制，实现跨情绪特征的查询-匹配位移检索<br/>5. 验证生成视频在表情准确性、自然度和唇同步方面的显著提升效果<br/><br/>总结：该研究提出CEM-Net，通过音频情绪增强和跨情绪记忆机制解决参考图像与音频情绪冲突问题，实现更准确自然的口型生成视频。|
|2508.11371v1|[Speech Emotion Recognition Using Fine-Tuned DWFormer:A Study on Track 1   of the IERPChallenge 2024](http://arxiv.org/abs/2508.11371v1)|**贡献点（分点）：**  <br/>1. **首次将人格特质与情感识别结合**：突破传统离散情绪标签预测模型的局限，探索个体差异对情感表达的影响。  <br/>2. **专注纯音频特征研究**：在Track 1任务中仅依赖音频信号，区别于其他任务的多模态（文本+音频）特征融合。  <br/>3. **提出数据增强与分数融合策略**：优化预训练模型DWFormer的性能，提升情感识别的准确性与鲁棒性。  <br/>4. **获得IEP Challenge 2024 Track 1冠军**：在权威竞赛中验证方法有效性，实现最优结果。  <br/><br/>**总结（100字内）：**  <br/>本研究将人格特质引入情感识别，结合纯音频特征与数据增强/分数融合策略，优化预训练模型DWFormer，在IEP Challenge 2024 Track 1中取得最佳成绩。|
|2508.11362v1|[Mitigating Category Imbalance: Fosafer System for the Multimodal Emotion   and Intent Joint Understanding Challenge](http://arxiv.org/abs/2508.11362v1)|**贡献点（分点）：**  <br/>1. **多模态数据增强**：结合文本、视频、音频三种模态的增强技术，缓解类别不平衡问题。  <br/>2. **新型损失函数**：提出SampleWeighted Focal Contrastive损失，优化少数类样本及语义相似难区分样本的识别。  <br/>3. **模型微调**：基于Hubert模型进行适配优化，提升多模态情绪与意图联合识别性能。  <br/>4. **模态竞争缓解策略**：引入模态丢弃（modal dropout）技术，减少跨模态信息干扰。  <br/>5. **集成推理方法**：采用多数投票（plurality voting）机制，提升最终预测的鲁棒性。  <br/>6. **实验验证**：在Track 2 Mandarin任务中取得次优性能（第二好结果），验证方法有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Fosafer方法，通过多模态数据增强、改进损失函数、模型微调及模态竞争缓解策略，解决语音情绪与意图联合识别中的类别不平衡问题，最终通过集成投票实现高效预测，取得Track 2 Mandarin挑战的次优性能。|
|2508.11187v1|[Expressive Speech Retrieval using Natural Language Descriptions of   Speaking Style](http://arxiv.org/abs/2508.11187v1)|总结：  <br/>提出基于表达风格（而非内容）的语音检索任务，设计跨模态联合嵌入框架，通过文本提示实现情感/风格匹配检索，验证在22种风格数据集上的有效性。<br/><br/>贡献点：  <br/>1. **任务创新**：首次定义"expressive speech retrieval"任务，区别于传统基于语义内容的语音检索，聚焦于语音表达风格的匹配（如情感、语气）。  <br/>2. **联合嵌入框架**：构建语音与文本描述的跨模态联合潜在空间，实现通过自由文本提示（非固定关键词）检索匹配的语音片段。  <br/>3. **跨模态对齐优化**：提出针对有效跨模态特征对齐的训练准则，提升语音与文本表征的一致性。  <br/>4. **提示增强方法**：设计文本提示增强策略，提升模型对任意文本查询的泛化能力。  <br/>5. **大规模实验验证**：在包含22种说话风格的多数据集上验证方法有效性，展示优于传统方法的Recall@k性能。|
|2508.09600v1|[OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue](http://arxiv.org/abs/2508.09600v1)|**贡献点总结（100字以内）:**  <br/>提出OSUM-EChat开源对话系统，通过三阶段训练策略和语言-语音双思考机制提升共情能力；构建EChat-200K数据集与EChat-eval评估框架；实验验证其在资源受限场景下优于现有端到端模型。<br/><br/>**分点贡献:**  <br/>1. **创新方法**：设计三阶段理解驱动训练策略，扩展大规模语音理解模型至对话任务，减少对大尺度数据的依赖。  <br/>2. **双模态机制**：融合语言与语音线索的双思考机制，结合认知链与对话生成，提升共情响应能力。  <br/>3. **数据支持**：构建EChat-200K数据集（丰富共情语音对话）与EChat-eval基准（全面评估共情能力）。  <br/>4. **实验验证**：在资源受限场景下，OSUM-EChat性能优于现有端到端模型，证实其有效性。|
|2508.09389v1|[ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual   Inputs](http://arxiv.org/abs/2508.09389v1)|**总结（100字以内）:**  <br/>提出独立文本到韵律特征映射模型，结合编码器-解码器与掩码机制，显著提升F0与能量预测精度，并成功集成至TTS系统，通过感知测试验证其在语音情感与语义建模任务中的有效性。<br/><br/>**贡献点（分点列出）:**  <br/>1. **提出独立模型框架**：开发可单独使用的文本到韵律特征(如F0、能量)映射模型，适用于语音合成等下游任务。  <br/>2. **创新编码解码结构**：引入声学特征与时间对齐文本文本的联合输入，通过部分掩码处理生成固定长度的韵律嵌入。  <br/>3. **实验验证性能提升**：在GigaSpeech数据集上对比主流风格编码器，证明模型在多粒度F0和能量预测中表现更优。  <br/>4. **TTS系统集成应用**：将预测的韵律特征整合至TTS系统，通过感知实验表明其优于基线模型，提升语音自然度。  <br/>5. **强调潜在应用价值**：突出模型在依赖韵律建模的语音任务中的潜力，为情感语音合成和个性化语音生成提供新思路。|
|2508.08925v1|[LPGNet: A Lightweight Network with Parallel Attention and Gated Fusion   for Multimodal Emotion Recognition](http://arxiv.org/abs/2508.08925v1)|**贡献点：**  <br/>1. 提出轻量化的LPGNet模型，通过并行注意力机制降低计算成本。  <br/>2. 引入LPIA模块，替代传统堆叠Transformer层，高效建模单模态与跨模态关系。  <br/>3. 设计双门控融合方法，动态滤波和整合多模态输入特征。  <br/>4. 完全去除说话人嵌入，实现对说话人身份的独立性，提升跨说话人泛化能力。  <br/>5. 在IEMOCAP数据集上验证，达到87%以上准确率和F1分数，参数更少且表现优于基线模型。  <br/><br/>**总结：**  <br/>LPGNet通过轻量化结构与创新融合机制，在对话情感识别中实现高效建模和跨说话人泛化，超越基线模型性能。|
|2508.07086v2|[SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means   Quantization](http://arxiv.org/abs/2508.07086v2)|总结：  <br/>本文提出SEF-MK框架，通过多k-means模型随机匿名化SSL表示，平衡用户隐私保护与攻击风险，在保护语言情感内容的同时提升隐私攻击的难度。<br/><br/>贡献点：  <br/>1. **提出SEF-MK框架**：首个无需说话者嵌入的语音匿名化方法，通过多k-means模型随机选择进行数据处理，避免传统单模型的局限性。  <br/>2. **多模型策略优化**：相比单模型，多k-means模型更有效保留用户语言和情感信息，提升隐私保护效果。  <br/>3. **攻击者视角分析**：揭示多k-means模型反而增强攻击者识别隐私的潜力，为安全评估提供新视角。  <br/>4. **系统设计指导**：基于对比实验，为用户在设计语音匿名化系统时提供针对攻击威胁的优化策略。|
|2508.07086v1|[SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means   Quantization](http://arxiv.org/abs/2508.07086v1)|贡献点总结（100字以内）:  <br/>提出SEF-MK框架无需说话者嵌入，通过多k-means模型随机应用保护语言内容，同时暴露隐私攻击风险，为用户优化匿名化系统设计提供了理论依据和新思路。<br/><br/>分点贡献：  <br/>1. **提出首个多模型语音匿名化框架**：SEF-MK通过为每个语音片段随机选择多个预训练于不同说话者子集的k-means模型，替代传统单模型方法，实现更灵活的特征混淆。  <br/>2. **双视角分析攻击与防御效果**：从用户隐私保护和攻击者角度验证方法有效性，揭示多模型策略在保留语言信息与增强攻击风险之间的权衡。  <br/>3. **实验证明方法优势与风险**：实验表明SEF-MK在用户视角下优于单模型，但攻击者能更高效利用多模型发起隐私攻击，为系统安全设计提供关键数据支持。  <br/>4. **推动隐私保护与攻击对抗研究**：通过展示新框架的双重特性（保护与风险），为平衡语音隐私与攻击防御提供理论框架和实践方向。|
|2508.06890v1|[Maestro-EVC: Controllable Emotional Voice Conversion Guided by   References and Explicit Prosody](http://arxiv.org/abs/2508.06890v1)|总结：  <br/>提出Maestro-EVC框架，实现语音内容、说话人身份与情感的独立控制，引入时间情感表示和显式韵律建模，有效捕捉并传递情感的时间动态，解决现有方法在情感分离和细粒度建模上的不足。<br/><br/>贡献点：  <br/>1. **提出Maestro-EVC框架**：首次实现对语音内容、说话人身份和情感的独立控制，通过多属性分离机制提升情感转换的可操控性。  <br/>2. **时间情感表示**：设计专门处理情感时间动态的表示方法，捕捉情感变化的细微特征。  <br/>3. **显式韵律建模与增强**：结合韵律增强技术，强化情感表达的语音韵律特征，提升在韵律不匹配场景下的鲁棒性。  <br/>4. **实验验证效果**：通过实验证明该框架在情感语音合成任务中实现高质量、可控的情感表达，优于现有方法。|
|2508.06321v1|[EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech   Emotion Recognition](http://arxiv.org/abs/2508.06321v1)|**贡献点（分点）:**  <br/>1. **提出混合模型架构**：结合LSTM与1D-CNN，优化Speech Emotion Recognition（SER）系统的性能与鲁棒性。  <br/>2. **创新数据增强策略**：融合传统方法（噪声添加、音高变化、时间拉伸）与新颖的组合增强管道，提升泛化能力并减少过拟合。  <br/>3. **多维特征提取**：采用RMSE、MFCC和ZCR构建高维特征向量，增强输入数据的多样性和判别性。  <br/>4. **实验验证有效性**：在IEMOCAP和RAVDESS数据集上，通过ReLU和ELU激活函数取得优于现有方法的准确率（95.78%-96.75%）。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出EmoAugNet混合模型，整合传统与组合数据增强策略及多维特征提取，显著提升SER系统的泛化能力与准确率，验证了其在情感语音识别领域的有效性。|
|2508.05385v1|[A Scalable Pipeline for Enabling Non-Verbal Speech Generation and   Understanding](http://arxiv.org/abs/2508.05385v1)|总结：该研究构建了一个包含10类非语言发声的38K数据集NonVerbalSpeech-38K，提出自动标注流程并验证其在语音合成与字幕任务中的有效性，推动非语言语音研究。<br/><br/>贡献点：<br/>1. 提出非语言语音数据集构建的自动流程，实现数据集的高效标注与多样化采集<br/>2. 发布包含38,718个样本（131小时）的NonVerbalSpeech-38K数据集，覆盖10类非语言发声<br/>3. 通过验证SOTA模型（F5-TTS/Qwen2-Audio）在非语言语音生成与理解任务中的性能提升，证明数据集的有效性|
|2508.04723v1|[Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated   Music through Portable EEG-fNIRS Fusion](http://arxiv.org/abs/2508.04723v1)|**贡献点总结（100字以内）：**  <br/>提出MEEtBrain框架解决音乐刺激约束、模态特异性及便携性问题，实现AI生成音乐与EEG-fNIRS同步采集，构建可公开共享的多模态情绪数据集，推动情绪分析研究与应用。  <br/><br/>**分点贡献：**  <br/>1. **突破音乐刺激约束**：通过AI生成音乐刺激实现大规模、无偏见的情绪诱导，解决版权与人工筛选导致的音乐样本局限性，确保音乐多样性。  <br/>2. **多模态信号融合**：开发可同时采集EEG与fNIRS数据的无线便携设备，结合两种神经信号提升情绪分析的准确性和鲁棒性。  <br/>3. **解决便携性瓶颈**：采用轻量无线头带式设计与干电极技术，降低设备复杂度和使用门槛，提升实际应用场景的可行性。  <br/>4. **公开共享数据集**：构建包含20名受试者14小时数据的多模态情绪数据库，并持续扩展至44名受试者，推动跨研究复用与技术应用。|
|2508.04481v1|[Emotion Detection Using Conditional Generative Adversarial Networks   (cGAN): A Deep Learning Approach](http://arxiv.org/abs/2508.04481v1)|总结：  <br/>本文提出基于cGANs的多模态情感检测方法，整合文本、音频和面部信息，通过生成对抗网络提升分类准确率，验证优于基线模型，展示了其在增强人机交互情感理解中的潜力。<br/><br/>贡献点：  <br/>1. **提出多模态情感检测框架**：首次将文本、音频和面部表情三模态数据整合，提升情感识别的全面性。  <br/>2. **创新使用cGAN架构**：设计条件生成对抗网络生成合成情感数据，增强模型对多模态特征的建模能力。  <br/>3. **验证方法有效性**：通过实验对比基线模型，证明所提方法在情感分类准确率上的显著提升。  <br/>4. **推动人机交互应用**：强调cGANs在提升系统情感理解能力方面的潜力，为更自然的人机交互提供技术支持。|
|2508.04230v1|[Towards interpretable emotion recognition: Identifying key features with   machine learning](http://arxiv.org/abs/2508.04230v1)|**贡献点：**  <br/>1. 提出一种基于机器学习算法的方法，用于在情感识别任务中识别和推广关键的可解释特征。  <br/>2. 构建了更广泛、更稳健的框架，突破了以往研究局限于狭窄上下文和结果不一致的局限性。  <br/>3. 针对无监督模型（如wav2vec2、HuBERT）在医疗等关键领域应用的可解释性瓶颈，提供可推广的解析路径。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过机器学习算法识别情感识别任务中的可解释特征，提出更广泛适用的框架，解决无监督模型在关键领域应用的可解释性问题，为医疗等场景提供更可靠的特征解析方法。|
|2508.03780v1|[Are Inherently Interpretable Models More Robust? A Study In Music   Emotion Recognition](http://arxiv.org/abs/2508.03780v1)|总结：本研究通过对比实验发现，设计上更注重可解释性的深度学习模型在对抗性扰动下更具鲁棒性，且计算成本低于对抗训练模型，为语音领域模型设计提供了新思路。<br/><br/>贡献点：<br/>1. **提出新研究问题**：首次探究"本质可解释性深度模型是否比传统黑盒模型更鲁棒"，揭示模型可解释性与对抗性鲁棒性之间的潜在关联。<br/>2. **构建对比实验框架**：基于音乐情感识别任务，系统比较了可解释模型、黑盒模型与对抗训练模型在对抗样本下的表现，建立明确的评估基准。<br/>3. **发现计算成本优势**：证明本质可解释性模型在达到与对抗训练模型相当的鲁棒性时，具有更低的计算开销，为实际应用提供效率优化依据。<br/>4. **验证理论假设**：实验证实可解释性模型的特征选择机制能有效抵御无关扰动，其对语义特征的聚焦可能减少对虚假相关性的依赖。<br/>5. **推动模型设计方向**：为语音处理领域提供新的模型优化视角，即通过提升模型可解释性可同时增强其对抗性鲁棒性。|
|2508.03543v2|[EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable   Text-to-Speech via Activation Steering](http://arxiv.org/abs/2508.03543v2)|总结：  <br/>本文提出EmoSteer-TTS，首个训练无关的细粒度语音情感控制方法，通过激活引导实现情感转换、插值和消除，构建了情感语音数据集并验证了其优于SOTA的效果。<br/><br/>贡献点：  <br/>1. 提出**训练无关（Training-Free）的TTS情感控制方法**：解决传统系统依赖高质数据集的局限，无需额外训练即可实现细粒度情感调控。  <br/>2. 设计**激活引导（Activation Steering）算法框架**：包含激活提取、情感标记搜索、推理时情感控制三阶段，兼容主流预训练模型（如F5-TTS、CosyVoice2）。  <br/>3. 构建**定制化情感语音数据集****：涵盖多说话人，支持生成多样化情感样本以优化控制效果。  <br/>4. 实现**连续且可解释的情感操控****：支持情感插值、消除等细粒度操作，显著提升情感生成的灵活性和稳定性。  <br/>5. 首次达成**无需训练的细粒度情感控制****：在TTS领域提出全新范式，优于当前最先进的方法。|
|2508.02849v1|[SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech   Codec](http://arxiv.org/abs/2508.02849v1)|总结（100字以内）:  <br/>本研究提出SecoustiCodec，通过跨模态对齐与语义-语音学分离，在低比特率下实现流式语音编码，解决了现有方法在语义完整性、重建能力和流式支持等方面的不足，取得了SOTA重建质量。  <br/><br/>贡献点:  <br/>1. **提出跨模态对齐的低比特率流式语音编解码器**：SecoustiCodec在单codebook空间内分离语义与非语义（如音色、情感）信息，支持实时流式处理。  <br/>2. **引入语音学编码机制弥补信息缺失**：通过桥接语义与声学编码的信息差，提升语音重建的完整性和保真度。  <br/>3. **设计语义专用高效量化方法**：基于VAE与FSQ，缓解token长尾分布问题，同时保持高codebook利用率。  <br/>4. **构建基于对比学习的语义分离框架**：在联合多模态帧级空间对齐文本与语音特征，有效去除非语义干扰。  <br/>5. **提出声学约束多阶段优化策略**：确保模型训练的稳定收敛，增强编解码器的鲁棒性与性能。|
|2508.02448v1|[Charting 15 years of progress in deep learning for speech emotion   recognition: A replication study](http://arxiv.org/abs/2508.02448v1)|总结：  <br/>该研究量化评估了语音情感识别15年的发展，揭示了Transformer架构后的性能瓶颈，探讨了模型选择对进展认知的影响，并为未来研究方向提供关键指引。<br/><br/>贡献点：  <br/>1. **系统量化评估**：首次通过大规模实证分析，系统量化了2009年起SER领域15年的技术进步程度。  <br/>2. **跨模态对比研究**：对比了纯音频模型（基于语音信号）与文本驱动模型（基于转录文本），揭示了两者在性能上的差异。  <br/>3. **Transformer瓶颈分析**：发现Transformer架构的引入导致性能提升趋缓，暗示深度模型存在效率与效果的权衡问题。  <br/>4. **认知偏差识别**：证明进展感知受模型对比基准影响，暴露了研究评价中的主观性问题。  <br/>5. **未来研究指引**：提出需从模型设计、数据利用及任务定义等维度探索SER技术的突破路径。|
|2508.02038v4|[Marco-Voice Technical Report](http://arxiv.org/abs/2508.02038v4)|总结：  <br/>本文提出集成语音克隆与情感控制的多模态语音合成系统Marco-Voice，创新性地结合说话者-情感解纠缠机制与旋转情感嵌入方法，构建高质量情感语料库CSEMOTIONS，并通过实验证明其在表达性与可控性上的显著提升，同时开放代码与数据集以促进研究。<br/><br/>**贡献点：**  <br/>1. **统一框架集成**：首次将语音克隆与情感控制合成整合于同一系统，实现对说话者身份和情感风格的联合建模与控制。  <br/>2. **说话者-情感解纠缠机制**：引入基于in-batch对比学习的解纠缠方法，支持独立调节说话者身份与情感表达。  <br/>3. **旋转情感嵌入技术**：提出平滑情感控制的旋转情感嵌入集成方法，提升情感变化的自然度与连续性。  <br/>4. **高质量数据集构建**：创建CSEMOTIONS数据集（10小时普通话语料，6名专业说话人，7类情感），为情感语音合成提供基准。  <br/>5. **性能验证与开源**：通过大量实验验证系统在语音清晰度、情感丰富度等指标上的优越性，并公开代码与数据集促进复现与应用。|
|2508.02038v3|[Marco-Voice Technical Report](http://arxiv.org/abs/2508.02038v3)|总结：  <br/>本文提出了一种统一的语音合成系统，结合语音克隆与情感控制，通过新型机制和数据集提升语音表达的可控性与自然度，并开源代码与数据。<br/><br/>贡献点：  <br/>1. **统一框架**：首次将语音克隆与情感控制语音合成整合于同一系统（Marco-Voice），解决身份保持与情感表达的协同挑战。  <br/>2. **分离机制**：引入跨批次对比学习（in-batch contrastive learning）和说话人-情感解耦技术，实现独立操控身份与情感风格。  <br/>3. **情感控制方法**：提出旋转情感嵌入（rotational emotional embedding）技术，支持平滑情感调控。  <br/>4. **高质数据集**：构建CSEMOTIONS数据集（10小时中文语音，6位专业说话人，7种情感类别），填补情感语音研究的数据需求。  <br/>5. **实验验证**：在客观和主观指标上验证系统有效性，证明其在语音清晰度与情感丰富度上的优越表现，并开源代码与数据集。|
|2508.02038v2|[Marco-Voice Technical Report](http://arxiv.org/abs/2508.02038v2)|总结：  <br/>该论文提出融合语音克隆与情感控制的语音合成系统Marco-Voice，创新性地引入解耦机制与旋转情感嵌入方法，构建高质量情感语料库CSEMOTIONS，显著提升语音生成的表达性、可控性和自然度。<br/><br/>贡献点：  <br/>1. **系统整合**：首次将语音克隆与情感控制统一于单一框架，实现高度表达、可控且自然的语音生成。  <br/>2. **解耦机制**：提出基于同一批次对比学习的说话人-情感分离方法，支持独立控制身份与情感风格。  <br/>3. **情感嵌入优化**：设计旋转情感嵌入集成技术，提升情感控制的平滑性与连续性。  <br/>4. **数据集构建**：创建CSEMOTIONS数据集（10小时中文语音，6位专业说话人，7种情感类别），为研究提供高质量资源。  <br/>5. **性能验证**：通过客观与主观指标验证系统效果，证明语音清晰度与情感丰富性的显著提升。  <br/>6. **开源共享**：公开代码与数据集，促进技术复现与进一步研究。|
|2508.02038v1|[Marco-Voice Technical Report](http://arxiv.org/abs/2508.02038v1)|**分点贡献：**<br/>1. 提出统一框架：整合了语音克隆与情感控制语音合成功能，实现多模态语音生成的一体化。<br/>2. 创新解缠机制：引入基于批量对比学习（in-batch contrastive learning）的说话人-情感分离技术，支持独立操控声纹特征与情感风格。<br/>3. 优化情感控制：设计旋转情感嵌入（rotational emotional embedding）方法，实现平滑自然的情感过渡。<br/>4. 构建高质量数据集：创建CSEMOTIONS中文情感语音数据集（含10小时、6位演讲者、7类情感），为研究提供标注资源。<br/>5. 实验验证有效性：在多维度指标（清晰度、情感丰富度）上展示系统Marco-Voice的显著提升，推动表达性语音合成技术发展。<br/><br/>**总结（100字内）：**  <br/>论文提出整合语音克隆与情感控制的Marco-Voice系统，通过新解缠机制和情感嵌入方法提升语音生成质量，并构建CSEMOTIONS数据集，验证了系统在表达性与自然度方面的突破。|
|2508.01960v1|[Non-Verbal Vocalisations and their Challenges: Emotion, Privacy,   Sparseness, and Real Life](http://arxiv.org/abs/2508.01960v1)|总结:  <br/>该论文系统梳理了非语言发声（NVVs）在心理学和语言学中的发展脉络，提出其研究面临隐私、伦理及语境建模等挑战，并主张采用语料库方法进行更真实的研究，但指出该方法仍存在数据量不足的局限。<br/><br/>贡献点:  <br/>1. **历史性回顾**：梳理NVVs在心理学和语言学领域近两百年的发展历程，揭示其从研究到被忽视再到情感研究复兴的演变过程。  <br/>2. **分类与功能分析**：首次系统归纳NVVs的类型（形式特征）及功能，通过典型例子"ah"阐明其在情感传递等场景中的作用。  <br/>3. **挑战识别**：明确指出NVVs研究的两大核心问题：隐私与伦理限制导致现实场景数据采集不足，以及孤立样本无法体现真实语境。  <br/>4. **方法论创新**：提出基于语料库的研究方法作为解决方案，强调其能提升建模真实性，同时客观指出该方法仍面临隐私和数据稀疏性问题。|
|2508.01644v1|[DRKF: Decoupled Representations with Knowledge Fusion for Multimodal   Emotion Recognition](http://arxiv.org/abs/2508.01644v1)|**贡献点：**  <br/>1. **提出DRKF方法**：首次结合解耦表示与知识融合策略，有效应对多模态情感识别中的模态异质性和情感线索不一致性问题。  <br/>2. **优化表示学习（ORL模块）**：通过对比互信息估计与渐进模态增强技术，分离任务相关的共享特征与模态特异性信息，缓解模态差异对性能的影响。  <br/>3. **知识融合（KF模块）**：设计轻量级自注意力融合编码器，动态识别主导模态并整合其他模态情感信息，提升融合表示的准确性。  <br/>4. **情感判别子模块（ED）**：引入机制处理情感不一致场景下的错误主导模态选择，保留情感不一致性特征以辅助最终分类。  <br/>5. **实验验证与开源**：在IEMOCAP、MELD、M3ED等主流数据集上达到SOTA性能，并公开代码供研究复现。|
|2507.21395v1|[Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition   with Cross-Modal Fusion](http://arxiv.org/abs/2507.21395v1)|**贡献点：**<br/>1. 提出Sync-TVA框架，集成图注意力机制以增强跨模态交互与模态贡献平衡。  <br/>2. 引入模态特定的动态增强模块，分别优化文本、音频、视觉模态的特征表示。  <br/>3. 构建异构跨模态图结构，建模多模态特征间的复杂语义关系。  <br/>4. 设计交叉注意力融合机制，实现多模态线索的全局对齐与协同推理。  <br/>5. 在MELD和IEMOCAP数据集上验证模型效能，尤其在类别不平衡场景下表现优于现有方法。  <br/><br/>**总结：**  <br/>本文提出Sync-TVA框架，通过动态增强与结构化跨模态融合解决多模态情感识别中的模态不平衡和跨模态交互问题，有效提升了模型在复杂数据下的情感识别性能。|
|2507.21138v1|[TTS-1 Technical Report](http://arxiv.org/abs/2507.21138v1)|总结：  <br/>本研究提出Inworld TTS-1系列模型，包含高效实时模型（1.6B参数）和高质高表达模型（8.8B参数），支持多语言、情感控制及高分辨率语音生成，并开源代码促进研究应用。<br/><br/>贡献点：  <br/>1. **双模型架构**：提出Inworld TTS-1与TTS-1-Max两个Transformer-based自回归TTS模型，分别针对效率与质量进行优化。  <br/>2. **大规模参数设计**：TTS-1-Max以8.8B参数实现语音质量与表达性的极致提升，适用于高要求应用场景。  <br/>3. **分阶段训练策略**：通过扩展训练计算并采用预训练-微调-RL对齐的分阶段方法，获得SOTA性能，依赖上下文学习生成高质量语音。  <br/>4. **高分辨率与低延迟**：支持48kHz超清语音生成，实现低延迟实时合成，适配设备端部署需求。  <br/>5. **多语言与情感控制**：覆盖11种语言，通过音频标记实现细粒度情感调节及非语言发声（如停顿、语气）。  <br/>6. **开源代码**：开源训练与模型代码，采用MIT协议，推动语音合成领域的研究与实际应用。|
|2507.20627v1|[Controllable Video-to-Music Generation with Multiple Time-Varying   Conditions](http://arxiv.org/abs/2507.20627v1)|**贡献点总结：**  <br/>提出多条件引导的V2M框架，结合两阶段训练策略与新颖模块，提升控制与对齐性能。<br/><br/>**分点贡献：**  <br/>1. **提出多条件引导框架**：首次引入多种时变条件控制机制，实现音乐生成过程的灵活调控。  <br/>2. **设计两阶段训练策略**：分阶段学习V2M基础与音视频时序同步，兼顾用户需求与模型稳定性。  <br/>3. **开发细粒度特征选择模块**：优化视觉特征提取与对齐，提升语义信息的精准匹配能力。  <br/>4. **构建渐进时间对齐注意力机制**：增强跨模态时序一致性，改善音视频同步效果。  <br/>5. **创新动态条件融合与控制引导解码器**：整合多条件信息并精准指导音乐创作，提升生成质量。  <br/>6. **实验验证有效性**：在主观与客观评估中均超越现有方法，证明控制与对齐性能的显著提升。|
|2507.19356v1|[Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of   ASR Transcripts and Speaker Diarization](http://arxiv.org/abs/2507.19356v1)|**贡献点分点：**  <br/>1. 提出基于时间戳对齐的ASR与SD联合框架，解决多模态情绪识别中的模态不匹配问题；  <br/>2. 设计跨模态融合机制，结合RoBERTa文本嵌入与Wav2Vec音频嵌入，并引入门控机制增强融合效果；  <br/>3. 在IEMOCAP数据集上验证了精确对齐对SER性能的显著提升，优于未同步的基线方法；  <br/>4. 系统性地证明时间戳对齐在对话场景下的关键作用，为鲁棒的多模态情绪分析提供理论支持。  <br/><br/>**总结（100字以内）：**  <br/>本文通过时间戳对齐和跨模态融合提升SER性能，结合预训练ASR与SD模型，利用门控机制优化文本-音频嵌入融合，实验表明精准对齐显著优于未同步方法，为多模态情绪分析提供了关键技术和理论依据。|
|2507.18119v2|[GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker   Characteristic Awareness](http://arxiv.org/abs/2507.18119v2)|**贡献点:**<br/>1. **提出GOAT-SLM模型**：首次将语音分析扩展至语言内容外的非语言特征（如方言、年龄、情绪），提升语音交互的多维理解能力。<br/>2. **双模态头架构设计**：分离语言建模与声学实现，实现语义理解与表达性语音生成的协同优化。<br/>3. **分阶段训练策略**：通过模块化、渐进式对齐语言、非语言及说话人特征信息，提升模型效率与多任务适应性。<br/>4. **多维度性能验证**：在TELEVAL基准测试中，验证模型在情感、方言及年龄敏感交互等非语义任务上的显著优势。<br/>5. **推动语音系统发展**：强调超越纯语言建模的重要性，促进自然、自适应且具备社会意识的语音交互技术演进。<br/><br/>**总结（100字以内）**：  <br/>本文提出GOAT-SLM，通过双模态架构与分阶段训练策略，首次融合语音的非语言特征（如方言、情感），在多维评估中优于开源模型，推动更自然、社会化的语音交互系统发展。|
|2507.18119v1|[GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker   Characteristic Awareness](http://arxiv.org/abs/2507.18119v1)|**总结（100字以内）**:  <br/>本研究提出GOAT-SLM，通过融合语调和说话者特征建模，创新性采用双模态头架构和分阶段训练策略，显著提升语音系统在情感、方言等非语义任务的性能，并推动更自然、社会感知的口语交互技术发展。<br/><br/>**贡献点**:  <br/>1. **提出GOAT-SLM模型**：首次构建具有语调（paralinguistic）和说话者特征（age, dialect, emotion）感知能力的端到端语音模型，突破传统仅关注语言内容的局限。  <br/>2. **双模态头架构设计**：创新性解耦语言建模与声学实现，实现“稳健语言理解+表达性语音生成”的双重目标。  <br/>3. **分阶段模块化训练策略**：通过大规模语音-文本语料逐步对齐语言、语调与说话者特征信息，提升模型效率与泛化能力。  <br/>4. **实验证明性能优势**：在TELEVAL多维基准测试中，模型在语义与非语义任务（如情感识别、方言适配）表现均衡，优于现有开源模型。  <br/>5. **强调非语言建模价值**：论证了建模语调、说话者特征对构建自然、社会感知的语音系统的重要性，推动领域方法革新。|
|2507.17563v1|[BoSS: Beyond-Semantic Speech](http://arxiv.org/abs/2507.17563v1)|**贡献点**:  <br/>1. 提出Spoken Interaction System Capability Levels（L1-L5）框架，系统化描述语音交互系统的演进层级。  <br/>2. 定义Beyond-Semantic Speech（BoSS）概念，强调隐式语义、情感及上下文动态在语音沟通中的关键作用。  <br/>3. 构建形式化BoSS框架，融合认知相关理论与机器学习模型，分析语音的时序与上下文特征。  <br/>4. 从五个维度评估BoSS属性，揭示当前语言模型对非语义信号的解析局限。  <br/>5. 指出提升BoSS研究的重要性，以推动更自然、情境感知的人机交互技术发展。  <br/><br/>**总结**:  <br/>论文提出BoSS框架与L1-L5层级体系，揭示现有语音模型在解析非语义信息上的不足，强调需加强隐式语义与上下文研究以实现更智能的语音交互。|
|2507.14915v3|[Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion   Modeling](http://arxiv.org/abs/2507.14915v3)|总结（100字以内）:  <br/>该论文提出SoulDance数据集与SoulNet框架，解决音乐对齐、多模态交互及全身动作建模的挑战，实现高质量3D舞蹈生成，提升情感表达与观众沉浸感。<br/><br/>（注：原文实际属于舞蹈生成领域，而非语音领域。以下是根据摘要内容整理的贡献点）  <br/><br/>贡献点分点列出:  <br/>1. **构建高精度音乐-舞蹈配对数据集**（SoulDance）：  <br/>   - 提供专业级3D舞蹈动作数据，包含精细标注的全身运动（身体、手部、面部）。  <br/>   - 解决音乐与舞蹈跨模态对齐与全身动作建模的训练数据不足问题。  <br/><br/>2. **提出SoulNet生成框架**：  <br/>   - 集成三大核心组件：  <br/>     - **分层残差向量量化（HRVQ）**：建模全身动作的复杂细粒度依赖关系。  <br/>     - **音乐对齐生成模型**：将分层运动单元组合为协调且富有表现力的舞蹈。  <br/>     - **音乐-动作检索模块**：作为跨模态对齐先验，确保生成过程中动作与音乐的时序同步与语义一致性。  <br/><br/>3. **实验验证有效性**：  <br/>   - 通过大量实验表明，SoulNet在生成高质量、音乐对齐及全身协调的3D舞蹈方面显著优于现有方法。|
|2507.14915v2|[Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion   Modeling](http://arxiv.org/abs/2507.14915v2)|总结（100字以内）:<br/>本文提出SoulDance数据集和SoulNet框架，实现高精度音乐-舞蹈配对生成，解决跨模态对齐和全身动作依赖建模难题，显著提升舞蹈情感表达与观众互动性。<br/><br/>贡献点：<br/>1. 构建首个专业级音乐-舞蹈配对数据集SoulDance，包含精细注释的全身3D动作<br/>2. 提出SoulNet框架，首次实现音乐对齐的全身协调舞蹈序列生成<br/>3. 创新性设计三阶段系统：(1)分层残差向量量化模型 (2)音乐对齐生成模块 (3)音乐-动作检索模块作为对齐先验<br/>4. 通过大规模实验验证，实现优于现有技术的高质量音乐同步舞蹈生成|
|2507.14915v1|[Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion   Modeling](http://arxiv.org/abs/2507.14915v1)|总结：  <br/>本文提出SoulDance数据集和SoulNet框架，解决音乐-舞蹈跨模态对齐与全身心运动建模难题，生成高质量、音乐协调的全身心3D舞蹈序列。<br/><br/>贡献点：  <br/>1. **构建首个高精度音乐-舞蹈配对数据集**  <br/>   - SoulDance通过专业动捕系统采集，包含细致标注的全身心舞蹈动作，填补了全身心3D舞蹈数据的空白。  <br/><br/>2. **提出SoulNet生成框架**  <br/>   - 包含三个核心模块：  <br/>     (1) 分层残差向量量化（建模全身、手部、面部的复杂运动依赖）；  <br/>     (2) 音乐对齐生成模型（整合分层动作单元为表达性舞蹈序列）；  <br/>     (3) 音乐-运动检索模块（作为跨模态对齐先验，确保生成过程中的时间同步与语义一致性）。  <br/><br/>3. **实验证明方法有效性**  <br/>   - SoulNet在生成高质量、音乐协调且全身心对齐的3D舞蹈序列方面显著优于现有方法。|
|2507.13626v2|[Unifying Listener Scoring Scales: Comparison Learning Framework for   Speech Quality Assessment and Continuous Speech Emotion Recognition](http://arxiv.org/abs/2507.13626v2)|**贡献点分点总结：**  <br/>1. **统一评分尺度建模**：提出统一的听者评分尺度，替代传统平均评分方法，避免因平均 ordinal 数据导致的信息失真。  <br/>2. **比较评分机制**：引入比较评分（对比不同 utterance 的评分关系），更精准地捕捉语音质量/情感的相对评价。  <br/>3. **多任务协同优化**：通过统一尺度设计，同时提升 Speech Quality Assessment (SQA) 和 Continuous Speech Emotion Recognition (CSER) 的预测性能，证明方法的有效性与鲁棒性。  <br/><br/>**总结（100字以内）**：  <br/>本文提出统一的听者评分建模方法，结合比较评分机制，解决了平均评分导致的偏差问题，同时提升SQA和CSER任务的预测性能，证明其有效性与鲁棒性。|
|2507.13626v1|[Unifying Listener Scoring Scales: Comparison Learning Framework for   Speech Quality Assessment and Continuous Speech Emotion Recognition](http://arxiv.org/abs/2507.13626v1)|总结：该论文提出一种基于比较评分的统一听者量表建模方法，有效解决传统平均评分法在SQA和CSER任务中的偏差问题，实验验证了方法的性能提升与鲁棒性。<br/><br/>贡献点：  <br/>1. **统一评分量表建模**：首次提出统一建模听者评分量表，替代传统平均评分法，避免因平均顺序数据导致的失真。  <br/>2. **比较评分机制**：利用比较评分捕捉语音片段间的相对评分关系，增强对听众主观差异的建模能力。  <br/>3. **跨任务有效性验证**：方法在SQA和CSER双任务中均表现出性能提升，证明其通用性和鲁棒性。|
|2507.13155v1|[NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal   Vocalizations with Emotion Annotations for Text-to-Speech](http://arxiv.org/abs/2507.13155v1)|总结：  <br/>提出首个全面的非语言发声与情感标注数据集NonverbalTTS，通过自动化检测与人工验证构建多任务处理框架，验证其在提升表达式语音合成性能方面的有效性，推动该领域开源研究进展。<br/><br/>贡献点：  <br/>1. **构建首个综合数据集**：推出NonverbalTTS，包含17小时多模态数据，涵盖10种非语言发声（如笑声、咳嗽）和8种情感类别，填补开源领域空白。  <br/>2. **多源数据整合与验证**：从VoxCeleb和Expresso等主流数据集中自动化提取非语言发声，结合人工校验确保标注质量。  <br/>3. **提出多任务处理流程**：设计集成自动语音识别（ASR）、非语言发声标注、情感分类及多标注融合的端到端系统，优化协同标注效率。  <br/>4. **验证数据有效性**：证明基于NVTTS微调的开源TTS模型在主观与客观指标（如语音相似度、非语言发声保真度）上可媲美闭源系统（如CosyVoice2）。  <br/>5. **开源推动与资源共享**：发布数据集及标注规范，为表达式语音合成研究提供标准基准，解决数据稀缺性瓶颈。|
|2507.09618v1|[THAI Speech Emotion Recognition (THAI-SER) corpus](http://arxiv.org/abs/2507.09618v1)|总结（100字以内）:  <br/>本研究构建了首个大规模泰语音情识别语料库THAI-SER，包含27854个utterances，涵盖5种情感类别。通过众包标注与严格质量控制，确保标注可靠性与人类识别准确率。并提供模型评估结果及公开数据代码，为语音情感分析提供标准化资源。  <br/><br/>贡献点分点：  <br/>1. **构建首个大规模泰语音情识别语料库**：THAI-SER是首个专门针对泰语音情识别的大型语料库，包含41小时36分钟的音频数据（27,854个utterances），覆盖多种录制环境（Zoom、两个录音室）。  <br/>2. **多场景与多类型数据采集**：数据来源于不同录音环境（Zoom、录音室），包含剧本录制与即兴对话，由200名专业演员（男女比例112:88，年龄18-55岁）及导演完成。  <br/>3. **系统情感标注与质量控制**：采用众包方式标注情感类别，并设计严格的过滤机制，确保多数同意得分（majority agreement）高于0.71，标注可靠性（Krippendorff's alpha）达0.692（高于推荐阈值0.667）。  <br/>4. **全面评估指标与结果**：通过inter-annotator reliability（0.692）和human recognition accuracy（0.772）两项指标验证数据质量，并提供模型在语料库及跨语料库场景下的性能评估。  <br/>5. **开放共享资源**：语料库及实验代码根据Creative Commons BY-SA 4.0协议公开，便于研究复现与跨领域应用。|
|2507.08012v1|[RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](http://arxiv.org/abs/2507.08012v1)|**贡献点分点总结：**  <br/>1. 提出一种基于提示的文本到语音模型的新型微调策略，同时解决控制受限和过度灵活的问题。  <br/>2. 通过主成分分析（PCA）挖掘合成样本的潜在特征，识别对输出变化贡献最大的潜在维度。  <br/>3. 将这些潜在特征作为新标签，用于二次微调以增强模型的可控性。  <br/>4. 在无情感披露模型中验证方法有效性，显著提升连续和离散特征的控制能力。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出一种基于PCA的微调方法，通过挖掘潜在特征提升TTS模型的可控性，解决了传统方法控制受限和过度灵活的问题，并在无情感披露模型中验证了其有效性。|
|2507.07806v1|[End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced   by Semi-supervised Learning](http://arxiv.org/abs/2507.07806v1)|**贡献点：**  <br/>1. 提出将半监督学习应用于语音情感和意图识别，有效利用大规模未标注数据与少量标注数据缓解标注成本问题。  <br/>2. 设计端到端的声学与语言模型，并通过多任务学习联合优化情感和意图识别任务。  <br/>3. 对比分析两种半监督策略（Fix-Match与Full-Match）的性能差异，验证其在语音情感和意图识别中的有效性。  <br/>4. 通过实验表明，基于半监督学习的融合模型在语音和文本数据的联合识别任务中分别提升12.3%和10.4%的平衡指标。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出半监督学习框架，结合声学与语言模型的多任务学习方法，解决语音数据标注难题。通过对比Fix-Match和Full-Match策略，验证其在情感和意图识别中的有效性，并显著提升模型性能（语音+12.3%，文本+10.4%）。|
|2507.07046v1|[A Novel Hybrid Deep Learning Technique for Speech Emotion Detection   using Feature Engineering](http://arxiv.org/abs/2507.07046v1)|**分点贡献：**<br/>1. 提出DCRF-BiLSTM模型，用于识别七种基本情感（neutral, happy, sad, angry, fear, disgust, surprise）。  <br/>2. 在五个基准数据集（RAVDESS, TESS, SAVEE, EmoDB, Crema-D）上分别取得97.83%、97.02%、95.10%、100%、100%的高准确率。  <br/>3. 首次在综合数据集（R+T+S+C+E）上评估单一SER模型，实现93.76%的总体准确率，超越先前研究结果。  <br/>4. 在(R+T+S)数据集上的准确率达98.82%，优于现有文献记录。  <br/>5. 证实了模型的鲁棒性与跨数据集泛化能力，支持其在语音情感识别领域的广泛应用。  <br/><br/>**总结：**  <br/>提出DCRF-BiLSTM模型，成功识别七种情感并在五个基准数据集取得高准确率，首次实现跨多数据集的全面评估，验证了模型的鲁棒性和泛化能力。|
|2507.06670v1|[STARS: A Unified Framework for Singing Transcription, Alignment, and   Refined Style Annotation](http://arxiv.org/abs/2507.06670v1)|**贡献点总结（100字以内）:**  <br/>提出首个统一的唱歌注释框架STARS，同步处理转录、对齐与风格标注，实现多层级全面注释，引入非自回归局部声学编码器提升表示学习，实验验证其性能优势，并推动可控SVS方法的发展。<br/><br/>---<br/><br/>**分点贡献:**  <br/>1. **首个统一框架**：STARS是首个同时处理唱歌转录、对齐和风格注释的统一框架，突破现有方法孤立处理各环节的局限。  <br/>2. **多层级注释**：提供包括音素-音频对齐、音符转录与时间定位、发声技巧识别、全局风格特征（情感/语速）在内的综合注释。  <br/>3. **创新编码器设计**：提出非自回归局部声学编码器，实现结构化的层次化表示学习，提升模型效率与准确性。  <br/>4. **性能验证与应用**：实验验证在多个评估维度优于现有方法，并证明其在SVS训练中显著提升感知自然度和风格控制能力。  <br/>5. **解决扩展性问题**：克服唱歌数据集创建中的规模化挑战，为可控SVS提供新方法基础。|
|2507.05911v1|[Differentiable Reward Optimization for LLM based TTS system](http://arxiv.org/abs/2507.05911v1)|**贡献点：**  <br/>1. 提出DiffRO方法，通过直接基于神经编码器令牌（neural codec tokens）优化奖励函数，替代传统RLHF依赖合成音频的方式。  <br/>2. 引入Gumbel-Softmax技术使奖励函数可微，简化RLHF训练流程并提升计算效率。  <br/>3. 设计多任务奖励（MTR）模型，提供多维度反馈以增强系统对指令的遵循能力。  <br/>4. 验证DiffRO在发音准确性上的显著提升，取得seed-tts-eval基准的SOTA WER结果。  <br/>5. 实现零样本控制情感和质量属性的能力，拓展TTS系统的应用灵活性。  <br/><br/>**总结（100字以内）:**  <br/>本文提出DiffRO方法，通过神经编码器令牌直接优化奖励函数，并结合Gumbel-Softmax和多任务奖励模型，显著提升TTS系统发音准确性和指令遵循能力，同时实现零样本情感质量控制。|
|2507.05177v2|[OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech   Language Model](http://arxiv.org/abs/2507.05177v2)|总结（100字以内）:  <br/>OpenS2S是首个开源透明的端到端情感语音交互模型，通过流式解码、自动数据构建等技术实现高效生成，结合大模型内容生成和可控TTS系统，构建了低成本、高多样性的训练语料库，推动情感语音系统研究。<br/><br/>贡献点：<br/>1. **首个开源透明架构**：提供完整的开源实现，包括数据集、模型权重、训练代码，打破情感语音模型的封闭性。<br/>2. **低延迟流式解码**：采用流式交错解码技术，实现实时语音生成，提升交互流畅性。<br/>3. **自动化数据构建**：设计低成本、高质量的自动数据生成管道，合成多样化的对话语料。<br/>4. **可控情感生成**：结合大语言模型与可调节文本到语音系统，实现语音表达的情感和说话者多样性。<br/>5. **最小化人工干预**：通过模型生成和自动化技术减少人工标注，提升数据构建效率。|
|2507.04598v1|[Multi-Step Prediction and Control of Hierarchical Emotion Distribution   in Text-to-Speech Synthesis](http://arxiv.org/abs/2507.04598v1)|**贡献点：**<br/>1. 提出分层情感分布（HD-ED）框架，支持多层级情感渲染的定量控制  <br/>2. 设计多步骤层级预测模块，实现从语句到音素的情感方差量化  <br/>3. 构建兼容性强的外部模块，适配多种TTS系统架构  <br/>4. 通过客观与主观评估验证方法有效性，显著提升情感表达能力  <br/><br/>**总结：**  <br/>本文提出分层情感分布框架，结合多步骤预测与外部模块设计，实现了从语句到音素的情感渲染定量控制，并通过实验验证方法有效性。|
|2507.04349v1|[TTS-CtrlNet: Time varying emotion aligned text-to-speech generation with   ControlNet](http://arxiv.org/abs/2507.04349v1)|**贡献点分点总结：**  <br/>1. 提出首个基于ControlNet的可控流匹配TTS框架（TTS-CtrlNet），无需全模型微调即可实现时间变化的情感控制。  <br/>2. 设计三个实用策略：模块化架构优化、情感特定的流步长调整、灵活控制尺度，提升情感控制的可扩展性与效果。  <br/>3. 实验验证了TTS-CtrlNet在情感相似性评分（Emo-SIM、Aro-Val SIM）上的性能优势，达到当前最优水平，同时保留原模型的零样本语音克隆与自然性能力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出TTS-CtrlNet，通过冻结预训练模型并引入可控模块，实现高效时间变化情感控制，保留原模型性能。提出三套优化策略并验证其有效性，达到SOTA情感相似性指标。|
|2507.04048v1|[CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization   via Soft Prompt Tuning](http://arxiv.org/abs/2507.04048v1)|**总结（100字以内）**  <br/>提出CLEP-DG框架，通过微调CLAP、引入文本驱动的ACPT策略及跨模态迁移方法，提升语音情感识别在多样化声学环境下的泛化能力和性能，优于现有模型。<br/><br/>**贡献点分点列出**  <br/>1. **提出CLEP-DG框架**：基于CLAP优化语音情感识别（SER）的鲁棒性，解决跨声学条件泛化不足的问题。  <br/>2. **微调CLAP适配情感数据集**：通过大规模情感语音数据集训练CLEP模型，增强对情感相关特征的编码能力。  <br/>3. **设计ACPT策略**：引入文本驱动的声学环境建模方法，无需额外标注音频即可优化可学习提示向量。  <br/>4. **跨模态迁移分类器**：利用文本嵌入训练分类器，并在推理阶段应用于音频编码器，缓解领域偏移问题。  <br/>5. **验证效果提升**：在五个基准数据集上实验证明，CLEP-DG超越CLAP基线方法，达到监督与领域泛化双场景SOTA。|
|2507.03382v1|[Speaker-agnostic Emotion Vector for Cross-speaker Emotion Intensity   Control](http://arxiv.org/abs/2507.03382v1)|**贡献点：**  <br/>1. 提出**speaker-agnostic情感向量**，解决传统方法在跨说话人时因情感向量不匹配导致的说话人一致性丢失问题。  <br/>2. 构建适用于**任意说话人**的通用情感表达模型，突破了单说话人情感向量的局限性。  <br/>3. 验证方法在**未见过说话人**场景下的有效性，证明其泛化能力。  <br/>4. 实现跨说话人情感强度控制的同时，保持**语音质量与可控性**，优于现有技术。  <br/><br/>**总结（100字内）：**  <br/>提出一种通用情感向量，解决跨说话人情感强度控制中的不一致性问题，提升语音质量与可控性，并验证其在未见过说话人场景下的有效性。|
|2507.03251v2|[Toward Efficient Speech Emotion Recognition via Spectral Learning and   Attention](http://arxiv.org/abs/2507.03251v2)|总结（100字以内）:  <br/>本文提出基于MFCC和1D-CNN的SER框架，结合数据增强及注意力机制，显著提升情感识别的准确率和泛化能力，为现实应用提供新基准。<br/><br/>贡献点分点如下：  <br/>1. **提出融合MFCC与人类听觉感知的桥梁**：通过使用Mel-Frequency Cepstral Coefficients（MFCCs）作为频谱特征，增强计算模型与人类听觉感知的关联性。  <br/>2. **设计1D-CNN框架集成数据增强技术**：引入数据增强策略提升模型鲁棒性与特征多样性，优化语音情感分类的泛化能力。  <br/>3. **创新通道与空间注意力机制**：在CNN架构中添加注意力模块，突出关键情感模式，增强对语音信号细微情感变化的捕捉能力。  <br/>4. **建立多数据集的性能新基准**：在SAVEE、RAVDESS、CREMA-D、TESS、EMO-DB和EMOVO等数据集上达到96.39%-99.82%的高准确率，验证方法有效性。  <br/>5. **强调实际应用潜力**：通过实验表明，该方法可提升SER在辅助技术与人机交互等实际场景中的适用性。|
|2507.03251v1|[Toward Efficient Speech Emotion Recognition via Spectral Learning and   Attention](http://arxiv.org/abs/2507.03251v1)|**贡献点**（分点）:  <br/>1. **提出融合MFCC频谱特征的SER框架**：首次将Mel-Frequency Cepstral Coefficients（MFCCs）作为核心频谱特征，弥合计算情感处理与人类听觉感知之间的差距。  <br/>2. **设计增强鲁棒性的1D-CNN模型**：引入数据增强技术并构建基于1D卷积神经网络的框架，提升模型对复杂语音信号的适应能力。  <br/>3. **集成通道与空间注意力机制**：通过注意力模块优化特征提取，突出关键情感模式，增强对细微情感变化的识别能力。  <br/>4. **多数据集性能验证**：在SAVEE、RAVDESS、CREMA-D、TESS、EMO-DB和EMOVO等多个基准数据集上取得SOTA结果，显著提升准确率。  <br/>5. **推动实际应用潜力**：验证深度学习方法在多样化数据上的泛化能力，证明其在辅助技术与人机交互中的可行性和先进性。  <br/><br/>**总结**（100字以内）:  <br/>本文提出基于MFCC和1D-CNN的SER框架，融合数据增强与注意力机制，显著提升情感识别准确率，验证了其在多数据集上的泛化能力，为实际应用提供新思路。|
|2507.03147v2|[DeepGesture: A conversational gesture synthesis system based on emotions   and semantics](http://arxiv.org/abs/2507.03147v2)|**贡献点**：  <br/>1. 提出DeepGesture框架：基于扩散模型，首次实现多模态信号（文本、语音、情感、种子运动）驱动的共语音手势生成。  <br/>2. 架构优化：改进语义对齐与情感表达能力，通过快速文本转录作为语义条件，显著提升生成手势的自然度。  <br/>3. 情感控制生成：采用情绪引导的分类器自由扩散技术，支持跨情感状态的可控手势生成与情感插值。  <br/>4. 可视化渲染：开发基于BVH的Unity渲染管线，实现高质量三维手势可视化。  <br/>5. 泛化能力：在ZeroEGGS数据集上验证，系统可泛化到分布外语音（含合成语音），提升数字人的多模态交互表现。  <br/><br/>**总结**：  <br/>本文提出DeepGesture框架，通过多模态信号驱动与情感控制生成，显著提升共语音手势的自然度与上下文适配性。|
|2507.02080v1|[TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation](http://arxiv.org/abs/2507.02080v1)|总结：  <br/>提出TAGF框架，通过时间感知的门控机制解决多模态情感识别中的噪声和模态对齐问题，有效整合跨模态特征并建模动态情绪变化，在Aff-Wild2数据集上取得竞争力实验结果。<br/><br/>贡献点：  <br/>1. **提出时间感知门控融合方法（TAGF）**：通过BiLSTM构建时间门控机制，动态调整递归注意力输出的权重，解决模态间时间对齐和噪声干扰问题。  <br/>2. **多步跨模态特征整合**：学习各递归步骤的相对重要性，有效融合多阶段跨模态信息，捕捉情感表达的时序演化过程。  <br/>3. **强鲁棒性与动态建模能力**：在现实场景下可靠建模动态情绪过渡，显著提升对跨模态时序不一致的鲁棒性。  <br/>4. **实验验证性能优势**：在Aff-Wild2数据集上与现有递归注意力模型对比，展示出竞争力的指标表现（如准确率、F1值等）。|