|Source|Title|Summary|
|---|---|---|
|2511.03601v1|[Step-Audio-EditX Technical Report](http://arxiv.org/abs/2511.03601v1)|总结：  <br/>提出首个基于LLM的开源音频编辑模型Step-Audio-EditX，创新采用大边距合成数据训练方法，突破传统表示级解耦范式，在情感编辑等细粒度任务中超越现有SOTA模型。<br/><br/>贡献点：  <br/>1. **首个开源LLM音频编辑模型**：首次构建支持情感、说话风格和语用学编辑的开源音频生成系统，具备零样本文本到语音（TTS）能力。  <br/>2. **大边距合成数据训练方法**：仅依赖大边距合成数据，无需嵌入式先验或辅助模块，简化训练流程并提升模型泛化性。  <br/>3. **迭代控制与高表达性设计**：通过大边距学习实现音频编辑的迭代交互与丰富表达，革新传统语音生成的技术路径。  <br/>4. **超越现有SOTA模型表现**：在情感编辑及细粒度控制任务中，性能优于MiniMax-2.6-hd和Doubao-Seed-TTS-2.0等主流模型。|
|2511.03425v1|[SyMuPe: Affective and Controllable Symbolic Music Performance](http://arxiv.org/abs/2511.03425v1)|**贡献点**  <br/>1. **提出SyMuPe框架**：首个专门针对情感化、可控符号钢琴表演的生成框架。  <br/>2. **创新模型架构**：PianoFlow采用条件流匹配技术，解决多掩码音乐表演修复任务。  <br/>3. **高质量训练数据**：构建并清洗包含2,968小时对齐乐谱与表达性MIDI的基准数据集。  <br/>4. **情感控制机制**：集成情感分类器与Flan-T5情感加权文本嵌入，实现文本-情感驱动的生成。  <br/>5. **性能评估突破**：在客观和主观测试中超越Transformer基线模型，接近人类演奏水平。  <br/>6. **实际应用潜力**：可应用于互动音乐系统，提升音乐表演的可访问性与用户参与度。  <br/><br/>**总结**：  <br/>本文提出SyMuPe框架及PianoFlow模型，通过条件流匹配与情感控制技术生成高质量、可操控的钢琴演奏，突破传统方法限制，具备实际交互应用价值。|
|2511.00850v1|[MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional   Intelligence ability of Spoken Dialogue Models](http://arxiv.org/abs/2511.00850v1)|总结（100字以内）:  <br/>提出Multi-Bench基准，首次系统评估多轮对话中情感智能能力，设计分层任务和可复现框架，揭示当前SDMs在情感应用与推理方面的不足，推动对话模型更注重交互与情感理解。<br/><br/>贡献点:  <br/>1. **首次提出多轮情感对话基准**：Multi-Bench是首个专门针对SDMs在多轮交互对话中情感智能能力的评估体系，填补了该领域的研究空白。  <br/>2. **分层任务设计**：构建包含基础（情感理解）与进阶（情感支持与应用）任务的分层框架，全面覆盖交互对话的多维度能力要求。  <br/>3. **大规模数据集支持**：提供涵盖情感识别、推理与复杂对话的5个任务及3.2K样本数据集，并配套可复现的评估流程。  <br/>4. **揭示模型性能差距**：通过实证分析发现现有SDMs在情感应用与多轮推理任务中的局限性，明确未来研究方向。|
|2511.00402v1|[Emotion Detection in Speech Using Lightweight and Transformer-Based   Models: A Comparative and Ablation Study](http://arxiv.org/abs/2511.00402v1)|**贡献点：**<br/><br/>1. **提出对比分析**：比较了轻量级Transformer模型DistilHuBERT与PaSST在语音情感识别任务中的表现。<br/>2. **使用CREMA-D数据集**：对六种核心情感进行分类，验证模型在实际数据上的性能。<br/>3. **基准测试**：与传统CNN-LSTM模型（使用MFCC特征）进行对比，评估模型在准确率和F1分数上的优势。<br/>4. **模型轻量化优势**：DistilHuBERT在保持极小模型体积（0.02 MB）的同时，表现出更高的准确率和F1分数。<br/>5. **分类头结构研究**：通过消融实验分析PaSST三种分类头（Linear、MLP、Attentive Pooling）对性能的影响。<br/>6. **发现情感识别难点**：指出“disgust”情感最难识别，为后续研究提供方向。<br/>7. **应用场景建议**：表明轻量级Transformer模型适合用于边缘设备的实时语音情感识别任务。<br/><br/>**总结（100字以内）：**  <br/>本研究比较了DistilHuBERT与PaSST在语音情感识别中的表现，发现DistilHuBERT在准确率和模型大小上均有优势，并通过消融实验揭示了分类头的影响，为边缘实时部署提供了有力支持。|
|2511.00256v1|[NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset   for Voice Conversion](http://arxiv.org/abs/2511.00256v1)|总结：<br/>本文提出首个面向情感语音转换的自发播客数据集NaturalVoices，包含超5000小时多模态标注数据，结合开放框架支持模型训练与评估，并揭示现有架构在处理大规模自发数据时的局限性。<br/><br/>贡献点：<br/>1. **首个专门面向情感意识语音转换的大型自发语料库**  <br/>   - 提供5049小时的自发播客录音，突破传统表演式数据集的局限性<br/>   - 专为建模自然声调与情感设计，填补语音转换领域资源缺口<br/><br/>2. **多维度自动标注体系**  <br/>   - 首创包含情感（类别与属性）、语音质量、转录文本、说话人身份、声学事件的全面标注<br/>   - 通过大规模数据捕捉情感表达的多样性与自然说话风格<br/><br/>3. **模块化开放处理框架**  <br/>   - 提供可扩展的标注工具与灵活过滤系统<br/>   - 支持构建适用于不同VC任务的定制化数据子集<br/><br/>4. **系统性性能评估**  <br/>   - 验证数据集对训练鲁棒、可泛化的VC模型的有效性<br/>   - 揭示当前语音转换架构在处理大规模自发数据时的不足<br/><br/>5. **双功能价值定位**  <br/>   - 构建语音转换研究的基准资源<br/>   - 为情感建模提供挑战性数据测试场景|
|2510.26823v1|[Cross-Corpus Validation of Speech Emotion Recognition in Urdu using   Domain-Knowledge Acoustic Features](http://arxiv.org/abs/2510.26823v1)|总结：  <br/>本研究首次在跨语料库框架下评估Urdu语音情感识别性能，揭示了自语料库验证的局限性，强调跨语料库评估对低资源语言的重要性，推动情感计算在少数语言社区的应用。<br/><br/>贡献点：  <br/>1. **首次跨语料库研究**：针对Urdu语音情感识别，探索跨语料库设置下的模型泛化能力，填补该领域的研究空白。  <br/>2. **标准特征集应用**：采用eGeMAPS和ComParE等标准声学特征集，提升模型比较的客观性和实验可重复性。  <br/>3. **分类器对比实验**：对比Logistic Regression与Multilayer Perceptron的分类效果，验证不同方法在跨语料库任务中的适用性。  <br/>4. **改进评估策略**：提出结合Unweighted Average Recall（UAR）与类别不平衡处理的评估方法，增强性能评估的准确性。  <br/>5. **验证局限性发现**：揭示自语料库验证可能高估模型性能（UAR偏差高达13%），强调跨语料库验证对模型鲁棒性的实际意义。|
|2510.21388v1|[Compressing Quaternion Convolutional Neural Networks for Audio   Classification](http://arxiv.org/abs/2510.21388v1)|**贡献点总结（100字以内）**：  <br/>提出基于四元数卷积的网络（QCNNs），有效捕捉多通道音频相关性；通过剪枝方法降低计算复杂度，比知识蒸馏更高效；在多个音频分类任务中表现出竞争力，参数减少80%，计算成本降低50%。  <br/><br/>**分点贡献**：  <br/>1. **引入四元数卷积**：利用四元数代数联合处理多通道音频信号，增强模型对音频多维结构的利用，提升特征学习效果。  <br/>2. **降低计算复杂度**：通过剪枝方法显著减少QCNNs的可学习参数与计算成本，相比传统CNNs有明显优势。  <br/>3. **优于知识蒸馏**：实验表明剪枝在性能上与知识蒸馏相当，甚至更优，且计算代价更低。  <br/>4. **保持高性能表现**：在多个音频分类基准（如AudioSet、GTZAN、ESC-50、RAVDESS）中，剪枝后的QCNNs表现优于或接近传统CNN和Transformer模型。  <br/>5. **适用于资源受限场景**：提升模型效率，为部署在计算资源有限平台上的音频分类提供可行方案。|
|2510.20513v1|[Decoding the Ear: A Framework for Objectifying Expressiveness from Human   Preference Through Efficient Alignment](http://arxiv.org/abs/2510.20513v1)|总结：DeEAR 提出一种基于人类偏好的语音表达性评估框架，涵盖情感、韵律与自发性三个维度，实现高效、准确的表达性评分，并推动了表达性语音数据集的构建。<br/><br/>贡献点：<br/><br/>1. 提出 DeEAR 框架，将人类对语音表达性的偏好转化为客观评分。<br/>2. 评估维度涵盖情感（Emotion）、韵律（Prosody）、自发性（Spontaneity）。<br/>3. 使用少于 500 个标注样本实现与人类感知高度一致（SRCC = 0.86）。<br/>4. 实现 S2S 模型之间的公平基准比较，识别表达性差距。<br/>5. 构建包含 14,000 个表达性语句的 ExpressiveSpeech 数据集，显著提升模型表达性得分。|
|2510.18036v1|[Transformer Redesign for Late Fusion of Audio-Text Features on   Ultra-Low-Power Edge Hardware](http://arxiv.org/abs/2510.18036v1)|**贡献点：**<br/><br/>1. 提出一种面向边缘设备的硬件感知情绪识别系统，适用于小型、低功耗和隐私敏感的场景。  <br/>2. 结合语音和语言特征，采用优化的 late-fusion 架构，专为 Edge TPU 设计。  <br/>3. 集成量化 Transformer 音频模型与冻结的 DSResNet-SE 关键词嵌入，实现实时推理。  <br/>4. 采用 MicroFrontend 和 MLTK 保证训练与部署时的频谱对齐，提升模型性能。  <br/>5. 在 1.8MB 内存和 21-23ms 延迟下实现 6.3% 的宏观 F1 改进，优于单模态基线。  <br/>6. 验证了在微控制器级别边缘平台实现高精度实时多模态情绪推理的可行性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种适用于微控制器类边缘平台的硬件感知多模态情绪识别系统，结合语音与语言特征，实现低内存、低延迟的实时推理，并在 IEMOCAP 数据集上取得显著性能提升。|
|2510.16893v1|[Investigating Safety Vulnerabilities of Large Audio-Language Models   Under Speaker Emotional Variations](http://arxiv.org/abs/2510.16893v1)|**贡献点：**<br/><br/>1. **提出首个基于情感的恶意语音指令数据集**：构建了涵盖多种情感与强度的恶意语音指令数据集，用于评估大音频语言模型的安全性。<br/><br/>2. **揭示情感对模型安全性的影响**：发现不同情感会引发不同程度的不安全响应，指出了情感在安全对齐中的关键作用。<br/><br/>3. **发现强度的非单调影响**：情感强度对不安全响应的影响并非单调，中等强度的表达反而带来最大风险。<br/><br/>4. **强调情感变异下的安全对齐必要性**：指出需要专门设计的对齐策略以确保模型在情感变化下的安全性，这对实际部署至关重要。|
|2510.16273v1|[MuseTok: Symbolic Music Tokenization for Generation and Semantic   Understanding](http://arxiv.org/abs/2510.16273v1)|总结：  <br/>提出MuseTok音乐分词方法，结合RQ-VAE与Transformer框架，实现音乐生成与理解任务的高效表征学习，并在语义理解任务中超越传统基线。<br/><br/>贡献点：  <br/>1. **提出MuseTok**：首次将残差向量量化变分自编码器（RQ-VAE）与Transformer框架结合，用于符号音乐的分词学习。  <br/>2. **高保真音乐表征**：生成的音乐代码可实现高质量音乐重建和准确的乐理理解。  <br/>3. **多任务验证**：在旋律提取、和弦识别、情感识别等生成与理解任务中验证有效性。  <br/>4. **性能优势**：在语义理解任务上优于现有表征学习基线，生成性能与传统方法相当。  <br/>5. **定性分析支持**：通过真实数据与合成数据实验证明代码能有效捕捉音乐概念。|
|2510.13906v1|[Switchboard-Affect: Emotion Perception Labels from Conversational Speech](http://arxiv.org/abs/2510.13906v1)|总结：  <br/>本研究通过创建自然情感语音数据集SWB-Affect，分析标注线索，并评估SER模型表现，揭示了现有数据集在情感表达真实性与标注透明度上的不足，强调了自然数据在模型评估中的重要性。<br/><br/>贡献点：  <br/>1. **提出自然情感数据集SWB-Affect**：基于Switchboard语料库，构建涵盖10类情绪（愤怒、蔑视、厌恶、恐惧、悲伤、惊讶、快乐、温柔、平静、中性）和3个维度属性（激活度、效价、支配度）的自然对话情感标注数据集。  <br/>2. **明确标注方法与指南**：设计详细的标注定义，明确指导标注者识别语音中的情感特征，提升标注过程的透明度和可复现性。  <br/>3. **分析情感线索**：系统研究词汇和副语言特征（如语调、停顿）对情感标注的影响，揭示人类感知情感的多维度线索。  <br/>4. **评估SER模型泛化能力**：发现当前先进模型在情绪分类（尤其愤怒）上存在显著泛化不足，凸显自然情感数据在评估中的必要性。  <br/>5. **公开数据促进研究**：释放SWB-Affect标注数据，为语音情感分析领域的进一步研究提供资源支持。|
|2510.13244v1|[MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive   Learning and Bar-Equivariant Contact-Aware Encoding](http://arxiv.org/abs/2510.13244v1)|**贡献点：**  <br/>1. 提出MotionBeat框架，实现音乐与运动对齐的表示学习。  <br/>2. 引入Embodied Contrastive Loss (ECL) 和Structural Rhythm Alignment Loss (SRAL) 两个新目标，增强节奏与结构的捕捉能力。  <br/>3. 采用bar-equivariant相位旋转和接触引导注意力机制，有效建模周期性节奏与同步运动事件。  <br/>4. 在音乐生成舞蹈、节拍跟踪、音乐标签、音乐类型与乐器分类、情感识别及音视频检索等任务中优于现有方法。  <br/><br/>**总结：**  <br/>MotionBeat通过融合节奏与运动信息，提升了音乐表示的学习效果，并在多个任务中取得优越性能。|
|2510.12819v1|[Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet   Vocalization Analysis](http://arxiv.org/abs/2510.12819v1)|总结：  <br/>本文提出首个连续Valence-Arousal框架，通过自动标注和多任务学习提升宠物语音情绪识别性能，解决离散分类的模糊性问题，并推动AI宠物情绪翻译器等应用发展。<br/><br/>贡献点：  <br/>1. **连续情绪建模**：构建Valence-Arousal（VA）二维空间模型，替代传统离散分类，有效捕捉情绪的强度与连续性变异。  <br/>2. **自动标注方法**：开发VA标签生成算法，实现42,553个宠物声音样本的大规模高效标注。  <br/>3. **多任务学习框架**：联合训练VA回归与辅助任务（情绪、体型、性别），通过任务协同优化特征学习。  <br/>4. **高精度模型性能**：提出Audio Transformer模型，在Valence和Arousal任务中达到r=0.9024和0.7155的高相关性，缓解离散类别混淆。  <br/>5. **应用拓展潜力**：为宠物情绪分析提供更精细的表示，推动人-宠物交互、兽医诊断及行为训练等领域的应用落地。|
|2510.11124v1|[Perturbation Self-Supervised Representations for Cross-Lingual Emotion   TTS: Stage-Wise Modeling of Emotion and Speaker](http://arxiv.org/abs/2510.11124v1)|总结：  <br/>本文提出EMM-TTS框架，通过双阶段处理与扰动SSL表示实现跨语言情感语音合成，引入SCL和SEALN模块增强控制与保持，结合显式声学特征优化性能，并验证其在自然度和情感迁移上的优势。<br/><br/>贡献点：  <br/>1. **提出双阶段框架**：基于扰动自监督学习（SSL）表示，分阶段实现情感表达与音色还原的解耦控制。  <br/>2. **分析说话人扰动策略**：评估formant shifting与speaker anonymization对情感与音色分离的影响。  <br/>3. **创新控制模块**：引入Speaker Consistency Loss（SCL）与Speaker-Emotion Adaptive Layer Normalization（SEALN）以提升语音保持与表达控制能力。  <br/>4. **融合多模态特征**：结合显式声学特征（F0、能量、时长）与预训练潜在特征，增强语音克隆性能。  <br/>5. **多维度验证效果**：通过主观与客观指标综合评估，证明系统在情感迁移、音色一致性和语音自然度上的优越性。|
|2510.10078v1|[Improving Speech Emotion Recognition with Mutual Information Regularized   Generative Model](http://arxiv.org/abs/2510.10078v1)|**贡献点：**<br/><br/>1. 提出一种基于跨模态信息传递和互信息正则化的数据增强框架，提升语音情感识别（SER）输入数据质量。  <br/>2. 扩展数据增强至多模态输入，利用互信息确保模态间依赖关系。  <br/>3. 实验在IEMOCAP、MSP-IMPROV和MSP-Podcast三个基准数据集上验证框架有效性。  <br/>4. 框架在无需跨模态信息的情况下也能生成有效输入，提升情感预测性能。|
|2510.09072v1|[Emotion-Disentangled Embedding Alignment for Noise-Robust and   Cross-Corpus Speech Emotion Recognition](http://arxiv.org/abs/2510.09072v1)|**贡献点：**  <br/>1. 提出EDRL方法，提取具有类别区分性的语音情感特征，同时保留情感类别间的共性。  <br/>2. 引入MEA方法，将特征投影到联合判别潜在子空间，增强与原始语音输入的协方差。  <br/>3. 在干净数据上训练情感分类器，并在噪声和跨语料数据上评估，验证方法的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EDRL与MEA结合的两阶段方法，提升语音情感识别在噪声和跨语料场景下的鲁棒性和泛化能力，通过联合潜在子空间对齐实现更有效的特征学习。|
|2510.08586v1|[Dynamic Stress Detection: A Study of Temporal Progression Modelling of   Stress in Speech](http://arxiv.org/abs/2510.08586v1)|**贡献点：**<br/><br/>1. 将心理压力建模为随时间演变的动态现象，而非静态标签。  <br/>2. 提出动态标注策略，从情感标签中生成细粒度的压力标注。  <br/>3. 引入基于交叉注意力的序列模型（Unidirectional LSTM 和 Transformer Encoder）捕捉时间上的压力变化。  <br/>4. 在 MuSE 和 StressID 数据集上显著优于现有基线（+5% 和 +18%）。  <br/>5. 方法在自定义真实场景数据集上具有良好的泛化能力。  <br/><br/>**总结（100字以内）：**  <br/>本文将语音压力检测建模为动态过程，提出动态标注策略与交叉注意力序列模型，显著提升检测准确率，并在真实数据集上表现良好。|
|2510.06072v1|[EmoHRNet: High-Resolution Neural Network Based Speech Emotion   Recognition](http://arxiv.org/abs/2510.06072v1)|**贡献点：**  <br/>1. 提出EmoHRNet，是HRNet在语音情感识别领域的创新应用。  <br/>2. 保持高分辨率表示，有效捕捉语音信号中细节与整体情感特征。  <br/>3. 在多个数据集（RAVDESS、IEMOCAP、EMOVO）上均取得优于现有模型的性能。  <br/>4. 实验结果表明，EmoHRNet在SER领域达到新基准。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoHRNet，将HRNet应用于语音情感识别，保持高分辨率表示以捕捉更全面的情感特征，在多个数据集上取得更高准确率，为SER领域提供新基准。|
|2510.05875v1|[LARA-Gen: Enabling Continuous Emotion Control for Music Generation   Models via Latent Affective Representation Alignment](http://arxiv.org/abs/2510.05875v1)|总结：  <br/>本论文提出LARA-Gen框架，通过引入LARA对齐机制和连续情感控制模块，实现音乐生成中的精细情感调控，并建立客观评价基准，显著提升情感一致性与生成质量。<br/><br/>贡献点：  <br/>1. **LARA-Gen框架设计**：引入Latent Affective Representation Alignment (LARA)机制，将模型内部隐藏状态与外部音乐理解模型对齐，实现连续情感控制。  <br/>2. **连续情感解耦模块**：基于valence-arousal空间设计情感控制模块，分离情感属性与文本内容，突破文本提示的瓶颈。  <br/>3. **情感生成基准建立**：构建包含专业测试集和Emotion Predictor的评估基准，支持音乐情感可控性的客观验证与比较。|
|2510.05758v1|[EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in   LLM-based TTS](http://arxiv.org/abs/2510.05758v1)|**贡献点：**  <br/>1. 提出EMORL-TTS框架，首次将全局情感强度控制（VAD空间）与局部强调调节相结合，实现细粒度情感控制。  <br/>2. 融合监督微调与强化学习，通过任务特定奖励机制优化情感类别、强度及强调的生成质量。  <br/>3. 系统研究强调位置对情感强度的影响机制，揭示情感表达中的局部调控规律。  <br/>4. 实验证明在保持LLM基线合成质量的前提下，显著提升情感准确性、强度区分和强调清晰度。  <br/><br/>**总结：**  <br/>EMORL-TTS通过统一强度与强调控制，结合强化学习提升情感生成效果，实现高质量且可控的语音情感合成。|
|2510.05749v1|[MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics   for Speech Emotion Recognition](http://arxiv.org/abs/2510.05749v1)|**贡献点总结：**<br/><br/>1. 提出MSF-SER模型，融合多粒度文本语义（LES、GS、ES）与声学特征。  <br/>2. 引入跨模态FiLM-modulated轻量FM-MOE结构，提升语义融合效果。  <br/>3. 在MSP-Podcast和IEMOCAP数据集上验证，显著提升情感维度预测性能。|
|2510.05191v2|[Provable Speech Attributes Conversion via Latent Independence](http://arxiv.org/abs/2510.05191v2)|**贡献点：**<br/><br/>1. 提出一种通用的语音属性转换框架，具有理论分析和保证。  <br/>2. 基于非概率自编码器，引入预测潜变量与可控变量的独立性约束。  <br/>3. 实现可控变量下的信号一致转换，保留原始内容并修改目标属性。  <br/>4. 在多个语音风格（如说话人身份、情感）上验证方法的通用性和有效性。  <br/><br/>**总结：**  <br/>本文提出了一种理论严谨的语音属性转换框架，通过独立性约束实现可控转换，有效保留内容并修改目标属性，适用于多种语音风格。|
|2510.04688v1|[A Study on the Data Distribution Gap in Music Emotion Recognition](http://arxiv.org/abs/2510.04688v1)|**贡献点：**<br/><br/>1. **多风格音乐情感数据集分析**：引入五个涵盖多种音乐风格的维度标注数据集，提升了研究的广泛性和代表性。  <br/>2. **揭示数据偏差问题**：通过实验分析，发现某些特征表示可能存在音乐类型主导和数据集偏差。  <br/>3. **提出有效框架**：结合Jukebox模型提取的嵌入和chroma特征，构建了一个简单且高效的音乐情感识别框架。  <br/>4. **提升跨数据集泛化能力**：通过多种训练集组合，显著提升了模型的跨数据集泛化性能。  <br/><br/>**总结（100字以内）：**  <br/>本文通过分析多风格音乐情感数据集，揭示数据偏差问题，并提出结合Jukebox嵌入与chroma特征的框架，提升模型跨数据集泛化能力。|
|2510.04251v1|[Machine Unlearning in Speech Emotion Recognition via Forget Set Alone](http://arxiv.org/abs/2510.04251v1)|总结：  <br/>提出基于对抗攻击的语音情感识别反学习方法，仅使用被遗忘数据进行微调，在保护隐私的同时保持模型性能。<br/><br/>贡献点：  <br/>1. **创新方法**：首次将对抗攻击策略应用于语音情感识别的机器反学习任务，无需依赖额外数据或数据重新分配。  <br/>2. **隐私保护**：解决语音数据隐私问题，允许仅通过被遗忘数据删除模型中的敏感信息，降低隐私泄露风险。  <br/>3. **计算高效性**：在大数据场景下减少计算资源消耗，避免传统方法对数据规模和分布式处理的高依赖。  <br/>4. **性能保持**：验证方法在去除敏感数据后仍能维持较高的情感识别测试集性能，平衡模型有效性与隐私需求。|
|2510.03986v1|[A Multilingual Framework for Dysarthria: Detection, Severity   Classification, Speech-to-Text, and Clean Speech Generation](http://arxiv.org/abs/2510.03986v1)|**贡献点：**<br/><br/>1. 提出一个统一的多语言AI框架，涵盖六项关键任务：二分类检测、严重程度分类、干净语音生成、语音转文本、情感检测与语音克隆。<br/>2. 在英语、俄语和德语数据集上进行了分析，采用频谱可视化和声学特征提取方法。<br/>3. 二分类检测准确率达97%，展示了语言泛化能力。<br/>4. 严重程度分类准确率同样达97%，模型注意力集中在低谐波上，具有可解释性。<br/>5. 俄语语音转文本系统在训练和测试集上分别达到0.03和0.06的L1损失，具备较高的语音可懂度。<br/>6. 通过跨语言迁移学习，在英语数据上微调俄语模型，显著降低损失，验证了低资源语言的可行性。<br/>7. 语音转文本系统使用三轮训练达到0.1367的词错误率（WER），支持后续的情感识别与语音克隆任务。<br/>8. 研究成果可应用于多语言环境下的构音障碍诊断与患者沟通改善。|
|2510.02171v1|[Go witheFlow: Real-time Emotion Driven Audio Effects Modulation](http://arxiv.org/abs/2510.02171v1)|**贡献点总结**  <br/>1. 提出witheFlow系统，实现基于生物信号与音频特征的实时音乐效果自适应调节，解决人机协作中情感表达的难点。  <br/>2. 系统设计为轻量级且本地运行，支持笔记本电脑部署，提升实际应用可行性。  <br/>3. 作为开源项目提供，兼容数字音频工作站及传感器，推动跨学科技术共享与研究复现。  <br/>4. 验证了通过跨模态数据（生物信号+音频）增强音乐表演情感交互的潜力，为未来人机协作提供新思路。  <br/><br/>**摘要总结**（100字以内）：  <br/>论文提出witheFlow系统，通过整合生物信号与音频特征实现实时音乐效果自适应调节，支持本地运行与开源，探索人机协作在音乐情感表达中的应用。|
|2510.01722v1|[Emotional Text-To-Speech Based on Mutual-Information-Guided   Emotion-Timbre Disentanglement](http://arxiv.org/abs/2510.01722v1)|**总结（100字以内）:**  <br/>本文提出了一种新的情感TTS方法，通过风格解缠技术实现细粒度音素级情感嵌入预测，并分离参考语音的内在属性，提升了生成语音的自然度和情感丰富度。<br/><br/>---<br/><br/>**贡献点：**<br/><br/>1. **引入细粒度情感嵌入预测**：实现音素级别的情感特征建模，捕捉更细致的情感细节。  <br/>2. **风格解缠技术**：通过解缠方法减少音色与情感特征之间的互信息，提升特征分离效果。  <br/>3. **提升情感TTS质量**：实验表明，该方法在生成自然、富有情感的语音方面优于基线系统。  <br/>4. **增强系统灵活性**：分离参考语音的内在属性，使情感TTS系统更具可控性和适应性。|
|2510.01284v1|[Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](http://arxiv.org/abs/2510.01284v1)|总结：  <br/>提出Ovi统一音频-视频生成框架，通过双DIT模块块级跨模态融合实现自然同步，结合预训练视频架构的音频塔提升生成质量，并开源完整资源。<br/><br/>贡献点：  <br/>1. **统一生成范式**：首次将音频与视频视为单一生成过程，替代传统多阶段架构或顺序合成方法。  <br/>2. **块级跨模态融合**：利用双DIT模块的时间与语义块级融合机制，解决同步问题并消除额外对齐步骤。  <br/>3. **音频塔架构设计**：采用与强预训练视频模型相同的架构初始化音频塔，增强跨模态对齐能力。  <br/>4. **大规模音频训练**：从头训练音频塔于百万余小时原始音频数据，实现逼真音效与情感语音生成。  <br/>5. **联合训练策略**：通过时间（scaled-RoPE嵌入）与时序语义（双向交叉注意力）联合优化，提升细粒度融合效果。  <br/>6. **高质量应用**：生成电影级视频片段，包含自然语音与上下文匹配的音效，并开源全部资源。|
|2510.01176v2|[Audio Driven Real-Time Facial Animation for Social Telepresence](http://arxiv.org/abs/2510.01176v2)|**贡献点总结（100字以内）**  <br/>提出实时音频驱动的3D面部动画系统，通过在线Transformer和蒸馏管道降低延迟至15ms内，结合扩散模型实现高保真表情生成，支持多模态交互（如情绪条件与眼动追踪），并在多语言场景中验证其高效性与准确性。  <br/><br/>**分点贡献**  <br/>1. **实时性架构创新**：设计在线Transformer（消除未来输入依赖）与蒸馏管道（单步加速去噪），实现<15ms的GPU延迟。  <br/>2. **扩散模型应用**：利用扩散模型的生成能力，捕捉自然语音所需的丰富面部表达，同时维持实时性能。  <br/>3. **多模态扩展性**：框架兼容情绪条件及VR设备中的多模态传感器（如头戴式眼动摄像头），提升交互场景的适应性。  <br/>4. **效率与准确性提升**：相比现有离线方法，推理速度提升100-1000倍，且在面部动画精度上表现更优。  <br/>5. **实际验证**：通过多语言演讲等实时VR场景演示，证明系统在真实应用中的有效性。|
|2510.01157v1|[Backdoor Attacks Against Speech Language Models](http://arxiv.org/abs/2510.01157v1)|**贡献点：**  <br/>1. 提出首个系统性研究音频后门攻击对语音语言模型的影响框架；  <br/>2. 验证攻击在4种语音编码器和3种数据集上的广泛有效性，覆盖ASR、情感识别、性别与年龄预测等任务；  <br/>3. 通过组件级分析明确识别出模型处理流程中最脆弱的环节；  <br/>4. 设计基于微调的防御方法，有效缓解被污染预训练编码器的威胁。  <br/><br/>**总结（100字以内）：**  <br/>本文首次系统研究音频后门攻击在语音模型中的传播，验证其在多任务和数据集上的高成功率，并通过组件分析定位漏洞，提出微调防御策略以应对预训练编码器被攻击的风险。|
|2509.25495v1|[EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for   Speech Emotion Recognition](http://arxiv.org/abs/2509.25495v1)|**贡献点：**<br/><br/>1. 提出一种轻量、无需训练的测试时自适应框架Emo-TTA；  <br/>2. 通过期望最大化（EM）过程增量更新类别条件统计量；  <br/>3. 利用音频语言模型的预测作为先验进行显式的测试分布估计；  <br/>4. 在不修改模型权重的情况下，提升跨域语音情感识别性能；  <br/>5. 在六个跨域基准数据集上验证了其有效性，优于现有TTA方法。  <br/><br/>**总结（100字以内）：**  <br/>本文提出无需训练的Emo-TTA框架，通过统计量更新实现测试时自适应，提升语音情感识别在跨域场景下的性能，验证了统计自适应的有效性。|
|2509.25416v1|[Emotion-Aligned Generation in Diffusion Text to Speech Models via   Preference-Guided Optimization](http://arxiv.org/abs/2509.25416v1)|**贡献点总结（100字以内）**  <br/>提出EASPO框架，通过分步情感偏好优化提升情感语音合成的表达力与自然度，结合EASPM时间条件模型实现自动偏好对构建与生成过程的可控情感塑造。<br/><br/>**分点列出贡献**  <br/>1. **提出新框架**：EASPO是首个将分步情感偏好纳入扩散TTS优化的后训练框架，解决现有方法依赖粗标签的问题。  <br/>2. **引入EASPM模型**：开发时间条件模型，自动评分中间去噪阶段的语音状态并构建偏好对，实现细粒度情感对齐。  <br/>3. **优化生成过程**：通过匹配分步情感偏好，提升语音生成的可控性，实现情感塑造的精确控制。  <br/>4. **验证有效性**：实验表明该方法在情感表达和语音自然度上优于现有技术，证明其优越性。|
|2509.24629v1|[Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech   Synthesis](http://arxiv.org/abs/2509.24629v1)|**总结（100字以内）**:  <br/>WeSCon是首个无需情感或语速过渡数据的自训练框架，实现预训练TTS模型的词级情感与语速控制，通过过渡平滑、动态注意力偏置和多轮推理，提升零样本合成能力。<br/><br/>**贡献点**:<br/><br/>1. **首次实现词级情感与语速控制**：提出WeSCon框架，是首个在零样本预训练TTS模型中实现词级情感及语速控制的方案，无需依赖带有情感或语速变化的标注数据。<br/><br/>2. **解决数据稀缺问题**：通过自训练策略，克服现有标注数据不足的限制，有效提升模型在无标注词级控制数据下的性能。<br/><br/>3. **引入过渡平滑策略**：设计过渡平滑机制，处理多情感转换的复杂性，使语音表达更自然流畅。<br/><br/>4. **动态语速控制机制**：提出动态语速控制方法，实现对说话速度的词级调控，增强语音的可变性和表达力。<br/><br/>5. **动态情感注意力偏置机制**：在推理过程中引入动态情感注意力偏置，简化推理流程，提升控制效率与效果。<br/><br/>6. **端到端自训练方法**：通过自训练和多轮推理，实现端到端的词级表达控制，保持模型原有的零样本合成能力。|
|2509.23759v2|[VioPTT: Violin Technique-Aware Transcription from Synthetic Data   Augmentation](http://arxiv.org/abs/2509.23759v2)|**贡献点总结（100字以内）：**  <br/>本文提出VioPTT模型，首次联合 violin transcription 和 playing technique prediction，实现轻量级端到端的演奏技巧转录，并发布高质量合成数据集MOSA-VPT，提升模型在真实录音中的泛化能力与性能。<br/><br/>**分点列出贡献：**<br/><br/>1. 提出VioPTT模型，首次在统一框架中联合处理小提琴音符转录与演奏技巧预测。<br/>2. 模型可转录音高起止点及演奏技巧，突破传统模型仅关注音高和时间的限制。<br/>3. 发布MOSA-VPT数据集，为小提琴演奏技巧的自动转录提供高质量合成标注数据。<br/>4. 利用该数据集，模型在真实世界录音中展现出良好的泛化能力和领先的转录性能。|