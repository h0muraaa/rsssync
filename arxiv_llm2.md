|Source|Title|Summary|
|---|---|---|
|2511.19275v1|[Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization](https://arxiv.org/abs/2511.19275v1)||
|2511.18869v1|[Multidimensional Music Aesthetic Evaluation via Semantically Consistent C-Mixup Augmentation](https://arxiv.org/abs/2511.18869v1)||
|2511.18833v1|[PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation](https://arxiv.org/abs/2511.18833v1)||
|2511.18725v1|[First Deep Learning Approach to Hammering Acoustics for Stem Stability Assessment in Total Hip Arthroplasty](https://arxiv.org/abs/2511.18725v1)||
|2511.18487v1|[InstructAudio: Unified speech and music generation with natural language instruction](https://arxiv.org/abs/2511.18487v1)||
|2511.17813v1|[Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813v1)||
|2511.17429v1|[Semantic and Semiotic Interplays in Text-to-Audio AI: Exploring Cognitive Dynamics and Musical Interactions](https://arxiv.org/abs/2511.17429v1)||
|2511.17425v1|[AI in Music and Sound: Pedagogical Reflections, Post-Structuralist Approaches and Creative Outcomes in Seminar Practice](https://arxiv.org/abs/2511.17425v1)||
|2511.17404v1|[The Artist is Present: Traces of Artists Resigind and Spawning in Text-to-Audio AI](https://arxiv.org/abs/2511.17404v1)||
|2511.17335v1|[Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM](https://arxiv.org/abs/2511.17335v1)||
|2511.16757v1|[Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation](https://arxiv.org/abs/2511.16757v1)||
|2511.16639v1|[Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs](https://arxiv.org/abs/2511.16639v1)|贡献点总结如下：  <br/>1. **提出首个基于离散音频编码单元的语音表示学习框架**（Codec2Vec），突破传统连续输入模型的限制。  <br/>2. **实现显著的效率提升**：降低存储需求达16.5倍，缩短训练时间2.3倍，增强数据隐私性。  <br/>3. **探索多目标训练策略**：通过掩码预测等方法验证框架有效性，采用消融实验分析不同训练目标的影响。  <br/>4. **验证性能竞争力**：在SUPERB基准上与连续输入模型表现相当，证明其可扩展性和实用性。|
|2511.16126v1|[SUNAC: Source-aware Unified Neural Audio Codec](https://arxiv.org/abs/2511.16126v1)|**贡献点总结**  <br/>1. **提出源感知编解码器**：设计可直接从混合音频中编码单个源的模型，无需显式分离步骤。  <br/>2. **支持用户驱动的源选择**：通过源类型提示实现用户可控的编解码选择，支持多源同类型并行编码。  <br/>3. **性能与效率优势**：在音质重合成与源分离任务中表现与传统分离-编解码方案相当，但计算成本显著降低。|
|2511.16046v1|[Train Short, Infer Long: Speech-LLM Enables Zero-Shot Streamable Joint ASR and Diarization on Long Audio](https://arxiv.org/abs/2511.16046v1)|总结：  <br/>提出首个仅用短音频训练的端到端流式语音大语言模型JEDIS-LLM，支持长音频的零样本联合ASR与说话人分割，在长音频任务中超越离线模型，实现高效实时处理。<br/><br/>贡献点：  <br/>1. **引入Speaker Prompt Cache（SPC）**：通过实时更新机制，使模型在未额外训练的情况下实现长音频流式推理，支持无缝使用预注册的说话人资料。  <br/>2. **单词级说话人监督融合**：在语音编码器训练中整合字级说话人信息，显著提升说话人分割性能。  <br/>3. **端到端流式联合处理**：构建首个端到端流式联合ASR与说话人分割系统，在短音频和长音频场景均优于现有方法。  <br/>4. **零样本长音频应用**：突破传统模型对长音频需额外训练的限制，仅以短音频训练即可实现长音频的流式联合任务，达成SOTA水平。|
|2511.15766v1|[A Generalized Weighted Overlap-Add (WOLA) Filter Bank for Improved Subband System Identification](https://arxiv.org/abs/2511.15766v1)|**总结（100字以内）**  <br/>本研究提出广义WOLA滤波器组，消除传统方法中子带滤波器的约束，分析其MSE性能与系统参数的关系，并设计低复杂度PT-WOLA实现，显著提升子带系统辨识性能。<br/><br/>**贡献点：**  <br/>1. **广义WOLA设计**：首次将子带滤波器在下采样前重新定位，突破传统WOLA对子带滤波器的约束，提升灵活性。  <br/>2. **理论分析**：建立广义WOLA在全带系统辨识中的MSE性能模型，揭示子带滤波器阶数、系统脉冲响应长度、下采样因子与原型滤波器间的量化关系。  <br/>3. **高效实现方案**：提出PT-WOLA方法，在保持与传统WOLA相近计算复杂度的同时，实现广义WOLA的高效应用，验证其性能提升效果。|
|2511.15145v1|[Auden-Voice: General-Purpose Voice Encoder for Speech and Language Understanding](https://arxiv.org/abs/2511.15145v1)|**贡献点**：<br/>1. **提出通用语音编码器**：构建了兼顾身份识别与语用线索的语音编码器，解决大模型中语音编码器对两者平衡不足的问题。<br/>2. **多任务训练验证**：通过系统评估发现多任务训练能生成更均衡的语音表征，而CLAP仅提升检索效果，未增强语用理解。<br/>3. **性能验证**：开发的Auden-Voice编码器在与大语言模型（LLMs）集成后表现出色，验证了其在实际应用中的有效性。<br/>4. **开源贡献**：提供代码与训练方案，通过音频理解工具包Auden推动相关领域的研究和应用。|
|2511.15131v1|[CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries](https://arxiv.org/abs/2511.15131v1)|**贡献点总结**：  <br/>1. **首个真实世界数据集**：提出CASTELLA，填补AMR领域缺乏真实场景数据的空白。  <br/>2. **大规模数据**：提供24倍于先前数据集的音频样本（训练集1,009，验证集213，测试集640）。  <br/>3. **人工标注提升可靠性**：采用人工标注而非合成数据，增强数据质量与评估可信度。  <br/>4. **建立基线模型**：基于CASTELLA构建AMR基线模型，推动后续研究对比基准。  <br/>5. **性能验证**：实验表明，合成数据预训练后微调模型在CASTELLA上性能显著优于纯合成数据训练模型（Recall1@0.7提升10.4点）。  <br/><br/>**摘要总结**（100字以内）：  <br/>CASTELLA是首个真实世界人工标注的AMR数据集，规模比现有数据集大24倍，有效提升模型可靠性。实验验证微调模型表现优于纯合成数据模型，为AMR研究提供实用基准。|
|2511.15038v1|[Aligning Generative Music AI with Human Preferences: Methods and Challenges](https://arxiv.org/abs/2511.15038v1)|总结：  <br/>本文提出将偏好对齐技术系统化应用于音乐生成，解决计算优化与人类审美间的鸿沟，整合MusicRL、DiffRhythm+及Text2midi-InferAlign等方法应对音乐独特挑战，并强调跨学科研究的重要性。<br/><br/>贡献点：  <br/>1. **系统化应用偏好对齐**：首次倡导将偏好对齐技术作为音乐生成的核心框架，弥补现有系统对人类细致偏好的适配不足。  <br/>2. **整合前沿方法**：综合MusicRL的大规模偏好学习、DiffRhythm+的多偏好对齐框架及Text2midi-InferAlign的推理优化技术，探索其在音乐生成中的协同作用。  <br/>3. **识别关键挑战**：明确音乐生成领域亟需解决的三大问题：长篇作品的可扩展性、多维度偏好建模的可靠性及主观质量评估的复杂性。  <br/>4. **跨学科研究倡议**：呼吁结合机器学习与音乐理论，推动AI系统从技术层面向更符合人类创造性与体验需求的方向发展。|
|2511.14969v1|[Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion](https://arxiv.org/abs/2511.14969v1)|**贡献点：**  <br/>1. **系统性数据质量控制**：提出针对MELD和IEMOCAP数据集的多模态质量验证流程，涵盖说话人身份、音频-文本对齐及面部检测。  <br/>2. **跨模态迁移学习**：利用说话人与面部识别的迁移学习，将身份区分性嵌入用于捕捉情感表达的个体特征。  <br/>3. **多阶段特征适配**：通过RecoMadeEasy(R)提取音视频嵌入，微调MPNet-v2生成情感感知文本表征，并结合情感特定MLP优化特征。  <br/>4. **MAMBA三模态融合方法**：首次应用于MERC任务，实现64.8%（MELD）和74.3%（IEMOCAP）的准确率，验证了多模态整合的有效性。  <br/>5. **低频情感类别研究基础**：为提升低频情绪类别的识别性能提供了数据与方法框架。  <br/><br/>**总结：**  <br/>本研究通过质量控制与迁移学习框架，融合音频、视觉及情感感知文本特征，提出MAMBA三模态方法，在多模态对话情感识别任务中实现高准确率，并为提升低频情绪分类提供基础。|
|2511.14939v1|[Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report](https://arxiv.org/abs/2511.14939v1)|**贡献点：**<br/>1. **模型评估**：系统性验证了Audio-MAE和三种PANN架构（CNN6/CNN10/CNN14）在新冠检测任务中的性能，对比Coswara和COUGHVID数据集的内在与跨域泛化能力。<br/>2. **人口统计平衡**：首次引入严格年龄与性别分层策略，消除模型对人口统计特征与新冠状态之间的错误关联（demographic leakage），提升评估真实性。<br/>3. **泛化挑战揭示**：发现跨数据集时模型表现显著下降（AUC 0.43-0.68），尤其是Audio-MAE性能崩溃（F1 0.00-0.08），凸显音频模型泛化能力不足。<br/>4. **数据量不足问题**：通过分层后样本量（1,219-2,160）的局限性，指出深度学习模型对大规模训练数据的依赖，限制实际应用潜力。<br/>5. **临床应用启示**：强调严格的人口统计控制对建立临床可靠模型的必要性，并揭示音频基新冠检测系统开发中的核心挑战。<br/><br/>**总结**：本研究揭示预训练音频模型在新冠检测中存在跨数据集泛化不足和人口统计偏差问题，强调严格数据分层对模型评估的重要性，同时指出小规模数据限制模型性能，为发展临床可用系统提供关键参考。|
|2511.14793v1|[OBHS: An Optimized Block Huffman Scheme for Real-Time Audio Compression](https://arxiv.org/abs/2511.14793v1)|**贡献点总结：**  <br/>1. **提出OBHS算法**：针对实时音频流的应用场景，设计了一种新的无损音频压缩方案。  <br/>2. **优化压缩方法**：结合分块Huffman编码、规范码表示及智能回退机制，平衡高压缩率与低计算复杂度。  <br/>3. **实验验证性能**：在静音丰富音频中实现高达93.6%的压缩比，并在多种音频类型（粉红噪声、音调、真实录音）中保持竞争力。  <br/>4. **线性复杂度设计**：算法时间复杂度为O(n)，可高效适配资源受限的实时应用场景。  <br/><br/>**摘要总结（100字内）**：  <br/>OBHS是一种高效的无损音频压缩算法，通过分块Huffman编码与规范码结合智能回退机制，在静音音频中实现高达93.6%压缩比，并保持线性复杂度，适用于实时流媒体场景。|
|2511.14600v1|[A Controllable Perceptual Feature Generative Model for Melody Harmonization via Conditional Variational Autoencoder](https://arxiv.org/abs/2511.14600v1)|**贡献点总结：**  <br/>1. **提出CPFG-Net网络与感知特征-和弦映射算法**：首次结合感知特征与和弦表示，实现旋律与和声的可控生成。  <br/>2. **可控生成系统**：支持从旋律预测序列化的感知特征与调性结构，生成逻辑连贯的和弦进行。  <br/>3. **新型数据集BCPT-220K**：基于古典音乐构建的感知特征数据集，提升模型训练效果。  <br/>4. **表现力与创造力验证**：实验表明模型在感知特征预测与和弦推理中兼具创新性与情感表达能力。  <br/>5. **跨模态扩展性**：符号化模型可便捷迁移至音频生成领域，推动音乐生成任务的泛化能力。  <br/><br/>**总结（100字内）**：  <br/>本文提出CPFG-Net模型与感知特征-和弦映射算法，通过BCPT-220K数据集实现旋律与和声的可控生成，验证了模型在音乐表现力与创造力上的优势，并拓展至音频生成场景。|
|2511.14410v1|[TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation](https://arxiv.org/abs/2511.14410v1)|总结：本文提出轻量级TTA模型，通过大规模多语言数据训练实现跨语言语音表征，在多个任务中超越Whisper，开源模型与训练方案。<br/><br/>贡献点：<br/>1. 提出轻量化TTA模型：针对Whisper在输入格式、模型规模和语义表现上的局限，设计专门强化语音语义理解的轻量级模型架构。<br/>2. 多语言大规模训练：使用358k小时语音数据，在ASR、ST和语音-文本对齐任务上进行联合训练，提升模型跨语言表示能力。<br/>3. 跨语言表征鲁棒性：通过多任务学习获得更稳定的跨语言语音表征，支持多语言场景的语音理解与生成。<br/>4. 表现超越基准：在ASR/ST、语音检索、ASR-LLM等基准测试中均展现优于Whisper的性能。<br/>5. 量化分析跨语言关系：系统研究跨语言能力与ASR/ST性能之间的关联机制，提供理论支持。<br/>6. 开源工具支持：将模型权重和训练方案集成至音频理解工具包Auden，推动领域技术发展。|
|2511.14390v2|[Accelerating Automatic Differentiation of Direct Form Digital Filters](https://arxiv.org/abs/2511.14390v2)||
|2511.14390v1|[Accelerating Automatic Differentiation of Direct Form Digital Filters](https://arxiv.org/abs/2511.14390v1)|总结：提出闭式反向传播方法，支持初始条件梯度，实现统一表达式计算。C++/CUDA实现显著加速，尤其在GPU上表现最佳。适用于低阶滤波器，时间域方法在速度上优于频域方法。<br/><br/>贡献点：<br/>1. 提出直接形式滤波器的自动微分通用公式，包含初始条件梯度计算<br/>2. 构建统一表达式同时表示滤波器与梯度计算，支持并行加速<br/>3. C++/CUDA实现相比Python实现速度提升超1000倍，GPU表现最优<br/>4. 证明时间域精确滤波与解析梯度方法在低阶滤波场景下优于频域方法|
|2511.14307v1|[Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions](https://arxiv.org/abs/2511.14307v1)|总结：  <br/>本文提出一种结合声学事件推理与指令调优大语言模型（LLM）的音频问答方法，通过结构化提示和GRPO算法训练，在DCASE 2025 Track 5中实现62.6%的开发集准确率。<br/><br/>贡献点：  <br/>1. **基于SSL的BEATs音频特征提取**：采用BEATs模型从帧级音频中提取特征，提升声学事件感知能力。  <br/>2. **多层级预测校准机制**：通过段级预测校准生成事件级结果，增强预测的准确性与鲁棒性。  <br/>3. **结构化提示融合策略**：将问题、候选答案与预测结果结合为结构化提示，优化LLM对AQA任务的响应。  <br/>4. **GRPO算法训练LLM**：使用简单奖励函数的GRPO算法微调Qwen2.5-7B-Instruct，提升模型在AQA任务中的表现。  <br/>5. **端到端方法验证**：首次在AQA任务中验证声学事件推理与指令调优LLM的结合有效性，取得62.6%准确率。|
|2511.14293v1|[Segmentwise Pruning in Audio-Language Models](https://arxiv.org/abs/2511.14293v1)|**贡献点：**  <br/>1. **跨领域迁移**：首次将视觉-语言领域的token pruning策略引入音频-语言模型，探索其在音频任务中的适用性。  <br/>2. **时间维度优化**：提出轻量级策略，综合考虑音频的时间序列特性以提升剪枝有效性。  <br/>3. **性能与效率平衡**：实验表明，在仅保留25%原始token的情况下，模型在Clotho v2和MMAU上的性能仅小幅下降（CIDEr降2%，准确率降4%），显著降低计算成本。  <br/><br/>**总结（100字以内）**：  <br/>该论文将视觉-语言token pruning策略迁移至音频领域，提出基于时间维度的轻量优化方法，实验验证在减少40% token数量时仅小幅牺牲性能，显著降低计算成本，为音频-语言模型的高效处理提供新思路。|
|2511.14138v2|[FxSearcher: gradient-free text-driven audio transformation](https://arxiv.org/abs/2511.14138v2)|**贡献点：**<br/>1. 提出FxSearcher框架，通过梯度无关方法突破传统可微音频效果的限制，实现基于文本提示的多样化音频变换。<br/>2. 整合贝叶斯优化与CLAP-based分数函数，提升音频效果配置搜索的效率与准确性。<br/>3. 引入引导提示机制，有效抑制音频伪影并增强生成结果与人类偏好的一致性。<br/>4. 构建AI驱动的客观评估框架，支持对生成音频质量的量化分析。<br/>5. 实验验证方法在客观指标与主观偏好上均优于现有技术，达成高性能音频变换。<br/><br/>**总结（100字以内）：**  <br/>本文提出FxSearcher框架，通过贝叶斯优化和CLAP模型实现文本引导的音频效果搜索，引入引导提示优化生成质量，构建AI评估体系验证效果，最终实现与人类偏好高度一致的多样化高质量音频变换。|
|2511.14138v1|[FxSearcher: gradient-free text-driven audio transformation](https://arxiv.org/abs/2511.14138v1)|**贡献点总结（100字以内）**  <br/>1. 提出无梯度的FxSearcher框架，突破传统方法对可微音频效果的依赖。  <br/>2. 集成贝叶斯优化与CLAP评分函数，提升音频效果搜索效率。  <br/>3. 引入引导提示机制，减少伪影并增强输出与人类偏好的一致性。  <br/>4. 构建AI评估框架，客观量化音频转换质量。  <br/>5. 实验验证方法在多个指标上与人类偏好高度匹配，提供公开演示。  <br/><br/>**简要总结**  <br/>FxSearcher通过无梯度优化与引导提示，实现高效音频效果搜索，结合AI评估体系验证其与人类偏好一致，推动高质量文本驱动音频转换。|
|2511.13863v2|[Segmenting Collision Sound Sources in Egocentric Videos](https://arxiv.org/abs/2511.13863v2)|**总结（100字以内）：**  <br/>提出碰撞声音源分割（CS3）任务，结合音频与第一视角视觉信息，利用CLIP和SAM2等基础模型实现弱监督分割，通过引入手部对象线索提升定位精度，在EPIC-CS3和Ego4D-CS3数据集上实现3-4.7倍mIoU提升。<br/><br/>**贡献点分点：**  <br/>1. **提出新任务CS3**：首次定义基于音频条件的碰撞声音源分割任务，旨在通过视频与音频协同分析识别碰撞物体。  <br/>2. **解决双对象依赖性挑战**：碰撞声由两个物体交互产生，需同时建模双方物理属性与声音特征，突破传统单一音频事件分析。  <br/>3. **利用第一视角视觉线索**：结合手部对象等egocentric特征筛选潜在碰撞源，提升复杂场景下的分割准确性。  <br/>4. **弱监督方法创新**：结合CLIP与SAM2等基础模型，提出无需精细标注的弱监督分割框架，降低数据标注成本。  <br/>5. **实验验证有效性**：在自建EPIC-CS3和Ego4D-CS3数据集上，方法在mIoU指标上超越基线3–4.7倍，验证其有效性。|
|2511.13863v1|[Segmenting Collision Sound Sources in Egocentric Videos](https://arxiv.org/abs/2511.13863v1)|总结：提出碰撞声音源分割新任务CS3，结合弱监督方法与视觉-听觉线索，基于CLIP/SAM2模型实现高精度分割，建立两个基准数据集并取得显著性能提升。<br/><br/>贡献点：  <br/>1. **提出新型任务**：定义Collision Sound Source Segmentation (CS3)任务，首次探索基于碰撞声音分割视觉场景中相关物体的多模态问题。  <br/>2. **弱监督方法创新**：设计无需精确标注的弱监督框架，利用foundation models（CLIP、SAM2）实现音频条件下的视觉分割。  <br/>3. **引入第一视角线索**：结合egocentric视频中“手部物体”等上下文信息，提升对潜在碰撞源的识别准确率。  <br/>4. **构建基准数据集**：提出EPIC-CS3和Ego4D-CS3两个新数据集，为任务研究提供标准评测平台。  <br/>5. **性能突破**：在对比实验中，方法在两个基准上分别提升3倍和4.7倍的mIoU，验证了技术有效性。|
|2511.13732v2|[Principled Coarse-Grained Acceptance for Speculative Decoding in Speech](https://arxiv.org/abs/2511.13732v2)||
|2511.13529v1|[Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529v1)|**贡献点总结：**  <br/>1. **提出匈牙利语新数据集**：构建BEA-Large（255小时自发语音，433名说话人）与BEA-Dialogue（85小时自然对话），填补低资源语言自发语料空白。  <br/>2. **丰富数据特性**：BEA-Large包含详细段级元数据，BEA-Dialogue支持说话人独立子集划分，助力对话ASR与说话人分割研究。  <br/>3. **建立可复现基线**：使用公开ASR模型（如Fast Conformer）验证性能，自发语音WER达14.18%，重复语音WER为4.8%。  <br/>4. **提供说话人分割基准**：Diarization误差率13.05%-18.26%，为后续研究提供参考。  <br/>5. **揭示挑战与方向**：指出对话ASR在不流畅性、重叠与非正式语境下的困难，推动多语言自发语料基准开发。  <br/><br/>**总结（100字以内）：**  <br/>本研究发布匈牙利语新数据集BEA-Large与BEA-Dialogue，填补低资源语言自发语料空白，建立基线模型并揭示对话ASR挑战，推动匈牙利语音技术发展，并为其他语言的自发语料基准提供方法论框架。|
|2511.13487v2|[Systematic Evaluation of Time-Frequency Features for Binaural Sound Source Localization](https://arxiv.org/abs/2511.13487v2)|**贡献点总结**（100字以内）:  <br/>本研究系统评估了双耳声源定位中时间频率特征设计的影响，揭示了特征组合对模型性能的关键作用，提出跨域泛化需融合多类型特征，验证了低复杂度CNN在优化特征输入下的竞争力，为声源定位特征工程提供了实用指导。<br/><br/>**分点贡献**:  <br/>1. **系统性评估方法**：首次系统研究时间频率特征设计对双耳声源定位（SSL）性能的影响，量化不同特征组合的贡献。  <br/>2. **特征组合对比**：对比了振幅（幅度谱、ILD）与相位（相位谱、IPD）特征的组合效果，发现特定两特征集（如ILD+IPD）在域内场景已足够。  <br/>3. **跨域泛化策略**：提出针对多样内容的跨域SSL需引入更丰富的特征（如结合通道谱与ILD/IPD），提升模型适应性。  <br/>4. **低复杂度模型验证**：证明在最优特征组合下，简化CNN模型可达到与复杂模型相当的性能，降低计算负担。  <br/>5. **实用指导意义**：为域内和跨域声源定位任务的特征设计提供理论依据和工程实践建议，强调特征选择优先于模型复杂度。|
|2511.13300v1|[PASE: Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement](https://arxiv.org/abs/2511.13300v1)|**贡献点：**<br/>1. **提出语音增强中的幻觉问题**：识别语言幻觉（无效音素结构）与声学幻觉（说话人身份/韵律不一致）两大挑战，强调语言幻觉为更根本性问题。  <br/>2. **创新框架PASE**：设计基于预训练WavLM的音素先验，通过表示蒸馏构建去噪专家模型，结合双流声码器策略（音素+声学表示），双重降低幻觉风险。  <br/>3. **方法突破**：提出利用语言模型的离散token分布建模语音结构，并首次将音素先验与声码器训练结合，解决噪声干扰下的先验污染问题。  <br/>4. **实验验证**：在感知质量与幻觉抑制方面显著超越现有生成模型及判别模型，证明框架的有效性与鲁棒性。  <br/><br/>**总结**（100字内）：  <br/>PASE通过音素先验与双流声码器设计，有效缓解语音增强中的语言与声学幻觉，显著提升感知质量并超越现有方法。|
|2511.13273v1|[Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs](https://arxiv.org/abs/2511.13273v1)|总结（100字以内）:  <br/>提出AMPBench基准测试，揭示LALMs在听觉运动感知上的系统性缺陷，发现模型在声源运动方向与轨迹识别中的不足，强调人类与模型在空间推理能力上的差距，为改进音频语言模型提供诊断工具与新方向。<br/><br/>贡献点：<br/>1. **提出首个听觉运动理解基准**（AMPBench）：首次设计专门评估音频模型对声源运动方向和轨迹推理能力的基准测试。  <br/>2. **揭示模型感知缺陷**：通过定量与定性分析发现，当前LALMs在运动线索识别和方向模式区分上存在显著不足（准确率低于50%）。  <br/>3. **量化人类与模型的差距**：明确指出模型在听觉空间推理中的根本性局限，强调与人类感知能力的鸿沟。  <br/>4. **提供改进方向**：为增强音频语言模型的空间认知能力提供诊断工具和理论启示。|
|2511.13219v2|[FoleyBench: A Benchmark For Video-to-Audio Models](https://arxiv.org/abs/2511.13219v2)||
|2511.13219v1|[FoleyBench: A Benchmark For Video-to-Audio Models](https://arxiv.org/abs/2511.13219v1)|**总结（100字以内）:**  <br/>提出FoleyBench，首个针对Foley音效生成的V2A基准数据集，包含5000个视频-音频-文本三元组，解决现有数据集评估与应用脱节的问题，支持细粒度模型分析和多维度性能评估。<br/><br/>**贡献点分点列出:**  <br/>1. **首个Foley专用基准**：构建了FoleyBench，首次针对Foley风格场景设计的大规模V2A评估数据集，填补领域空白。  <br/>2. **高质量数据集**：包含5000个视频-音频-文本三元组，确保音频与视觉事件因果关联，提升评估相关性。  <br/>3. **自动化构建**：通过可扩展的自动化管道从互联网视频（YouTube/Vimeo）中提取数据，实现高效规模化采集。  <br/>4. **分类体系优化**：基于Foley声学分类的标签设计，覆盖更精准的声效类别，增强对Foley需求的适配性。  <br/>5. **元数据支持分析**：提供源复杂度、UCS/AudioSet类别和视频长度等元数据，支持模型性能的细粒度研究与失败模式分析。  <br/>6. **多维度模型评估**：对SOTA V2A模型进行基准测试，涵盖音频质量、对齐度、时序同步及文本一致性等关键指标。|
|2511.13078v1|[A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078v1)|总结:  <br/>本文提出EMSGlass系统，结合EMSNet多模态多任务模型与EMSServe低延迟框架，首次实现高精度、高效率的EMS场景多模态处理，通过真实数据验证提升应急响应能力，并为下一代AI救护车系统提供方向。<br/><br/>贡献点:  <br/>1. **首个多模态多任务模型**：EMSNet整合文本、生命体征、场景图像等多源数据，首次实现EMS场景下的统一实时理解。  <br/>2. **低延迟推理框架**：EMSServe基于PyTorch设计，通过模态感知拆分与特征缓存机制，优化异步数据处理，实现1.9-11.7倍速度提升。  <br/>3. **高效多模态执行**：针对EMS设备异构性，提出自适应推理方案，解决多模态数据在复杂环境中的实时处理难题。  <br/>4. **真实场景验证**：基于实际EMS数据集训练，模型在五项关键任务中表现优于单模态基线，验证了多模态融合的有效性。  <br/>5. **用户研究反馈**：通过专业EMT实测，证明系统提升情境感知、决策速度与操作效率，并为AI-EMS系统发展提供方向。|
|2511.12552v1|[Eardrum sound pressure prediction from ear canal reflectance based on the inverse solution of Webster's horn equation](https://arxiv.org/abs/2511.12552v1)|**贡献点：**  <br/>1. 提出基于一维模型的个体耳道面积函数估算方法，通过有限差分近似解决 Webster's horn 方程逆解问题。  <br/>2. 优化空间分辨率终止标准，弥补典型测量中高频缺失的不足，提升逆解精度。  <br/>3. 将模拟输入阻抗扩展至3.5 MHz（对应0.1 mm空间分辨率），实现更精确的耳道面积函数计算。  <br/>4. 调整低通滤波截止频率，使其与带限输入阻抗最高频率匹配，增强频率适应性。  <br/>5. 建立稳健的面积函数终止判据，基于近似耳道长度实现准确截断。  <br/>6. 验证一维电声模型对三维耳道传递阻抗的复制能力，证实其有效性与实用性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出了基于一维模型优化个体耳道面积函数的新方法，通过调整空间分辨率和低通截止频率，提高了高频精度并验证了模型对三维传递阻抗的适用性。|
|2511.12404v1|[SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs](https://arxiv.org/abs/2511.12404v1)|**贡献点**<br/>1. **开源与透明性**：提供开源平台SynthGuard，突破现有闭源检测工具的限制，增强技术可复现性和研究可及性。  <br/>2. **多模态支持**：统一兼容图像和音频分析，解决传统工具模态单一的问题，应对AI生成媒体的多样化风险。  <br/>3. **融合传统与MLLM技术**：结合传统检测方法与多模态大语言模型（MLLMs），提升检测能力与跨模态推理的准确性。  <br/>4. **可解释性设计**：引入可解释推理机制，帮助用户理解检测决策逻辑，增强公众对AI生成内容的信任与认知。  <br/>5. **交互式教育功能**：开发面向公众和教育者的交互界面，降低技术门槛，推动AI生成内容的识别科普与教学应用。  <br/><br/>**总结（100字以内）**：  <br/>SynthGuard是首个开源、多模态的AI生成媒体检测平台，整合传统方法与MLLMs，提供可解释性及交互式界面，解决现有工具闭源、模态受限及透明度不足问题，助力公众与研究者共同应对虚假信息挑战。|
|2511.12347v1|[VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing](https://arxiv.org/abs/2511.12347v1)|总结：  <br/>VoiceCraft-X是首个统一多语言语音编辑与零样本TTS的自回归模型，通过跨语言文本处理与时间对齐token机制实现高效生成与编辑，展现了低数据下的强泛化能力与实际应用价值。<br/><br/>贡献点：  <br/>1. **多语言统一框架**：首次将11种语言（英语、汉语、韩语等）的语音编辑与零样本TTS合成整合为单一模型，支持跨语言任务协同处理。  <br/>2. **无音素文本处理**：基于Qwen3大语言模型，实现无需音素标注的跨语言文本语义理解与生成。  <br/>3. **时间对齐token机制**：创新性地设计文本与语音token的时间对齐重排策略，提升生成质量与任务连贯性。  <br/>4. **高效生成与编辑**：在同一框架下完成高质量语音生成与现有录音编辑，降低多任务开发复杂度。  <br/>5. **低数据泛化能力**：在有限语言数据条件下仍保持鲁棒性能，验证统一自回归模型在复杂语音任务中的潜力。|
|2511.12285v1|[How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer](https://arxiv.org/abs/2511.12285v1)|总结：  <br/>本研究首次系统分析SSL语音模型在多种音调语言中的迁移效果，揭示任务类型对音调建模时间跨度的影响，为多语言语音处理提供关键洞见。<br/><br/>贡献点：  <br/>1. **跨语言研究**：首次针对 Burmese、Thai、Lao 和 Vietnamese 等非汉语音调语言，系统探讨SSL模型对音调线索的捕捉与迁移机制。  <br/>2. **基线建立**：量化不同语言的音调线索时间跨度（Burmese/Thai约100ms，Lao/Vietnamese约180ms），为音调建模提供跨语言基准。  <br/>3. **任务影响分析**：通过探针与梯度分析，发现下游任务（如ASR vs. 语调/声学任务）显著影响模型对音调时间跨度的建模策略。  <br/>4. **迁移机制揭示**：阐明任务类型如何塑造模型的时序关注焦点，为优化低资源语言的SSL模型迁移提供理论依据。|
|2511.11825v1|[Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion](https://arxiv.org/abs/2511.11825v1)|**贡献总结（100字内）**：  <br/>提出基于Transformer和混合ViT的双输入声学-图像特征融合框架，解决非稳态噪声下的单通道降噪问题，兼顾计算效率与实现实时性，实验验证在感知质量与语音可懂度上接近干净参考信号。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出混合ViT框架**：结合声学特征与图像特征，通过双输入融合有效建模时域与频谱依赖性，解决非稳态噪声下的降噪难题。  <br/>2. **轻量化设计**：针对嵌入式设备优化，确保实时应用的计算效率和部署可行性。  <br/>3. **多指标验证**：采用PESQ、STOI、Seg SNR和LLR四大标准评估体系，全面验证方法在质量与可懂度上的提升。  <br/>4. **跨数据集实验**：基于Librispeech、UrbanSound8K和Google Audioset等真实噪声场景，验证方法在复杂环境下的鲁棒性。  <br/>5. **性能接近参考信号**：实验结果表明，降噪后语音质量与可懂度显著优于噪声输入，接近干净语音基准。|
|2511.11811v1|[Lessons Learned from Developing a Privacy-Preserving Multimodal Wearable for Local Voice-and-Vision Inference](https://arxiv.org/abs/2511.11811v1)|总结（100字以内）:  <br/>本文提出一种隐私保护的耳戴式多模态可穿戴系统，整合摄像头、麦克风和扬声器于30克设备中，通过手机实现本地AI推理，解决数据隐私和计算效率问题，为嵌入式AI系统设计提供关键经验。<br/><br/>贡献点：  <br/>1. **隐私保护系统架构**：首次将耳戴式多模态感知设备与可信手机边缘节点结合，通过本地处理保障用户数据隐私。  <br/>2. **紧凑硬件设计**：在30克限制下实现摄像头、麦克风和扬声器集成，突破微型化多模态感知设备的物理挑战。  <br/>3. **离线推理方案**：开发支持唤醒词触发的全本地多模态推理系统，无需云端传输，运行量化后的视觉-语言和大语言模型。  <br/>4. **关键设计经验**：通过迭代原型验证，总结功耗管理、低延迟通信、设备连接性和社会接受度等嵌入式AI系统的优化策略。|
|2511.11473v1|[Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473v1)|总结：提出无需用户提示的主动助听系统，通过双耳音频与自言自语锚点结合双模型架构，实现实时对话分离与多场景泛化，推动助听设备向动态适应方向发展。<br/><br/>贡献点：<br/>1. **首次实现无需显式指令的主动对话分离**：系统可自动识别并分离佩戴者与对话伙伴的声音，无需用户手动触发。<br/>2. **创新性使用自言自语作为锚点**：通过分析佩戴者的自我语音，结合回合制对话行为，推断对话参与者并抑制背景声音。<br/>3. **双模型架构设计**：轻量流模型（12.5ms更新）实现实时低延迟处理，慢速模型捕捉长程对话动态，兼顾效率与精度。<br/>4. **真实场景验证**：基于11名参与者6.8小时的双耳设备数据集，验证系统在多对话环境中的泛化能力。<br/>5. **推进主动式助听设备应用**：为助听器提供动态适应对话场景的能力，提升实际交互体验。|
|2511.11106v1|[AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization](https://arxiv.org/abs/2511.11106v1)|总结：  <br/>本文提出AccKV框架，通过层适应聚焦和跨校准技术优化AV-LLMs的KV缓存，有效解决模态混淆与对齐问题，显著提升推理效率同时保持模型准确性。<br/><br/>贡献点：  <br/>1. **揭示注意力机制的层间特性**：发现AV-LLMs在高层更倾向于关注视频模态，而非严格依赖任务类型，为优化策略提供新视角。  <br/>2. **提出跨模态信息冲突的理论依据**：指出直接整合音频（时序KV）与视频（时空KV）会导致信息混淆和性能下降，强调模态处理的差异性需求。  <br/>3. **设计AccKV框架**：  <br/>   - 采用**层适应聚焦**技术，根据各层特征选择性保留关键模态的KV缓存；  <br/>   - 引入**跨校准机制**，整合低效KV后对齐模态优先级，选择性清除低优先级缓存；  <br/>   - 实验证明在提升计算效率的同时保持模型精度。|
|2511.11039v1|[TimeAudio: Bridging Temporal Gaps in Large Audio-Language Models](https://arxiv.org/abs/2511.11039v1)|**贡献点：**<br/>1. 提出TimeAudio方法，解决LALMs在时间戳表示、架构设计和数据限制方面的不足，提升长音频的细粒度理解能力。<br/>2. 引入独特的时间标记机制与绝对时间感知编码，实现音频内容与精确时间信息的绑定，增强时间敏感推理。<br/>3. 设计分段级token合并模块，显著降低音频token冗余，提高信息提取效率与模型处理长音频的能力。<br/>4. 构建首个专门针对时间任务的语音数据集，并开发配套的细粒度评估指标体系，推动该领域研究。<br/><br/>**总结（100字内）：**  <br/>TimeAudio通过改进时间表示、架构和数据，提升LALMs对长音频的细粒度理解，解决时间定位与冗余问题，并构建专用数据集与评价体系，显著增强模型在密集字幕、时间接地等任务中的性能。|
|2511.11000v2|[DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition](https://arxiv.org/abs/2511.11000v2)|总结：  <br/>提出DialogGraph-LLM框架，结合MR-DAN与多模态模型实现端到端音频意图识别，并设计自适应半监督策略提升效率，在多数据集上验证其优越性，适用于监督不足的音频场景。<br/><br/>贡献点：  <br/>1. **提出DialogGraph-LLM框架**：首创将多关系对话注意网络（MR-DAN）与多模态基础模型（如Qwen2.5-Omni-7B）结合，实现直接从音频到意图的端到端推理。  <br/>2. **自适应半监督学习策略**：设计基于LLM的置信度感知伪标签生成机制，通过全局和类别置信度双阈值过滤，结合熵值样本选择优化训练效率。  <br/>3. **实证效果与应用价值**：在MarketCalls和MIntRec 2.0数据集上验证框架的优越性，证明其在实际音频对话场景中的高效性与实用性，尤其适用于标注数据稀缺的领域。|
|2511.11000v1|[DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition](https://arxiv.org/abs/2511.11000v1)|总结：  <br/>提出DialogGraph-LLM框架，结合MR-DAN与多模态模型，通过自适应半监督策略提升长对话意图识别，在MarketCalls与MIntRec 2.0数据集上超越基线，验证了其在少监督场景下的实用价值。<br/><br/>贡献点：  <br/>1. **提出DialogGraph-LLM框架**：融合多关系对话注意力网络（MR-DAN）与多模态基础模型，实现端到端的音频到意图推理。  <br/>2. **创新半监督学习策略**：基于LLM的置信度感知伪标签生成，结合全局/类别置信度双阈值过滤与熵选择机制，提升小样本场景下的模型性能。  <br/>3. **实验验证有效性**：在自有数据集MarketCalls和公开基准MIntRec 2.0上显著优于音频/文本驱动基线，证明框架在实际场景中的高效性与实用性。  <br/>4. **开源代码与应用价值**：代码开源，为音频密集型领域（如客服、会议记录）的少监督意图识别提供可复用解决方案。|
|2511.10913v1|[Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio](https://arxiv.org/abs/2511.10913v1)|总结（100字以内）:  <br/>本文提出HARMGEN攻击框架，针对TTS系统的数据-语音模态差异设计内容滥用攻击，揭示现有安全机制的缺陷，并呼吁构建跨模态防御体系。<br/><br/>贡献点:  <br/>1. **提出新型内容滥用攻击**：首次系统性探索TTS系统的内容滥用威胁，设计基于语义混淆（Concat, Shuffle）和音频模态漏洞（Read, Spell, Phoneme）的攻击方法。  <br/>2. **揭示对抗性攻击有效性**：通过实验验证攻击显著降低拒绝率（57-93%检测率），并提升生成语音的毒性，证明传统安全措施对TTS系统的不足。  <br/>3. **分析多维度防御机制**：评估音频平台的反应性防御（如内容审核）和TTS提供商的主动性防御，发现深度伪造检测失效及对抗扰动可绕过审查。  <br/>4. **强调跨模态安全需求**：指出需在训练与部署全链路构建针对文本-语音模态差异的联合防护体系，填补内容中心滥用的研究空白。|
|2511.10793v1|[Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces](https://arxiv.org/abs/2511.10793v1)|总结：  <br/>本文提出RHYME框架，通过非欧几何空间对齐跨范式合成语音的结构失真，实现合成无关的深度伪造检测，超越现有方法并取得新SOTA。<br/><br/>贡献点：  <br/>1. **跨范式通用性**：首次针对传统TTS与现代扩散/流匹配生成器等多范式合成语音设计通用深度伪造检测框架，突破单一生成模型的局限。  <br/>2. **结构失真建模**：提出合成语音在嵌入空间存在共享的几何结构失真假设，通过几何感知建模提升跨模型泛化能力。  <br/>3. **非欧几何融合**：创新性采用双曲空间（建模层次结构）与球面空间（捕捉周期性伪影）联合建模，结合黎曼巴里中心平均实现合成无关对齐。  <br/>4. **统一检测框架**：RHYME融合多预训练语音编码器的表征，通过几何投影构建跨范式统一检测模型，显著优于单模型与同质融合基线。  <br/>5. **性能突破**：在跨范式音频深度伪造检测任务中取得最优性能，刷新领域内最新技术水平。|
|2511.10790v1|[Towards Attribution of Generators and Emotional Manipulation in Cross-Lingual Synthetic Speech using Geometric Learning](https://arxiv.org/abs/2511.10790v1)|**贡献点：**  <br/>1. 提出MiCuNet框架，首次结合语义-声调线索与频谱动态，用于合成语音的情感和操控特征细粒度追溯。  <br/>2. 引入混合曲率投影机制（Hyperbolic-Euclidean-Spherical空间融合），增强多模态特征整合能力。  <br/>3. 设计可学习的时间门控机制，动态调节不同时间尺度下的特征权重以提升模型表现。  <br/>4. 首次在多语言（英文和中文）EmoFake数据集上实现多任务学习，同时预测原始情感、操控情感和操控源。  <br/>5. 方法在多任务场景下显著优于传统融合策略，验证了曲率自适应框架在合成语音分析中的有效性。  <br/><br/>**总结：**  <br/>本研究提出MiCuNet框架，通过混合曲率投影与时间门控机制，实现合成语音情感与操控特征的多任务细粒度溯源，超越传统方法并开创曲率自适应应用先河。|
|2511.10692v1|[StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak](https://arxiv.org/abs/2511.10692v1)|**贡献点**：  <br/>1. 提出StyleBreak框架：首个系统性研究人类语音表达特征对LAMs对齐鲁棒性的框架，针对性设计对抗攻击策略。  <br/>2. 两阶段风格感知转换：独立扰动文本与音频，控制语言、副语言及超语言属性，提升攻击多样性与有效性。  <br/>3. 查询自适应策略网络：自动搜索对抗性语音风格，优化攻击效率，降低人工干预依赖。  <br/>4. 实验验证：首次量化LAMs在语音属性攻击下的脆弱性，证明StyleBreak在多范式攻击中显著优于现有方法。  <br/><br/>**总结（100字内）**：  <br/>StyleBreak通过两阶段风格感知转换与自适应策略网络，系统性揭示人类语音表达对LAMs对齐的影响，填补了音频对抗攻击研究空白并显著提升攻击效能。|
|2511.10639v1|[Direction-of-Arrival and Noise Covariance Matrix joint estimation for beamforming](https://arxiv.org/abs/2511.10639v1)|总结：  <br/>提出联合DoA与噪声协方差矩阵估计方法，通过准线性解简化流程并提升波束成形性能，跨频率bin的DoA估计增强混响环境鲁棒性，均优于经典MUSIC算法及信号增强技术。<br/><br/>贡献点：  <br/>1. 联合估计DoA与噪声协方差矩阵（NCM），专为波束成形应用设计；  <br/>2. 基于现有NCM框架，提出准线性解替代传统穷举搜索，简化估计过程；  <br/>3. 引入跨所有频率bin的DoA估计技术，提升混响环境下的鲁棒性；  <br/>4. 在中高角度场景下，相较MUSIC等经典方法实现更低的角误差与更强的信号增强；  <br/>5. 通过理论与实证指标验证，框架在噪声抑制与干扰消除方面优于其他信号增强技术。|
|2511.10289v1|[Music Flamingo: Scaling Music Understanding in Audio Language Models](https://arxiv.org/abs/2511.10289v1)|总结：<br/>提出Music Flamingo，构建MF-Skills数据集，融合音乐理论与强化学习，实现多维度音乐理解，推动领域发展。<br/><br/>贡献点：<br/>1. 提出Music Flamingo：首个针对音乐理解的通用音频-语言模型，突破传统模型仅处理表面信息的局限<br/>2. 构建MF-Skills数据集：通过多阶段标注流程，首次系统涵盖和声、结构、音色、歌词、文化等五维音乐属性<br/>3. 创新训练方法：结合MF-Think冷启动框架与GRPO强化学习，提升模型音乐推理能力<br/>4. 多模态增强：基于Audio Flamingo 3进行架构改进，强化音乐特征提取与跨模态对齐能力<br/>5. 行业突破：在10+音乐理解基准测试中取得SOTA，建立人类级音乐感知新范式|
|2511.10262v1|[MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models](https://arxiv.org/abs/2511.10262v1)|**贡献点总结（分点）：**  <br/>1. 提出MTR-DuplexBench，首个面向全双工语音语言模型（FD-SLM）的多轮对话基准测试，填补现有单轮评估的不足。  <br/>2. 通过分段处理连续对话，实现逐回合（turn-by-turn）的细粒度评估，解决多轮对话中回合边界模糊与上下文不一致问题。  <br/>3. 覆盖多维度评估指标，包括对话质量、动态性、指令遵循与安全性，推动FD-SLM在复杂交互场景下的能力验证。  <br/>4. 实验揭示现有FD-SLM在多轮次任务中存在性能波动，验证新基准的必要性与有效性，为后续研究提供方向。  <br/><br/>**总结（100字以内）：**  <br/>本文提出MTR-DuplexBench，首个针对全双工语音语言模型的多轮对话基准，通过分段评估解决回合边界与上下文一致性的挑战，揭示模型在多轮交互中的不足，推动更全面的对话能力研究。|
|2511.10222v2|[Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard](https://arxiv.org/abs/2511.10222v2)|**贡献点**  <br/>1. **提出SACRED-Bench**：首个针对跨模态语音-音频攻击的鲁棒性评估基准，覆盖复杂音频输入场景。  <br/>2. **设计新型攻击机制**：包括语音重叠、音频混合及多样化指令格式，模拟真实攻击场景中的隐蔽威胁。  <br/>3. **提出多模态防御模型**：SALMONN-Guard联合检测语音、音频与文本，将攻击成功率从66%降至20%。  <br/>4. **揭示安全挑战**：实验验证现有LLM在跨模态攻击下的脆弱性，推动音频感知安全防御研究。  <br/><br/>**总结**（100字以内）：  <br/>提出SACRED-Bench基准及SALMONN-Guard防御，揭示语音-音频跨模态攻击的脆弱性，显著降低攻击成功率，推动多模态LLM的安全性研究。|
|2511.10222v1|[Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard](https://arxiv.org/abs/2511.10222v1)|【贡献点】<br/>1. 提出SACRED-Bench基准测试：首个针对LLMs跨模态音频攻击的评估框架，通过语音-音频合成机制发现传统防护手段的盲区。<br/>2. 创新攻击机制设计：包含三种新型攻击方式——语音重叠/多说话人对话、语音-音频混合、多样化口语指令格式，可绕过文本过滤器。<br/>3. 实验验证模型脆弱性：测试显示Gemini 2.5 Pro等顶尖模型在音频攻击下仍存在66%的成功率，揭示跨模态攻击的严重安全隐患。<br/>4. 提出多模态防护模型SALMONN-Guard：通过联合分析语音、音频和文本降低攻击成功率至20%，为音频感知防御提供新思路。<br/>5. 强调音频安全研究必要性：呼吁学界关注语音模型的音频输入风险，推动更全面的多模态安全防护体系发展。<br/><br/>【总结】该研究提出首个针对LLMs跨模态音频攻击的基准测试SACRED-Bench，并开发SALMONN-Guard防护模型，揭示当前模型在音频攻击场景下的脆弱性，推动音频安全防护研究。|
|2511.10168v1|[A Study of Binaural Deep Beamforming With Interpretable Beampatterns Guided by Time-Varying RTF](https://arxiv.org/abs/2511.10168v1)|**总结（100字以内）:**  <br/>提出基于SI-SDR损失的动态波束成形框架，结合实时RTF跟踪实现空间一致性增强，并验证其在不同指导方式下的有效性，推动助听器和可穿戴设备的应用。<br/><br/>**贡献点分点列出:**  <br/>1. **动态波束成形框架**：首次提出通过深度学习模型在动态声学环境中估计时变波束成形权重，提升语音增强效果。  <br/>2. **SI-SDR损失优化**：采用SI-SDR（Scale-Invariant Signal-to-Distortion Ratio）损失函数，直接优化语音增强目标，提高信号质量。  <br/>3. **RTF连续跟踪机制**：引入持续跟踪的相对转移函数（RTFs）作为指导，确保模型对移动目标说话人的空间行为建模。  <br/>4. **多场景对比验证**：通过三种设置（真实RTF、估计RTF、无指导）对比实验，验证RTF指导对空间一致性与方向跟踪的重要性。  <br/>5. **双耳信号输出**：模型生成双耳信号以保留空间线索，增强实际应用（如助听器、可穿戴设备）的沉浸感与定位准确性。|
|2511.09995v1|[Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS](https://arxiv.org/abs/2511.09995v1)|总结：  <br/>该研究针对FM-based zero-shot TTS在speaker表示不足的问题，提出TLA-SA方法，通过时间与层级自适应对齐提升一致性，并在多数据集和模型架构上验证其有效性。<br/><br/>贡献点：  <br/>1. **揭示speaker信息分布特性**：通过实证分析发现speaker信息在时间步和网络层的非均匀分配，指出现有方法对speaker表示的不足。  <br/>2. **提出TLA-SA损失函数**：联合利用时间维度和网络层级的speaker信息变化，增强跨时间步和层的speaker一致性。  <br/>3. **跨场景有效性验证**：在研究级和工业级数据集上验证方法有效性，并适用于解码器-only语言模型和无LM的FM-TTS系统。|
|2511.09958v1|[Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2511.09958v1)|总结：  <br/>本文提出Audio-VLA模型，融合接触音频提升机器人动态过程感知，并引入TCR指标系统评估操作过程，改进仿真环境以增强实验真实性，实验证明其在多任务中优于视觉模型。<br/><br/>贡献点：  <br/>1. 提出 Audio-VLA 模型，利用接触音频解决视觉 VLA 模型在感知交互动态过程中的局限性。  <br/>2. 引入 Task Completion Rate (TCR) 指标，构建更全面的动态操作过程评估体系。  <br/>3. 设计多模态投影层，对齐视觉、音频和语言特征到统一空间，提升跨模态理解能力。  <br/>4. 改进 RLBench 和 LIBERO 仿真环境，通过碰撞音频生成提供更真实的交互反馈。|
|2511.09232v1|[POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation](https://arxiv.org/abs/2511.09232v1)|总结（100字以内）:  <br/>提出POTSA框架，结合OT与并行语音对，通过Bias Compensation、token-level OT约束及层调度策略，显著提升多语言S2TT性能，尤其在零样本场景下表现优异，并减少数据需求。  <br/><br/>贡献点：  <br/>1. **提出POTSA框架**：基于跨语言并行语音对和最优传输（OT），解决高/低资源语言间翻译性能偏倚问题。  <br/>2. **引入Bias Compensation模块**：粗粒度对齐跨语言语音表征，缓解语义共性缺失导致的偏倚。  <br/>3. **Token-level OT约束**：通过Q-Former建立细粒度表征一致性，增强跨语言对齐精度。  <br/>4. **层调度策略**：动态聚焦语义关键层，提升OT约束的语义有效性。  <br/>5. **高效实验验证**：在FLEURS数据集上实现SOTA，仅需10小时平行语料即超越现有方法，尤其在零样本语言上提升5.05 BLEU。|
|2511.09090v1|[Diff-V2M: A Hierarchical Conditional Diffusion Model with Explicit Rhythmic Modeling for Video-to-Music Generation](https://arxiv.org/abs/2511.09090v1)|总结（100字以内）：  <br/>本文提出Diff-V2M框架，通过分层交叉注意力机制与时间步感知融合策略，解决视频到音乐生成中的节奏建模与多特征整合问题，验证低分辨率ODF在节奏建模的有效性，并在多数据集上取得SOTA性能。<br/><br/>贡献点：  <br/>1. **提出Diff-V2M框架**：构建基于分层条件扩散模型的通用V2M生成框架，解决时序对齐与特征整合难题。  <br/>2. **创新节奏建模方法**：评估并开发节奏预测器，直接从视频中提取低分辨率ODF作为更优节奏信号。  <br/>3. **分层交叉注意力机制**：分离情感特征（第一层）与语义/节奏特征（第二层）的整合过程，提升情感与内容一致性。  <br/>4. **时间步感知融合策略**：引入FiLM和加权融合，实现语义与节奏信号的自适应平衡，增强特征交互。  <br/>5. **实验验证效果**：在in-domain和out-of-domain数据集上超越现有方法，显著提升客观指标与主观评价表现。|
|2511.09084v1|[Towards Effective and Efficient Non-autoregressive decoders for Conformer and LLM-based ASR using Block-based Attention Mask](https://arxiv.org/abs/2511.09084v1)|总结（100字以内）:  <br/>本文提出一种新型块式非自回归注意力掩码解码器（AMD），兼容Conformer和LLM基ASR系统，在保持高识别准确率的同时显著提升解码速度，并通过实时因子优化平衡性能与效率。<br/><br/>贡献点：  <br/>1. **提出块式非自回归解码器**：设计AMD结构，支持输出标签块内并行推理与块间单调左向右预测，突破传统自回归模型的串行限制。  <br/>2. **融合多概率模型**：开发单次beam search算法，动态整合CTC、自回归解码器与AMD的概率，提升解码鲁棒性。  <br/>3. **跨模型验证有效性**：在Conformer、WavLM集成及LLM基ASR系统中验证AMD，实现3种配置下的解码加速（1.44x-2.31x）及WER降低（0.13%-0.62%）。  <br/>4. **性能-效率可调平衡**：提供灵活的性能与计算效率权衡机制，适配不同ASR架构的需求。|
|2511.08723v1|[ParaS2S: Benchmarking and Aligning Spoken Language Models for Paralinguistic-aware Speech-to-Speech Interaction](https://arxiv.org/abs/2511.08723v1)|**总结（100字以内）**  <br/>提出ParaS2S框架，结合RL和基准测试ParaS2SBench，通过GRPO方法提升S2S模型对语用线索的响应能力，在内容和风格适配性上超越监督微调模型，减少标注依赖。  <br/><br/>**贡献点分点总结**  <br/>1. **提出ParaS2S框架**：首个直接在波形层面优化内容与说话风格的RL框架，处理语用线索（情感/语调/说话人属性）。  <br/>2. **构建ParaS2SBench基准**：全面评估S2S模型的输出适配性，与人类判断一致，提供自动评估工具。  <br/>3. **引入GRPO方法**：通过分组相对策略优化，利用未标注语音数据进行模型探索与学习，减少标注依赖。  <br/>4. **实验验证有效性**：在ParaS2SBench中，RL方法在内容与风格适配性上比监督微调提升11%，超越现有模型且标注需求更低。|
|2511.08496v3|[HQ-SVC: Towards High-Quality Zero-Shot Singing Voice Conversion in Low-Resource Scenarios](https://arxiv.org/abs/2511.08496v3)|总结：<br/>本文提出HQ-SVC框架，通过联合建模内容与说话人特征、引入可微信号处理与扩散技术，显著提升零样本声乐转换质量与效率，同时拓展至语音超分辨率任务，表现优越于现有方法。<br/><br/>贡献点：<br/>1. **联合特征提取**：首次采用解耦编码器同时建模内容和说话人特征，避免传统方法分离建模导致的声学信息丢失。<br/>2. **音高音量建模**：通过细化音高和音量建模增强输出保真度，提升转换后的语音自然度与旋律一致性。<br/>3. **可微信号处理**：结合扩散技术实现渐进式输出优化，增强模型对语音细节的控制能力。<br/>4. **高效框架设计**：降低计算资源需求，显著提升零样本声乐转换的效率。<br/>5. **多任务通用性**：无需额外训练即支持语音超分辨率任务，证明框架在语音增强领域的泛化能力。|
|2511.08496v2|[HQ-SVC: Towards High-Quality Zero-Shot Singing Voice Conversion in Low-Resource Scenarios](https://arxiv.org/abs/2511.08496v2)|总结：  <br/>HQ-SVC通过联合编码器、音高音量建模与可微信号处理技术，解决了零样本唱歌声音转换中信息丢失和资源消耗问题，实现了高质量转换与高效性能，并拓展至语音超分辨率任务。<br/><br/>贡献点：  <br/>1. **联合特征提取**：设计解耦编码器，同步提取内容与说话者特征，避免传统方法因分离建模导致的关键信息损失。  <br/>2. **保真度增强**：引入音高与音量建模模块，保留声学细节，提升转换后的语音自然度与质量。  <br/>3. **渐进式优化**：结合可微信号处理与扩散技术，分阶段优化输出，提高生成效果的稳定性与精细度。  <br/>4. **高效性与通用性**：框架无需微调，显著提升计算效率；同时支持语音超分辨率任务，超越专用音频超分辨率方法。|
|2511.08389v1|[Unifying Model and Layer Fusion for Speech Foundation Models](https://arxiv.org/abs/2511.08389v1)|贡献点总结（100字以内）：  <br/>提出跨模型层融合接口模块，统一多层与多模型融合策略；通过实验验证在ASR和声调分析任务中的有效性；分析方法的可扩展性，强调上游模型选择的重要性；为Speech Foundation Models的应用提供新思路。  <br/><br/>分点贡献：  <br/>1. **统一融合策略**：设计接口模块实现多模型与多层表示的跨模态融合，整合现有两种主流方法（多层融合、多模型融合）。  <br/>2. **提升下游任务性能**：在自监督与监督模型框架下，通过实验验证方法在ASR、声调分析等任务中的优越性。  <br/>3. **可扩展性分析**：探讨模型规模与数量对融合效果的影响，强调上游模型选择的关键性。  <br/>4. **方法有效性验证**：证明接口模块在合适模型组合下能显著提升性能，为Speech Foundation Models的实践提供参考。|
|2511.08252v3|[Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models](https://arxiv.org/abs/2511.08252v3)|贡献点：<br/>1. 揭示AudioLDM 2中自注意力图对保持源音乐时间结构（旋律/节奏）的关键作用，对比交叉注意力图在特征修改中的局限性；<br/>2. 提出Melodia训练免技术，通过选择性操控特定层自注意力图并构建注意力库，在无需文本描述前提下实现音乐特征精准修改；<br/>3. 引入两个新型评估指标，完善音乐编辑方法的客观与主观评价体系；<br/>4. 通过跨数据集实验验证方法有效性，证明其在文本一致性与结构完整性上的优势；<br/>5. 深化对扩散模型内部机制的理解，为音乐生成与编辑提供更精确的控制手段。<br/><br/>总结：该研究通过分析注意力机制揭示音乐生成模型的关键结构保持机制，提出无需训练的Melodia技术实现音乐特征修改与结构保护，同时引入新评估指标，显著提升了音乐编辑效果与模型可解释性。|
|2511.08252v2|[Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models](https://arxiv.org/abs/2511.08252v2)|总结（100字以内）:  <br/>该研究提出Melodia技术，通过分析AudioLDM2模型中的自注意力地图实现音乐编辑时保留源音乐时序结构，无需文本描述即可精准修改音乐特征，同时引入两个新评估指标，实验验证其在文本保真与结构完整性上优于现有方法。<br/><br/>贡献点：  <br/>1. **揭示注意力机制差异**：发现跨注意力地图包含音乐特征细节但易致无效修改，而自注意力地图对时序结构保留关键。  <br/>2. **提出训练无关编辑技术**：Melodia通过选择性操控特定层的自注意力图并利用注意力存储库，在无文本描述前提下精准修改音乐属性。  <br/>3. **设计新型评估指标**：提出两个客观与主观结合的指标，提升音乐编辑方法评估的准确性与全面性。  <br/>4. **实验证明有效性**：在多数据集上验证方法在文本遵循性和结构完整性上的优越性，推动音乐生成模型的可解释性与可控性研究。|
|2511.08252v1|[Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models](https://arxiv.org/abs/2511.08252v1)|总结（100字以内）:  <br/>该研究揭示了音乐生成模型中自注意力图对保持时间结构的重要性，提出无需训练的Melodia方法实现精准属性修改，设计两种新评估指标，验证了方法在文本一致性与结构完整性上的优越性，提升模型控制能力。<br/><br/>贡献点：  <br/>1. **揭示注意力机制作用**：分析AudioLDM 2中自注意力图与交叉注意力图的功能差异，发现自注意力图对源音乐时间结构（旋律/节奏）的保真关键作用。  <br/>2. **提出Training-free编辑方法**：推出Melodia技术，通过选择性操控特定层的自注意力图并利用注意力仓库存储源信息，实现音乐属性修改而不破坏原始结构。  <br/>3. **设计新评估指标**：提出两种新颖的量化指标，完善音乐编辑方法的客观评价体系，实验验证其在多数据集上的有效性。  <br/>4. **提升模型可解释性与可控性**：深化对扩散模型内部机制的理解，为音乐生成提供更精准的结构控制手段。|
|2511.08093v1|[Quantizing Whisper-small: How design choices affect ASR performance](https://arxiv.org/abs/2511.08093v1)|**贡献点：**  <br/>1. **跨库统一评估**：首次对Whisper-small的Post-training Quantization（PTQ）方法进行多库（PyTorch、Optimum-Quanto、HQQ、bitsandbytes）综合比较，系统分析量化方案、方法、粒度和位宽的影响。  <br/>2. **动态int8量化最优**：发现动态int8量化（Quanto库）在模型压缩（57%）与精度（WER降低）间取得最佳平衡，显著提升部署效率。  <br/>3. **静态量化局限性**：揭示静态量化在Transformer架构下表现较差，可能因缺乏动态校准导致精度损失。  <br/>4. **激进量化格式权衡**：提出nf4/int3等高压缩格式（71%）在噪声环境下会牺牲精度，但适合对精度要求较低的场景。  <br/>5. **无重训练部署方案**：验证无需模型重训练的PTQ方法可有效降低模型尺寸和推理成本，推动Whisper-small在边缘设备的高效应用。  <br/><br/>**总结（100字内）：**  <br/>本研究通过跨库评估，揭示动态int8量化（Quanto）在Whisper-small中实现最佳压缩-精度平衡，为边缘部署提供高效方案，同时指出激进量化格式的适用场景及静态量化的局限性。|
|2511.08092v1|[Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR](https://arxiv.org/abs/2511.08092v1)|总结：  <br/>本研究提出one-shot剪枝方法作为ASR模型的隐式正则化手段，通过敏感度分析揭示模型结构的不对称性，并在无需微调的情况下实现显著性能提升，同时支持更激进的压缩，为模型架构设计提供新视角。<br/><br/>贡献点：  <br/>1. **挑战传统观点**：提出one-shot剪枝不仅是压缩技术，更是ASR模型的隐式正则化方法，提升泛化性能。  <br/>2. **敏感度诊断结合**：首次结合梯度与Fisher信息诊断，实现组件级精准剪枝策略。  <br/>3. **暴露架构不对称性**：发现解码器FFN对剪枝敏感，而自注意力与末层编码器冗余可安全移除，提升模型效率。  <br/>4. **无需微调的性能提升**：在未微调情况下，50%剪枝量显著降低LibriSpeech的WER（绝对2.38%，相对20.44%）。  <br/>5. **低稀疏度性能保障**：在40%稀疏度下保持接近基线精度，突破传统全局剪枝的性能瓶颈。  <br/>6. **设计工具价值**：论证剪枝位置的定位与剪枝量同等重要，推动将剪枝纳入模型架构设计核心考量。|
|2511.08040v1|[Automatic Music Mixing using a Generative Model of Effect Embeddings](https://arxiv.org/abs/2511.08040v1)|总结：  <br/>MEGAMI提出了一种生成框架，通过嵌入生成和域适应方法解决音乐混音的多解性问题，实现接近人类水平的跨流派混音质量。<br/><br/>贡献点：  <br/>1. 提出MEGAMI框架，首次将音乐混音建模为条件分布生成任务，而非确定性回归，解决多解性问题。  <br/>2. 引入track-agnostic效果处理器，利用每轨生成的嵌入控制混音风格。  <br/>3. 设计permutation-equivariant架构，支持任意未标记轨道的混音处理。  <br/>4. 采用域适应技术，实现干声与湿声录音的联合训练。  <br/>5. 通过分布度量和听测验证，在客观指标和主观质量上均超越现有方法并接近人类水平。|
|2511.08012v1|[DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes](https://arxiv.org/abs/2511.08012v1)|**贡献点分点总结：**  <br/>1. **提出基于LLMs的空间音频数据集**：构建了首个结合大语言模型辅助的新型数据集，显著提升声学场景的多样性与真实感，突破传统合成数据的局限。  <br/>2. **设计轻量级DOA模型LightDOA**：引入深度可分离卷积结构，针对多通道输入优化模型，成功在保持低计算复杂度的同时实现高精度与鲁棒性。  <br/>3. **实验验证模型有效性**：在多样化声学场景中验证LightDOA的性能，证明其优于现有方法，并适用于资源受限的实际应用。  <br/>4. **推动DOA研究方向革新**：强调了LLMs生成空间音频数据在提升模型泛化能力与研究效率中的潜力，为后续研究提供新思路。  <br/><br/>**总结（100字内）：**  <br/>本文提出基于LLMs的新型空间音频数据集，设计轻量级模型LightDOA，并验证其在多种场景下的高效性与鲁棒性，为资源受限环境中的DOA估计提供了创新解决方案。|
|2511.07931v1|[SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931v1)|**主要贡献**  <br/>- 提出 **SpeechJudge** 套件：数据集、评估基准、生成奖励模型三位一体，聚焦语音自然度。  <br/>- 构建 **SpeechJudge‑Data**：99 K 人类偏好对，涵盖多语言、多风格、多个零样本文本到语音（TTS）模型，并标注可懂度与自然度。  <br/>- 设立 **SpeechJudge‑Eval** 基准，揭示现有客观指标和 AudioLLM 在自然度判断上的不足（最高 < 70% 与人工一致）。  <br/>- 开发 **SpeechJudge‑GRM**：基于 Qwen2.5‑Omni‑7B 的生成奖励模型，采用两阶段后训练——带链式思考的监督微调（SFT） + 基于 GRPO 的强化学习（RL）。  <br/>- 在基准上取得 **77.2%**（推理时 @10 缩放至 79.4%）的准确率，优于传统 Bradley‑Terry 奖励模型（72.7%）。  <br/>- 验证 SpeechJudge‑GRM 可作为奖励函数用于语音生成模型的后训练，实现模型对齐人类偏好。  <br/><br/>**摘要（100字以内）**  <br/>提出 SpeechJudge 套件，提供 99 K 人类偏好数据、自然度评估基准及基于 Qwen2.5 的生成奖励模型，在基准上实现 77%+ 准确率并可用于引导语音生成模型对齐人类感知。|
|2511.07268v1|[Generating Piano Music with Transformers: A Comparative Study of Scale, Data, and Metrics](https://arxiv.org/abs/2511.07268v1)|总结：  <br/>该论文系统比较了符号钢琴音乐生成中的关键设计因素，分析了量化指标与人类评估的相关性，并提出了一款高性能Transformer模型，在图灵测试中被评价为人类创作水平。<br/><br/>贡献点：  <br/>1. **系统性设计比较**：全面评估数据集多样性、模型架构、参数规模及训练策略对生成音乐质量的影响。  <br/>2. **量化指标分析**：探讨多种量化指标与人类听觉判断的一致性，建立客观评估框架。  <br/>3. **人类评估结合**：通过听力实验验证模型表现，确保评估结果与主观感知的关联性。  <br/>4. **高性能模型提出**：开发950M参数Transformer模型，基于80K跨流派MIDI文件训练，在生成质量上接近人类创作水平。|
|2511.07253v1|[Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models](https://arxiv.org/abs/2511.07253v1)|**贡献点总结**（100字以内）:  <br/>提出Omni-AVSR统一框架，整合多粒度训练与LoRA策略，实现ASR/VSR/AVSR任务的高效处理与弹性推理，减少资源消耗并保持鲁棒性，分析模型扩展性以平衡性能与效率。<br/><br/>---<br/><br/>**分点贡献点**:<br/>1. **提出统一框架**：首次构建支持ASR、VSR和AVSR的统一音频-视觉LLM模型，避免多模型独立训练带来的资源冗余与协同缺失。  <br/>2. **高效多粒度训练**：采用Matryoshka表示学习范式，通过多音频/视觉粒度联合训练降低资源消耗，提升跨模态适应性。  <br/>3. **参数高效适应策略**：探索三种LoRA-based方法，在模型共享与任务特异性之间取得平衡，显著减少微调参数量。  <br/>4. **资源效率优化**：单模型训练部署资源消耗远低于现有方法，在保持高精度的同时实现弹性推理能力。  <br/>5. **噪声鲁棒性验证**：模型在嘈杂环境下的表现稳定，证明其对真实场景的适应性。  <br/>6. **扩展性分析**：系统研究模型规模对性能/效率的权衡，为实际应用提供理论依据与优化方向。|
|2511.07185v3|[Neural Directional Filtering Using a Compact Microphone Array](https://arxiv.org/abs/2511.07185v3)|**贡献点总结（分点）：**<br/>1. 提出神经方向滤波（NDF）方法，通过深度学习实现预定义方向性模式的声学捕获，突破传统波束成形对麦克风数量和阵列孔径的依赖。<br/>2. 实现**频率不变**的方向性模式，即使在空间混叠频率以上仍保持性能。<br/>3. 支持**高阶方向性模式**的近似，且可**动态调整方向**。<br/>4. 引入数据驱动的评估指标，提升方向性模式和方向因子的量化分析能力。<br/>5. 在未见条件下具备良好的**泛化能力**，实验优于传统波束成形和参数化方法。<br/><br/>**总结（100字内）：**  <br/>论文提出神经方向滤波方法，通过深度学习实现灵活、高阶方向性模式捕获，突破传统波束成形限制，在高频混叠下仍保持性能，且具备方向调整与泛化能力，实验验证优于主流方法。|
|2511.07185v2|[Neural Directional Filtering Using a Compact Microphone Array](https://arxiv.org/abs/2511.07185v2)|**贡献点**:  <br/>1. **提出神经方向滤波（NDF）方法**：利用深度神经网络生成单通道复掩膜，通过参考麦克风实现虚拟方向性麦克风，突破传统波束成形器在紧凑阵列中的方向性限制。  <br/>2. **引入数据依赖的训练策略与评估指标**：设计数据驱动的训练方案，并提出用于量化方向性模式（如直接性因子）的指标，提升模型可解释性与优化效率。  <br/>3. **实现频率不变的直接性模式**：NDF在空间混叠频率以上仍保持频率不变性，解决了传统方法的频率依赖性问题。  <br/>4. **支持高阶与多样化方向性控制**：可拟合复杂高阶方向性模式，并通过动态调整实现多方向聚焦。  <br/>5. **具备强泛化能力**：方法在未见过的环境或条件下仍能保持性能，适应性强。  <br/>6. **实验验证优越性**：通过对比实验证明NDF在传统波束成形与参数化方法上的性能提升，证明其有效性。  <br/><br/>**总结**: 本研究提出基于深度神经网络的神经方向滤波方法，突破紧凑麦克风阵列方向性限制，实现频率不变、高阶模式拟合和多方向控制，并通过实验验证其优越性。|
|2511.07156v1|[Conditional Diffusion as Latent Constraints for Controllable Symbolic Music Generation](https://arxiv.org/abs/2511.07156v1)|**贡献点总结（100字以内）：**  <br/>本研究提出将去噪扩散过程作为插件式潜在约束，首次实现无条件符号音乐生成模型在多音乐属性（音符密度、音高范围、轮廓、节奏复杂度）上的灵活控制，实验表明该方法在保持生成质量与多样性的同时，优于传统正则化和现有潜在约束架构。  <br/><br/>**分点贡献：**  <br/>1. **引入扩散约束框架**：提出将去噪扩散过程作为插件式潜在约束，用于无条件符号音乐生成模型，实现对生成音乐的灵活控制。  <br/>2. **多属性通用性**：首次系统性地将该方法应用于多样化的音乐属性（如音符密度、音高范围、轮廓、节奏复杂度），突破了现有方法依赖单一模态的局限。  <br/>3. **隐式先验设计**：构建基于小型条件扩散模型的隐式概率先验库，作用于冻结的无条件主干网络的潜在空间，提升生成控制的精确性。  <br/>4. **性能优势**：实验表明扩散驱动约束在目标属性与生成属性的相关性、感知质量和多样性方面均优于传统正则化方法和现有潜在约束架构。|
|2511.07135v1|[Generating Novel and Realistic Speakers for Voice Conversion](https://arxiv.org/abs/2511.07135v1)|**贡献点总结：**<br/>1. 提出SpeakerVAE方法，通过深度分层变分自编码器生成全新说话人表示，突破传统VC系统需依赖目标语音数据的限制。<br/>2. 实现无需共训练或微调基础VC系统的灵活插件化设计，兼容FACodec和CosyVoice2等主流模型。<br/>3. 生成的新型说话人音色质量与训练数据相当，显著提升未见语音的转换效果。<br/>4. 构建轻量级框架，降低生成新型语音的计算成本，拓展VC在隐私保护和创意应用中的场景。|
|2511.07118v1|[On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation](https://arxiv.org/abs/2511.07118v1)|**贡献点总结：**  <br/>1. **提出控制平衡问题**：指出传统方法（如变分信息瓶颈）在平衡KLD（KL散度）与AR（属性正则化）损失时存在挑战，导致模型无法兼顾生成可控性和潜在分布的合理性。  <br/>2. **设计属性转换方法**：通过引入合适的属性变换，实现对KLD和AR损失的联合优化，解决现有模型难以同时满足两个正则化目标的局限性。  <br/>3. **验证效果**：在符号音乐生成任务中验证该方法，证明其能有效提升模型对连续音乐属性的可控性，同时保持潜在空间的结构化与正则化。  <br/><br/>**摘要总结（100字内）**：  <br/>该论文提出通过属性转换协调KLD与AR损失，解决显式潜在变量模型中正则化目标难以平衡的问题，从而提升符号音乐生成的可控性与潜在空间结构化质量。|
|2511.07099v1|[E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis](https://arxiv.org/abs/2511.07099v1)|**贡献点：**  <br/>1. 提出**E2E-VGuard框架**，首次系统性防御基于LLM的端到端语音合成与ASR驱动场景下的新型攻击。  <br/>2. 引入**编码器集成与特征提取器**，实现音色保护；设计**ASR针对性对抗样本**攻击发音。  <br/>3. 结合**心理声学模型**，确保扰动不可感知，平衡安全性与音频质量。  <br/>4. 评估覆盖**16开源合成器+3商业API**，支持中英文数据集，验证防御有效性。  <br/>5. 实现真实场景部署验证，并开放代码及演示页面。  <br/><br/>**总结（100字内）：**  <br/>该研究提出E2E-VGuard框架，针对LLM驱动的端到端语音合成及ASR场景攻击，通过音色保护、发音破坏与心理声学扰动设计，实现高效防御，并在多系统与数据集上验证其有效性，为语音安全提供新方案。|
|2511.06606v2|[SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding and Reasoning into Large Audio-Language Models](https://arxiv.org/abs/2511.06606v2)|总结：  <br/>SPUR通过FOA编码器和定制数据集，将空间感知能力注入LALMs，显著提升空间问答和多说话人识别性能，同时保持音频理解能力。<br/><br/>贡献点：  <br/>1. 提出SPUR方法：一种轻量级插件方案，通过最小架构改动增强LALMs的空间感知能力。  <br/>2. FOA编码器设计：将(W,X,Y,Z)声道映射为旋转感知、以听者为中心的空间特征，通过多模态适配器集成。  <br/>3. SPUR-Set数据集：结合开源FOA录音与可控模拟，聚焦相对方向、高度、距离及重叠，支持监督式空间推理训练。  <br/>4. 性能验证：微调后模型在空间问答和多说话人属性任务中表现提升，且保留通用音频理解能力。  <br/>5. 消融实验：通过广泛消融验证了方法的有效性和各组件的重要性。|
|2511.06606v1|[SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding and Reasoning into Large Audio-Language Models](https://arxiv.org/abs/2511.06606v1)|**贡献点：**  <br/>1. **提出SPUR方法**：通过轻量架构改造，将单耳LALMs扩展为具备空间感知能力的模型，无需大幅调整原有结构。  <br/>2. **FOA编码器设计**：将FOA通道（W,X,Y,Z）映射为旋转感知、以听者为中心的空间特征，增强对方向、高度、距离等空间属性的捕捉。  <br/>3. **SPUR-Set数据集**：构建首个结合开放数据与模拟的多模态数据集，聚焦相对空间关系（方向、高度、距离、重叠），支持监督式空间推理。  <br/>4. **性能提升与兼容性**：微调后在空间问答和多说话人归因任务中表现优异，同时保留模型对通用音频的理解能力。  <br/>5. **消融实验验证**：通过系统消融证明方法有效性，为后续空间感知研究提供基础。  <br/><br/>**总结**（100字以内）：  <br/>SPUR通过轻量插件增强LALMs空间感知能力，结合FOA编码器和定制数据集，提升多说话人定位与空间问答性能，保持通用音频理解。|
|2511.06592v1|[MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592v1)|总结：该研究揭示音频大语言模型在临床决策中存在严重模态偏差，易受语音特征（如年龄、性别、情感）影响而非医疗证据，强调需构建偏见感知架构以避免医疗公平性问题。<br/><br/>贡献点：<br/>1. 首次系统评估音频LLM在临床场景的模态偏差，发现手术建议差异可达35%；<br/>2. 量化揭示年龄（12%）、性别与情感对模型决策的显著影响，尤其年龄偏差在多数模型中持续存在；<br/>3. 验证显式推理可缓解性别偏差，但情感识别能力不足导致其影响未被有效捕捉；<br/>4. 提出偏见感知架构的必要性，为临床部署音频模型提供关键风险警示。|
|2511.04376v3|[MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers](https://arxiv.org/abs/2511.04376v3)|**贡献点：**  <br/>1. 提出MusRec模型，首次实现零样本文本到音乐的编辑能力，无需特定任务训练。  <br/>2. 支持对真实音乐的多样编辑任务，突破现有模型仅限合成音乐的限制。  <br/>3. 在音乐内容保持、结构一致性及编辑保真度方面优于现有方法，验证了模型效果。  <br/>4. 为现实场景中的可控音乐编辑提供理论基础与实用方案。  <br/><br/>**总结（100字以内）：**  <br/>MusRec通过融合修正流和扩散变压器技术，首次实现真实音乐的零样本文本编辑，解决了现有模型依赖合成数据、需精确提示等局限，显著提升内容保留与结构一致性，推动可控音乐编辑的实际应用。|
|2511.03601v2|[Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601v2)|**贡献点：**  <br/>1. 提出首个开源LLM音频模型Step-Audio-EditX，支持情感、说话风格、语音学特征等多维度迭代编辑及零样本TTS。  <br/>2. 核心创新：仅依赖大边距合成数据训练，无需嵌入式先验或辅助模块，简化模型复杂度。  <br/>3. 大边距学习方法实现高表达性与迭代控制，突破传统表示层解耦范式。  <br/>4. 在情感编辑等任务上超越MiniMax-2.6-hd与Doubao-Seed-TTS-2.0，验证方法有效性。  <br/><br/>**总结（100字内）：**  <br/>Step-Audio-EditX是首个开源LLM音频模型，通过大边距合成数据训练，支持多维度音频编辑及零样本TTS，超越现有模型性能，推动语音编辑技术发展。|
|2510.25235v2|[Disentangling the effects of peripheral hearing loss and higher-level processes on speech intelligibility in older adults](https://arxiv.org/abs/2510.25235v2)|总结：  <br/>该研究提出基于WHIS模拟器和GESI客观测量的方法，揭示老年听众在语音可懂度表现中的个体差异，强调高级处理能力在补偿听力损失中的关键作用。<br/><br/>贡献点：  <br/>1. **提出新方法**：开发了WHIS模拟器，用于模拟特定老年受试者的听力损失特征，以分离外周听力损失与高级处理对语音可懂度的影响。  <br/>2. **对比实验设计**：通过理想比值掩码（IRM）增强与未处理条件下的语音材料，系统评估不同听力条件下语音可懂度表现。  <br/>3. **发现个体差异**：结果表明部分老年受试者语音可懂度表现优于年轻正常听力者，反映高级处理能力的个体化差异。  <br/>4. **验证GESI有效性**：证明GESI客观测量能准确预测年轻和老年听众的语音可懂度表现，拓展其应用范围。  <br/>5. **参数预测应用**：利用年轻听众实验参数预测老年群体语音可懂度，揭示高级处理效率对表现差异的影响。  <br/>6. **建立研究框架**：构建了独立于听力水平的对比实验框架，为研究老年群体高级语音处理能力提供新工具和方法。|
|2510.12377v2|[A Phase Synthesizer for Decorrelation to Improve Acoustic Feedback Cancellation](https://arxiv.org/abs/2510.12377v2)|总结：提出结合频率偏移与相位调制的统一去相关框架，并扩展相位调制技术，应用于车内语音通信系统以提升稳定性与语音质量。<br/><br/>贡献点：<br/>1. **提出统一去相关框架**：首次将频率偏移和相位调制方法结合，构建名为"phase synthesizer"的新型去相关技术，基于DFT滤波器银行实现。<br/>2. **扩展相位调制技术**：引入可变延迟线机制改进传统相位调制，借鉴音效处理中的vibrato和chorus技术提升方法灵活性。<br/>3. **系统验证与性能提升**：在车内语音通信场景中验证该方法，通过自适应频域卡尔曼滤波器实现系统稳定性增强和PESQ语音质量指标提升。|
|2510.12175v3|[Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis](https://arxiv.org/abs/2510.12175v3)||
|2510.12175v2|[Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis](https://arxiv.org/abs/2510.12175v2)|总结：  <br/>提出Audio Palette模型，通过引入四项时间变化声学控制信号和LoRA参数高效适配，实现高质量、可解释的可控音频生成，支持艺术家导向的音频创作。<br/><br/>贡献点：<br/>1. **首个四维时间可控音频生成模型**：首次引入时间变化的loudness（响度）、pitch（音高）、spectral centroid（频谱中心）和timbre（音色）控制信号，实现对声学特征的精确、可解释操控。<br/>2. **参数高效适配方法**：基于LoRA技术，在AudioSet的精简子集上训练（仅需0.85%原始参数），显著降低计算成本并保持生成质量。<br/>3. **保持音质与语义对齐**：在实现细粒度控制的同时，维持与基线模型相似的FAD和LAION-CLAP指标表现，确保高质量和文本语义一致性。<br/>4. **模块化可扩展框架**：设计支持序列条件、内存高效且具备三尺度分类器自由引导机制的可扩展生成流水线，提升音频研究的灵活性与实用性。<br/>5. **开源可控音频合成新范式**：为开放研究环境下的可控声音设计和表演型音频合成提供稳健基础，推动艺术家主导的音频创作流程。|
|2510.06625v2|[Pitch Estimation With Mean Averaging Smoothed Product Spectrum And Musical Consonance Evaluation Using MASP](https://arxiv.org/abs/2510.06625v2)|**贡献点：**<br/>1. 提出MASP谱（改进的谐波积谱），通过全局均值平滑增强对缺失泛音频谱的鲁棒性，提升音高估计准确性。<br/>2. 将MASP算法扩展为谐波性度量H，用于评估二/三音和声的音乐和声性，结果与音乐理论感知一致。<br/>3. 揭示音高与和声感知可能共享基于频谱的底层机制，为语音与音乐感知研究提供新视角。<br/><br/>**总结（100字内）：**  <br/>提出MASP谱改进音高估计，扩展为谐波性度量评估和声，揭示音高与和声感知共享频谱机制，推动语音及音乐感知研究。|
|2510.04593v2|[UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models](https://arxiv.org/abs/2510.04593v2)|**贡献点总结：**  <br/>1. 提出UniVoice框架，首次将ASR与TTS整合为统一模型，突破传统独立处理模式。  <br/>2. 采用连续表示替代离散语音标记，减少信息损失并提升任务衔接性。  <br/>3. 结合自回归建模（ASR）与流匹配（高保真生成），兼顾效率与质量。  <br/>4. 设计双注意力机制，动态切换因果掩码与双向掩码以协调模型差异。  <br/>5. 引入文本前缀条件语音填充方法，实现高保真度零样本语音克隆。  <br/>6. 实验验证方法在ASR与零样本TTS任务中性能超越单任务模型。  <br/>7. 探索端到端语音理解与生成的新范式，推动语音处理技术发展。  <br/><br/>**总结（100字内）：**  <br/>UniVoice通过连续表示与双注意力机制，首次统一ASR与TTS任务，实现高效高质量语音处理，尤其在零样本语音克隆中表现突出，为端到端语音理解生成提供新思路。|
|2510.04584v1|[Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584v1)|**贡献点**  <br/>1. 系统评估了现有 MCQA 框架在三大基准（MMAU、MMAR、MMSU）上的鲁棒性，揭示模型对选项顺序、问题与选项的同义改写均极为敏感。  <br/>2. 对四种主流 LALM（Audio Flamingo 2/3、Qwen2.5‑Omni‑7B‑Instruct、Kimi‑Audio‑7B‑Instruct）进行实验，验证上述敏感性具有模型普遍性。  <br/>3. 归纳出 MCQA 评估中隐藏的变量对结果产生的偏差，指出单一准确率指标的局限性。  <br/>4. 提出更简洁、稳健的评估协议和度量方式，能够同时考虑选项顺序与文本改写的影响，提供更细致、可解释的 LALM 评估报告。  <br/><br/>**100字以内总结**  <br/>研究表明现有 MCQA 评估对选项顺序、题目及选项改写极为敏感，提出更稳健的评估协议和指标，实现对大型音频语言模型更细致的评估。|
|2510.01157v2|[Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157v2)|**贡献点总结（100字内）：**  <br/>1. 提出首个系统性研究语音模型中音频后门攻击的框架。  <br/>2. 证实攻击在多编码器（4种）和数据集（3种）上的高成功率（90.76%-99.41%）。  <br/>3. 通过组件分析定位模型中最易受攻击的阶段。  <br/>4. 提出基于微调的防御策略以缓解中毒预训练编码器的威胁。|
|2509.23238v3|[WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms](https://arxiv.org/abs/2509.23238v3)|总结：提出WavJEPA及改进版WavJEPA-Nat，实现高效、鲁棒的原始波形通用音频表征学习，突破传统频谱图方法的局限性。<br/><br/>贡献点：<br/>1. **创新方法**：提出WavJEPA，首次将联合嵌入预测架构应用于波形域，解决频谱图方法存在的计算延迟和相位信息丢失问题；<br/>2. **性能优势**：在多种下游任务中显著超越现有时域音频基础模型，且提升幅度远超计算资源消耗；<br/>3. **鲁棒性增强**：开发WavJEPA-Nat（多通道扩展），通过模拟自然场景训练，显著提升模型在噪声/混响环境下的鲁棒性；<br/>4. **应用潜力**：验证原始波形表征方法在通用音频任务中的可行性，为低延迟、高效率的时域音频基础模型提供新范式。|
|2509.22651v1|[VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651v1)|**本文的主要贡献**  <br/><br/>1. **提出全新基准 VoiceAssistant‑Eval**  <br/>   - 首个覆盖“听、说、看”三大交互模态的综合评测框架。  <br/>   - 包含 10,497 条精挑细选的样例，覆盖 13 类任务（自然声、音乐、口语对话、多轮交互、角色扮演、图像视觉等）。  <br/><br/>2. **系统量化评估 21 种开源模型与 GPT‑4o‑Audio**  <br/>   - 同时测量回答内容质量、语音输出质量以及多模态一致性。  <br/>   - 首次给出公开可比的“听力准确率”“说话能力”“视听融合”指标。  <br/><br/>3. **揭示模型能力格局与关键发现**  <br/>   - 专有模型并非在所有任务上均优于开源模型。  <br/>   - 大多数模型在说话任务表现突出，但在音频理解上仍显薄弱。  <br/>   - 结构合理的中小模型（如 Step‑Audio‑2‑mini，7B）可在听力准确率上超越更大模型（如 LLaMA‑Omni2‑32B‑Bilingual）。  <br/><br/>4. **辨识当前技术瓶颈**  <br/>   - 多模态（音+图）输入、角色声线模仿等任务仍难以突破。  <br/>   - 稳健性与安全对齐仍存在显著缺口，需进一步研究。  <br/><br/>5. **公开代码与数据**  <br/>   - 在 https://mathllm.github.io/VoiceAssistantEval/ 提供完整基准实现，便于社区复现与扩展。  <br/><br/>---<br/><br/>**100字以内总结**  <br/>本文推出首个覆盖听、说、看三模态的 VoiceAssistant‑Eval 基准，提供 10 k+ 示例与 13 类任务，系统评测 21 种开源模型和 GPT‑4o‑Audio。实验发现专有模型并非全胜，模型在说话表现佳而音频理解不足；小模型亦能逼近大模型。基准揭示多模态输入、角色模仿等难点，用于指导下一代 AI 助手研发。代码、数据已公开。|