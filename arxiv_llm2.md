|Source|Title|Summary|
|---|---|---|
|2511.02234v1|[An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning   Performance in an Audio MLLM](http://arxiv.org/abs/2511.02234v1)|**贡献点总结：**<br/><br/>1. 提出一种新的音频MLLM训练方法，即插叙式指令调优（interleaved instruction tuning）；  <br/>2. 构建了一个专注于音频语义推理的新基准数据集SHARD；  <br/>3. 验证插叙式提示能提升音频语义推理性能，但可能影响音频标签能力。|
|2511.01261v1|[Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech   Role-Play](http://arxiv.org/abs/2511.01261v1)|**贡献点：**<br/><br/>1. 提出Speech-DRAME-EvalBench：首个包含双语人工标注数据的语音角色扮演评估基准。  <br/>2. 开发DRAME-Eval模型：优于零样本和少样本ALLM的细粒度语音评估模型。  <br/>3. 构建Speech-DRAME-RoleBench：首个使用DRAME-Eval自动评分的角色扮演基准，用于评估语音基础模型。  <br/>4. 引入两种互补评估策略：Archetype Evaluation（角色原型评估）与Realism Evaluation（现实感评估）。  <br/>5. 实现更高评分一致性：DRAME-Eval与人工评分的皮尔逊相关系数显著提升。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Speech-DRAME框架，涵盖评估基准、模型和角色扮演基准，引入两种评估策略，显著提升语音角色扮演模型的评估效果与一致性，为该领域提供首个全面可复现的评估基础。|
|2510.26096v1|[ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for   Audio-Language Models](http://arxiv.org/abs/2510.26096v1)|**总结（100字以内）:**<br/>本文提出首个针对语音语言模型的防御框架ALMGuard，通过识别通用触发器和优化Mel频域扰动，有效降低jailbreak攻击成功率至4.6%，同时保持模型正常功能，建立新SOTA。<br/><br/>**贡献点:**<br/>1. **首个专用防御框架**：提出ALMGuard，首次专门针对Audio-Language Models（ALMs）设计安全防护，填补该领域研究空白。<br/>2. **通用触发器识别**：基于安全对齐的shortcut假设，设计通用Shortcut Activation Perturbations（SAPs）作为有效触发器，激活安全机制抵御攻击。<br/>3. **频域扰动优化**：提出Mel-Gradient Sparse Mask（M-GSM），通过限制敏感Mel频段扰动，实现攻击触发器过滤与语音理解性能的平衡。<br/>4. **鲁棒性验证**：通过理论分析与实验，证明方法对已知和未知jailbreak攻击均具有高鲁棒性，优于传统防御手段。<br/>5. **性能提升**：在四种模型上实验显示，ALMGuard将攻击成功率降至4.6%，且保持与基准相当的正常任务性能，达新SOTA。|
|2510.24693v1|[STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D   Intelligence](http://arxiv.org/abs/2510.24693v1)|**贡献点总结（100字以内）:**<br/><br/>STAR-Bench提出音频4D智能的定义，设计包含基础感知与时空推理的测试框架，强调对难以用语言描述的音频特征的评估。通过高质量数据和严格的评估流程，揭示模型与人类在感知、知识和推理上的差距，并指出开源模型的局限性。|
|2510.23558v1|[ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language   Models](http://arxiv.org/abs/2510.23558v1)|**贡献点：**<br/><br/>1. 提出ISA-Bench，首个系统评估LALM指令敏感性的动态基准。<br/>2. 从指令描述、输出格式和任务组合三个维度全面分析指令敏感性。<br/>3. 揭示现有先进LALMs在指令敏感性方面存在显著问题，影响任务表现。<br/>4. 提出通过复杂指令变体数据集微调Qwen2-Audio以提升指令跟随能力。<br/>5. 指出微调过程中可能出现显著的灾难性遗忘问题，强调指令鲁棒性的重要性。|
|2510.22603v2|[Mitigating Attention Sinks and Massive Activations in Audio-Visual   Speech Recognition with LLMs](http://arxiv.org/abs/2510.22603v2)|贡献点总结：<br/>本研究首次揭示了多模态语音识别中注意力sink和大规模激活现象，发现其与MLP层固定特征索引相关，并通过解相关损失有效缓解，显著提升高下采样场景下的识别性能。<br/><br/>分点贡献：<br/>1. 首次将注意力sink和大规模激活现象引入多模态语音识别领域（ASR/VSR/AVSR）<br/>2. 揭示大规模激活源自MLP层且对应固定特征索引，突破单模态研究局限<br/>3. 发现中间低语义token与BOS token存在高余弦相似性，导致注意力扩散<br/>4. 提出简单有效的解相关损失函数，降低BOS与其他token的相似性<br/>5. 在高音频-视觉特征下采样场景下实现WER提升，且保持低下采样率稳定性|
|2510.22455v1|[Evaluating Multimodal Large Language Models on Core Music Perception   Tasks](http://arxiv.org/abs/2510.22455v1)|**贡献点总结（100字以内）：**  <br/>本文评估了三个SOTA音乐大模型在音频和MIDI输入下的音乐理解能力，揭示了模型在音频感知上的局限性，并指出推理策略对性能提升有限。研究为构建更可靠的音频优先音乐系统提供了指导。|
|2510.20850v1|[Can large audio language models understand child stuttering speech?   speech summarization, and source separation](http://arxiv.org/abs/2510.20850v1)|总结：  <br/>本研究评估大型音频语言模型在处理儿童断续语音中的表现，提出双场景任务设计与混合评估方法，为临床与教育应用提供指导，并支持模型复现。<br/><br/>贡献点：  <br/>1. **分析差异影响**：揭示儿童语音（含断续现象）与成人语音在声学、语调及语言发展上的差异对ASR/NLP的挑战。  <br/>2. **场景任务设计**：提出两种测试场景（多说话者访谈、单儿童阅读），结合单通道源分离与儿童专属摘要任务，针对性评估LALMs性能。  <br/>3. **混合评估方法**：引入LLM作为评判者、人类专家评分及BERTScore多维度验证，量化模型与人类的一致性以评估可靠性。  <br/>4. **临床应用指导**：明确LALMs在生成儿童专属摘要时的适用条件与局限性，为医疗与教育场景提供部署依据。  <br/>5. **可复现支持**：开源提示模板与评估脚本，便于实验复现与模型优化。|
|2510.20504v1|[Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate   Speech Coding](http://arxiv.org/abs/2510.20504v1)|总结：  <br/>提出一种语义优先的语音编解码方法，通过简化Whisper模型实现高质量语音重建，无需外部监督，效果优于传统语义监督编解码器。<br/><br/>贡献点如下：  <br/><br/>1. 提出语义优先的语音编解码方法，与现有基于语义监督的方法相反。  <br/>2. 通过简化Whisper模型的结构，释放其在语音建模方面的潜力。  <br/>3. 构建了无需外部监督的新型编解码器SimWhisper-Codec。  <br/>4. 在相同比特率下，SimWhisper-Codec在语义保持和语音质量上均优于Mimi Codec和SpeechTokenizer等语义监督编解码器。  <br/>5. 提供开源实现，便于研究和应用。|
|2510.20113v1|[SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment   Assistance](http://arxiv.org/abs/2510.20113v1)|**贡献点：**<br/><br/>1. 提出SpeechAgent系统，专门针对语音障碍用户设计的移动辅助沟通工具。  <br/>2. 结合大语言模型（LLM）驱动的推理与先进的语音处理模块，实现个性化支持。  <br/>3. 构建结构化的部署流程，支持移动和边缘设备的实时语音处理。  <br/>4. 保证低延迟与高准确率，提升系统的实用性与用户体验。  <br/>5. 在真实语音障碍数据集和边缘设备延迟评估上验证了系统的有效性与可行性。  <br/><br/>**总结：**  <br/>SpeechAgent是一个针对语音障碍用户的移动辅助沟通系统，结合LLM与语音处理技术，具备实时性与高精度，为日常交流提供有效支持。|
|2510.19055v1|[The MUSE Benchmark: Probing Music Perception and Auditory Relational   Reasoning in Audio LLMS](http://arxiv.org/abs/2510.19055v1)|**贡献点：**  <br/>1. 提出MUSE基准，包含10项任务，用于评估MLLMs在音乐感知方面的基础能力。  <br/>2. 评估四个SOTA模型与200人的大样本人类基线，揭示模型间表现差异及与人类的显著差距。  <br/>3. 发现Gemini Pro在基础感知任务中表现较好，而Qwen和Audio-Flamingo 3表现接近随机，存在严重感知缺陷。  <br/>4. 指出Chain-of-Thought提示方法效果不稳定，甚至有害。  <br/>5. 提供了一个关键工具，用于评估音乐表示的不变性及推动更鲁棒的AI系统发展。<br/><br/>**总结（100字以内）：**  <br/>MUSE基准评估了MLLMs在音乐理解中的表现，揭示其与人类的显著差距和模型间性能差异，指出部分模型存在严重感知缺陷，并质疑CoT提示的有效性，为提升AI音乐理解能力提供重要参考。|
|2510.18416v1|[SegTune: Structured and Fine-Grained Control for Song Generation](http://arxiv.org/abs/2510.18416v1)|总结：  <br/>提出SegTune框架，实现音乐生成的段级控制与风格一致性，通过创新的时间广播机制和LLM时长预测器提升生成质量，构建大规模数据管道和新评估指标，验证了方法的优越性。<br/><br/>贡献点：<br/>1. **段级控制能力**：允许用户或大模型指定与歌曲段落对齐的局部音乐描述，实现细粒度音乐结构控制。<br/>2. **时空注入机制**：通过时间广播将段落提示分配至对应时间窗口，结合全局提示确保整体风格连贯。<br/>3. **LLM时长预测器**：引入基于大语言模型的自回归时长预测模块，生成精确的句级时间戳歌词（LRC格式）。<br/>4. **大规模数据集**：构建高精度歌词-音乐对齐的歌曲数据管道，为训练和评估提供高质量资源。<br/>5. **新评估指标**：提出针对段级对齐度与声乐属性一致性的量化评估标准，完善音乐生成评价体系。<br/>6. **实验验证优势**：在可控性、音乐连贯性等指标上超越现有基线方法，证明SegTune的有效性。|
|2510.18308v1|[ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control   for Expressive Text-to-Speech Generation](http://arxiv.org/abs/2510.18308v1)|**贡献点总结（分点）：**<br/><br/>1. 提出轻量且可解释的ParaStyleTTS框架，实现仅通过文本提示控制语音风格。  <br/>2. 引入双层级风格适应架构，分别建模韵律和语用风格。  <br/>3. 支持对情感、性别、年龄等语音风格因素的精细控制。  <br/>4. 在不同提示格式下保持风格一致性，适用于实际部署场景。  <br/>5. 相比基于大语言模型的方法，具有更高的效率（30倍快，8倍参数，2.5倍内存节省）。  <br/>6. 实验表明其生成语音质量与SOTA系统相当，且在鲁棒性和可控性方面表现更优。|
|2510.18169v1|[Hearing Health in Home Healthcare: Leveraging LLMs for Illness Scoring   and ALMs for Vocal Biomarker Extraction](http://arxiv.org/abs/2510.18169v1)|贡献点：<br/>1. 提出基于LLM的多模态整合框架，将非结构化SOAP笔记与结构化生命体征数据融合为全局健康评分<br/>2. 设计针对家庭护理场景的多阶段语音预处理流程，实现目标说话人短语音片段的精准提取<br/>3. 开发ALM用于语音生物标记的自然语言描述生成，并建立其与健康状态的关联分析<br/>4. 通过对比实验验证LLM在健康评分预测中的有效性，发现SOAP笔记信息量显著优于生命体征数据<br/>5. 首次证明ALM在家庭护理录音中能够识别健康相关声学模式，提供可读性高的医学发现<br/><br/>总结：本研究提出LLM与ALM结合的新方法，实现家庭护理语音数据的多维分析，证实SOAP笔记对健康评估更重要，并首次揭示ALM在识别医疗声学模式方面的潜力，为智能健康监测提供创新解决方案。|
|2510.16718v1|[U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity   Speech Generation](http://arxiv.org/abs/2510.16718v1)|**贡献点：**  <br/>1. 提出U-Codec，一种超低帧率（5Hz）的神经语音编解码器，实现高质量语音重建与快速生成。  <br/>2. 引入基于Transformer的帧间长时依赖模块，增强低帧率下的语音质量。  <br/>3. 系统研究残差向量量化（RVQ）深度与码本大小，确定最优配置。  <br/>4. 将LLM-based TTS扩展至5Hz，实现32层RVQ，显著提升推理速度。  <br/>5. 实验验证5Hz离散令牌可用于快速且高保真语音合成，具有可行性。  <br/><br/>**总结：**  <br/>U-Codec在5Hz超低帧率下实现高质量语音合成，显著提升TTS推理速度，验证了低帧率离散令牌的可行性。|
|2510.15227v1|[LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution   Designed for Speech Large Language Models](http://arxiv.org/abs/2510.15227v1)|**贡献点：**<br/><br/>1. 提出LongCat-Audio-Codec，适用于工业级端到端语音大语言模型的音频编解码方案。  <br/>2. 采用解耦模型架构与多阶段训练策略，提升语义建模与音学特征提取能力。  <br/>3. 实现超低帧率（16.67 Hz）和低比特率（0.43-0.87 kbps）的音频编码。  <br/>4. 在低比特率下仍能保持高质量语音合成，实现编码效率与解码质量的平衡。  <br/>5. 提供开源的推理代码和模型检查点，便于工业应用与研究复现。  <br/><br/>**总结（100字以内）：**  <br/>本文提出LongCat-Audio-Codec，通过解耦架构和多阶段训练实现高效语音编码与高质量合成，支持超低帧率和比特率，适用于工业级大语言模型，并开源代码与模型参数。|
|2510.14664v1|[SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality   Evaluation](http://arxiv.org/abs/2510.14664v1)|总结：  <br/>该论文提出SpeechLLM-as-Judges范式，构建多语言SpeechEval数据集，并开发SQ-LLM模型，通过结构化评估和链式推理提升语音质量评价能力，验证了其跨语言和任务的泛化性能，资源开源。<br/><br/>贡献点：  <br/>1. **提出新型评估范式**：首次将大语言模型（LLMs）用于结构化、可解释的语音质量评价，突破传统标量评分和二进制决策的局限。  <br/>2. **构建大规模多语言数据集**：推出SpeechEval数据集（32,207语音片段、128,754注释），覆盖质量评估、对比、改进建议、深度伪造检测四大任务，支持跨语言研究。  <br/>3. **设计专用语音质量感知模型**：开发SQ-LLM，结合链式推理（Chain-of-Thought）与奖励优化（Reward Optimization），提升语音质量分析能力。  <br/>4. **验证跨语言与任务泛化性**：实验表明SQ-LLM在多语言和多任务场景中均表现优异，证实该范式在语音质量评估领域的潜力。  <br/>5. **开源研究资源**：提供SpeechEval数据集及模型代码，推动社区研究与应用。|
|2510.13558v1|[Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts   Steering Module](http://arxiv.org/abs/2510.13558v1)|总结（100字以内）:  <br/>SteerMoE提出一种新型音频-语言对齐框架，冻结音频编码器和LLM解码器，仅训练轻量级引导模块，实现高效、模块化的跨模态对齐。<br/><br/>贡献点：  <br/>1. **提出SteerMoE框架**：基于柏拉图表示假设，设计可模块化、参数高效的音频-语言对齐方法。  <br/>2. **冻结大模型参数**：仅训练轻量级引导模块，避免对LLM进行昂贵的微调。  <br/>3. **动态专家选择机制**：采用MoE路由策略，动态应用学习到的引导向量以逐步转换音频表示。  <br/>4. **无需修改LLM词汇**：在连续嵌入空间操作，保留LLM原有的词汇和推理能力。  <br/>5. **高效且强性能**：实验表明在ASR、音频理解及函数调用任务中均表现优异，具有高计算效率。|
|2510.12995v2|[Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs](http://arxiv.org/abs/2510.12995v2)|**总结**：本文提出一种结合自回归建模与连续标记扩散的语音合成方法，通过双重头架构和两阶段训练策略提升性能，取得SOTA结果。<br/><br/>**贡献点**：<br/><br/>1. 提出在MLLM框架下使用连续语音表示进行TTS，保留语音的连续性特征，避免离散标记带来的信息损失。  <br/>2. 设计双头架构，包含一个用于生成连续语音表示的扩散头和保留语言模型头以实现多任务能力。  <br/>3. 引入掩码训练策略，缓解自回归解码中的曝光偏差问题。  <br/>4. 提出两阶段训练方法，冻结语言模型以稳定优化，提升扩散头的学习效果。  <br/>5. 在LibriSpeech测试集上取得SOTA的自回归性能，WER降至1.95%，并实现46%的相对WER下降。|
|2510.12851v1|[Adaptive vector steering: A training-free, layer-wise intervention for   hallucination mitigation in large audio and multimodal models](http://arxiv.org/abs/2510.12851v1)|**贡献点：**<br/><br/>1. 提出Adaptive Vector Steering (AVS)方法，用于增强音频语言模型生成内容与音频内容的对齐。<br/>2. 揭示输出正确性与内部表示之间的强相关性。<br/>3. 在两个模型（Gemma、Qwen）和两个基准测试（Audio Hallucination QA、MMAU）中验证了有效性。<br/>4. 显著提升模型在Audio Hallucination QA数据集上的F1分数和MMAU数据集的准确率。<br/>5. 首次将向量引导技术应用于缓解音频生成中的幻觉问题。|
|2510.11646v1|[BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis](http://arxiv.org/abs/2510.11646v1)|**贡献点：**<br/><br/>1. 提出BridgeTTS，一种基于自回归框架的零样本语音合成系统，采用双语音表示范式BridgeCode。  <br/>2. 通过预测稀疏语音标记并重建丰富连续特征，缓解自回归模型的速度与质量之间的权衡。  <br/>3. 引入联合优化策略，同时优化标记级和特征级目标，提升语音自然度与可懂度。  <br/>4. 在保持语音质量与说话人相似度的同时显著加速合成过程，实验结果表现优异。  <br/><br/>**总结（100字以内）：**  <br/>BridgeTTS通过双表示范式和联合优化策略，有效解决自回归零样本TTS中的速度-质量权衡与监督不匹配问题，实现高质量、高效的语音合成。|
|2510.11330v1|[Diffusion-Link: Diffusion Probabilistic Model for Bridging the   Audio-Text Modality Gap](http://arxiv.org/abs/2510.11330v1)|**总结（100字以内）**:  <br/>提出Diffusion-Link方法，首次应用于自动音频字幕生成，有效减少模态差距并提升零样本与监督任务表现，证明扩散模型在跨模态耦合中的潜力优于知识密集型设计。  <br/><br/>**贡献点**:  <br/>1. **提出Diffusion-Link方法**：设计首个基于扩散模型的模态桥接模块，通过生成式映射将音频嵌入对齐到文本嵌入分布，填补语音-语言跨模态研究的空白。  <br/>2. **首次应用于AAC任务**：将扩散模型引入自动音频字幕生成（AudioCaps），突破传统知识检索框架，探索扩散模型在非文本模态的扩展应用。  <br/>3. **模态差距分析创新**：系统性分析模态差距，证明Diffusion-Link在相似度和几何度量上表现最优，实现音频嵌入向文本分布的集体迁移。  <br/>4. **显著性能提升**：在零样本和监督场景下，未引入外部知识即达到SOTA，零样本任务相对提升达52.5%，监督任务提升7.5%。  <br/>5. **轻量级实现设计**：模块仅需三个残差MLP块，保持计算效率，为多模态与大语言模型的高效耦合提供新范式。|
|2510.08618v1|[Look before Transcription: End-to-End SlideASR with Visually-Anchored   Policy Optimization](http://arxiv.org/abs/2510.08618v1)|总结：  <br/>该论文提出SlideASR任务，设计VAPO方法优化视觉引导的语音识别，构建SlideASR-Bench基准数据集，解决了传统方法在领域专业术语识别中的不足。<br/><br/>贡献点：  <br/>1. **定义新任务**：提出SlideASR任务，利用幻灯片的视觉信息提升领域术语识别准确性。  <br/>2. **提出VAPO方法**：首创Visually-Anchored Policy Optimization（VAPO），通过先OCR后转录的推理流程优化模型性能。  <br/>3. **多奖励机制**：设计四类奖励（格式合规、OCR精度、ASR质量、视觉一致性）引导强化学习训练。  <br/>4. **构建基准数据集**：创建SlideASR-Bench，包含合成与真实数据集，为研究提供实体丰富的标准测试环境。|
|2510.08373v1|[DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching](http://arxiv.org/abs/2510.08373v1)|**贡献点总结（100字以内）：**  <br/>本文提出DialoSpeech，结合LLM和Chunked Flow Matching，解决多轮对话中自然性、上下文连贯性和交互动态生成难题，支持中英文及跨语言语音合成，并构建了可扩展的双轨对话数据集。<br/><br/>---<br/><br/>**分点贡献：**<br/><br/>1. **提出DialoSpeech架构**：融合大语言模型与Chunked Flow Matching技术，实现自然、富有表现力的对话语音合成。<br/>2. **支持多语言与跨语言合成**：模型同时支持中文和英文，并具备跨语言语音合成能力。<br/>3. **解决交互动态生成难题**：有效处理对话中的轮次切换、语音重叠和说话人一致性等交互特性。<br/>4. **构建双轨对话数据集**：设计数据处理流程，创建双轨对话数据集，支持可扩展训练与实验验证。<br/>5. **实验验证效果优越**：在多项指标上超越现有基线模型，提供生成人类对话语音的有效解决方案。|
|2510.07355v1|[AV-EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in   Omni-modal LLMS with Audio-visual Cues](http://arxiv.org/abs/2510.07355v1)|**贡献点：**<br/><br/>1. 提出AV-EMO-Reasoning基准，系统评估LLMs的情感一致性。  <br/>2. 构建包含单轮与多轮的合成音视频语料库，涵盖真实场景。  <br/>3. 引入连续、分类和感知三种评估指标，全面衡量情感推理能力。  <br/>4. 实验证明视觉线索能显著提升情感一致性，优于纯语音基线。  <br/>5. 显示LLMs能利用音视频线索生成更具情感意识的对话内容。  <br/>6. 指出不同评估指标捕捉的情感维度不同，体现模型在不同任务中的互补性。  <br/>7. 推动情感感知对话系统的可复现评估，促进更自然的AI交互发展。  <br/><br/>**总结：**  <br/>该研究提出AV-EMO-Reasoning基准，系统评估LLMs的情感推理能力，证明音视频融合有助于提高情感一致性，并为自然人机交互提供评估标准。|
|2510.07096v1|[Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided   Sarcastic Speech Synthesis](http://arxiv.org/abs/2510.07096v1)|**贡献点：**<br/><br/>1. 提出首个基于LLM的 sarcastic speech synthesis 框架，结合语义与韵律信息。<br/>2. 使用 LoRA 微调 LLaMA 3 生成语义嵌入，捕捉语用矛盾和语篇线索。<br/>3. 引入 RAG 模块检索韵律示例，提供讽刺表达的参考模式。<br/>4. 在 VITS 框架中融合双条件，提升语音自然度与语境适配性。<br/>5. 在客观与主观评估中均优于现有基线，增强讽刺表达性和下游检测效果。 <br/><br/>**总结（100字以内）：**  <br/>本文提出一种基于LLM和RAG的讽刺语音合成框架，结合语义与韵律信息，提升语音自然度与表达性，并在检测任务中表现优异。|
|2510.05542v1|[Sci-Phi: A Large Language Model Spatial Audio Descriptor](http://arxiv.org/abs/2510.05542v1)|**贡献点总结：**<br/><br/>1. 提出首个能够完成空间场景描述的音频大语言模型 Sci-Phi。<br/>2. 引入双空间与频谱编码器，实现对多声源和环境参数的联合估计。<br/>3. 通过4000小时合成Ambisonics数据训练，支持多声源识别与描述。<br/>4. 在多种挑战性条件下（如混响、信噪比、声源相似性）表现出良好的鲁棒性。<br/>5. 在真实房间冲激响应上保持良好性能，具备实际应用潜力。|
|2510.04593v1|[UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with   Large Language Models](http://arxiv.org/abs/2510.04593v1)|**贡献点总结（100字以内）：**  <br/>本文提出UniVoice，通过连续表示统一ASR与TTS任务，结合自回归和流匹配的优势，设计双注意力机制与文本前缀条件语音填充方法，实现高质量零样本语音克隆，并在ASR和零样本TTS任务中达到或超越单任务模型性能。|
|2510.04584v1|[Robustness assessment of large audio language models in multiple-choice   evaluation](http://arxiv.org/abs/2510.04584v1)|总结：  <br/>该论文指出现有MCQA框架在评估LALMs时存在局限，提出改进的评估协议和指标以更全面捕捉模型对选项顺序、问题改写等细微变化的敏感性，提升评估的准确性和细节呈现。<br/><br/>贡献点：  <br/>1. **发现现有评估框架的缺陷**：揭示当前MCQA评估方法对选项顺序、问题表述等微小变化不敏感，导致结果偏差，无法真实反映模型性能。  <br/>2. **系统性研究多模型与多基准**：基于三个基准（MMAU、MMAR、MMSU）和四个典型LALMs（Audio Flamingo 2/3、Qwen2.5-Omni-7B-Instruct、Kimi-Audio-7B-Instruct）开展全面实验分析。  <br/>3. **量化模型敏感性差异**：证明模型不仅受选项顺序影响，还对问题及选项的改写敏感，揭示评估中需要考虑的多维变量。  <br/>4. **提出改进的评估方案**：设计更简洁有效的评估协议和指标，支持对细微变化的刻画，提供更细致的LALMs性能分析报告。|
|2510.04463v1|[Evaluating Self-Supervised Speech Models via Text-Based LLMS](http://arxiv.org/abs/2510.04463v1)|**贡献点：**  <br/>1. 提出一种无需额外训练或调参的SSL模型任务无关评估新方法。  <br/>2. 利用大语言模型（LLMs）进行评估，通过输入离散token序列和SSL模型的领域提示获取均对数似然。  <br/>3. 实验证明该方法与自动语音识别任务表现具有相关性。  <br/>4. 发现LLMs可作为SSL评估工具，并提供对说话人验证任务有用的推理时嵌入。  <br/><br/>**总结：**  <br/>本研究提出一种基于大语言模型的SSL任务无关评估方法，无需额外训练，有效关联ASR任务，并揭示LLMs在说话人验证中的潜在应用。|
|2510.01698v3|[TalkPlay-Tools: Conversational Music Recommendation with LLM Tool   Calling](http://arxiv.org/abs/2510.01698v3)|总结：  <br/>提出统一的LLM音乐推荐框架，整合多检索方法与数据库过滤，通过工具调用实现多模态用户意图理解及推荐组件协调，验证其跨场景竞争力，开创交互式推荐新范式。<br/><br/>贡献点：  <br/>1. 构建统一的检索-重排序流程，整合布尔过滤（SQL）、稀疏检索（BM25）、密集检索（嵌入相似性）和生成检索（语义ID）等多组件技术。  <br/>2. 设计基于工具调用的端到端推荐系统，实现用户意图解析、工具规划与多数据库过滤方法的协同集成。  <br/>3. 引入动态工具选择机制，通过预测工具类型、执行顺序及参数，支持多模态推荐场景的高效适配。  <br/>4. 验证框架在多样化推荐任务中的竞争力，推动对话式音乐推荐系统向更灵活、精准的方向发展。|
|2509.16971v2|[AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for   Coarse-to-Fine Audio Deep Reasoning](http://arxiv.org/abs/2509.16971v2)|总结（100字以内）:  <br/>提出AudioGenie-Reasoner，首个统一训练无关多智能体系统，通过模仿人类粗到细认知过程和主动迭代文档优化，解决音频深度推理中感知与推理分离问题，达到SOTA性能并开源代码。<br/><br/>贡献点分点列出：  <br/>1. **首个统一训练无关多智能体系统**：设计AudioGenie-Reasoner（AGR），首次实现音频感知与推理的协同，无需额外训练数据。  <br/>2. **感知-推理范式转换**：将音频深度推理转化为复杂文本理解任务，利用大语言模型的潜力。  <br/>3. **模仿人类认知流程**：引入粗到细的文本证据链处理机制，分阶段提取信息并逐步优化。  <br/>4. **主动迭代文档优化循环**：通过工具增强路线和专用智能体，持续补充缺失信息并完善推理链。  <br/>5. **显著性能提升**：在多个基准测试中达到现有开源模型的SOTA表现。  <br/>6. **开源实现**：提供完整代码，促进研究复现与应用。|
|2509.16622v2|[Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](http://arxiv.org/abs/2509.16622v2)|**贡献点：**  <br/>1. 提出将扩散模型LLaDA作为外部处理模块用于ASR，显著降低Whisper-LLaMA的词错误率（WER）。  <br/>2. 在LibriSpeech上，最佳级联系统实现2.25%/4.94%的WER，比基线提高12.3%。  <br/>3. 强调音频条件嵌入对提升ASR性能的重要性，纯文本LLaDA无法有效提升准确率。  <br/>4. 评估LLaDA作为独立解码器在扩散及半自回归解码下的表现，部分配置实现更快推理速度。  <br/><br/>**总结：**  <br/>本研究探索LLaDA在ASR中的应用，展示其在提升识别准确率和推理速度方面的潜力。|
|2509.13927v2|[DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models](http://arxiv.org/abs/2509.13927v2)|**贡献点：**  <br/>1. 提出DSpAST，一种基于SpatialAST的空间音频编码器，通过解耦表示同时捕捉声源类型、方向和距离信息，提升空间音频推理性能。  <br/>2. 在参数量上仅增加0.2%，显著降低模型复杂度，兼顾高效性与任务能力。  <br/>3. 在SpatialSoundQA任务中验证模型有效性，实验结果表明DSpAST显著优于SpatialAST。  <br/><br/>**总结（100字以内）：**  <br/>DSpAST通过解耦表示优化空间音频编码，仅微增参数即实现对声源类型、方向和距离的高效捕捉，在SpatialSoundQA任务中显著提升性能，超越传统SpatialAST模型。|
|2509.13145v2|[UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System   Based on Multimodal Large Language Model](http://arxiv.org/abs/2509.13145v2)|总结：  <br/>本文提出基于多模态大语言模型的言语康复系统，整合超声舌成像与语音信号，构建专用数据集并引入时空融合训练策略，显著提升发音障碍分析精度与临床反馈有效性。<br/><br/>贡献点：  <br/>1. **构建高质量领域专用数据集**：包含超声舌成像与语音信号的对话对，解决发音信息获取和融合不足的问题。  <br/>2. **提出MLLM基础的言语康复系统**：结合超声成像与语音信号，实现精准、实时的发音运动反馈。  <br/>3. **设计时空融合训练策略**：通过同步处理超声视频与语音信号，提升模型对发音障碍的细粒度分析能力。  <br/>4. **生成可操作的临床反馈**：基于模型分析结果，提供针对发音障碍的互动性治疗建议。  <br/>5. **实验证明有效性**：在发音分析与临床评估任务中验证系统的性能优势。|
|2509.12508v3|[Fun-ASR Technical Report](http://arxiv.org/abs/2509.12508v3)|贡献点总结：<br/>Fun-ASR通过整合大规模数据、大模型架构、LLM与强化学习技术，解决了LLM在语音识别中的幻觉问题，实现了在真实行业数据集上的SOTA表现，并针对流式处理、噪声鲁棒性等实际应用场景进行了专项优化。<br/><br/>分点贡献：<br/>1. 提出集成强化学习的LLM-based语音识别框架（Fun-ASR），有效缓解LLM幻觉问题<br/>2. 构建大规模语音-文本对数据集与大模型容量的协同训练机制<br/>3. 实现流式语音识别、噪声环境下的鲁棒识别、语种切换等实际场景的专项优化<br/>4. 开发支持热词自定义等工业级功能的部署系统<br/>5. 在真实行业评估数据集上取得优于现有LLM-ASR系统的性能表现|