|Source|Title|Summary|
|---|---|---|
|2511.10639v1|[Direction-of-Arrival and Noise Covariance Matrix joint estimation for beamforming](https://arxiv.org/abs/2511.10639v1)|总结：  <br/>提出联合DoA与噪声协方差矩阵估计方法，通过准线性解简化流程并提升波束成形性能，跨频率bin的DoA估计增强混响环境鲁棒性，均优于经典MUSIC算法及信号增强技术。<br/><br/>贡献点：  <br/>1. 联合估计DoA与噪声协方差矩阵（NCM），专为波束成形应用设计；  <br/>2. 基于现有NCM框架，提出准线性解替代传统穷举搜索，简化估计过程；  <br/>3. 引入跨所有频率bin的DoA估计技术，提升混响环境下的鲁棒性；  <br/>4. 在中高角度场景下，相较MUSIC等经典方法实现更低的角误差与更强的信号增强；  <br/>5. 通过理论与实证指标验证，框架在噪声抑制与干扰消除方面优于其他信号增强技术。|
|2511.10289v1|[Music Flamingo: Scaling Music Understanding in Audio Language Models](https://arxiv.org/abs/2511.10289v1)|总结：<br/>提出Music Flamingo，构建MF-Skills数据集，融合音乐理论与强化学习，实现多维度音乐理解，推动领域发展。<br/><br/>贡献点：<br/>1. 提出Music Flamingo：首个针对音乐理解的通用音频-语言模型，突破传统模型仅处理表面信息的局限<br/>2. 构建MF-Skills数据集：通过多阶段标注流程，首次系统涵盖和声、结构、音色、歌词、文化等五维音乐属性<br/>3. 创新训练方法：结合MF-Think冷启动框架与GRPO强化学习，提升模型音乐推理能力<br/>4. 多模态增强：基于Audio Flamingo 3进行架构改进，强化音乐特征提取与跨模态对齐能力<br/>5. 行业突破：在10+音乐理解基准测试中取得SOTA，建立人类级音乐感知新范式|
|2511.10262v1|[MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models](https://arxiv.org/abs/2511.10262v1)|**贡献点总结（分点）：**  <br/>1. 提出MTR-DuplexBench，首个面向全双工语音语言模型（FD-SLM）的多轮对话基准测试，填补现有单轮评估的不足。  <br/>2. 通过分段处理连续对话，实现逐回合（turn-by-turn）的细粒度评估，解决多轮对话中回合边界模糊与上下文不一致问题。  <br/>3. 覆盖多维度评估指标，包括对话质量、动态性、指令遵循与安全性，推动FD-SLM在复杂交互场景下的能力验证。  <br/>4. 实验揭示现有FD-SLM在多轮次任务中存在性能波动，验证新基准的必要性与有效性，为后续研究提供方向。  <br/><br/>**总结（100字以内）：**  <br/>本文提出MTR-DuplexBench，首个针对全双工语音语言模型的多轮对话基准，通过分段评估解决回合边界与上下文一致性的挑战，揭示模型在多轮交互中的不足，推动更全面的对话能力研究。|
|2511.10222v1|[Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard](https://arxiv.org/abs/2511.10222v1)|【贡献点】<br/>1. 提出SACRED-Bench基准测试：首个针对LLMs跨模态音频攻击的评估框架，通过语音-音频合成机制发现传统防护手段的盲区。<br/>2. 创新攻击机制设计：包含三种新型攻击方式——语音重叠/多说话人对话、语音-音频混合、多样化口语指令格式，可绕过文本过滤器。<br/>3. 实验验证模型脆弱性：测试显示Gemini 2.5 Pro等顶尖模型在音频攻击下仍存在66%的成功率，揭示跨模态攻击的严重安全隐患。<br/>4. 提出多模态防护模型SALMONN-Guard：通过联合分析语音、音频和文本降低攻击成功率至20%，为音频感知防御提供新思路。<br/>5. 强调音频安全研究必要性：呼吁学界关注语音模型的音频输入风险，推动更全面的多模态安全防护体系发展。<br/><br/>【总结】该研究提出首个针对LLMs跨模态音频攻击的基准测试SACRED-Bench，并开发SALMONN-Guard防护模型，揭示当前模型在音频攻击场景下的脆弱性，推动音频安全防护研究。|
|2511.10168v1|[A Study of Binaural Deep Beamforming With Interpretable Beampatterns Guided by Time-Varying RTF](https://arxiv.org/abs/2511.10168v1)|**总结（100字以内）:**  <br/>提出基于SI-SDR损失的动态波束成形框架，结合实时RTF跟踪实现空间一致性增强，并验证其在不同指导方式下的有效性，推动助听器和可穿戴设备的应用。<br/><br/>**贡献点分点列出:**  <br/>1. **动态波束成形框架**：首次提出通过深度学习模型在动态声学环境中估计时变波束成形权重，提升语音增强效果。  <br/>2. **SI-SDR损失优化**：采用SI-SDR（Scale-Invariant Signal-to-Distortion Ratio）损失函数，直接优化语音增强目标，提高信号质量。  <br/>3. **RTF连续跟踪机制**：引入持续跟踪的相对转移函数（RTFs）作为指导，确保模型对移动目标说话人的空间行为建模。  <br/>4. **多场景对比验证**：通过三种设置（真实RTF、估计RTF、无指导）对比实验，验证RTF指导对空间一致性与方向跟踪的重要性。  <br/>5. **双耳信号输出**：模型生成双耳信号以保留空间线索，增强实际应用（如助听器、可穿戴设备）的沉浸感与定位准确性。|
|2511.09995v1|[Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS](https://arxiv.org/abs/2511.09995v1)|总结：  <br/>该研究针对FM-based zero-shot TTS在speaker表示不足的问题，提出TLA-SA方法，通过时间与层级自适应对齐提升一致性，并在多数据集和模型架构上验证其有效性。<br/><br/>贡献点：  <br/>1. **揭示speaker信息分布特性**：通过实证分析发现speaker信息在时间步和网络层的非均匀分配，指出现有方法对speaker表示的不足。  <br/>2. **提出TLA-SA损失函数**：联合利用时间维度和网络层级的speaker信息变化，增强跨时间步和层的speaker一致性。  <br/>3. **跨场景有效性验证**：在研究级和工业级数据集上验证方法有效性，并适用于解码器-only语言模型和无LM的FM-TTS系统。|
|2511.09958v1|[Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2511.09958v1)|总结：  <br/>本文提出Audio-VLA模型，融合接触音频提升机器人动态过程感知，并引入TCR指标系统评估操作过程，改进仿真环境以增强实验真实性，实验证明其在多任务中优于视觉模型。<br/><br/>贡献点：  <br/>1. 提出 Audio-VLA 模型，利用接触音频解决视觉 VLA 模型在感知交互动态过程中的局限性。  <br/>2. 引入 Task Completion Rate (TCR) 指标，构建更全面的动态操作过程评估体系。  <br/>3. 设计多模态投影层，对齐视觉、音频和语言特征到统一空间，提升跨模态理解能力。  <br/>4. 改进 RLBench 和 LIBERO 仿真环境，通过碰撞音频生成提供更真实的交互反馈。|
|2511.09232v1|[POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation](https://arxiv.org/abs/2511.09232v1)|总结（100字以内）:  <br/>提出POTSA框架，结合OT与并行语音对，通过Bias Compensation、token-level OT约束及层调度策略，显著提升多语言S2TT性能，尤其在零样本场景下表现优异，并减少数据需求。  <br/><br/>贡献点：  <br/>1. **提出POTSA框架**：基于跨语言并行语音对和最优传输（OT），解决高/低资源语言间翻译性能偏倚问题。  <br/>2. **引入Bias Compensation模块**：粗粒度对齐跨语言语音表征，缓解语义共性缺失导致的偏倚。  <br/>3. **Token-level OT约束**：通过Q-Former建立细粒度表征一致性，增强跨语言对齐精度。  <br/>4. **层调度策略**：动态聚焦语义关键层，提升OT约束的语义有效性。  <br/>5. **高效实验验证**：在FLEURS数据集上实现SOTA，仅需10小时平行语料即超越现有方法，尤其在零样本语言上提升5.05 BLEU。|
|2511.09090v1|[Diff-V2M: A Hierarchical Conditional Diffusion Model with Explicit Rhythmic Modeling for Video-to-Music Generation](https://arxiv.org/abs/2511.09090v1)|总结（100字以内）：  <br/>本文提出Diff-V2M框架，通过分层交叉注意力机制与时间步感知融合策略，解决视频到音乐生成中的节奏建模与多特征整合问题，验证低分辨率ODF在节奏建模的有效性，并在多数据集上取得SOTA性能。<br/><br/>贡献点：  <br/>1. **提出Diff-V2M框架**：构建基于分层条件扩散模型的通用V2M生成框架，解决时序对齐与特征整合难题。  <br/>2. **创新节奏建模方法**：评估并开发节奏预测器，直接从视频中提取低分辨率ODF作为更优节奏信号。  <br/>3. **分层交叉注意力机制**：分离情感特征（第一层）与语义/节奏特征（第二层）的整合过程，提升情感与内容一致性。  <br/>4. **时间步感知融合策略**：引入FiLM和加权融合，实现语义与节奏信号的自适应平衡，增强特征交互。  <br/>5. **实验验证效果**：在in-domain和out-of-domain数据集上超越现有方法，显著提升客观指标与主观评价表现。|
|2511.09084v1|[Towards Effective and Efficient Non-autoregressive decoders for Conformer and LLM-based ASR using Block-based Attention Mask](https://arxiv.org/abs/2511.09084v1)|总结（100字以内）:  <br/>本文提出一种新型块式非自回归注意力掩码解码器（AMD），兼容Conformer和LLM基ASR系统，在保持高识别准确率的同时显著提升解码速度，并通过实时因子优化平衡性能与效率。<br/><br/>贡献点：  <br/>1. **提出块式非自回归解码器**：设计AMD结构，支持输出标签块内并行推理与块间单调左向右预测，突破传统自回归模型的串行限制。  <br/>2. **融合多概率模型**：开发单次beam search算法，动态整合CTC、自回归解码器与AMD的概率，提升解码鲁棒性。  <br/>3. **跨模型验证有效性**：在Conformer、WavLM集成及LLM基ASR系统中验证AMD，实现3种配置下的解码加速（1.44x-2.31x）及WER降低（0.13%-0.62%）。  <br/>4. **性能-效率可调平衡**：提供灵活的性能与计算效率权衡机制，适配不同ASR架构的需求。|
|2511.08723v1|[ParaS2S: Benchmarking and Aligning Spoken Language Models for Paralinguistic-aware Speech-to-Speech Interaction](https://arxiv.org/abs/2511.08723v1)|**总结（100字以内）**  <br/>提出ParaS2S框架，结合RL和基准测试ParaS2SBench，通过GRPO方法提升S2S模型对语用线索的响应能力，在内容和风格适配性上超越监督微调模型，减少标注依赖。  <br/><br/>**贡献点分点总结**  <br/>1. **提出ParaS2S框架**：首个直接在波形层面优化内容与说话风格的RL框架，处理语用线索（情感/语调/说话人属性）。  <br/>2. **构建ParaS2SBench基准**：全面评估S2S模型的输出适配性，与人类判断一致，提供自动评估工具。  <br/>3. **引入GRPO方法**：通过分组相对策略优化，利用未标注语音数据进行模型探索与学习，减少标注依赖。  <br/>4. **实验验证有效性**：在ParaS2SBench中，RL方法在内容与风格适配性上比监督微调提升11%，超越现有模型且标注需求更低。|
|2511.08496v2|[HQ-SVC: Towards High-Quality Zero-Shot Singing Voice Conversion in Low-Resource Scenarios](https://arxiv.org/abs/2511.08496v2)|总结：  <br/>HQ-SVC通过联合编码器、音高音量建模与可微信号处理技术，解决了零样本唱歌声音转换中信息丢失和资源消耗问题，实现了高质量转换与高效性能，并拓展至语音超分辨率任务。<br/><br/>贡献点：  <br/>1. **联合特征提取**：设计解耦编码器，同步提取内容与说话者特征，避免传统方法因分离建模导致的关键信息损失。  <br/>2. **保真度增强**：引入音高与音量建模模块，保留声学细节，提升转换后的语音自然度与质量。  <br/>3. **渐进式优化**：结合可微信号处理与扩散技术，分阶段优化输出，提高生成效果的稳定性与精细度。  <br/>4. **高效性与通用性**：框架无需微调，显著提升计算效率；同时支持语音超分辨率任务，超越专用音频超分辨率方法。|
|2511.08389v1|[Unifying Model and Layer Fusion for Speech Foundation Models](https://arxiv.org/abs/2511.08389v1)|贡献点总结（100字以内）：  <br/>提出跨模型层融合接口模块，统一多层与多模型融合策略；通过实验验证在ASR和声调分析任务中的有效性；分析方法的可扩展性，强调上游模型选择的重要性；为Speech Foundation Models的应用提供新思路。  <br/><br/>分点贡献：  <br/>1. **统一融合策略**：设计接口模块实现多模型与多层表示的跨模态融合，整合现有两种主流方法（多层融合、多模型融合）。  <br/>2. **提升下游任务性能**：在自监督与监督模型框架下，通过实验验证方法在ASR、声调分析等任务中的优越性。  <br/>3. **可扩展性分析**：探讨模型规模与数量对融合效果的影响，强调上游模型选择的关键性。  <br/>4. **方法有效性验证**：证明接口模块在合适模型组合下能显著提升性能，为Speech Foundation Models的实践提供参考。|
|2511.08252v1|[Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models](https://arxiv.org/abs/2511.08252v1)|总结（100字以内）:  <br/>该研究揭示了音乐生成模型中自注意力图对保持时间结构的重要性，提出无需训练的Melodia方法实现精准属性修改，设计两种新评估指标，验证了方法在文本一致性与结构完整性上的优越性，提升模型控制能力。<br/><br/>贡献点：  <br/>1. **揭示注意力机制作用**：分析AudioLDM 2中自注意力图与交叉注意力图的功能差异，发现自注意力图对源音乐时间结构（旋律/节奏）的保真关键作用。  <br/>2. **提出Training-free编辑方法**：推出Melodia技术，通过选择性操控特定层的自注意力图并利用注意力仓库存储源信息，实现音乐属性修改而不破坏原始结构。  <br/>3. **设计新评估指标**：提出两种新颖的量化指标，完善音乐编辑方法的客观评价体系，实验验证其在多数据集上的有效性。  <br/>4. **提升模型可解释性与可控性**：深化对扩散模型内部机制的理解，为音乐生成提供更精准的结构控制手段。|
|2511.08093v1|[Quantizing Whisper-small: How design choices affect ASR performance](https://arxiv.org/abs/2511.08093v1)|**贡献点：**  <br/>1. **跨库统一评估**：首次对Whisper-small的Post-training Quantization（PTQ）方法进行多库（PyTorch、Optimum-Quanto、HQQ、bitsandbytes）综合比较，系统分析量化方案、方法、粒度和位宽的影响。  <br/>2. **动态int8量化最优**：发现动态int8量化（Quanto库）在模型压缩（57%）与精度（WER降低）间取得最佳平衡，显著提升部署效率。  <br/>3. **静态量化局限性**：揭示静态量化在Transformer架构下表现较差，可能因缺乏动态校准导致精度损失。  <br/>4. **激进量化格式权衡**：提出nf4/int3等高压缩格式（71%）在噪声环境下会牺牲精度，但适合对精度要求较低的场景。  <br/>5. **无重训练部署方案**：验证无需模型重训练的PTQ方法可有效降低模型尺寸和推理成本，推动Whisper-small在边缘设备的高效应用。  <br/><br/>**总结（100字内）：**  <br/>本研究通过跨库评估，揭示动态int8量化（Quanto）在Whisper-small中实现最佳压缩-精度平衡，为边缘部署提供高效方案，同时指出激进量化格式的适用场景及静态量化的局限性。|
|2511.08092v1|[Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR](https://arxiv.org/abs/2511.08092v1)|总结：  <br/>本研究提出one-shot剪枝方法作为ASR模型的隐式正则化手段，通过敏感度分析揭示模型结构的不对称性，并在无需微调的情况下实现显著性能提升，同时支持更激进的压缩，为模型架构设计提供新视角。<br/><br/>贡献点：  <br/>1. **挑战传统观点**：提出one-shot剪枝不仅是压缩技术，更是ASR模型的隐式正则化方法，提升泛化性能。  <br/>2. **敏感度诊断结合**：首次结合梯度与Fisher信息诊断，实现组件级精准剪枝策略。  <br/>3. **暴露架构不对称性**：发现解码器FFN对剪枝敏感，而自注意力与末层编码器冗余可安全移除，提升模型效率。  <br/>4. **无需微调的性能提升**：在未微调情况下，50%剪枝量显著降低LibriSpeech的WER（绝对2.38%，相对20.44%）。  <br/>5. **低稀疏度性能保障**：在40%稀疏度下保持接近基线精度，突破传统全局剪枝的性能瓶颈。  <br/>6. **设计工具价值**：论证剪枝位置的定位与剪枝量同等重要，推动将剪枝纳入模型架构设计核心考量。|
|2511.08040v1|[Automatic Music Mixing using a Generative Model of Effect Embeddings](https://arxiv.org/abs/2511.08040v1)|总结：  <br/>MEGAMI提出了一种生成框架，通过嵌入生成和域适应方法解决音乐混音的多解性问题，实现接近人类水平的跨流派混音质量。<br/><br/>贡献点：  <br/>1. 提出MEGAMI框架，首次将音乐混音建模为条件分布生成任务，而非确定性回归，解决多解性问题。  <br/>2. 引入track-agnostic效果处理器，利用每轨生成的嵌入控制混音风格。  <br/>3. 设计permutation-equivariant架构，支持任意未标记轨道的混音处理。  <br/>4. 采用域适应技术，实现干声与湿声录音的联合训练。  <br/>5. 通过分布度量和听测验证，在客观指标和主观质量上均超越现有方法并接近人类水平。|
|2511.08012v1|[DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes](https://arxiv.org/abs/2511.08012v1)|**贡献点分点总结：**  <br/>1. **提出基于LLMs的空间音频数据集**：构建了首个结合大语言模型辅助的新型数据集，显著提升声学场景的多样性与真实感，突破传统合成数据的局限。  <br/>2. **设计轻量级DOA模型LightDOA**：引入深度可分离卷积结构，针对多通道输入优化模型，成功在保持低计算复杂度的同时实现高精度与鲁棒性。  <br/>3. **实验验证模型有效性**：在多样化声学场景中验证LightDOA的性能，证明其优于现有方法，并适用于资源受限的实际应用。  <br/>4. **推动DOA研究方向革新**：强调了LLMs生成空间音频数据在提升模型泛化能力与研究效率中的潜力，为后续研究提供新思路。  <br/><br/>**总结（100字内）：**  <br/>本文提出基于LLMs的新型空间音频数据集，设计轻量级模型LightDOA，并验证其在多种场景下的高效性与鲁棒性，为资源受限环境中的DOA估计提供了创新解决方案。|
|2511.07931v1|[SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931v1)|**主要贡献**  <br/>- 提出 **SpeechJudge** 套件：数据集、评估基准、生成奖励模型三位一体，聚焦语音自然度。  <br/>- 构建 **SpeechJudge‑Data**：99 K 人类偏好对，涵盖多语言、多风格、多个零样本文本到语音（TTS）模型，并标注可懂度与自然度。  <br/>- 设立 **SpeechJudge‑Eval** 基准，揭示现有客观指标和 AudioLLM 在自然度判断上的不足（最高 < 70% 与人工一致）。  <br/>- 开发 **SpeechJudge‑GRM**：基于 Qwen2.5‑Omni‑7B 的生成奖励模型，采用两阶段后训练——带链式思考的监督微调（SFT） + 基于 GRPO 的强化学习（RL）。  <br/>- 在基准上取得 **77.2%**（推理时 @10 缩放至 79.4%）的准确率，优于传统 Bradley‑Terry 奖励模型（72.7%）。  <br/>- 验证 SpeechJudge‑GRM 可作为奖励函数用于语音生成模型的后训练，实现模型对齐人类偏好。  <br/><br/>**摘要（100字以内）**  <br/>提出 SpeechJudge 套件，提供 99 K 人类偏好数据、自然度评估基准及基于 Qwen2.5 的生成奖励模型，在基准上实现 77%+ 准确率并可用于引导语音生成模型对齐人类感知。|
|2511.07268v1|[Generating Piano Music with Transformers: A Comparative Study of Scale, Data, and Metrics](https://arxiv.org/abs/2511.07268v1)|总结：  <br/>该论文系统比较了符号钢琴音乐生成中的关键设计因素，分析了量化指标与人类评估的相关性，并提出了一款高性能Transformer模型，在图灵测试中被评价为人类创作水平。<br/><br/>贡献点：  <br/>1. **系统性设计比较**：全面评估数据集多样性、模型架构、参数规模及训练策略对生成音乐质量的影响。  <br/>2. **量化指标分析**：探讨多种量化指标与人类听觉判断的一致性，建立客观评估框架。  <br/>3. **人类评估结合**：通过听力实验验证模型表现，确保评估结果与主观感知的关联性。  <br/>4. **高性能模型提出**：开发950M参数Transformer模型，基于80K跨流派MIDI文件训练，在生成质量上接近人类创作水平。|
|2511.07253v1|[Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models](https://arxiv.org/abs/2511.07253v1)|**贡献点总结**（100字以内）:  <br/>提出Omni-AVSR统一框架，整合多粒度训练与LoRA策略，实现ASR/VSR/AVSR任务的高效处理与弹性推理，减少资源消耗并保持鲁棒性，分析模型扩展性以平衡性能与效率。<br/><br/>---<br/><br/>**分点贡献点**:<br/>1. **提出统一框架**：首次构建支持ASR、VSR和AVSR的统一音频-视觉LLM模型，避免多模型独立训练带来的资源冗余与协同缺失。  <br/>2. **高效多粒度训练**：采用Matryoshka表示学习范式，通过多音频/视觉粒度联合训练降低资源消耗，提升跨模态适应性。  <br/>3. **参数高效适应策略**：探索三种LoRA-based方法，在模型共享与任务特异性之间取得平衡，显著减少微调参数量。  <br/>4. **资源效率优化**：单模型训练部署资源消耗远低于现有方法，在保持高精度的同时实现弹性推理能力。  <br/>5. **噪声鲁棒性验证**：模型在嘈杂环境下的表现稳定，证明其对真实场景的适应性。  <br/>6. **扩展性分析**：系统研究模型规模对性能/效率的权衡，为实际应用提供理论依据与优化方向。|
|2511.07185v2|[Neural Directional Filtering Using a Compact Microphone Array](https://arxiv.org/abs/2511.07185v2)|**贡献点**:  <br/>1. **提出神经方向滤波（NDF）方法**：利用深度神经网络生成单通道复掩膜，通过参考麦克风实现虚拟方向性麦克风，突破传统波束成形器在紧凑阵列中的方向性限制。  <br/>2. **引入数据依赖的训练策略与评估指标**：设计数据驱动的训练方案，并提出用于量化方向性模式（如直接性因子）的指标，提升模型可解释性与优化效率。  <br/>3. **实现频率不变的直接性模式**：NDF在空间混叠频率以上仍保持频率不变性，解决了传统方法的频率依赖性问题。  <br/>4. **支持高阶与多样化方向性控制**：可拟合复杂高阶方向性模式，并通过动态调整实现多方向聚焦。  <br/>5. **具备强泛化能力**：方法在未见过的环境或条件下仍能保持性能，适应性强。  <br/>6. **实验验证优越性**：通过对比实验证明NDF在传统波束成形与参数化方法上的性能提升，证明其有效性。  <br/><br/>**总结**: 本研究提出基于深度神经网络的神经方向滤波方法，突破紧凑麦克风阵列方向性限制，实现频率不变、高阶模式拟合和多方向控制，并通过实验验证其优越性。|
|2511.07156v1|[Conditional Diffusion as Latent Constraints for Controllable Symbolic Music Generation](https://arxiv.org/abs/2511.07156v1)|**贡献点总结（100字以内）：**  <br/>本研究提出将去噪扩散过程作为插件式潜在约束，首次实现无条件符号音乐生成模型在多音乐属性（音符密度、音高范围、轮廓、节奏复杂度）上的灵活控制，实验表明该方法在保持生成质量与多样性的同时，优于传统正则化和现有潜在约束架构。  <br/><br/>**分点贡献：**  <br/>1. **引入扩散约束框架**：提出将去噪扩散过程作为插件式潜在约束，用于无条件符号音乐生成模型，实现对生成音乐的灵活控制。  <br/>2. **多属性通用性**：首次系统性地将该方法应用于多样化的音乐属性（如音符密度、音高范围、轮廓、节奏复杂度），突破了现有方法依赖单一模态的局限。  <br/>3. **隐式先验设计**：构建基于小型条件扩散模型的隐式概率先验库，作用于冻结的无条件主干网络的潜在空间，提升生成控制的精确性。  <br/>4. **性能优势**：实验表明扩散驱动约束在目标属性与生成属性的相关性、感知质量和多样性方面均优于传统正则化方法和现有潜在约束架构。|
|2511.07135v1|[Generating Novel and Realistic Speakers for Voice Conversion](https://arxiv.org/abs/2511.07135v1)|**贡献点总结：**<br/>1. 提出SpeakerVAE方法，通过深度分层变分自编码器生成全新说话人表示，突破传统VC系统需依赖目标语音数据的限制。<br/>2. 实现无需共训练或微调基础VC系统的灵活插件化设计，兼容FACodec和CosyVoice2等主流模型。<br/>3. 生成的新型说话人音色质量与训练数据相当，显著提升未见语音的转换效果。<br/>4. 构建轻量级框架，降低生成新型语音的计算成本，拓展VC在隐私保护和创意应用中的场景。|
|2511.07118v1|[On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation](https://arxiv.org/abs/2511.07118v1)|**贡献点总结：**  <br/>1. **提出控制平衡问题**：指出传统方法（如变分信息瓶颈）在平衡KLD（KL散度）与AR（属性正则化）损失时存在挑战，导致模型无法兼顾生成可控性和潜在分布的合理性。  <br/>2. **设计属性转换方法**：通过引入合适的属性变换，实现对KLD和AR损失的联合优化，解决现有模型难以同时满足两个正则化目标的局限性。  <br/>3. **验证效果**：在符号音乐生成任务中验证该方法，证明其能有效提升模型对连续音乐属性的可控性，同时保持潜在空间的结构化与正则化。  <br/><br/>**摘要总结（100字内）**：  <br/>该论文提出通过属性转换协调KLD与AR损失，解决显式潜在变量模型中正则化目标难以平衡的问题，从而提升符号音乐生成的可控性与潜在空间结构化质量。|
|2511.07099v1|[E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis](https://arxiv.org/abs/2511.07099v1)|**贡献点：**  <br/>1. 提出**E2E-VGuard框架**，首次系统性防御基于LLM的端到端语音合成与ASR驱动场景下的新型攻击。  <br/>2. 引入**编码器集成与特征提取器**，实现音色保护；设计**ASR针对性对抗样本**攻击发音。  <br/>3. 结合**心理声学模型**，确保扰动不可感知，平衡安全性与音频质量。  <br/>4. 评估覆盖**16开源合成器+3商业API**，支持中英文数据集，验证防御有效性。  <br/>5. 实现真实场景部署验证，并开放代码及演示页面。  <br/><br/>**总结（100字内）：**  <br/>该研究提出E2E-VGuard框架，针对LLM驱动的端到端语音合成及ASR场景攻击，通过音色保护、发音破坏与心理声学扰动设计，实现高效防御，并在多系统与数据集上验证其有效性，为语音安全提供新方案。|
|2511.06606v1|[SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding and Reasoning into Large Audio-Language Models](https://arxiv.org/abs/2511.06606v1)|**贡献点：**  <br/>1. **提出SPUR方法**：通过轻量架构改造，将单耳LALMs扩展为具备空间感知能力的模型，无需大幅调整原有结构。  <br/>2. **FOA编码器设计**：将FOA通道（W,X,Y,Z）映射为旋转感知、以听者为中心的空间特征，增强对方向、高度、距离等空间属性的捕捉。  <br/>3. **SPUR-Set数据集**：构建首个结合开放数据与模拟的多模态数据集，聚焦相对空间关系（方向、高度、距离、重叠），支持监督式空间推理。  <br/>4. **性能提升与兼容性**：微调后在空间问答和多说话人归因任务中表现优异，同时保留模型对通用音频的理解能力。  <br/>5. **消融实验验证**：通过系统消融证明方法有效性，为后续空间感知研究提供基础。  <br/><br/>**总结**（100字以内）：  <br/>SPUR通过轻量插件增强LALMs空间感知能力，结合FOA编码器和定制数据集，提升多说话人定位与空间问答性能，保持通用音频理解。|
|2511.06592v1|[MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592v1)|总结：该研究揭示音频大语言模型在临床决策中存在严重模态偏差，易受语音特征（如年龄、性别、情感）影响而非医疗证据，强调需构建偏见感知架构以避免医疗公平性问题。<br/><br/>贡献点：<br/>1. 首次系统评估音频LLM在临床场景的模态偏差，发现手术建议差异可达35%；<br/>2. 量化揭示年龄（12%）、性别与情感对模型决策的显著影响，尤其年龄偏差在多数模型中持续存在；<br/>3. 验证显式推理可缓解性别偏差，但情感识别能力不足导致其影响未被有效捕捉；<br/>4. 提出偏见感知架构的必要性，为临床部署音频模型提供关键风险警示。|
|2510.25235v2|[Disentangling the effects of peripheral hearing loss and higher-level processes on speech intelligibility in older adults](https://arxiv.org/abs/2510.25235v2)|总结：  <br/>该研究提出基于WHIS模拟器和GESI客观测量的方法，揭示老年听众在语音可懂度表现中的个体差异，强调高级处理能力在补偿听力损失中的关键作用。<br/><br/>贡献点：  <br/>1. **提出新方法**：开发了WHIS模拟器，用于模拟特定老年受试者的听力损失特征，以分离外周听力损失与高级处理对语音可懂度的影响。  <br/>2. **对比实验设计**：通过理想比值掩码（IRM）增强与未处理条件下的语音材料，系统评估不同听力条件下语音可懂度表现。  <br/>3. **发现个体差异**：结果表明部分老年受试者语音可懂度表现优于年轻正常听力者，反映高级处理能力的个体化差异。  <br/>4. **验证GESI有效性**：证明GESI客观测量能准确预测年轻和老年听众的语音可懂度表现，拓展其应用范围。  <br/>5. **参数预测应用**：利用年轻听众实验参数预测老年群体语音可懂度，揭示高级处理效率对表现差异的影响。  <br/>6. **建立研究框架**：构建了独立于听力水平的对比实验框架，为研究老年群体高级语音处理能力提供新工具和方法。|
|2510.12377v2|[A Phase Synthesizer for Decorrelation to Improve Acoustic Feedback Cancellation](https://arxiv.org/abs/2510.12377v2)|总结：提出结合频率偏移与相位调制的统一去相关框架，并扩展相位调制技术，应用于车内语音通信系统以提升稳定性与语音质量。<br/><br/>贡献点：<br/>1. **提出统一去相关框架**：首次将频率偏移和相位调制方法结合，构建名为"phase synthesizer"的新型去相关技术，基于DFT滤波器银行实现。<br/>2. **扩展相位调制技术**：引入可变延迟线机制改进传统相位调制，借鉴音效处理中的vibrato和chorus技术提升方法灵活性。<br/>3. **系统验证与性能提升**：在车内语音通信场景中验证该方法，通过自适应频域卡尔曼滤波器实现系统稳定性增强和PESQ语音质量指标提升。|
|2510.04584v1|[Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584v1)|**贡献点**  <br/>1. 系统评估了现有 MCQA 框架在三大基准（MMAU、MMAR、MMSU）上的鲁棒性，揭示模型对选项顺序、问题与选项的同义改写均极为敏感。  <br/>2. 对四种主流 LALM（Audio Flamingo 2/3、Qwen2.5‑Omni‑7B‑Instruct、Kimi‑Audio‑7B‑Instruct）进行实验，验证上述敏感性具有模型普遍性。  <br/>3. 归纳出 MCQA 评估中隐藏的变量对结果产生的偏差，指出单一准确率指标的局限性。  <br/>4. 提出更简洁、稳健的评估协议和度量方式，能够同时考虑选项顺序与文本改写的影响，提供更细致、可解释的 LALM 评估报告。  <br/><br/>**100字以内总结**  <br/>研究表明现有 MCQA 评估对选项顺序、题目及选项改写极为敏感，提出更稳健的评估协议和指标，实现对大型音频语言模型更细致的评估。|
|2510.01157v2|[Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157v2)|**贡献点总结（100字内）：**  <br/>1. 提出首个系统性研究语音模型中音频后门攻击的框架。  <br/>2. 证实攻击在多编码器（4种）和数据集（3种）上的高成功率（90.76%-99.41%）。  <br/>3. 通过组件分析定位模型中最易受攻击的阶段。  <br/>4. 提出基于微调的防御策略以缓解中毒预训练编码器的威胁。|
|2509.23238v3|[WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms](https://arxiv.org/abs/2509.23238v3)|总结：提出WavJEPA及改进版WavJEPA-Nat，实现高效、鲁棒的原始波形通用音频表征学习，突破传统频谱图方法的局限性。<br/><br/>贡献点：<br/>1. **创新方法**：提出WavJEPA，首次将联合嵌入预测架构应用于波形域，解决频谱图方法存在的计算延迟和相位信息丢失问题；<br/>2. **性能优势**：在多种下游任务中显著超越现有时域音频基础模型，且提升幅度远超计算资源消耗；<br/>3. **鲁棒性增强**：开发WavJEPA-Nat（多通道扩展），通过模拟自然场景训练，显著提升模型在噪声/混响环境下的鲁棒性；<br/>4. **应用潜力**：验证原始波形表征方法在通用音频任务中的可行性，为低延迟、高效率的时域音频基础模型提供新范式。|
|2509.22651v1|[VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651v1)|**本文的主要贡献**  <br/><br/>1. **提出全新基准 VoiceAssistant‑Eval**  <br/>   - 首个覆盖“听、说、看”三大交互模态的综合评测框架。  <br/>   - 包含 10,497 条精挑细选的样例，覆盖 13 类任务（自然声、音乐、口语对话、多轮交互、角色扮演、图像视觉等）。  <br/><br/>2. **系统量化评估 21 种开源模型与 GPT‑4o‑Audio**  <br/>   - 同时测量回答内容质量、语音输出质量以及多模态一致性。  <br/>   - 首次给出公开可比的“听力准确率”“说话能力”“视听融合”指标。  <br/><br/>3. **揭示模型能力格局与关键发现**  <br/>   - 专有模型并非在所有任务上均优于开源模型。  <br/>   - 大多数模型在说话任务表现突出，但在音频理解上仍显薄弱。  <br/>   - 结构合理的中小模型（如 Step‑Audio‑2‑mini，7B）可在听力准确率上超越更大模型（如 LLaMA‑Omni2‑32B‑Bilingual）。  <br/><br/>4. **辨识当前技术瓶颈**  <br/>   - 多模态（音+图）输入、角色声线模仿等任务仍难以突破。  <br/>   - 稳健性与安全对齐仍存在显著缺口，需进一步研究。  <br/><br/>5. **公开代码与数据**  <br/>   - 在 https://mathllm.github.io/VoiceAssistantEval/ 提供完整基准实现，便于社区复现与扩展。  <br/><br/>---<br/><br/>**100字以内总结**  <br/>本文推出首个覆盖听、说、看三模态的 VoiceAssistant‑Eval 基准，提供 10 k+ 示例与 13 类任务，系统评测 21 种开源模型和 GPT‑4o‑Audio。实验发现专有模型并非全胜，模型在说话表现佳而音频理解不足；小模型亦能逼近大模型。基准揭示多模态输入、角色模仿等难点，用于指导下一代 AI 助手研发。代码、数据已公开。|
|2509.15654v2|[EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model for Generalized Speech Emotion Recognition](https://arxiv.org/abs/2509.15654v2)|**贡献点**  <br/>1. 提出 **EMO‑RL** 框架，首次将强化学习系统化地引入大规模音频‑语言模型（LALM）用于情绪识别。  <br/>2. 设计 **Emotion Similarity‑Weighted Reward (ESWR)**，依据情绪之间的相似度对奖励进行加权，缓解情绪边界模糊导致的收敛不稳定。  <br/>3. 引入 **Explicit Structured Reasoning (ESR)** 模块，在相对较小（7B 参数）模型上显式构建情绪推理结构，提升推理能力。  <br/>4. 采用 **group‑relative policy optimization** 结合情绪约束，实现高效的策略更新与多情绪平衡。  <br/>5 **实验验证**：在 MELD 与 IEMOCAP 上取得 SOTA 表现；跨数据集实验显示出显著的迁移与泛化能力。<br/><br/>**100字以内总结**  <br/>EMO‑RL 通过情绪相似加权奖励与显式结构化推理，稳健提升大规模音频语言模型在情感计算中的识别与推理能力，在多个情绪数据集上实现新最佳并展现强泛化。|