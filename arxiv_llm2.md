|Source|Title|Summary|
|---|---|---|
|2509.19755v1|[Can Audio Large Language Models Verify Speaker Identity?](http://arxiv.org/abs/2509.19755v1)|总结：  <br/>本文提出将说话人验证转化为音频问答任务，设计硬对样本采样策略与轻量微调方法，提升ALLMs的零样本SV性能，并成功应用于文本相关SV任务，验证其作为统一鲁棒模型的潜力。<br/><br/>贡献点：  <br/>1. **任务重构**：首创将说话人验证（SV）框架重构为音频问答任务，拓展ALLMs在语音领域的应用范式。  <br/>2. **硬对采样策略**：提出基于规则的硬样本对生成方法，增强训练数据挑战性以提升模型鲁棒性。  <br/>3. **轻量微调优化**：通过监督微调显著改善ALLMs的零样本SV性能，但与传统模型仍存在性能差距。  <br/>4. **文本相关SV应用**：首次探索文本依赖SV，联合验证身份与语音内容，效果接近级联ASR-SV系统。  <br/>5. **统一模型潜力**：论证ALLMs经适配后可作为通用鲁棒SV模型，兼具音频理解与身份验证能力。|
|2509.19745v1|[PART: Progressive Alignment Representation Training for Multilingual   Speech-To-Text with LLMs](http://arxiv.org/abs/2509.19745v1)|**贡献点：**  <br/>1. **提出PART框架**：设计了一个多阶段、多任务的框架，首次将同语言和跨语言对齐分离，解决多语言语音与文本表示对齐的挑战。  <br/>2. **动态参数激活**：在跨语言训练阶段动态激活LLM参数，避免传统方法中冻结参数导致的跨语言收敛问题，提升模型灵活性。  <br/>3. **文本任务增强**：通过分阶段引入文本相关任务（如翻译、理解），进一步强化多语言能力，优化语音模态的跨语言泛化与语言特异性。  <br/>4. **实验验证有效性**：在CommonVoice 15、Fleurs、Wenetspeech和CoVoST2等多语言数据集上验证，证明PART在性能和平衡性上优于现有方法。  <br/>5. **通用性与扩展性**：方法适用于多种语音模态对齐任务，展示了其在多语言场景下的广泛应用潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出PART框架，通过动态激活参数和分阶段任务分离，有效解决多语言语音与文本对齐问题，实验验证其性能优于传统方法，具备语言特异性与跨语言泛化能力，适用于多种语音模态任务。|
|2509.19676v1|[Thinking While Listening: Simple Test Time Scaling For Audio   Classification](http://arxiv.org/abs/2509.19676v1)|总结（100字以内）：  <br/>提出"听中思"框架提升音频分类性能，解决思考机制集成和新架构设计两个核心问题，验证测试时扩展的有效性，并发现轻量级方法可超越大参数文本推理模型。<br/><br/>贡献点：  <br/>1. 提出"思考-听"（think-while-listening）框架，将推理能力引入音频分类流程以提升性能  <br/>2. 分析并解决两个核心问题：现有模型的推理机制改进与全新架构的端到端设计  <br/>3. 证明测试时扩展（test-time scaling）能带来持续准确率提升，样本数量增加时效果更显著  <br/>4. 通过对比实验发现：仅重训练小模型嵌入矩阵的轻量方法可超越大参数文本推理模型（如GPT-2 vs. GPT-OSS-20B/Qwen3-14B）的零样本推理表现|
|2509.19592v1|[Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech   Generation](http://arxiv.org/abs/2509.19592v1)|总结：  <br/>该研究提出两种基于局部Transformer的架构，支持高效帧堆叠解码，并系统分析了并行与迭代策略的权衡，为语音生成模型的部署提供实用选择指南。<br/><br/>贡献点：  <br/>1. 提出两种LT架构：自回归Transformer（逐帧生成代码本）与MaskGIT-based Transformer（迭代掩码预测），解决多代码本依赖问题。  <br/>2. 引入帧堆叠机制，使主Transformer预测多帧联合，结合LT解码代码本，在保持感知质量前提下提升生成速度。  <br/>3. 系统分析不同吞吐量和质量需求下，平行与迭代采样策略的性能权衡，揭示其适用场景。  <br/>4. 提出基于部署优先级（如计算效率与合成保真度）的实用解码策略选择指南，指导实际应用优化。|
|2509.19469v1|[MusiCRS: Benchmarking Audio-Centric Conversational Recommendation](http://arxiv.org/abs/2509.19469v1)|总结：  <br/>本文提出首个音乐领域对话推荐基准MusiCRS，整合音频与对话数据，支持多模态评估，揭示系统在音频推理上的局限，推动跨模态知识整合的进展。<br/><br/>贡献点：  <br/>1. **首个音频重心基准数据集**：MusiCRS是首个将真实Reddit用户对话与对应音频轨道关联的对话推荐数据集，填补音乐领域的研究空白。  <br/>2. **多模态多样性**：涵盖477个高质量对话（涉及古典、嘻哈、电子等多音乐流派）和3,589个音乐实体，提供YouTube音频链接以支持音频内容验证。  <br/>3. **系统性评估框架**：支持音频-only、query-only及音频+query（多模态）三种输入模态配置，便于对比音频大模型、检索框架与传统方法的性能。  <br/>4. **揭示核心挑战**：实验表明现有系统过度依赖文本信号，无法有效处理音频内容的抽象推理，指出现有跨模态知识整合的理论瓶颈。  <br/>5. **开放资源支持**：提供数据集、评估代码与基线，促进后续研究与模型开发。|
|2509.19001v1|[HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS](http://arxiv.org/abs/2509.19001v1)|**贡献点：**  <br/>1. 提出HD-PPT框架，将语音合成转化为结构化、分层的任务，解决单级文本指令与多级语音标记间的模态控制难题。  <br/>2. 设计新型语音编解码器，通过ASR和CLAP目标监督，提取细粒度的提示偏好和内容偏好语音标记。  <br/>3. 引入分层解码策略，按语义、细粒度风格、声学表示顺序生成语音标记，实现可控性与自然度的平衡。  <br/>4. 实验证明该方法在指令遵循度和语音自然度上达到SOTA，验证了精确可控语音合成的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HD-PPT框架，通过分层解码和新型语音编解码器解决TTS精确控制问题，提升指令遵循度与语音自然度，为可控语音合成提供新方法。|
|2509.18700v1|[Enhancing Automatic Chord Recognition through LLM Chain-of-Thought   Reasoning](http://arxiv.org/abs/2509.18700v1)|总结：  <br/>本文提出利用大语言模型整合多MIR工具的创新框架，通过文本化音频信息提升和弦识别性能，在多个数据集上实现1-2.77%的准确率提升，为音乐信息检索的多工具协作开辟新方向。<br/><br/>贡献点：  <br/>1. **提出LLM作为MIR工具整合桥梁**：首次将语言模型应用于连接音乐源分离、调识别、和弦识别等不同MIR模块，实现跨技术协同。  <br/>2. **文本化音频信息处理流程**：将音频衍生的音乐信息转化为文本，使LLM具备推理和修正能力，提升和弦识别任务的准确性。  <br/>3. **五阶段链式思维框架**：设计系统化的5阶段流程，利用音乐理论知识整合多MIR工具输出，优化和弦识别结果。  <br/>4. **实验证明有效性**：在三个数据集上验证方法，显示在MIREX等指标上实现1-2.77%的整体准确率提升。  <br/>5. **推动多工具协作研究**：为音乐信息检索任务中的多工具集成与协同提供新思路，拓展LLM在音频分析的应用场景。|
|2509.18606v1|[FlexSED: Towards Open-Vocabulary Sound Event Detection](http://arxiv.org/abs/2509.18606v1)|总结：  <br/>本文提出FlexSED，首个开放词汇声事件检测系统，结合SSL音频模型与CLAP文本编码器，通过编码器-解码器结构和自适应融合策略实现连续训练，利用LLM优化标签选择，有效解决零样本/少样本检测与文本查询问题，并开源代码模型支持研究应用。<br/><br/>贡献点：  <br/>1. **开放词汇SED系统**：首次构建支持自由文本查询的开放词汇声事件检测框架，突破传统多类分类的限制。  <br/>2. **跨模态融合架构**：集成预训练音频SSL模型与CLAP文本编码器，采用编码器-解码器结构及自适应融合策略，提升多模态信息处理能力。  <br/>3. **连续训练机制**：设计从预训练权重进行高效微调的连续训练框架，增强模型对未见类别的泛化能力。  <br/>4. **LLM辅助标签选择**：引入大语言模型解决训练数据标签缺失问题，优化事件查询生成与监督策略。  <br/>5. **强零样本/少样本能力**：在AudioSet-Strong数据集上验证，显著优于传统模型，同时具备灵活的文本查询与小样本适配性。  <br/>6. **开源促进应用**：公开代码与预训练模型，推动开放词汇SED技术在实际场景中的研究与落地。|
|2509.18570v1|[HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for   Multi-Task Speech Language Modeling](http://arxiv.org/abs/2509.18570v1)|**贡献点总结（100字以内）**：  <br/>提出HarmoniFuse框架，通过组件选择和提示适应机制，解决多任务语音模型的任务干扰问题，实现异构任务需求的协同处理，并引入批处理交错训练策略，提升ASR与SER性能，为真实数据约束下的多任务语音理解提供可扩展解决方案。<br/><br/>**分点贡献**：  <br/>1. **提出HarmoniFuse框架**：首次设计组件选择性和提示适应性机制，通过动态融合任务相关语音表征，有效缓解多任务间的干扰与性能退化问题。  <br/>2. **任务感知的组件选择**：采用门控语音编码器提取任务特定的声学特征，精准区分ASR与SER所需的不同信息类型（语言内容 vs. 语言情感线索）。  <br/>3. **动态融合模块**：基于任务特征自适应聚合Transformer层，实现模型对多任务需求的灵活响应与协同建模。  <br/>4. **批处理交错训练策略**：无需联合标注，通过分离ASR与SER数据集的训练方式提升模型泛化能力，降低数据标注成本。  <br/>5. **实验验证有效性**：在真实数据约束下验证框架的提升效果，证明其在多任务语音理解中的可扩展性与鲁棒性。|
|2509.18569v1|[Explore the Reinforcement Learning for the LLM based ASR and TTS system](http://arxiv.org/abs/2509.18569v1)|总结：本研究提出轻量级RL框架并验证其在语音模型中的效果，通过奖励函数设计与数据构建优化提升ASR和TTS性能，结合GRPO与DiffRO方法实现突破，证明RL在小数据场景下仍有显著优势。<br/><br/>贡献点：<br/>1. 提出首个针对音频处理的轻量级强化学习框架，适配音频输入-输出的LLMs<br/>2. 首次系统验证RL在ASR任务中的有效性，探索规则奖励函数与数据构建策略<br/>3. 对比GRPO与DiffRO方法，创新性融合两者优势提升TTS性能<br/>4. 验证RL在有限训练数据和优化步数下的泛化能力<br/>5. 提供音频领域RL应用的实践范式，填补该方向研究空白|
|2509.17901v1|[Does Audio Matter for Modern Video-LLMs and Their Benchmarks?](http://arxiv.org/abs/2509.17901v1)|贡献点总结（100字以内）:  <br/>揭示视频理解中音频的冗余性，提出音频编码器与轻量级压缩技术，设计新数据集推动评估改进，指出学术研究与实际需求的差距，并开源代码促进研究发展。  <br/><br/>分点贡献：  <br/>1. **音频冗余性分析**：发现多数现有视频理解任务的基准测试可通过单帧图像解决，音频信息在当前模型评价中作用有限。  <br/>2. **音频处理优化**：基于LLaVA-OneVision架构，集成Whisper音频编码器，并采用Mamba-based压缩技术解决音频token爆炸问题。  <br/>3. **新数据集与工具发布**：推出AVQA-Hard和Music-AVQA-Hard两个更具挑战性的数据集，并开源模型及代码，推动更可信的音频-视觉模型评估。  <br/>4. **学术与实际需求的差距**：揭示学术界对音频-视觉模型的评估方式与实际应用场景的不匹配，提供针对性改进工具。|
|2509.16990v1|[Advancing Speech Understanding in Speech-Aware Language Models with GRPO](http://arxiv.org/abs/2509.16990v1)|总结（100字以内）:  <br/>本文提出基于GRPO的SALLM训练方法，应用于开放格式语音理解任务。通过BLEU奖励信号优化模型，实验证明效果优于标准SFT，并探索离策略样本的整合以提升性能。<br/><br/>贡献点：  <br/>1. **方法创新**：提出Group Relative Policy Optimization (GRPO)框架用于训练Speech-Aware Large Language Models (SALLMs)。  <br/>2. **任务扩展**：将GRPO从多选任务延伸至开放格式任务（如语音问答、语音翻译），更贴合生成能力评估。  <br/>3. **奖励机制**：首次将BLEU作为奖励信号，优化模型生成质量。  <br/>4. **实证验证**：通过实验表明该方法在关键指标上显著优于标准监督微调（SFT）。  <br/>5. **研究方向**：探索离策略样本在GRPO中的潜在应用，为后续改进提供新思路。|
|2509.16975v1|[Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs](http://arxiv.org/abs/2509.16975v1)|总结：  <br/>提出首个基于自然语言的音频编辑评估框架，结合多模态大语言模型的微调与提示技术，实现更准确、可解释的评价，同时开源代码和演示。<br/><br/>贡献点：<br/>1. **首个自然语言驱动框架**：构建了首个利用自然语言描述进行音频编辑评估的自动化框架，替代传统客观指标，提升感知分析深度。  <br/>2. **双任务微调与推理增强**：引入两个微调任务结合Chain-of-Thought提示与轻量级指令调整，显著提升多音频理解与逐步推理能力。  <br/>3. **性能优势验证**：实验表明框架在准确性、可解释性上优于基线方法，且与主观评测和客观指标高度一致。  <br/>4. **开源实现**：提供代码和演示，便于复现与实际应用，推动相关领域的研究与技术落地。|
|2509.16971v1|[AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for   Coarse-to-Fine Audio Deep Reasoning](http://arxiv.org/abs/2509.16971v1)|总结：提出首个无需训练的多智能体音频推理系统AGR，通过文本化文档与迭代优化机制提升性能，实现SOTA结果并开源代码。<br/><br/>贡献点：<br/>1. **首个统一训练-free多智能体系统**：构建了首个无需额外训练的音频深度推理框架AGR，实现感知与推理的协同处理。<br/>2. **范式转换**：创新性地将音频深度推理转化为复杂文本理解任务，挖掘大语言模型的潜力。<br/>3. **人类认知模拟**：采用"粗到细"分层推理范式，通过音频→文本文档的初步转化和迭代优化机制模拟人类认知过程。<br/>4. **主动探索机制**：设计工具增强的文档优化环路，结合专用智能体实现信息缺失检测与证据链增强。<br/>5. **性能突破**：在多个基准测试中取得SOTA效果，验证了方法的有效性和先进性。<br/>6. **开源贡献**：提供完整代码库促进研究复现与技术发展。|
|2509.16649v1|[AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval](http://arxiv.org/abs/2509.16649v1)|总结：本文提出基于双编码器和对比学习的音频-文本检索系统，结合知识蒸馏与大语言模型数据增强技术，引入聚类辅助任务提升性能，最终在Clotho数据集上取得46.62%和48.83%的mAP@16成绩。<br/><br/>贡献点：  <br/>1. 提出双编码器架构，通过对比学习对齐音频与文本表征。  <br/>2. 引入知识蒸馏与大语言模型（LLM）驱动的数据增强策略（包括回译与LLM mix）。  <br/>3. 设计聚类辅助分类任务，优化模型微调效果。  <br/>4. 在Clotho开发测试集上实现单系统46.62%、四系统集成48.83%的mAP@16性能指标。|
|2509.16622v1|[Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](http://arxiv.org/abs/2509.16622v1)|总结：  <br/>本研究通过引入扩散模型LLaDA用于语音识别，提出三种掩码策略并验证音频嵌入的重要性，展示了其在LibriSpeech数据集上显著降低WER的效果，同时探讨了扩散模型在ASR中的优化潜力。<br/><br/>贡献点：  <br/>1. **比较DLLMs与传统方法**：验证扩散模型LLaDA在ASR任务中相比自回归解码器（如Whisper-LLaMA）的竞争力，尤其在WER指标上表现突出。  <br/>2. **提出三种掩码策略**：探索随机掩码、低置信度掩码和半自回归策略，证明LLaDA在提升Whisper-LLaMA转录质量方面的作用。  <br/>3. **强调音频条件嵌入关键性**：通过对比纯文本LLaDA与音频条件模型，证明声学特征对模型性能的直接影响，凸显音频嵌入的重要性。  <br/>4. **评估独立解码器性能**：测试LLaDA作为独立ASR解码器的效果，显示其在推理速度上的优势，尽管识别准确率略低于Whisper-LLaMA。  <br/>5. **提供实证改进方向**：为扩散模型在语音识别中的应用提供实验依据，揭示其潜在优化路径，如结合扩散与半自回归解码技术。|
|2509.15775v1|[EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large   Language Model](http://arxiv.org/abs/2509.15775v1)|总结：本文提出EmoQ框架，通过EmoQ-Former和MAL实现多模态情感融合与协同优化，结合软提示注入技术提升性能，并在IEMOCAP和MELD数据集上达到SOTA，为SER提供新范式。<br/><br/>贡献点：  <br/>1. 提出EmoQ框架：融合单模态情感信息与多模态特征对齐问题，构建端到端多模态情感识别系统。  <br/>2. 引入EmoQ-Former模块：生成跨模态查询嵌入，实现多模态信息的有效融合。  <br/>3. 设计多目标情感学习（MAL）策略：通过协同优化提升模型对复杂情感的判别能力。  <br/>4. 提出软提示注入机制：将多模态表征动态融入大语言模型，增强情感推理的准确性。  <br/>5. 验证有效性：在IEMOCAP和MELD数据集上取得当前最优性能（SOTA），证明框架优势。  <br/>6. 建立新范式：为语音情感识别领域的多模态融合提供理论与技术参考。|
|2509.15680v1|[Mamba-2 audio captioning: design space exploration and analysis](http://arxiv.org/abs/2509.15680v1)|总结：  <br/>本研究提出基于Mamba-2的音频描述模型，系统探索了LLM规模、LoRA秩和连接器设计，实现参数更少但性能优于大模型，并首次深入分析多个因素对音频captioning的影响。<br/><br/>贡献点：  <br/>1. 构建首个基于Mamba-2大语言模型的音频captioning系统，利用其SOTA状态空间模型结构；  <br/>2. 系统性研究LLM参数规模、LoRA秩和连接器设计，结合Mamba-2的线性时间复杂度优势；  <br/>3. 实现参数更少但性能更强的音频描述效果，超越同数据集上训练的更大模型；  <br/>4. 首次全面分析LLM参数数量、音频编码器微调策略、特征多样性及降/扩技术对性能的影响。|
|2509.15667v1|[VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](http://arxiv.org/abs/2509.15667v1)|**贡献点：**  <br/>1. 提出首个将预训练解码器型LLM与声学编码器-解码器架构（如Whisper）融合的框架，构建语音增强的大语言模型。  <br/>2. 引入**音频条件文本空间**作为中间对齐机制，替代直接使用音频嵌入，提高多模态表示对齐效果。  <br/>3. 实现完全基于连续文本表示空间的融合方法，通过跨模态注意力机制结合Whisper与LLM隐藏状态。  <br/>4. 支持**离线和实时流式模式**，扩展模型的应用场景。  <br/>5. 首次发布希腊语音LLM**VoxKrikri**，并在希腊ASR任务中达到SOTA性能，平均提升20%相对基准。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出多模态融合框架，通过音频条件文本空间实现跨模态对齐，支持离线与流式处理，构建首个希腊语音LLM（VoxKrikri），并在希腊ASR任务中取得SOTA性能，平均提升20%。|
|2509.15654v2|[EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced   Audio-Language Model for Generalized Speech Emotion Recognition](http://arxiv.org/abs/2509.15654v2)|总结：  <br/>本研究提出EMO-RL框架，通过创新的强化学习策略解决语音情感识别中的收敛不稳定和模型规模限制问题，实现情感推理能力的显著提升，并在多个数据集上取得SOTA结果，验证了其优秀的泛化能力。<br/><br/>贡献点：  <br/>1. **提出EMO-RL框架**：首次结合强化学习与语音情感识别，解决因情感边界模糊导致的收敛不稳定性和小模型推理能力不足的双重挑战。  <br/>2. **创新性模块设计**：引入**情感相似性加权奖励（ESWR）**和**显式结构化推理（ESR）**，通过情感约束优化提升模型对情感信号的敏感性和推理准确性。  <br/>3. **基于预训练模型的优化方法**：采用**组相对策略优化**，将情感约束融入策略更新过程，增强模型在复杂情感场景下的适应性。  <br/>4. **验证效果与泛化能力**：在MELD和IEMOCAP数据集上达到当前最优性能，并通过跨数据集实验证明了方法的强泛化能力。|
|2509.15654v1|[EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced   Audio-Language Model for Generalized Speech Emotion Recognition](http://arxiv.org/abs/2509.15654v1)|贡献点分点总结：<br/>1. 提出EMO-RL框架，将强化学习与情感建模结合，解决LALMs在情感计算中的性能瓶颈<br/>2. 创新设计情绪相似度加权奖励机制（ESWR）和显式结构化推理模块（ESR）<br/>3. 建立基于预训练LALMs的组相对策略优化方法，通过情绪约束提升模型稳定性<br/>4. 在MELD和IEMOCAP数据集取得SOTA效果，验证跨数据集泛化能力显著优于现有方法<br/><br/>总结（99字）：本研究提出EMO-RL框架，通过ESWR和ESR创新技术提升LALMs在情感识别与推理中的性能，解决收敛不稳和小模型局限问题，在多个数据集上取得最佳效果并展现强泛化能力。|
|2509.15253v1|[Emotion-Aware Speech Generation with Character-Specific Voices for   Comics](http://arxiv.org/abs/2509.15253v1)|总结：  <br/>本文提出了一种端到端的漫画语音生成系统，融合图像处理、大语言模型和TTS技术，实现角色特定且情感感知的语音合成，推动漫画的自动化语音生成与沉浸式阅读体验。<br/><br/>贡献点：  <br/>1. **端到端语音生成框架**：首次构建从漫画全卷到角色情感语音的完整生成流程，无需人工标注对话与情感信息。  <br/>2. **多模态信息整合**：通过图像处理模块提取角色身份和情感强度，结合大语言模型分析剧情上下文进行对话归属与情感分析。  <br/>3. **个性化语音合成**：采用定制化语音配置，根据角色特征及情感状态生成对应声音，增强语音表现力。  <br/>4. **自动化语音覆盖**：实现漫画语音的自动生成，为互动式阅读体验提供技术支撑。|
|2509.14804v1|[Towards Building Speech Large Language Models for Multitask   Understanding in Low-Resource Languages](http://arxiv.org/abs/2509.14804v1)|总结：本文提出首个针对泰语的自监督语音编码器XLSR-Thai、高效多任务对齐方法U-Align及跨语言数据生成管道Thai-SUP，有效解决泰语低资源下的语音语言模型性能问题，并开源相关成果以推动研究。<br/><br/>贡献点：<br/>1. XLSR-Thai：首个基于自监督学习的泰语语音编码器，通过36,000小时泰语数据训练，突破现有编码器在低资源语言上的局限。<br/>2. U-Align：提出更资源高效且支持多任务的语音-文本对齐方法，替代传统ASR对齐范式，降低训练成本。<br/>3. Thai-SUP：构建首个超1,000小时的泰语语音理解数据集，通过高资源语言数据生成实现低资源语言数据扩充。<br/>4. 综合验证：通过多项实验验证方法有效性，推动泰语语音语言模型的多任务理解能力提升。<br/>5. 开源贡献：公开XLSR-Thai和Thai-SUP，为低资源语言语音研究提供基准与工具。|
|2509.14666v1|[Spatial Audio Motion Understanding and Reasoning](http://arxiv.org/abs/2509.14666v1)|总结：  <br/>该论文提出基于空间音频编码器的多事件检测与定位方法，结合音频定位模型与大语言模型实现动态音频场景推理，并构建首个空间音频运动理解基准数据集，验证了框架的有效性。<br/><br/>贡献点：  <br/>1. **提出空间音频编码器**：实现多声源事件检测及帧级方向（DoA）与距离估计。  <br/>2. **设计跨模态音频定位模型**：通过交叉注意力机制对齐音频特征与语义文本嵌入，提升对未知事件的泛化能力。  <br/>3. **引入LLM空间推理框架**：利用结构化空间属性作为条件，支持复杂动态音频场景的查询解答。  <br/>4. **构建基准数据集**：首次提出用于评估空间音频运动理解与推理的专用数据集，并与基线模型对比验证性能。|
|2509.14270v1|[SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation   Pipeline for Training Text to Speech Models](http://arxiv.org/abs/2509.14270v1)|贡献点总结（100字以内）:<br/>提出SpeechWeave方法，解决TTS数据获取难题，提升数据多样性、文本规范化正确率及语音一致性，实现可扩展的高质量合成数据生成。<br/><br/>分点贡献：<br/>1. 构建多语言、领域特定的合成语音数据生成系统SpeechWeave，解决真实数据获取困难问题<br/>2. 通过创新生成机制提升数据多样性，实验显示在10-48%的 linguistic/phonetic指标上优于基准<br/>3. 实现97%的文本规范化正确率，避免现有工具异常和模式遗漏问题<br/>4. 保证语音标准化一致性，同时具备大规模数据生成的可扩展性能力，适用于商业TTS系统|
|2509.13927v1|[DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models](http://arxiv.org/abs/2509.13927v1)|总结：  <br/>提出DSpAST，通过解缠绕表示学习实现高效空间语音编码，仅增加0.2%参数，在空间音频推理任务中显著提升性能，优于SpatialAST。<br/><br/>贡献点：  <br/>1. **提出DSpAST模型**：基于SpatialAST，设计了能够学习解缠绕空间音频表示的编码器，提升多任务处理能力。  <br/>2. **参数效率优化**：仅需0.2%额外参数，显著降低计算资源需求，保持模型轻量化。  <br/>3. **多模态信息解耦**：有效分离声音事件类型、方向和距离等独立信息，增强任务针对性。  <br/>4. **实验验证性能提升**：在SpatialSoundQA数据集上与BAT系统结合，证明DSpAST在空间音频推理中的优越性。  <br/>5. **推动语音与语言模型融合**：为大语言模型处理空间音频任务提供高效、通用的前端编码器方案。|
|2509.13145v1|[UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System   Based on Multimodal Large Language Model](http://arxiv.org/abs/2509.13145v1)|总结：<br/>该论文提出基于多模态大语言模型的语音康复系统，创新性整合超声舌成像与语音信号，构建高质量领域数据集，并设计时空融合训练策略，实现精准的发音障碍分析与可操作反馈生成。<br/><br/>贡献点：<br/>1. **提出多模态语音康复系统**：融合超声舌成像与语音信号，实现实时、交互式的发音运动反馈，突破传统方法在实时性与反馈精度的局限。<br/>2. **构建高质量领域数据集**：创建包含UTI-语音对话对的高精度数据集，支持模型的临床适配性优化与细粒度分析。<br/>3. **设计时空融合训练策略**：提出将超声视频与语音信号进行联合时空对齐的训练方法，提升模型对发音障碍的识别与反馈生成能力。|
|2509.12508v2|[FunAudio-ASR Technical Report](http://arxiv.org/abs/2509.12508v2)|总结：  <br/>本文提出FunAudio-ASR系统，通过整合大规模数据、大模型容量、LLMs协同与强化学习，解决LLMs幻觉问题，实现跨复杂场景的SOTA性能，并针对实际部署优化流式处理、噪声鲁棒性、代码切换等能力。<br/><br/>贡献点：  <br/>1. **提出面向复杂场景的LLM-based ASR系统**：设计FunAudio-ASR，结合数据扩展、模型容量提升、LLMs深度整合与强化学习，突破传统ASR架构限制，实现多样化场景下的SOTA表现。  <br/>2. **针对性部署优化**：增强流式处理、噪声鲁棒性、代码切换、热词定制等能力，满足工业级应用需求（如实时性、抗干扰性、多语言支持）。  <br/>3. **解决LLMs幻觉问题**：通过系统性改进（如强化学习与数据协同），减少LLMs在ASR中的幻觉现象，提升实际场景的用户体验与可靠性。  <br/>4. **验证实际性能优势**：实验表明，FunAudio-ASR在真实行业数据集上表现显著优于开源基准上的LLM-based ASR系统，证明其在工业场景的适用性。|
|2509.12508v1|[FunAudio-ASR Technical Report](http://arxiv.org/abs/2509.12508v1)|**贡献点分点列表：**  <br/>1. **提出FunAudio-ASR系统**：首个大规模结合数据扩展、模型容量增强、LLM集成和强化学习的ASR框架，实现复杂场景下的SOTA性能。  <br/>2. **解决LLM幻觉问题**：通过强化学习优化，减少LLM在ASR中的幻觉现象，提升实际应用的用户体验。  <br/>3. **增强实际部署能力**：针对流式处理、噪声鲁棒性、代码切换、热词自定义等工业需求进行针对性优化，满足真实场景要求。  <br/>4. **验证行业适用性**：在真实应用数据集上表现优于开源基准，证明系统在实际场景中的有效性与鲁棒性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出FunAudio-ASR系统，结合大规模数据、LLM和强化学习，优化流式、噪声等工业需求，解决LLM幻觉问题，在真实应用数据集上实现SOTA性能，验证其在复杂场景下的有效性与实用性。|
|2509.11124v1|[STASE: A spatialized text-to-audio synthesis engine for music generation](http://arxiv.org/abs/2509.11124v1)|**贡献点：**  <br/>1. 提出STASE系统，支持用户自定义空间属性的文本到音频生成，突破传统单声道/固定立体声限制。  <br/>2. 引入LLM作为代理，直接解析文本中的空间线索（如方位、距离等），替代依赖潜在空间操控的方法。  <br/>3. 设计解耦架构：将语义理解与物理空间渲染分离，提升空间生成的可解释性和用户可控性。  <br/>4. 构建双路径处理机制：Description Prompts显式映射空间信息，Abstract Prompts结合RAG模块检索空间模板。  <br/>5. 系统化讨论STASE实现细节与当前生成空间音频评估中的挑战，推动领域研究。  <br/><br/>**总结（100字以内）：**  <br/>STASE通过LLM解析文本空间线索，解耦语义理解与物理渲染，支持用户自定义空间属性，结合RAG模块增强灵活性，为生成空间音频提供新方法并探讨评估挑战。|
|2509.09685v1|[TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal   Conversational Music Recommendation](http://arxiv.org/abs/2509.09685v1)|总结：  <br/>TalkPlayData 2是一款合成多模态对话音乐推荐数据集，通过多代理协作生成，支持音频与图像多模态交互，有效提升了生成推荐模型的训练效果。<br/><br/>贡献点：  <br/>1. **提出多代理数据生成框架**：构建由不同角色和功能的LLM代理组成的系统，通过定制提示和信息分层访问生成多样化对话数据。  <br/>2. **实现多模态对话模拟**：所有LLM代理均具备处理音频和图像的能力，支持多模态音乐推荐场景的真实交互模拟。  <br/>3. **动态对话目标定向生成**：通过微调Listener LLM的对话目标，覆盖更广泛的推荐场景，提升数据集的场景适应性。  <br/>4. **开源与评估验证**：公开数据集及生成代码，并通过LLM评估和主观实验验证其在音乐推荐模型训练中的有效性。|
|2509.09550v2|[Finite Scalar Quantization Enables Redundant and Transmission-Robust   Neural Audio Compression at Low Bit-rates](http://arxiv.org/abs/2509.09550v2)|**贡献点总结:**  <br/>1. 提出NeuCodec，首个基于Finite Scalar Quantization (FSQ)的神经音频编解码器，替代传统Residual Vector Quantization (RVQ)方法。  <br/>2. 通过编码器蒸馏实验证明，不同编码器可生成差异显著的码序列，同时保持相同的量化器、解码器和重建质量，展示编码鲁棒性。  <br/>3. 比较FSQ与RVQ在噪声信道下的抗扰动能力，证实FSQ在比特级扰动鲁棒性上具有显著优势。  <br/><br/>**研究摘要（100字以内）:**  <br/>该论文提出基于FSQ的NeuCodec，首次揭示编码器蒸馏中不同编码器的码序列差异性，同时证明FSQ在噪声传输下的抗扰动能力优于RVQ，为语音处理提供了更高效的编码方案。|
|2509.09174v1|[EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for   Speech-to-Speech LLMs](http://arxiv.org/abs/2509.09174v1)|总结：  <br/>提出EchoX模型，通过整合声学与语义学习，解决SLLMs的推理能力下降问题，验证其在知识问答任务上的优越性能，并开源项目促进应用。<br/><br/>贡献点：  <br/>1. **提出解决声学-语义差距的新方法**：通过动态生成语音训练目标，弥合现有SLLMs在特征表示空间中的声学与语义鸿沟。  <br/>2. **融合双模态学习机制**：结合声学和语义学习，使模型同时保留语音处理能力与强推理能力。  <br/>3. **验证性能优势**：在六千小时训练数据下，EchoX于多个知识型问答基准上达到先进水平。  <br/>4. **开源实践**：提供可复现实验的代码库，推动语音大模型研究与应用。|
|2509.07526v1|[Competitive Audio-Language Models with Data-Efficient Single-Stage   Training on Public Data](http://arxiv.org/abs/2509.07526v1)|总结：  <br/>Falcon3-Audio通过高效架构和少量数据实现语音-语言任务的高性能，挑战了传统依赖复杂训练策略和大规模数据的范式。<br/><br/>贡献点：  <br/>1. **创新架构**：提出Falcon3-Audio系列模型，整合指令调优的LLM与Whisper编码器，构建音频-语言统一框架。  <br/>2. **数据效率突破**：仅需<30K小时公开音频数据（5K个唯一语料）即可达到MMAU基准最优性能（64.14分），与R1-AQA持平。  <br/>3. **参数效率优胜**：1B参数模型性能可比肩2B-13B参数的主流模型，体现显著的压缩潜力与资源节约。  <br/>4. **简化训练流程**：采用单阶段训练方法，无需课程学习、多编码器或多注意力模块等复杂设计。  <br/>5. **透明性设计**：基于公开模型与标准数据集，提升模型可解释性与复现性。  <br/>6. **验证性能鲁棒性**：通过消融实验证明，简化的模型结构仍能实现与超大规模数据训练模型相当的性能。|
|2509.06452v1|[AudioBoost: Increasing Audiobook Retrievability in Spotify Search with   Synthetic Query Generation](http://arxiv.org/abs/2509.06452v1)|**贡献点总结：**  <br/>1. 提出AudioBoost系统，通过合成查询生成解决Spotify有声书冷启动检索问题。  <br/>2. 采用大语言模型（LLM）根据有声书元数据生成探索性查询，提升用户搜索意图匹配。  <br/>3. 将合成查询同步索引至查询自动补全（QAC）与检索引擎，实现查询优化与结果增强。  <br/>4. 离线实验验证合成查询有效提升可检索性，且质量较高。  <br/>5. 在线A/B测试显示：有声书曝光量+0.7%、点击率+1.22%、探索性查询完成率+1.82%。  <br/><br/>**摘要核心贡献（100字以内）：**  <br/>提出AudioBoost系统，利用LLM生成合成查询解决Spotify有声书冷启动检索问题，提升用户探索体验并通过实验验证显著效果。|
|2509.04744v1|[WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning](http://arxiv.org/abs/2509.04744v1)|总结：  <br/>本文提出首个真实场景下的多模态符号音乐推理基准WildScore，通过系统分类和多选题评估框架揭示了当前模型在音乐分析中的潜力与挑战，并开放了数据集与代码。<br/><br/>贡献点：  <br/>1. **首个真实场景音乐推理基准**：WildScore作为首个基于实际音乐作品和用户生成问题的多模态符号音乐分析数据集，打破了实验室环境的局限。  <br/>2. **系统音乐学分类框架**：构建包含高阶与细粒度音乐学本体的知识体系，为多模态音乐任务提供结构化评估依据。  <br/>3. **多选题评估方法**：将复杂音乐推理任务转化为多选题形式，实现对模型视觉-符号理解能力的可控与可扩展测试。  <br/>4. **揭示模型表现模式**：通过实证测试发现主流模型在音乐符号推理中的优势与不足，为未来研究提供方向指引。  <br/>5. **开源开放**：公开数据集和代码，推动多模态音乐分析领域的研究复用与技术迭代。|
|2509.04488v1|[Serialized Output Prompting for Large Language Model-based Multi-Talker   Speech Recognition](http://arxiv.org/abs/2509.04488v1)|总结（100字以内）:  <br/>本文提出一种通过结构化提示（SOP）提升多说话人语音识别性能的方法，设计分离器与串行CTC模块，并采用三阶段训练策略，克服现有LLM在复杂多说话人场景下的性能瓶颈。<br/><br/>贡献点:  <br/>1. **首次探索结构化提示设计**：提出基于序列化输出提示（SOP）的框架，优化LLM在多说话人场景任务定义中的表现。  <br/>2. **创新网络结构**：引入Separator与串行CTC层，按“先说话先后输出”机制分离混合语音中的多说话人内容。  <br/>3. **提示生成策略**：通过贪婪搜索解码序列化CTC输出，构建结构化提示SOP以增强LLM推理能力。  <br/>4. **三阶段训练方法**：设计包含序列化输出训练、信息提取和SOP自适应的训练策略，提升模型鲁棒性。  <br/>5. **实验证明有效性**：在LibriMix数据集上验证，SOP方法显著优于现有LLM基础系统，尤其在三说话人场景中表现突出。|
|2509.04392v1|[Denoising GER: A Noise-Robust Generative Error Correction with LLM for   Speech Recognition](http://arxiv.org/abs/2509.04392v1)|总结：  <br/>提出Denoising GER框架，通过噪声自适应编码器、异构特征补偿动态融合和强化学习策略，显著提升语音识别后处理在噪声环境中的准确率和泛化能力。<br/><br/>贡献点：  <br/>1. **噪声自适应声学编码器**：增强模型对复杂噪声场景的适应性，提高鲁棒性。  <br/>2. **异构特征补偿动态融合（HFCDF）机制**：优化多模态信息整合效率，提升信息利用率。  <br/>3. **强化学习（RL）训练策略**：增强模型预测能力，改进生成错误纠正效果。  <br/>4. **实验验证**：在真实噪声环境中验证了框架的有效性与泛化能力，证明其优于现有方法。|
|2509.03940v1|[VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based   Role-Playing Agents](http://arxiv.org/abs/2509.03940v1)|总结：  <br/>提出首个面向语音角色扮演对话的综合基准VoxRole，解决副语言特征缺失与评估标准不足问题，通过自动化构建方法实现多维度对话数据与角色建模，并揭示当前模型在角色一致性方面的表现差异。<br/><br/>贡献点：  <br/>1. **构建首个语音基准**：提出VoxRole，填补语音角色扮演领域标准化评估的空白，涵盖多轮对话和丰富的角色属性。  <br/>2. **引入副语言特征**：系统性整合语调、韵律等语音特征，突破传统文本模态限制，提升角色情感与身份表达的真实性。  <br/>3. **创新自动化流程**：设计双阶段自动构建管道（音频对齐剧本+LLM角色建模），高效生成高质量多维度角色对话数据。  <br/>4. **多维度性能评估**：通过VoxRole全面测试当前语音对话模型，揭示其在长期角色一致性等核心能力上的优劣势。|
|2509.03021v1|[A Study on Zero-Shot Non-Intrusive Speech Intelligibility for Hearing   Aids Using Large Language Models](http://arxiv.org/abs/2509.03021v1)|总结（100字以内）:  <br/>本文提出基于大语言模型GPT-Whisper-HA的零样本非侵入式助听器语音评估方法，通过结合听力损失模拟和双ASR模块提升预测准确性，实验验证其在主观理解力评估中优于现有模型。<br/><br/>贡献点:  <br/>1. **提出GPT-Whisper-HA模型**：作为GPT-Whisper的扩展，首次将大语言模型应用于助听器（HA）的零样本非侵入式语音评估任务。  <br/>2. **个性化音频处理**：引入MSBG听力损失模型和NAL-R模拟，基于个体听力图动态调整音频输入处理流程。  <br/>3. **双ASR与分数融合机制**：使用两个自动语音识别模块生成音频-文本表示，并结合GPT-4o预测双评分后平均，提升预测鲁棒性。  <br/>4. **性能验证与应用潜力**：实验表明其相对RMSE降低2.59%，证明了大语言模型在助听器语音评估中的有效性。|
|2509.02521v2|[FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via   Dual Training](http://arxiv.org/abs/2509.02521v2)|总结：  <br/>本论文提出自然独白和双训练策略，开发了FLM-Audio全双工对话聊天机器人，显著提升响应质量与聊天体验并减少数据需求。<br/><br/>贡献点：  <br/>1. **引入自然独白结构**：将连续句子与等待间隔结合，模拟人类对话认知行为，保留语义完整性。  <br/>2. **提出双训练范式**：通过交替排列独白与音频位置（前导/后置），实现更优的语义对齐训练。  <br/>3. **构建全双工模型FLM-Audio**：基于7B参数的聊天机器人，融合自然独白与双训练策略，支持原生全双工对话。  <br/>4. **验证高效性**：实验表明模型在更少训练数据下实现更高质量的对话响应与更好的用户体验。|
|2509.02349v2|[AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation](http://arxiv.org/abs/2509.02349v2)|总结：  <br/>提出语义与声学token的合理定义，并构建多维度系统评估框架，实现对语音编码器的全面公平比较，揭示关键指标间的关联性。<br/><br/>贡献点：  <br/>1. **定义优化**：明确语义token与声学token的区分，解决现有研究中定义模糊的问题。  <br/>2. **系统评估框架**：设计覆盖音频重建、码本ID稳定性、解码器困惑度和下游任务性能的综合评估体系。  <br/>3. **指标关联性分析**：验证重建指标、码本稳定性、困惑度与下游任务表现之间的强相关性。  <br/>4. **跨领域比较**：提供统一标准，支持不同编码器在语义和声学层面的公平对比，突破传统领域局限。|
|2509.01787v1|[AHAMask: Reliable Task Specification for Large Audio Language Models   without Instructions](http://arxiv.org/abs/2509.01787v1)|总结：提出AHAMask方法，通过屏蔽注意力头实现无指令声学任务功能控制，参数高效且性能优于传统指令方法，揭示了LALMs中存在功能路径。<br/><br/>贡献点：<br/>1. 提出AHAMask框架，利用解码器-only LLM的注意力头屏蔽机制，实现无指令触发特定声学任务功能。<br/>2. 通过参数高效训练策略，仅需微调与注意力头数量相等的参数即可获得任务功能特异性掩码。<br/>3. 实验证明该方法在单任务和复合任务中表现优异，可媲美或超越基于指令的范式。<br/>4. 发现LALMs内部存在"功能路径"现象，即注意力头可通过选择性激活实现任务功能分化。|
|2509.00654v1|[The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation](http://arxiv.org/abs/2509.00654v1)|贡献点总结（100字以内）：<br/>本研究提出无需艺术家名字的轻量描述符作为风格控制手段，验证其在文本到音乐生成中的可行性与效果。通过对比实验与新评估指标，揭示现有政策限制可能无法完全阻断风格模仿，并构建了跨艺术家描述符表，定义了"无名字差距"概念，为合规生成提供新思路。<br/><br/>分点贡献：<br/>1. 提出使用大语言模型生成的轻量可读描述符替代艺术家名字，解决政策合规性与风格控制的矛盾<br/>2. 建立包含15个参考片段、三种提示条件（基线/名字/描述符）的系统性评估框架<br/>3. 开发新型min-distance attribution度量指标，提升风格控制效果评估的精确度<br/>4. 证实描述符可有效恢复艺术家名字提示的风格控制效果，揭示政策限制的局限性<br/>5. 通过跨艺术家迁移实验，证明描述符编码具有针对性的风格特征<br/>6. 构建包含10位当代艺术家的描述符词表，展示风格控制标记的广泛适用性|
|2509.00078v1|[ChipChat: Low-Latency Cascaded Conversational Agent in MLX](http://arxiv.org/abs/2509.00078v1)|**贡献点总结（100字以内）：**  <br/>提出ChipChat，整合流式语音识别、增强LLM、TTS与说话人建模，实现无需GPU的亚秒级延迟和隐私保护，验证级联系统的优化潜力。<br/><br/>**详细贡献点分点列出：**  <br/>1. **创新架构设计**：提出ChipChat，作为首个低延迟级联系统（CS），通过架构优化突破传统瓶颈。  <br/>2. **多模块集成**：整合流式对话语音识别（混合专家）、状态-动作增强LLM、TTS、神经声码器和说话人建模模块。  <br/>3. **高效实现**：基于MLX框架，在无专用GPU的设备（如Mac Studio）上实现亚秒级响应延迟。  <br/>4. **隐私保障**：全设备端到端处理确保用户数据隐私，适用于边缘计算场景。  <br/>5. **性能验证**：实验证明级联系统经优化后可超越端到端方法的延迟限制，推动语音AI代理实际应用。|
|2508.20805v1|[Exploring Machine Learning and Language Models for Multimodal Depression   Detection](http://arxiv.org/abs/2508.20805v1)|**贡献点：**  <br/>1. **首次参与多模态人格感知抑郁检测挑战**：针对该领域首个公开挑战赛提出系统性解决方案，推动多模态抑郁检测研究。  <br/>2. **多模型对比分析**：系统评估XGBoost、Transformer架构与大型语言模型（LLMs）在音频、视频、文本特征上的表现差异。  <br/>3. **模态信号建模洞察**：揭示不同模型在捕捉抑郁相关信号时的优劣，为跨模态特征融合提供实证依据。  <br/>4. **多模态表示策略优化**：提出有效的多模态表征方法，提升心理健康预测的模型性能与泛化能力。  <br/><br/>**总结（100字以内）：**  <br/>本文首次参与多模态人格感知抑郁检测挑战，对比XGBoost、Transformer及LLMs在音频、视频和文本特征上的效果，分析其优劣，并提出改进的多模态表征策略，为心理健康预测提供方法论参考。|
|2508.20660v1|[CodecBench: A Comprehensive Benchmark for Acoustic and Semantic   Evaluation](http://arxiv.org/abs/2508.20660v1)|**贡献点**  <br/>1. **提出CodecBench基准数据集**：首次构建多领域、多场景的复杂音频编码评估基准，覆盖语音语言模型所需的多样化应用场景（如多说话人、背景噪声等）。  <br/>2. **弥补现有评估不足**：针对传统音频编解码器评价依赖简单指标和场景的缺陷，设计全面的评测体系以更准确评估音频的声学与语义能力。  <br/>3. **多维度评价框架**：从声学和语义两个核心维度出发，跨四个数据领域验证音频编解码器的性能，推动技术发展。  <br/>4. **开源代码促进研究**：提供公开实现代码，便于社区复现与扩展，加速语音领域音频编解码技术的创新。  <br/><br/>**总结**：  <br/>提出CodecBench基准数据集，填补多模态语音模型音频编码评估空白，推动复杂场景下音频编解码技术发展与应用。|
|2508.20088v1|[AudioStory: Generating Long-Form Narrative Audio with Large Language   Models](http://arxiv.org/abs/2508.20088v1)|总结（100字以内）:  <br/>提出AudioStory框架，通过解耦桥接与残差查询机制、端到端训练，解决长叙事音频生成的时空连贯与组合推理难题，并建立多领域基准数据集，显著提升指令遵循与音频质量。<br/><br/>贡献点：  <br/>1. **统一框架设计**：首次将大语言模型（LLMs）与文本到音频（TTA）系统结合，生成结构化的长音频叙事，突破传统方法对短音频的限制。  <br/>2. **解耦桥接机制**：将LLM与音频生成的协作拆分为“桥接查询”（事件内语义对齐）和“残差查询”（跨事件一致性维护），实现场景过渡与情感连贯。  <br/>3. **端到端训练**：整合指令理解与音频生成流程，消除模块化训练依赖，提升模型组件间的协同效率。  <br/>4. **多领域基准数据集**：构建AudioStory-10K基准，涵盖动画音景、自然声叙事等场景，推动长音频生成领域的评估与研究。  <br/>5. **性能验证**：在单音频与叙事生成任务中均超越现有TTA基线，在指令遵循能力和音频保真度上表现最优。|
|2508.19514v1|[MQAD: A Large-Scale Question Answering Dataset for Training Music Large   Language Models](http://arxiv.org/abs/2508.19514v1)|总结（100字以内）:  <br/>本文提出MQAD音乐问答数据集，基于Million Song Dataset构建，融合多模态模型和LLMs技术，提供时间变化的音乐信息与结构分析，并引入主观评估指标，推动音乐理解研究。<br/><br/>贡献点分点列表:<br/>1. **构建首个音乐问答数据集MQAD**  <br/>   - 基于Million Song Dataset（MSD），覆盖270,000首歌曲的丰富音乐特征（节拍、和弦、调性、结构、乐器、流派等）  <br/>   - 包含300万条多样化问答对和描述，填补音乐领域大尺度语义标注数据的空白  <br/><br/>2. **创新多模态构建方法**  <br/>   - 结合音乐信息检索（MIR）模型提取高阶音乐特征  <br/>   - 采用大型语言模型（LLMs）生成自然语言问答对  <br/>   - 集成LLaMA2与Whisper多模态架构，实现音乐信号与文本的联合建模  <br/><br/>3. **提出新型评估体系**  <br/>   - 设计针对音乐问答任务的主观评价指标  <br/>   - 支持对音乐结构理解能力的量化分析  <br/>   - 为后续研究提供更全面的性能评估基准  <br/><br/>4. **验证数据集有效性**  <br/>   - 模型在MQAD上训练后，显著超越传统音乐音频描述方法  <br/>   - 通过实验证明数据集对音乐结构分析的促进作用|
|2508.18655v3|[Empathy Omni: Enabling Empathetic Speech Response Generation through   Large Language Models](http://arxiv.org/abs/2508.18655v3)|总结: 本研究提出Emotion Omni模型与情感对话数据集，显著提升语音助手的情感表达与语音质量，验证了小数据训练下的有效性。<br/><br/>贡献点:<br/>1. 提出Emotion Omni模型，突破传统语音LLMs仅转换文本的局限，实现语音情绪理解与情感响应生成  <br/>2. 构建首个专门用于情感对话的200k样本数据集，支持情感语音助手的开发  <br/>3. 在无需大规模预训练的条件下，实现与现有模型相当的指令遵循能力  <br/>4. 语音质量（UTMOS:4.41）与情感评分（Emotion GPT Score:3.97）均优于现有技术  <br/>5. 提供完整技术演示，推动语音情感交互落地应用|
|2508.18655v2|[Empathy Omni: Enabling Empathetic Speech Response Generation through   Large Language Models](http://arxiv.org/abs/2508.18655v2)|**总结（100字以内）：**  <br/>本文提出Emotion Omni模型，无需大规模预训练即可实现情感语音交互，构建了200k情感对话数据集，并在语音质量和情感表达上超越现有方法，验证了其在指令遵循与同理心生成上的有效性。<br/><br/>**贡献点分点：**  <br/>1. **提出Emotion Omni模型**：首次设计可在有限数据下生成同理心语音响应的模型，提升人机交互的情感理解能力。  <br/>2. **构建情感对话数据集**：开发数据管道创建200k规模的情感对话数据集，支持情感语音助手的训练与评估。  <br/>3. **无需大规模训练的性能**：在无大模型预训练条件下实现与现有模型相当的指令遵循能力，降低计算成本。  <br/>4. **优越的语音与情感质量**：实验表明其语音保真度（UTMOS:4.41）和情感表达分数（Emotion GPT Score:3.97）优于现有方法。|
|2508.18655v1|[Emotion Omni: Enabling Empathetic Speech Response Generation through   Large Language Models](http://arxiv.org/abs/2508.18655v1)|**贡献点（分点）:**  <br/>1. **提出Emotion Omni模型架构**：首次设计专门用于理解用户语音中情感和副语言线索，并生成同理心语音响应的模型，突破传统语音LLMs仅依赖文本转换的局限。  <br/>2. **构建高效情感对话数据集**：基于开源TTS框架开发数据生成流水线，创建包含200k条数据的情感对话语料库，显著降低对大规模训练数据的依赖。  <br/>3. **实现低成本模型训练**：通过优化数据生成和模型设计，使语音LLM在有限数据和低计算资源下仍能生成具有情感理解能力的对话响应。  <br/>4. **提供应用示范**：开源演示系统验证模型的实际效果，推动情感导向人机交互技术的落地与普及。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出Emotion Omni模型，通过创新架构和高效数据生成方法，降低情感语音助手开发的数据与计算成本，提升对话中的情感理解能力，并提供开源应用示例。|
|2508.18337v2|[EAI-Avatar: Emotion-Aware Interactive Talking Head Generation](http://arxiv.org/abs/2508.18337v2)|**贡献点总结：**  <br/>1. 提出EAI-Avatar框架，实现双向情感自适应对话形象生成。  <br/>2. 设计Transformer-based头遮罩生成器，生成时间一致的运动特征序列。  <br/>3. 引入交互式对话树结构，动态捕捉对话状态与情感信息，指导表情合成。  <br/>4. 通过逆向遍历提取历史情感线索，提升虚拟形象的情感表达连贯性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EAI-Avatar框架，通过情感感知和对话状态建模，实现双向互动中时间一致且情感丰富的虚拟形象生成，提升了AI对话的自然性和实用性。|
|2508.18337v1|[EAI-Avatar: Emotion-Aware Interactive Talking Head Generation](http://arxiv.org/abs/2508.18337v1)|**贡献点总结**  <br/>1. 提出EAI-Avatar框架，首次实现基于情感的双向对话交互虚拟头像生成。  <br/>2. 设计Transformer-based头掩码生成器，生成任意长度且时间一致的头部运动掩码。  <br/>3. 引入交互对话树结构，通过逆向遍历提取情感线索，指导表情合成与状态转移。  <br/>4. 通过实验验证方法在情感适应性、交互连贯性及生成效果上的显著优势。  <br/><br/>**摘要总结**（100字以内）:  <br/>提出EAI-Avatar框架，融合LLM对话生成与交互对话树，实现双向情感适应虚拟头像生成，实验验证其在情感表现与交互连贯性方面的优越性。|
|2508.18295v1|[H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech   Recognition Systems](http://arxiv.org/abs/2508.18295v1)|**贡献点：**<br/>1. 提出新型热词预检索模块（H-PRM），通过计算声学相似性筛选关键热词候选，解决大规模热词识别率下降问题。  <br/>2. 开发插件式解决方案，可无缝集成至传统ASR模型（如SeACo-Paraformer），显著提升热词召回后识别率（PRR）。  <br/>3. 将H-PRM扩展至Audio LLMs，采用提示方法实现热词定制的通用性与灵活性。  <br/>4. 实验证明H-PRM优于现有方法，为ASR热词定制提供新范式与研究方向。  <br/><br/>**总结（100字以内）：**  <br/>本文提出H-PRM模块，通过声学相似性筛选热词，解决大规模热词识别问题，并实现与传统模型及Audio LLMs的兼容，显著提升识别率，为ASR领域提供新方向。|
|2508.17863v1|[Speech Discrete Tokens or Continuous Features? A Comparative Analysis   for Spoken Language Understanding in SpeechLLMs](http://arxiv.org/abs/2508.17863v1)|总结（100字内）:  <br/>本文首次系统比较SpeechLLMs中离散token与连续特征的性能，在统一实验框架下评估其在六项口语理解任务中的表现，发现连续特性更优，并通过多维度分析揭示其学习模式差异，为领域发展提供新方向。<br/><br/>贡献点:  <br/>1. **提出统一对比框架**：首次在相同实验条件下公平比较基于自监督学习的离散token与连续特征方法，消除研究偏差。  <br/>2. **多模型规模验证**：通过小规模(Qwen1.5-0.5B)与大规模(Llama3.1-8B)LLM对比，验证方法在不同规模下的泛化性能。  <br/>3. **多维度深度分析**：从效率、SSL层、LLM层、鲁棒性四个层面系统解析两种方法的差异，揭示其学习特性本质。  <br/>4. **实证性能结论**：首次实证证明连续特征在多数语音处理任务中优于离散token，为SpeechLLM设计提供数据支持。|
|2508.17494v1|[Improving French Synthetic Speech Quality via SSML Prosody Control](http://arxiv.org/abs/2508.17494v1)|总结：  <br/>提出首个端到端法语SSML插入方法，结合级联模型提升韵律控制，显著改善合成语音自然度并获用户偏好。<br/><br/>贡献点：  <br/>1. 首次构建端到端流程，通过SSML标签控制法语语音的音高、语速、音量与停顿。  <br/>2. 设计级联架构，采用两个QLoRA微调的Qwen 2.5-7B模型分别完成句子断裂预测和韵律目标回归。  <br/>3. 在14小时法语播客数据集上验证性能，使句子断裂F1达99.2%，音高/语速/音量MAE降低25-40%。  <br/>4. 通过18人主观测试（9小时音频）显示自然度显著提升（MOS 3.20→3.87，p<0.005），15/18听众偏好增强合成。  <br/>5. 公开代码实现，便于学术研究与商业应用复现和扩展。|
|2508.15882v1|[Beyond Transcription: Mechanistic Interpretability in ASR](http://arxiv.org/abs/2508.15882v1)|**贡献点总结（100字以内）:**  <br/>本研究首次将基于logit lens、linear probing和activation patching等可解释性方法应用于ASR系统，揭示了音频-语义信息在模型层间的演变机制，发现了导致重复幻觉和语义偏见的编码器-解码器交互，为提升ASR模型透明度与鲁棒性提供了新方向。<br/><br/>**分点贡献:**  <br/>1. **方法迁移**：将语言模型领域的可解释性方法（logit lens、linear probing、activation patching）首次系统性引入自动语音识别（ASR）领域，填补了该方向的研究空白。  <br/>2. **新发现**：揭示了ASR系统中音频与语义信息在不同层间的动态演化规律，识别出导致重复幻觉（repetition hallucinations）的编码器-解码器交互机制，以及语义偏见（semantic biases）在深层音频表示中的编码现象。  <br/>3. **应用价值**：通过实验证明了可解释性技术在提升ASR模型性能与可解释性方面的潜力，为未来研究模型透明度和鲁棒性提供了理论依据与实践方向。|
|2508.12626v1|[Exploring the Feasibility of LLMs for Automated Music Emotion Annotation](http://arxiv.org/abs/2508.12626v1)|总结：  <br/>本研究探索使用GPT-4o自动生成音乐情感标注，构建GiantMIDI-Piano数据集并采用多维评估方法，验证其可行性与可靠性，指出模型虽在准确性上不足但具有成本效益，可作为可扩展的替代方案。<br/><br/>贡献点：  <br/>1. **提出使用大语言模型进行音乐情感注释的新方法**：首次尝试将GPT-4o应用于音乐情感标注任务，探索其在非文本领域的潜力。  <br/>2. **构建专有音乐情感标注数据集**：开发GiantMIDI-Piano数据集，并采用四象限valence-arousal框架进行系统标注。  <br/>3. **对比人类专家标注与模型输出**：通过与三位人类专家的标注对比，分析GPT生成标注的准确性及情感分类的细微差别。  <br/>4. **设计多维度评估体系**：引入标准准确率、加权准确率、标注者一致性指标及标签分布相似度等综合评估方法。  <br/>5. **揭示模型局限性与应用前景**：指出GPT在情感标注中的不足，但强调其成本效益和高效性，为大规模数据标注提供参考。|
|2508.12591v1|[Beyond Modality Limitations: A Unified MLLM Approach to Automated   Speaking Assessment with Effective Curriculum Learning](http://arxiv.org/abs/2508.12591v1)|总结：  <br/>本研究首次系统性探索MLLM在语音评估中的应用，提出Speech-First Multimodal Training方法，解决交付评估的挑战，显著提升整体评估性能，为ASA开辟新路径。<br/><br/>贡献点：  <br/>1. **首次系统性研究**：提出首个针对MLLM全面语音评估（ASA）的系统性研究框架，整合音频与文本信息。  <br/>2. **模态局限分析**：明确传统ASA系统在文本与音频模态间的固有缺陷，揭示MLLM的综合优势。  <br/>3. **针对性训练策略**：设计Speech-First Multimodal Training (SFMT)，通过课程学习强化语音建模，优化跨模态融合。  <br/>4. **性能提升验证**：在基准数据集实验证明，MLLM系统整体评估性能从PCC 0.783提升至0.846，交付评估提升4%。  <br/>5. **方法创新与应用**：为语音评估领域提供基于MLLM的新范式，推动多模态学习在语音处理中的实际落地。|
|2508.11818v1|[Audio Flamingo Sound-CoT Technical Report: Improving Chain-of-Thought   Reasoning in Sound Understanding](http://arxiv.org/abs/2508.11818v1)|总结（100字以内）:  <br/>本论文提出AF-Reasoning-Eval基准与AF-CoT-Train数据集，验证了链式思维微调Audio Flamingo系列在音频语言模型中的有效性，填补了该领域的研究空白。<br/><br/>贡献点：  <br/>1. **提出首个针对音频语言模型的链式思维推理评估基准（AF-Reasoning-Eval）**，涵盖常识推理与贴近选项辨别能力的测评。  <br/>2. **构建自动转换pipeline**，将现有音频问答与分类任务数据转化为显式推理链，生成包含124万样本的训练数据集（AF-CoT-Train）。  <br/>3. **验证Audio Flamingo系列模型在链式思维微调下的性能提升**，在多个音频推理任务中展现显著改进。  <br/>4. **探索链式思维训练对音频理解能力的增强机制**，为语音领域引入新的训练范式。|
|2508.11326v1|[MoE-TTS: Enhancing Out-of-Domain Text Understanding for   Description-based TTS via Mixture-of-Experts](http://arxiv.org/abs/2508.11326v1)|**贡献点总结（100字以内）:**  <br/>提出MoE-TTS模型，采用基于模态的专家混合策略，通过冻结预训练文本LLM并接入语音专用权重，有效提升对域外描述的语音生成准确性。实验表明其性能超越现有闭源产品，辅以演示链接促进实际应用。<br/><br/>**分点贡献：**  <br/>1. **提出MoE-TTS模型**：首次将模态对齐机制引入描述型TTS领域，通过融合预训练文本LLM与语音模态专用权重，解决域外文本描述理解难题。  <br/>2. **模态适配方法创新**：设计基于专家混合（MoE）的架构，保持文本LLM冻结以避免负迁移，同时引入语音模态的细粒度适配权重，增强声学生成能力。  <br/>3. **实验验证优越性**：通过对比实验表明，MoE-TTS在处理精心设计域外描述任务中超越主流闭源系统，并显著提升语音生成与描述内容的一致性。  <br/>4. **实际应用引导**：提供可评估的演示资源及测试数据，推动领域内对模型鲁棒性和表现的实证研究，促进TTS技术的实际落地。|
|2508.10414v1|[MCP2OSC: Parametric Control by Natural Language](http://arxiv.org/abs/2508.10414v1)|**贡献点总结（100字以内）**  <br/>该研究提出MCP2OSC系统，结合LLM与参数化OSC控制，通过自然语言提示实现高效的人机协作，解决传统文本提示和参数控制的不足，为多媒体设备提供通用的LLM驱动控制机制。<br/><br/>---<br/><br/>**分点贡献**  <br/>1. **填补控制方式鸿沟**：设计MCP服务器与自然语言提示准则，融合文本提示的直观性和参数控制的精确性，实现对OSC控制的参数化探索。  <br/>2. **开发MCP2OSC系统**：基于Claude构建集成化工具，支持通过自然语言生成、解释、搜索、可视化、验证和调试OSC消息，以及管理地址模式。  <br/>3. **实验证明有效性**：通过14个实际问答案例与通用模板，验证系统在复杂任务中的适用性与灵活性。  <br/>4. **创新应用视角**：首次从网络协议层面利用LLM的能力，直接处理和生成可读性高的 OSC 消息，推动创意 MCP 应用。  <br/>5. **提升人机协作**：引入灵活精度控制和语言接口，增强人类创造力与 OSC 开发效率，为多媒体控制提供通用机制潜力。|
|2508.08961v2|[DualSpeechLM: Towards Unified Speech Understanding and Generation via   Dual Speech Token Modeling with Large Language Models](http://arxiv.org/abs/2508.08961v2)|**贡献点：**<br/>1. 提出Understanding-driven Speech Tokenizer（USTokenizer），通过文本LLMs提取高阶语义信息，增强语音与文本的模态一致性，减少对大规模配对数据的依赖。<br/>2. 设计DualSpeechLM框架，同时建模语音标记（USToken）和声学标记，实现语音理解与生成的统一端到端建模。<br/>3. 引入语义监督损失与Chain-of-Condition（CoC）策略，提升模型训练稳定性及语音生成效果。<br/>4. 实验证明方法有效，在统一模型中实现理解和生成任务的协同增强。<br/><br/>**总结：**  <br/>该研究通过USTokenizer与DualSpeechLM提出语音-文本统一建模新框架，结合语义监督与CoC策略，解决了模态对齐和任务冲突问题，实验验证了其在语音理解与生成任务中的互补优势。|
|2508.08961v1|[DualSpeechLM: Towards Unified Speech Understanding and Generation via   Dual Speech Token Modeling with Large Language Models](http://arxiv.org/abs/2508.08961v1)|贡献点总结（100字以内）:<br/>提出理解驱动的语音分词器(USTokenizer)与双标记建模框架DualSpeechLM，通过语义对齐和跨模态建模解决语音与文本模态差异及任务需求冲突问题，创新性引入语义监督损失和Chain-of-Condition策略，实验验证了统一模型在语音理解与生成任务中的互补增强效果。<br/><br/>分点贡献：<br/>1. 提出Understanding-driven Speech Tokenizer（USTokenizer），通过文本LLM提取高阶语义信息，提升语音标记与文本标记的模态一致性，降低跨模态对齐难度<br/>2. 设计DualSpeechLM双标记建模框架，实现语音理解（USToken输入）与语音生成（声学标记输出）的端到端统一处理<br/>3. 创新性提出语义监督损失函数和Chain-of-Condition（CoC）训练策略，有效提升模型训练稳定性与语音生成性能<br/>4. 实验证明该方法能建立理解与生成任务的互补关系，展示出统一模型中双向增强的可行性|
|2508.08095v1|[Dual Information Speech Language Models for Emotional Conversations](http://arxiv.org/abs/2508.08095v1)|**贡献点：**  <br/>1. **问题识别**：指出基于冻结LLM的SLM在捕捉非语言线索和上下文理解上的不足。  <br/>2. **方法创新**：提出两种异构适配器，分别处理非语言与语言信息的解耦。  <br/>3. **训练策略优化**：设计弱监督训练策略，提升模型效率并减少对任务特定数据的依赖。  <br/>4. **信息解耦与表示**：通过结构化表示实现非语言与语言信息的分离，增强SLM对语音的理解能力。  <br/>5. **上下文保留**：利用可控随机性避免生成任务特定向量，维持上下文理解能力。  <br/>6. **实验验证**：在情感对话任务中验证方法有效性，展示参数与数据效率优势。  <br/><br/>**总结：**  <br/>本文提出异构适配器与弱监督训练策略，有效解耦语音中的非语言信息并提升上下文理解能力，在情感对话任务中取得高效且竞争力的性能。|
|2508.08039v2|[Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning](http://arxiv.org/abs/2508.08039v2)|**贡献点分点总结：**  <br/>1. **提出Audio-Thinker框架**：首个针对LALMs的强化学习框架，专门提升音频问答中的推理能力，关注适应性、一致性和有效性。  <br/>2. **引入自适应推理准确性奖励**：根据任务复杂度动态调整推理策略，增强模型对不同场景的适应性。  <br/>3. **设计外部奖励模型**：评估推理过程的整体一致性和质量，提升推理结果的可靠性。  <br/>4. **结合基于推理的奖励机制**：在训练中区分有效与错误的推理路径，优化学习效率。  <br/>5. **实验验证有效性**：在多个基准任务中，Audio-Thinker显著优于现有推理导向模型，展现更强的推理和泛化能力。  <br/><br/>**总结（100字以内）：**  <br/>提出Audio-Thinker框架，通过自适应奖励与外部评估模型提升LALMs的推理适应性、一致性和有效性，实验证明其在音频问答任务中表现更优。|
|2508.08039v1|[Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning](http://arxiv.org/abs/2508.08039v1)|**贡献点（分点）:**  <br/>1. **提出Audio-Thinker框架**：设计了一种强化学习框架，专门用于提升大音频语言模型（LALMs）在音频问答任务中的推理能力，聚焦于增强模型的适应性、一致性和有效性。  <br/>2. **引入动态奖励机制**：开发了适应性思维准确度奖励，使模型能够根据任务复杂度动态调整推理策略，优化推理过程的精确性。  <br/>3. **结合外部评估模型**：通过外部奖励模型评估推理过程的整体一致性和质量，同时利用基于思维的奖励区分有效与错误推理路径，提升训练效率。  <br/>4. **实验证明效果显著**：在多个基准任务中，模型表现优于现有推理导向的LALMs，验证了其在推理能力和泛化性上的突破。  <br/><br/>**总结（100字以内）:**  <br/>本文提出Audio-Thinker框架，通过动态奖励机制和外部评估模型提升LALMs的音频问答推理能力，显著增强其适应性、一致性和泛化效果，实验证明优于现有方法。|
|2508.06372v1|[SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with   Multimodal Large Language Models](http://arxiv.org/abs/2508.06372v1)|**总结（100字以内）:**  <br/>本文提出SpeakerLM，一种统一的多模态大语言模型，通过端到端架构整合语音对齐与识别，引入灵活注册机制，结合多阶段训练策略，显著提升数据扩展能力和场景适应性，超越现有级联方法。  <br/><br/>**贡献点:**  <br/>1. **统一模型架构**：首次将Speaker Diarization（SD）与Automatic Speech Recognition（ASR）融合为端到端的多模态大语言模型（SpeakerLM），解决级联框架的误差传播问题。  <br/>2. **灵活注册机制**：设计可适配不同注册条件的机制，支持多样化说话人注册场景，增强模型泛化能力。  <br/>3. **多阶段训练策略**：基于大规模真实数据的渐进式训练方法，有效提升模型性能及数据扩展能力。  <br/>4. **跨域性能优异**：在in-domain和out-of-domain公开数据集上均超越SOTA级联基线，验证模型的鲁棒性与广泛适用性。|
|2508.06277v1|[Large Language Model Data Generation for Enhanced Intent Recognition in   German Speech](http://arxiv.org/abs/2508.06277v1)|总结：  <br/>本论文提出结合自适应Whisper模型与合成文本数据的新方法，解决老年德语语音命令意图识别的资源不足问题，验证了生成模型在数据质量上的优势，并确保方法可重复性。<br/><br/>贡献点：  <br/>1. **应用场景拓展**：首次针对老年德语说话者的语音命令进行意图识别研究，突破传统方法对短命令和英语的依赖。  <br/>2. **方法创新**：提出整合自适应Whisper ASR模型与Transformer语言模型的新框架，利用合成文本数据生成技术提升性能。  <br/>3. **数据质量验证**：通过跨数据集测试，证明合成数据显著增强分类鲁棒性及对多样说话风格及未见词汇的适应能力。  <br/>4. **模型对比发现**：实验证实小型领域专用模型LeoLM在德语意图识别任务中优于大型通用模型ChatGPT。  <br/>5. **数据生成透明化**：提供详细的合成数据生成与训练流程文档，确保研究过程可复现、可验证。|
|2508.06262v1|[Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech   Synthesis](http://arxiv.org/abs/2508.06262v1)|**贡献点**  <br/>1. 构建**Llasa+**模型：提出加速与流式文本到语音（TTS）框架，解决LLM-based TTS在推理延迟和流式合成上的局限性。  <br/>2. 引入**双MTP模块**：在冻结骨干网络后添加两个可插拔的多token预测模块，实现单步AR生成多个token，显著提升生成速度。  <br/>3. 设计**验证算法**：基于冻结骨干网络开发新型校验机制，减少MTP可能引发的错误传播，确保生成质量不下降。  <br/>4. **因果解码器**：支持从token流实时重建语音，实现流式合成能力。  <br/>5. **通用性框架**：MTP与验证模块可扩展至其他LLM-based模型，提升通用性。  <br/>6. **实验验证**：在仅使用LibriTTS训练数据的情况下，Llasa+实现1.48X速度提升且生成质量不变，验证有效性。  <br/>7. **开源共享**：公开代码与模型，促进研究复现与应用。  <br/><br/>**总结**：本研究提出Llasa+，通过双MTP模块与验证机制加速TTS生成，同时实现流式合成，展现高效性与通用性。|
|2508.05835v1|[NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](http://arxiv.org/abs/2508.05835v1)|贡献点总结（100字以内）:  <br/>本文提出NanoCodec，一种低帧率（12.5 FPS）、高效率的音频编解码器，通过消融实验验证帧率、比特率与因果性对重建质量的影响，显著提升Speech LLM的低延迟训练与推理性能，建立新基准。<br/><br/>详细贡献点:  <br/>1. **提出低帧率编解码器**：针对高帧率导致的慢速训练和推理问题，设计仅需12.5 FPS的低帧率方案，减少自回归步骤。  <br/>2. **系统消融研究**：分析帧率、比特率和因果性对编解码器重建质量的独立影响，揭示关键参数优化方向。  <br/>3. **性能突破**：NanoCodec在多个比特率范围内优于现有方法，实现高质量压缩与低延迟的平衡。  <br/>4. **应用价值**：为Speech LLM的训练和推理提供更高效的音频处理方案，推动语音领域模型部署优化。|
|2508.04795v2|[Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a   Frozen LLM](http://arxiv.org/abs/2508.04795v2)|总结：  <br/>该研究提出无需微调的跨模态后处理方法，结合音频与语言模型以添加说话人元数据标签，通过轻量连接器实现高效特征融合，且冻结LLAMA模型可直接比较x-vectors，获得优异的说话人识别性能。<br/><br/>贡献点：  <br/>1. **提出元数据增强新范式**：首次探索在对话转录后加入说话人属性（年龄、性别、情绪）的元数据标签，作为传统语法修正的补充。  <br/>2. **无需微调的跨模态融合**：通过冻结音频模型（如Whisper/WavLM）与冻结LLM（如LLAMA）结合，实现多模态特征协同，避免任务特定微调。  <br/>3. **轻量高效连接器设计**：采用高效模块化架构，桥接音频与语言表示，兼顾性能与计算效率，保持系统快速响应能力。  <br/>4. **x-vector直接比较技术**：验证冻结LLAMA模型可直接处理x-vectors，实现8.8%的等错误率（EER），突破传统需微调模型的限制。|
|2508.04795v1|[Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a   Frozen LLM](http://arxiv.org/abs/2508.04795v1)|### 贡献点：<br/>1. **提出无微调的跨模态框架**：结合冻结的音频模型（如Whisper/WavLM）和语言模型（LLAMA），通过轻量连接器实现元数据标签（年龄、性别、情感等）的自动添加，无需对任一模型进行任务特定微调。  <br/>2. **支持时变与全局元数据标记**：设计可区分全局属性（如性别）和时间相关属性（如情感）的标签系统，增强对话转录的上下文丰富性。  <br/>3. **提升说话人画像性能**：在未微调的前提下，达到8.8%的等错误率（EER），验证了其在性别识别等任务中的有效性。  <br/>4. **保持模块化与高效性**：通过轻量连接器维持系统独立性，同时实现高吞吐量和低计算成本，适合实际部署场景。  <br/><br/>### 总结（100字以内）：  <br/>本文提出无需微调的跨模态框架，结合音频与语言模型生成说话人元数据标签，支持时变与全局属性，并在性别识别任务中达到8.8%的EER，兼顾高效性与模块化，为对话分析提供了新方法。|
|2508.04721v1|[Toward Low-Latency End-to-End Voice Agents for Telecommunications Using   Streaming ASR, Quantized LLMs, and Real-Time TTS](http://arxiv.org/abs/2508.04721v1)|总结：  <br/>本文提出低延迟电信AI语音代理管道，集成四类专用模型并构建专项数据集，实现了RTF<1.0的实时语音处理能力，为电信自动化服务提供了新解决方案。<br/><br/>贡献点：  <br/>1. **构建低延迟语音AI管道**：首次设计适用于电信实时交互场景的端到端AI语音代理系统，支持呼叫中心、IVR和客户支持等场景。  <br/>2. **四类专用模型开发**：提出TSLAM（量化LLM）、T-VEC（嵌入模型）、TTE（ASR模型）、T-Synth（TTS模型）四类电信专用模型，实现领域适配的语音处理能力。  <br/>3. **融合多模态技术**：整合流式ASR、会话智能、基于文档的检索增强生成（RAG）和实时TTS，形成完整的语音交互闭环。  <br/>4. **专题数据集构建**：创建包含500个电信问题的RFCs数据集，用于模拟真实场景并评估系统性能。  <br/>5. **突破性实时性能**：通过优化使TSLAM、TTE和T-Synth的实时因子（RTF）低于1.0，满足企业级低延迟部署需求。  <br/>6. **推动下一代AI应用**：为自动化客服、诊断等提供技术基础，推动电信领域智能化升级。|
|2508.04141v2|[Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech](http://arxiv.org/abs/2508.04141v2)|总结：  <br/>提出结合自回归与非自回归模块的TTS框架，设计并行结构提升零样本语音合成的质量与效率。<br/><br/>贡献点：  <br/>1. **混合模块架构创新**：首次将自回归（AR）与非自回归（NAR）模块结合，整体协调语义与音频信息的独立性与相互依赖性。  <br/>2. **Parallel Tokenizer设计**：提出并行分词器，实现语义和音频令牌的同步生成，提升生成效率。  <br/>3. **Coupled NAR模型改进**：基于AR输出构建耦合NAR模型，通过动态预测详细令牌增强语义-音频关联性。  <br/>4. **Parallel GPT系统实现**：基于上述架构开发Parallel GPT，实现在零样本场景下的高质量语音合成。  <br/>5. **实验验证有效性**：在英语和中文数据集上验证模型性能，显著优于现有零样本TTS方法，且提供语音演示。|
|2508.04141v1|[Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech](http://arxiv.org/abs/2508.04141v1)|**贡献点：**  <br/>1. 提出结合自回归（AR）与非自回归（NAR）模块的TTS框架，解决语义与音频特征独立及相互依赖关系的协调问题。  <br/>2. 设计**Parallel Tokenizer**，首次实现语义与音频高层token的并行生成，提升合成效率。  <br/>3. 引入**Coupled NAR模型**，基于AR模型输出动态预测细节token，增强语义-音频关联性建模能力。  <br/>4. 开发**Parallel GPT**，通过该架构显著提高零样本TTS的生成质量和效率。  <br/>5. 在英文和中文数据集上验证模型有效性，证明其优于现有零样本TTS方法。  <br/>6. 提供可交互的语音演示（含链接），直观展示模型优势。  <br/><br/>**总结：**  <br/>本文通过融合AR与NAR模块及创新tokenizer设计，提出Parallel GPT框架，显著提升零样本TTS的生成质量与效率，并在多语言数据集上验证其有效性。|
|2508.04096v1|[Efficient Scaling for LLM-based ASR](http://arxiv.org/abs/2508.04096v1)|总结:<br/>本论文提出EFIN分阶段训练策略，通过先预训练语音编码器提升LLM-ASR的计算效率，降低资源消耗，并建立错误率与计算量的缩放定律，为模型扩展提供理论指导。<br/><br/>贡献点:<br/>1. 发现预训练语音编码器优于联合微调：通过对比实验验证预训练语音编码器能显著提升LLM-ASR的扩展效率。<br/>2. 提出EFIN分阶段训练框架：设计"编码器先训练-再与LLM集成"的多阶段策略，实现性能与计算成本的平衡。<br/>3. 实测性能提升：在同等计算预算下，EFIN相比现有方法将CERR降低21.1%，FLOPs减少49.9%。<br/>4. 建立计算缩放定律：首次推导出ASR错误率与计算资源的函数关系，为模型优化提供理论依据。|
|2508.03983v1|[MiDashengLM: Efficient Audio Understanding with General Audio Captions](http://arxiv.org/abs/2508.03983v1)|总结：  <br/>本研究提出开源音频-语言模型MiDashengLM，通过自研ACAVCaps数据集实现多模态音频理解，并利用高效开源编码器提升性能与透明性。<br/><br/>贡献点：  <br/>1. **首个开源LALM框架**：基于公开数据构建，突破封闭数据和专有模型限制，提升泛化与可访问性。  <br/>2. **新型ACAVCaps训练数据集**：专门用于生成通用音频描述，增强模型对复杂音频场景的感知能力。  <br/>3. **全开源实现**：预训练与监督微调均采用公开数据集，确保算法透明性和实验可复现性。  <br/>4. **多模态融合方法**：突破传统ASR为中心的音频-文本对齐，统一融合语音、环境声、音乐信息为文本表征。  <br/>5. **高效性能优化**：实现4倍TTFT加速与20倍吞吐量提升，显著优于现有模型效率。  <br/>6. **开源代码与模型**：提供模型权重和代码库，推动音频语言模型的社区研究与应用。|
|2508.03365v2|[When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs](http://arxiv.org/abs/2508.03365v2)|贡献点总结（100字以内）:  <br/>提出WhisperInject两阶段音频攻击框架，通过RL-PGD优化生成有害响应，结合PGD实现隐蔽操控，实验验证成功率超86%，揭示新型音频原生威胁，突破理论研究局限。<br/><br/>分点贡献：  <br/>1. **提出两阶段对抗攻击框架**：创新性设计WhisperInject，包含奖励导向优化（RL-PGD）与负载注入（Payload Injection）两个阶段，系统性实现对音频语言模型的操控。  <br/>2. **开发新型优化方法**：引入RL-PGD算法，通过强化学习与投影梯度下降结合，引导目标模型绕过自身安全协议生成有害输出。  <br/>3. **实现隐蔽性攻击技术**：利用PGD在天气查询、问候等良性音频中嵌入不可察觉扰动，确保攻击内容对人类听众无害。  <br/>4. **验证攻击有效性**：在StrongREJECT、LlamaGuard及人工评估等严格框架下，成功攻击Qwen2.5-Omni-3B、7B及Phi-4-Multimodal模型，成功率超86%。  <br/>5. **揭示新型安全威胁**：首次证明音频本身可成为AI攻击载体，提出针对音频语言模型的实用、隐蔽的操控手段，推动安全研究从理论到实际应用。|
|2508.03365v1|[When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs](http://arxiv.org/abs/2508.03365v1)|总结（100字以内）:  <br/>提出WhisperInject框架，通过RL-PGD与Payload Injection双阶段方法，有效利用音频输入的不可感知扰动绕过AI安全机制，实验证明对主流音频模型的高成功率，揭示实际音频攻击威胁。<br/><br/>贡献点分点列出:<br/>1. **提出WhisperInject框架**：首次构建针对音频语言模型的两阶段对抗攻击系统，结合隐蔽性扰动与目标响应生成，突破传统语音攻击方法的局限性。<br/>2. **创新优化方法**：引入基于奖励的RL-PGD算法（第一阶段）引导模型规避自身安全协议，再通过PGD（第二阶段）实现音频载体中的精细扰动注入，提升攻击效率与隐蔽性。<br/>3. **实验证明有效性**：在StrongREJECT、LlamaGuard和人工评估基准下，验证对Qwen2.5-Omni-3B、7B及Phi-4-Multimodal等多模型的攻击成功率超86%，展示攻击的广泛适用性。<br/>4. **揭示实际威胁**：将理论攻击转化为可实际实施的音频攻击模式，首次证明通过自然语音载体（如天气查询）可隐蔽操控AI行为，对语音安全研究具有警示意义。|
|2508.02175v2|[Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through   Latent Acoustic Pattern Triggers](http://arxiv.org/abs/2508.02175v2)|总结：  <br/>该论文提出针对语音大模型的HIN攻击框架，揭示音频特征在安全攻击中的独特脆弱性，构建AudioSafe基准并发现三类关键漏洞，推动语音安全研究发展。<br/><br/>贡献点：  <br/>1. **提出HIN框架**：设计首个针对音频的后门攻击方法，通过声学触发（如时间动态修改、谱定制噪声）植入隐蔽触发器，影响ALLM的音频特征编码器。  <br/>2. **构建AudioSafe基准**：开发涵盖九种风险类型的评估体系，系统性研究音频特征对ALLM安全的影响。  <br/>3. **揭示三类关键漏洞**：发现（1）环境噪声和语音速率可导致超90%攻击成功；（2）ALLMs对不同声学特征敏感性差异显著；（3）污染样本对模型损失曲线影响微弱，凸显攻击隐蔽性。|
|2508.02175v1|[Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through   Latent Acoustic Pattern Triggers](http://arxiv.org/abs/2508.02175v1)|**贡献点：**  <br/>1. **提出HIN框架**：设计首个针对音频特征的后门攻击方法，通过时间动态修改和频谱噪声注入等音频特定操作，构建隐蔽且高效的触发器。  <br/>2. **开发AudioSafe基准**：构建首个评估音频特征引发风险的基准，涵盖9种风险类型，系统检验ALLM的安全性。  <br/>3. **揭示关键漏洞**：通过实验发现三类显著漏洞：（I）环境噪声与语速变化可达成超90%攻击成功率；（II）ALLM对不同特征的敏感性差异显著；（III）中毒样本导致的损失波动极小，攻击隐蔽性强。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HIN框架及AudioSafe基准，系统评估音频特征对ALLM的潜在风险，揭示了环境噪声、语速变化引发的高攻击成功率及模型对触发器敏感性差异等关键漏洞，强调音频安全的特殊挑战。|
|2508.01659v1|[From Contrast to Commonality: Audio Commonality Captioning for Enhanced   Audio-Text Cross-modal Understanding in Multimodal LLMs](http://arxiv.org/abs/2508.01659v1)|总结:  <br/>本文提出Audio Commonality Captioning (ACC)任务，解决ADC语义差距问题，提升音频-文本跨模态理解，并在多样化下游任务中保持模型性能，验证其鲁棒性与任务适配性。<br/><br/>贡献点:  <br/>1. **提出新型任务**：提出Audio Commonality Captioning (ACC)作为Audio Difference Captioning (ADC)的互补替代，通过捕捉音频片段的共性语义而非强调差异，解决跨模态对齐中的语义鸿沟问题。  <br/>2. **缓解灾难性遗忘**：设计ACC任务以避免ADC在微调过程中因偏离预训练目标而导致的灾难性遗忘，提升模型在下游任务中的稳定性。  <br/>3. **增强泛化能力**：实验证明ACC在语音和音乐相关任务（如VSC、SER、MIC、MGC）中优于ADC，能更有效地保持预训练阶段的通用能力。  <br/>4. **验证跨模态鲁棒性**：验证ACC在MLLMs框架下对音频-文本联合理解的提升，实现通用性与任务特定性能之间的平衡。|
|2508.01181v1|[Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion   Reasoning](http://arxiv.org/abs/2508.01181v1)|总结（100字以内）:  <br/>本研究提出CA-MER数据集和MoSEAR框架，通过平衡音频与视觉模态信息，解决多模态情感模型在情感冲突场景下的偏差问题，显著提升模型性能且无模态权衡。<br/><br/>贡献点:<br/>1. **提出CA-MER基准**：构建首个针对情感冲突场景的多模态情感理解基准，包含视频对齐、音频对齐和一致三类子集，模拟真实情感不一致情况。  <br/>2. **揭示模态依赖问题**：发现现有SOTA情感MLLMs在冲突场景中系统性过度依赖音频信号，忽略视觉模态关键信息。  <br/>3. **设计MoSEAR框架**：提出参数高效的双模块方法（MoSE+AR），通过门控机制和注意力重分配实现模态平衡整合。  <br/>4. **验证性能提升**：在多个基准测试中（包括MER2023、EMER、DFEW和CA-MER），证明MoSEAR在冲突场景下达到SOTA，且无模态间性能妥协。|
|2508.01178v1|[Advancing the Foundation Model for Music Understanding](http://arxiv.org/abs/2508.01178v1)|**贡献点总结（100字以内）**  <br/>提出MuFun统一音乐理解模型及MuCUE基准，创新性地联合处理音乐和歌词，通过大规模多任务数据训练，在跨模态理解中实现领先性能与泛化能力。<br/><br/>**分点贡献**  <br/>1. **统一模型框架**：构建首个面向整体音乐理解的MuFun基础模型，打破传统MIR领域中专用模型单任务优化的局限性。  <br/>2. **跨模态联合处理**：设计新型架构，同时处理乐器音轨与歌词文本，实现对音乐内容的多维度感知。  <br/>3. **大规模多任务训练**：基于涵盖音乐分类、标签、问答等任务的大型数据集进行训练，提升模型的通用性与适应性。  <br/>4. **新基准提出**：创建MuCUE评估体系，系统性衡量多方面音乐理解能力，推动领域标准化与模型对比。  <br/>5. **性能验证优势**：在MuCUE任务中显著超越现有音频大模型，验证了模型的先进性与泛化能力。|
|2508.01166v1|[Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented   Conversational LLM-Based ASR](http://arxiv.org/abs/2508.01166v1)|总结：  <br/>本文提出MARS方法，通过多模态检索与选择机制优化对话场景下的语言模型语音识别，有效降低冗余信息干扰，在较少训练数据下超越传统大模型性能。<br/><br/>贡献点：  <br/>1. 提出MARS多模态检索-选择框架，解决对话ASR中历史上下文冗余导致的混淆问题  <br/>2. 开发结合音频与文本相似性的双重排序方法，提升上下文选择的准确性  <br/>3. 首创性地将多模态检索技术引入对话ASR系统，增强对对话连贯性的理解  <br/>4. 在Interspeech 2025数据集验证中，证明方法在1.5K小时训练数据下优于179K小时数据训练的SOTA系统  <br/>5. 通过减少无效上下文输入，显著降低计算成本并提升实时对话识别效率|
|2507.23511v2|[MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio   Understanding Tasks](http://arxiv.org/abs/2507.23511v2)|总结：  <br/>提出MECAT基准和DATE评估指标，通过多专家协作与推理提升细粒度音频理解能力，结合单样本语义相似性与跨样本区分性，全面分析模型表现并开源数据与代码。<br/><br/>贡献点：  <br/>1. **提出MECAT多专家构建基准**：首次设计针对细粒度音频理解的综合评估数据集，整合领域专家模型分析与大语言模型的Chain-of-Thought推理，提供多视角、细粒度的音频描述和开放式问答任务。  <br/>2. **创新DATE评估指标**：开发结合单样本语义相似性与跨样本区分性能力的新型评估方法，通过区分泛化术语与详细描述，更精准衡量模型输出质量。  <br/>3. **系统评估SOTA音频模型**：揭示当前模型在细粒度理解任务中的性能上限与缺陷，为后续研究提供客观依据和方向指引。  <br/>4. **开放数据与代码**：公开MECAT数据集及评估代码，推动语音领域模型的可比性研究与技术迭代。|
|2507.23511v1|[MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio   Understanding Tasks](http://arxiv.org/abs/2507.23511v1)|总结：  <br/>本研究提出MECAT基准和DATE指标，解决现有音频模型评估体系的不足，提升细粒度音频理解能力，并展示全面评估结果。<br/><br/>贡献点：  <br/>1. **提出MECAT基准**：首个多专家构建的细粒度音频理解基准，整合专家模型分析与Chain-of-Thought推理，生成多视角、细粒度的音频描述和开放集问答对。  <br/>2. **设计DATE评估指标**：通过结合单样本语义相似度与跨样本可区分性，强化对具体细节的奖励机制，有效区分通用和高精度输出。  <br/>3. **系统评估最新模型**：提供全面的模型性能分析，揭示当前技术能力与局限性，为研究方向提供新的洞见。  <br/>4. **开源数据与代码**：通过公开的GitHub仓库，促进技术复现与社区验证，推动领域发展。|
|2507.23365v1|["I made this (sort of)": Negotiating authorship, confronting   fraudulence, and exploring new musical spaces with prompt-based AI music   generation](http://arxiv.org/abs/2507.23365v1)|总结：本文通过实验性音乐创作探索AI生成平台的局限性，结合LLM进行自我反思，提出对作者身份与人机协作的批判性思考，并建立新的音乐研究方法论。<br/><br/>贡献点：<br/>1. **实验性实践探索**：通过两部音乐专辑直接质疑当前提示驱动AI音乐生成的局限性，揭示其对"未经练习/打磨/制作"音乐的生成能力缺失。<br/>2. **LLM作为对话工具**：创新性地将大型语言模型用于自我访谈，产生跨人机的批判性对话，深化对AI创作与人类主体性的讨论。<br/>3. **身份协商理论框架**：提出"音乐身份变迁"的分析视角，探讨人在面对超越自身能力的AI生成系统时的创作主体性危机。<br/>4. **新音乐空间建构**：揭示艺术创作中人类与AI协作可能开辟的新型音乐表达领域，拓展AI艺术研究的边界。<br/>5. **方法论创新**：建立基于LLM中介的自我反思研究范式，为AI时代艺术创作方法论提供新的实践路径。|
|2507.20666v1|[MIMII-Agent: Leveraging LLMs with Function Calling for Relative   Evaluation of Anomalous Sound Detection](http://arxiv.org/abs/2507.20666v1)|总结：  <br/>提出基于LLMs的合成方法，生成机器类型特定异常声音，无需真实数据，突破传统与先进模型的局限，验证UASD系统评估的有效性。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的异常生成框架**：首次将大语言模型应用于机器类型异常声音的合成，通过文本描述自动选择音频变换函数，无需依赖真实异常数据。  <br/>2. **解决传统方法局限**：突破关键词标注法依赖人工定义标签、生成声音不现实且可扩展性差的问题，提升异常数据生成的灵活性。  <br/>3. **弥补先进模型缺陷**：无需真实异常训练数据，解决现有生成模型（如MIMII-Gen）在数据稀缺场景下的有效性不足问题。  <br/>4. **验证相对评估有效性**：通过对比真实与合成异常数据的检测难度趋势，证明所提方法可准确反映UASD系统在不同机器类型上的性能差异。  <br/>5. **支持无监督评估研究**：为无监督系统在缺乏真实异常数据时的相对性能评估提供可靠工具，推动异常检测研究的实用性与普适性。|