|Source|Title|Summary|
|---|---|---|
|2510.26096v1|[ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for   Audio-Language Models](http://arxiv.org/abs/2510.26096v1)|**贡献点：**<br/><br/>1. 提出首个针对Audio-Language Models（ALMs）的专用防御框架ALMGuard。  <br/>2. 设计一种识别通用Shortcut Activation Perturbations (SAPs)的方法，用于激活安全对齐的快捷方式。  <br/>3. 提出Mel-Gradient Sparse Mask (M-GSM) 技术，限制扰动范围以提高防御效果。  <br/>4. 在保持模型性能的前提下，显著降低高级别ALM特定 jailbreak 攻击的成功率，实现当前最优防御效果。  <br/><br/>**总结（100字以内）：**  <br/>ALMGuard是首个针对ALMs的专用防御框架，通过识别并激活安全快捷方式，有效抵御jailbreak攻击，维持模型在良性任务上的性能，达到当前最佳防御效果。|
|2510.24693v1|[STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D   Intelligence](http://arxiv.org/abs/2510.24693v1)|总结（100字以内）:  <br/>STAR-Bench提出音频4D智能概念，结合基础声学感知与整体时空推理任务，通过程序合成和四阶段人类标注数据，揭示现有模型在细粒度感知与物理世界理解上的局限，为音频模型发展提供新方向。<br/><br/>贡献点:  <br/>1. **定义音频4D智能**：首次将音频理解扩展到时间-空间动态推理，涵盖声音在三维空间与时间维度的感知与关系分析。  <br/>2. **构建新型基准**：设计STAR-Bench，融合基础声学感知（六属性）与时空推理（连续/离散过程、静态定位、多源关系、动态轨迹）双重任务。  <br/>3. **高质量数据生成**：采用程序合成+物理模拟（基础任务）与四阶段人类标注筛选（整体任务）的双方法，确保数据有效性。  <br/>4. **暴露模型局限**：实验显示闭源模型受限于细粒度感知，开源模型在感知、知识和推理能力上均存在显著差距。  <br/>5. **推动领域发展**：通过量化评估，为音频模型增强物理世界理解能力提供关键方向与改进路径。|
|2510.23558v1|[ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language   Models](http://arxiv.org/abs/2510.23558v1)|**贡献点总结：**  <br/>提出ISA-Bench基准测试，系统评估LALMs对指令敏感性的三大维度，揭示SOTA模型存在显著指令依赖问题，通过微调Qwen2-Audio缓解该问题但引发灾难性遗忘，为提升指令鲁棒性提供标准化评估框架。<br/><br/>**分点贡献：**  <br/>1. **首个系统性评估框架**：构建ISA-Bench动态基准，从指令描述、输出格式、任务组合三轴全面评估LALMs的指令敏感性。  <br/>2. **揭示指令依赖问题**：通过基准测试发现，现有SOTA模型在指令微调时表现显著下降，影响基础音频理解任务。  <br/>3. **针对性改进方法**：设计复杂指令-变体数据集，对Qwen2-Audio进行微调，显著提升其指令遵循性能。  <br/>4. **识别副作用**：指出改进方法导致的灾难性遗忘现象，即模型可能丢失原有任务能力。  <br/>5. **推动领域发展**：为LALMs的指令敏感性研究与优化提供标准化评估基础，强调指令鲁棒性在实际应用中的重要性。|
|2510.22603v2|[Mitigating Attention Sinks and Massive Activations in Audio-Visual   Speech Recognition with LLMs](http://arxiv.org/abs/2510.22603v2)|**贡献点分点列出：**<br/><br/>1. **首次研究多模态语音识别中的注意力汇聚（attention sinks）**  <br/>2. **发现注意力汇聚不仅出现在BOS token，还存在于中间低语义token**  <br/>3. **揭示大规模激活主要来源于MLP层，并对应固定特征索引**  <br/>4. **提出一种简单的去相关损失函数，降低BOS与其他token的余弦相似度**  <br/>5. **有效缓解中间token的注意力汇聚和大规模激活问题**  <br/>6. **在高音频-视觉特征下采率下提升WER，且在低下采率下保持稳定**<br/><br/>**总结（100字以内）：**  <br/>本文首次在多模态语音识别中分析注意力汇聚现象，揭示其来源，并提出去相关损失函数以缓解问题，提升模型在高下采率下的性能。|
|2510.22603v1|[Mitigating Attention Sinks and Massive Activations in Audio-Visual   Speech Recognition with LLMS](http://arxiv.org/abs/2510.22603v1)|**贡献点列表：**  <br/>1. **首次提出多模态注意力sink现象**：在音频-视觉语音识别中发现注意力sink不仅存在于BOS token，还出现在中间低语义token，揭示了LLM内部动态的新特征。  <br/>2. **识别巨大激活的来源与机制**：分析表明，巨大激活由MLP层产生，且对应固定特征索引，为理解多模态表示的冗余性提供了依据。  <br/>3. **揭示BOS与其他token的关联性**：中间sink token与BOS token具有高余弦相似度，导致注意力和激活的级联放大，解释了性能波动的潜在原因。  <br/>4. **提出decorrelation损失函数**：设计简单有效的损失，降低BOS与其他token的相似度，从而缓解中间sink问题并提升模型鲁棒性。  <br/>5. **验证方法的场景适应性**：在高音频-视觉特征下采样场景中显著降低WER，同时保持低下采样率下的稳定性，证明其对资源效率的优化作用。  <br/><br/>**总结（100字以内）：**  <br/>本研究首次揭示多模态语音识别中的注意力sink及巨大激活现象，提出decorrelation损失有效缓解问题，并在高下采样场景下提升识别性能，保持模型稳定性。|
|2510.22455v1|[Evaluating Multimodal Large Language Models on Core Music Perception   Tasks](http://arxiv.org/abs/2510.22455v1)|**贡献点**（分点列出）:  <br/>1. **建立音乐理解评估基准**：首次系统性评估三个SOTA多模态大语言模型（Gemini 2.5 Pro/Flash、Qwen2.5-Omni）在三项核心音乐能力（Syncopation Scoring、Transposition Detection、Chord Quality Identification）上的表现。  <br/>2. **分离影响因素**：明确区分了三个关键变量（感知输入差异、示例暴露策略、推理方法），揭示模型性能的多维限制。  <br/>3. **拓展LogicLM框架**：将LogicLM（结合LLM与符号求解器的结构化推理）应用于音乐领域，推动符号推理与感知任务的结合。  <br/>4. **揭示感知与推理的差异**：发现MIDI输入下模型性能接近上限，而音频输入显著下降，指出当前系统在音频“听觉”理解上的不足。  <br/>5. **验证推理策略有效性**：表明推理策略（如CoT、LogicLM）和少样本提示对音频任务提升有限，为改进音乐理解模型提供方向。  <br/>6. **提出方法与数据集**：通过实验数据和方法设计，明确感知-推理边界的划分，为构建以音频为核心的音乐系统提供可操作指导。  <br/><br/>**总结（100字以内）**:  <br/>本研究通过基准测试揭示多模态大语言模型在音乐理解中的感知与推理差异，指出当前系统更擅长符号推理而非音频感知，并提出改进方法与数据集以推动音频优先的音乐系统发展。|
|2510.20850v1|[Can large audio language models understand child stuttering speech?   speech summarization, and source separation](http://arxiv.org/abs/2510.20850v1)||
|2510.20504v1|[Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate   Speech Coding](http://arxiv.org/abs/2510.20504v1)|总结：  <br/>本文提出SimWhisper-Codec，通过简化Whisper模型实现语义优先的语音编解码，无需外部监督，在相同比特率下优于现有语义监督编解码器，验证了语义优先方法的有效性。<br/><br/>贡献点：  <br/>1. **提出语义优先范式**：首次尝试从语义模型反向设计语音编解码器，而非传统声学模型增强语义监督。  <br/>2. **架构简化策略**：通过针对性简化Whisper的结构，释放其在声学建模上的潜力，提升编码效率。  <br/>3. **端到端无监督设计**：构建无需外部语义监督的编解码器（SimWhisper-Codec），仅依赖冻结的Whisper编码器。  <br/>4. **性能验证**：在相同比特率下，实验表明其在语义保留和声学质量上均优于Mimi Codec和SpeechTokenizer等主流语义监督模型。  <br/>5. **开源实现**：提供代码库，便于复现与进一步研究，推动语音编解码领域方法创新。|
|2510.19055v1|[The MUSE Benchmark: Probing Music Perception and Auditory Relational   Reasoning in Audio LLMS](http://arxiv.org/abs/2510.19055v1)|**贡献点：**<br/>1. 提出MUSE基准：创建首个针对音乐感知核心能力的评估基准，包含10个任务，用于系统性测试多模态大语言模型（MLLMs）在音乐理解中的基础技能。  <br/>2. 对比实验：对四个SOTA模型（Gemini Pro、Qwen2.5-Omni、Audio-Flamingo 3）与200人规模人类基准进行对比，揭示当前模型在音乐理解中的能力差异和与人类的显著差距。  <br/>3. 模型表现分析：发现Gemini Pro在基础感知任务中表现较好，但Qwen和Audio Flamingo 3接近随机水平，指出现有模型存在严重的音乐感知缺陷。  <br/>4. CoT提示的局限性：首次验证Chain-of-Thought（CoT）提示在音乐理解任务中的不稳定性，表明其可能对模型性能产生负面影响。  <br/>5. 推动研究发展：为评估不变的音乐表示提供工具，推动AI在音乐领域更鲁棒的模型设计与研究。  <br/><br/>**总结（100字内）：**  <br/>本论文提出MUSE基准，评估MLLMs在音乐理解中的核心能力，揭示模型与人类的显著差距及CoT提示的局限性，为改进音乐感知AI系统提供关键工具和方向。|
|2510.18416v1|[SegTune: Structured and Fine-Grained Control for Song Generation](http://arxiv.org/abs/2510.18416v1)|**总结（100字以内）**:  <br/>SegTune提出非自回归框架，实现歌曲生成的结构化与可控性，通过段落级提示注入和全局风格引导提升音乐一致性和精确对齐，结合LLM-based持续时间预测及大规模数据集与新评估指标，显著优于现有方法。<br/><br/>**贡献点**:  <br/>1. **提出SegTune框架**：首个非自回归的结构化可控歌曲生成模型，支持段落级与全局提示的协同控制。  <br/>2. **段落级控制机制**：允许用户或LLM指定与歌曲段落对齐的本地音乐描述，实现更精细的结构控制。  <br/>3. **时间广播提示注入**：通过将段落提示广播到对应时间窗口，结合全局提示确保风格连贯性。  <br/>4. **LLM-based持续时间预测器**：自回归生成LRC格式时间戳歌词，提升歌词与音乐的精确对齐能力。  <br/>5. **大规模数据集构建**：设计高质量歌曲与对齐提示的数据管道，增强模型训练与泛化能力。  <br/>6. **新评估指标**：提出针对段落对齐度和声乐属性一致性的评估体系，量化模型性能。  <br/>7. **实验验证优势**：实验证明SegTune在可控性与音乐连贯性上优于现有基线方法。|
|2510.18308v1|[ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control   for Expressive Text-to-Speech Generation](http://arxiv.org/abs/2510.18308v1)|总结：  <br/>提出ParaStyleTTS框架，通过文本提示控制说话风格，无需参考音频，实现轻量、可解释的高效TTS生成，具备与LLM相近的性能但显著提升推理效率和资源利用率。  <br/><br/>贡献点：  <br/>1. **无需参考音频**：基于文本提示实现说话风格控制，解决隐私泄露与音频获取不便问题。  <br/>2. **双层级风格建模**：创新性分离韵律（prosodic）与语用（paralinguistic）风格建模，支持情感、性别、年龄等多维度细粒度调控。  <br/>3. **高效性能**：推理速度提升30倍，参数减少至1/8，CUDA内存需求降低至1/2.5，适用于低资源与实时场景。  <br/>4. **强鲁棒性与可控制性**：对提示语敏感度低，风格表现稳定，优于现有LLM方法在实际部署中的适用性。|
|2510.18169v1|[Hearing Health in Home Healthcare: Leveraging LLMs for Illness Scoring   and ALMs for Vocal Biomarker Extraction](http://arxiv.org/abs/2510.18169v1)|总结：该研究提出利用LLMs和ALMs整合语音数据，生成健康评分和口语化声学特征描述，为家庭护理数据提供新分析方法。<br/><br/>贡献点：<br/>1. 首次整合SOAP笔记与生命体征数据，生成综合健康评分用于跨访问比较<br/>2. 开发多阶段语音预处理流程，实现目标说话人短语音片段的精准提取<br/>3. 应用Audio Language Model生成可读性高的声学生物标志物描述<br/>4. 证实SOAP笔记在健康评估中的信息优势，较生命体征数据更有效<br/>5. 提供首个基于家庭护理录音的ALM健康模式识别实证研究<br/>6. 构建了处理异构家庭护理数据的端到端语音健康分析框架|
|2510.16718v1|[U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity   Speech Generation](http://arxiv.org/abs/2510.16718v1)|**贡献点分点列表：**  <br/>1. **提出U-Codec架构**：首次设计超低帧率（5Hz）语音编解码器，实现高保真重建与快速语音生成。  <br/>2. **引入Transformer帧间依赖模块**：通过Transformer编码器捕捉长时序列依赖，解决低帧率下语音质量退化问题。  <br/>3. **优化残差向量量化（RVQ）配置**：系统性探索RVQ深度与码本规模，确定最佳参数组合以平衡压缩与质量。  <br/>4. **扩展LLM-based TTS框架**：将LLM的自回归TTS模型从3层RVQ@50Hz升级至32层RVQ@5Hz，显著提升推理速度。  <br/>5. **实验证明可行性**：在保持语音自然性和相似性的同时，U-Codec使TTS推理速度比高帧率编解码器快3倍，验证低帧率压缩的实用性。<br/><br/>**总结（100字以内）：**  <br/>U-Codec通过Transformer与RVQ优化，在5Hz超低帧率下实现高保真语音合成，显著提升LLM-based TTS的推理速度，验证低帧率压缩的可行性与有效性。|
|2510.15227v1|[LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution   Designed for Speech Large Language Models](http://arxiv.org/abs/2510.15227v1)|总结：  <br/>LongCat-Audio-Codec提出一种高效音频编码方案，结合分离架构与多阶段训练策略，实现超低帧率和比特率编码，同时保持高质量语音合成与低延迟流式处理，平衡编码效率与解码质量。<br/><br/>贡献点：  <br/>1. **解耦架构与多阶段训练策略**：通过分离模型设计和分阶段训练，提升语义建模、声学特征提取及流式合成能力。  <br/>2. **超低编码效率**：支持16.67 Hz帧率、0.43-0.87 kbps比特率，显著降低传输与存储成本。  <br/>3. **高可懂度与低比特率合成**：在低比特率下仍能生成高质量语音，实现编码效率与语音质量的平衡。  <br/>4. **低延迟流式处理能力**：适用于实时语音应用，满足工业级端到端模型的高效需求。|
|2510.14664v1|[SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality   Evaluation](http://arxiv.org/abs/2510.14664v1)|总结：  <br/>提出SpeechLLM-as-Judges范式，通过多语言数据集SpeechEval和SQ-LLM模型实现结构化、可解释的语音质量评估，验证其跨任务和语言的泛化性能，并开源资源促进研究。<br/><br/>贡献点：  <br/>1. 提出基于LLM的结构化语音质量评估框架SpeechLLM-as-Judges，增强评估的可解释性。  <br/>2. 构建包含32,207条多语言语音及128,754注释的SpeechEval数据集，覆盖质量评估、对比、改进建议和深度伪造检测。  <br/>3. 开发SQ-LLM模型，结合链式推理和奖励优化提升语音质量分析能力。  <br/>4. 通过跨任务和多语言实验验证方法有效性，推动语音评估技术发展，并开放资源促进研究。|
|2510.13558v1|[Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts   Steering Module](http://arxiv.org/abs/2510.13558v1)|总结：  <br/>SteerMoE提出了一种参数高效、模块化的音频-语言对齐框架，通过动态MoE路由器和冻结模型组件，在保持LLM能力的同时实现多任务强表现。<br/><br/>贡献点：  <br/>1. **提出SteerMoE框架**：基于柏拉图表示假说，构建首个音频-语言对齐的模块化解决方，兼具参数效率与灵活性。  <br/>2. **冻结关键组件**：仅训练轻量级steering模块，保留音频编码器和LLM解码器的预训练参数，避免全模型微调。  <br/>3. **动态MoE路由机制**：通过门控路由器动态选择并应用学习到的steering向量，实现连续音频表示向LLM语义空间的渐进适配。  <br/>4. **无需修改LLM词汇**：基于连续嵌入空间对齐，完全兼容LLM原有架构，保留其高阶推理和代理能力。  <br/>5. **多任务验证效果**：在ASR、音频理解及定性函数调用任务中验证了其高效性与强表现，为音频-语言系统提供新范式。|
|2510.12995v2|[Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs](http://arxiv.org/abs/2510.12995v2)||
|2510.12995v1|[Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs](http://arxiv.org/abs/2510.12995v1)|总结（100字以内）：<br/>该研究提出基于连续语音表示的TTS方法，通过双头架构、掩码训练及两阶段优化显著提升性能，在LibriSpeech测试集中取得1.95% WER和46%相对WER降低的SOTA结果。<br/><br/>贡献点：<br/>1. 提出首个将扩散模型与MLLM结合的TTS框架，采用帧级严格自回归的连续语音表示生成模块<br/>2. 设计双头架构保留语言模型多任务能力，实现内容控制与语音生成的协同优化<br/>3. 创新性引入掩码训练策略，有效缓解自回归解码中的暴露偏差问题<br/>4. 提出两阶段训练机制，通过冻结语言模型稳定优化过程，提升模型泛化能力|
|2510.12851v1|[Adaptive vector steering: A training-free, layer-wise intervention for   hallucination mitigation in large audio and multimodal models](http://arxiv.org/abs/2510.12851v1)|总结：本文首次提出面向音频领域的向量转向方法AVS，有效缓解模型幻觉问题，并通过实验验证其在多个任务上的提升效果。<br/><br/>贡献点：<br/>1. 提出Adaptive Vector Steering (AVS)方法，通过调节模型内部状态增强音频生成与内容的关联性<br/>2. 揭示音频生成正确性与内部表征间的强相关性，为模型优化提供理论依据<br/>3. 在Audio Hallucination QA与MMAU两个基准测试中验证AVS有效性，实现8%相对准确率提升<br/>4. 首次将向量转向技术应用于音频领域，开创性地解决音频语言模型的幻觉问题|
|2510.11646v1|[BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis](http://arxiv.org/abs/2510.11646v1)|总结：  <br/>BridgeTTS通过双重表示范式解决零样本TTS的速度质量权衡与监督不匹配问题，在保持高质量合成的同时显著提升合成速度。<br/><br/>贡献点：  <br/>1. 提出**BridgeTTS框架**，基于BridgeCode的双重语音表示范式，实现稀疏标记与连续特征的协同建模。  <br/>2. **解决速度-质量矛盾**：通过预测稀疏标记减少自回归迭代，同时重建丰富连续特征以保障合成质量。  <br/>3. **缓解监督不匹配问题**：联合优化标记级（token-level）和特征级（feature-level）目标，提升语音自然度与可懂度。  <br/>4. **实验验证有效性**：在质量、说话人相似度及合成速度方面均取得显著提升，提供公开语音演示。|
|2510.11330v1|[Diffusion-Link: Diffusion Probabilistic Model for Bridging the   Audio-Text Modality Gap](http://arxiv.org/abs/2510.11330v1)|总结：  <br/>Diffusion-Link首次将扩散模型应用于自动音频字幕生成（AAC），通过减少模态差距提升多模态编码器与LLM耦合效果，在零样本和全监督任务中均取得SOTA性能，验证了扩散式跨模态桥接的有效性。<br/><br/>贡献点：  <br/>1. 提出Diffusion-Link模块，基于扩散模型实现音频与文本模态的生成式对齐，填补模态鸿沟。  <br/>2. 首次将扩散模型应用于AAC任务，突破传统知识检索中心方法的局限。  <br/>3. 通过模态差距分析，证明其在相似性与几何度量上显著优于现有扩散方法。  <br/>4. 在AudioCaps数据集上，零样本与全监督场景下均实现SOTA，相对提升达52.5%和7.5%。  <br/>5. 构建轻量网络（三残差MLP块），与冻结多模态编码器结合，无需外部知识即可提升性能。|
|2510.08618v1|[Look before Transcription: End-to-End SlideASR with Visually-Anchored   Policy Optimization](http://arxiv.org/abs/2510.08618v1)|总结：  <br/>本文提出SlideASR任务及VAPO方法，通过视觉锚定推理提升学术领域语音识别准确率，并构建实体丰富的基准数据集以促进研究。  <br/><br/>贡献点：  <br/>1. **定义SlideASR任务**：首次提出结合幻灯片视觉信息的语音识别任务，解决领域术语识别难题。  <br/>2. **提出VAPO方法**：设计基于"Think-Answer"格式的视觉锚定强化学习框架，优化OCR与语音转录的协同。  <br/>3. **四奖励机制**：通过格式合规性、OCR精度、ASR质量、视觉一致性四个维度进行多目标强化优化。  <br/>4. **构建SlideASR-Bench**：创建包含合成数据和真实场景的实体丰富基准，支持方法验证与对比研究。  <br/>5. **端到端范式**：建立高效、可扩展的视觉-语音联合识别范式，突破传统多阶段流程的局限性。|
|2510.07355v1|[AV-EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in   Omni-modal LLMS with Audio-visual Cues](http://arxiv.org/abs/2510.07355v1)|**贡献点总结（100字以内）**  <br/>本文提出AV-EMO-Reasoning基准测试，系统评估LLMs的音频-视觉情感一致性，发现视觉线索可显著提升情感表达，揭示模型在不同评估指标下的互补优势，推动更自然的人机交互研究。<br/><br/>**分点贡献：**  <br/>1. **提出首个系统评估框架**：构建AV-EMO-Reasoning基准测试，首次全面评估LLMs在情感推理任务中对音频与视觉多模态线索的整合能力。  <br/>2. **多模态数据集设计**：整合单轮/多轮合成音频-视觉语料库及真实场景数据，覆盖不同交互复杂度的测试样本。  <br/>3. **多维度评估指标**：引入连续性、分类性及感知性指标，从定量与定性角度全面分析情感一致性。  <br/>4. **揭示视觉线索的关键作用**：实验验证视觉信息对提升情感表达的显著效果，证明其对音频-视觉融合模型的必要性。  <br/>5. **互补性分析**：发现不同指标家族对情感判断的捕获差异，强调自动评分与人为感知的协同价值。  <br/>6. **推动实际应用**：为情绪感知对话系统提供可复现实验标准，助力开发更自然、适应性的人机交互技术。|
|2510.05542v1|[Sci-Phi: A Large Language Model Spatial Audio Descriptor](http://arxiv.org/abs/2510.05542v1)|总结：提出首个具备全面空间场景感知能力的音频LLM Sci-Phi，通过双空间频谱编码器实现多声源描述与环境特征估计，展现出良好的鲁棒性和真实场景泛化能力。<br/><br/>贡献点：<br/>1. 模型结构创新：首次设计具有空间编码器与频谱编码器的双通道架构，突破单通道输入限制<br/>2. 全参数估计能力：实现对最多4个方向声源、背景声和房间特性的完整参数化描述<br/>3. 大规模训练数据：基于4000小时合成一阶Ambisonics录音构建元数据驱动的训练集<br/>4. 多维度评估体系：采用排列不变协议和15项指标全面验证内容、定位、时序、响度、混响等感知性能<br/>5. 强泛化能力：在真实房间冲激响应场景下仅出现轻微性能下降，验证了模型的实际应用价值|
|2510.04593v1|[UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with   Large Language Models](http://arxiv.org/abs/2510.04593v1)|总结：  <br/>本文提出UniVoice统一语音模型框架，通过连续表示整合ASR与TTS，结合自回归和流匹配优势，设计双注意力机制并支持零样本语音克隆，实验验证其性能优于单任务模型。<br/><br/>贡献点：  <br/>1. 提出UniVoice统一框架，将语音识别与合成整合到单一LLM中，打破传统分离处理模式。  <br/>2. 采用连续语音表示替代离散标记化，降低信息损失，提升模型整体性能。  <br/>3. 融合自回归建模（ASR）与流匹配（TTS）优势，实现高质量语音生成与准确识别。  <br/>4. 设计双注意力机制，动态切换因果掩码与双向掩码，解决两类模型的内在矛盾。  <br/>5. 引入文本前缀条件的语音填充方法，支持高保真零样本语音克隆任务。  <br/>6. 实验证明方法在ASR和零样本TTS任务中达到或超越现有单任务模型效果。  <br/>7. 为端到端语音理解和生成提供新思路，推动语音处理技术一体化发展。|
|2510.04584v1|[Robustness assessment of large audio language models in multiple-choice   evaluation](http://arxiv.org/abs/2510.04584v1)|总结：  <br/>本文提出改进的MCQA评估框架，揭示LALM对选项顺序和同义改写敏感，通过系统研究三基准四模型，设计更细致的评估协议与指标，增强对模型性能的全面分析。<br/><br/>贡献点：  <br/>1. 指出现有MCQA框架忽略选项顺序变化对结果的影响，导致评估结果不准确。  <br/>2. 系统研究三个基准（MMAU、MMAR、MMSU）和四个模型（Audio Flamingo 2/3、Qwen2.5-Omni-7B-Instruct、Kimi-Audio-7B-Instruct）的敏感性差异。  <br/>3. 证明模型对问题及选项的同义改写同样敏感，提出需考虑细微变化的评估标准。  <br/>4. 设计更简单的评估协议与指标，提升对LALM性能的详细分析能力。|
|2510.04463v1|[Evaluating Self-Supervised Speech Models via Text-Based LLMS](http://arxiv.org/abs/2510.04463v1)|**贡献点：**  <br/>1. 提出基于大语言模型（LLM）的SSL任务无关评估方法，无需额外训练或调参即可通过领域线索生成可靠评分。  <br/>2. 在自动语音识别（ASR）任务中验证LLM评分与性能的显著相关性，为SSL模型评估提供新思路。  <br/>3. 表明LLM不仅能用于SSL评估，还能通过推理时嵌入支持说话人验证任务，扩展其跨领域应用价值。  <br/><br/>**总结（100字以内）：**  <br/>提出利用LLM评估SSL模型的新方法，通过领域线索生成无需训练的可靠评分，验证其在ASR任务中的有效性，并拓展至说话人验证，提升LLM在语音领域的多功能性。|
|2510.04162v1|[Drax: Speech Recognition with Discrete Flow Matching](http://arxiv.org/abs/2510.04162v1)|总结：  <br/>提出Drax离散流匹配框架，实现高效并行解码；设计音频条件概率路径模拟推理错误轨迹；理论分析揭示泛化差距与训练/推理占用差异的关系；实验验证在保持准确率的同时提升效率，推动NAR ASR发展。<br/><br/>贡献点：  <br/>1. **首个将离散流匹配应用于ASR的框架**：Drax是首个针对自动语音识别的离散流匹配模型，支持高效并行解码。  <br/>2. **音频条件概率路径设计**：通过构建音频条件概率路径，引导模型学习推理过程中可能的中间错误轨迹，而非直接使用随机噪声。  <br/>3. **理论分析指导设计**：揭示训练与推理占用分布差异导致泛化差距的核心原因，并提出通过累积速度误差进行控制的理论框架。  <br/>4. **性能优势验证**：实验证明Drax在保持与SOTA模型相当的识别准确率的同时，显著优化了准确率-效率权衡，为NAR ASR提供新方向。|
|2510.04136v1|[MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition](http://arxiv.org/abs/2510.04136v1)|总结：  <br/>MoME结合Matryoshka表示学习与稀疏MoE，实现动态压缩率调整和跨尺度泛化，提升AVSR效率与鲁棒性，提供资源感知的可扩展框架。<br/><br/>贡献点：  <br/>1. **提出动态压缩机制**：解决传统token压缩方法压缩率固定、输出长度固定的问题，支持推理时灵活平衡信息密度与效率。  <br/>2. **创新框架设计**：引入稀疏Mixture-of-Experts（MoE）与Matryoshka表示学习结合（MoME），动态分配模型容量至不同粒度与模态。  <br/>3. **共享路由器机制**：通过跨粒度专家激活一致性，使压缩序列能利用低压缩层级的表示，增强模型泛化能力与可解释性。  <br/>4. **高鲁棒性与低参数量**：实验证明在噪声环境下保持鲁棒性，同时显著减少参数量，优于现有MRL方法。  <br/>5. **多任务统一优化**：在AVSR、ASR、VSR任务中均达到SOTA性能，实现资源感知的高效语音识别解决方案。|
|2510.02995v1|[AudioToolAgent: An Agentic Framework for Audio-Language Models](http://arxiv.org/abs/2510.02995v1)|总结：提出AudioToolAgent框架，通过中央LLM代理整合音频模型工具，提升音频理解任务的多步推理能力，实现SOTA准确率并支持灵活扩展。<br/><br/>贡献点：  <br/>1. 提出AudioToolAgent框架，利用中央LLM代理协调音频语言模型与工具适配器实现多步骤推理  <br/>2. 开发支持工具选择、问答链构建及输出验证的代理机制  <br/>3. 在MMAU/MMAR/MMAU-Pro数据集上达到当前最优性能（74.10%/68.80%/57.96%）  <br/>4. 采用蒙特卡洛采样方法系统评估374种配置，发现高效工具组合  <br/>5. 模块化设计实现工具灵活集成，降低数据与训练成本|
|2510.01698v3|[TalkPlay-Tools: Conversational Music Recommendation with LLM Tool   Calling](http://arxiv.org/abs/2510.01698v3)|总结：  <br/>提出一种基于LLM的统一工具调用框架，整合多种检索方法并实现多模态支持，通过动态工具规划提升音乐推荐系统的灵活性与性能。<br/><br/>贡献点：  <br/>1. **统一工具调用架构**：构建LLM驱动的端到端音乐推荐系统，整合布尔过滤、稀疏检索、密集检索和生成式检索多种方法。  <br/>2. **工具规划机制**：动态预测工具类型、执行顺序及参数，实现用户意图与推荐流程的精准匹配。  <br/>3. **多模态与数据库整合**：支持多样化交互模态，无缝集成SQL等数据库过滤技术，增强系统兼容性。  <br/>4. **选择性检索策略**：根据用户查询灵活调用适配的检索方法，提升推荐效果的多样性与准确性。  <br/>5. **新范式提出**：为对话式推荐系统提供统一检索-重排序框架，推动生成式推荐与传统组件的协同应用。|
|2510.01698v2|[TalkPlay-Tools: Conversational Music Recommendation with LLM Tool   Calling](http://arxiv.org/abs/2510.01698v2)|总结：  <br/>提出基于LLM的统一音乐推荐框架，通过工具调用整合多检索方法，实现多模态支持与高效对话式推荐。<br/><br/>贡献点：  <br/>1. **统一框架设计**：构建LLM驱动的音乐推荐系统，融合检索-重排序流程，整合布尔过滤、稀疏检索、密集检索和生成性检索等多模块。  <br/>2. **工具调用机制**：引入工具规划模块，动态选择工具类型、执行顺序及参数，实现对用户需求的精准适配。  <br/>3. **多模态支持**：通过协调不同数据库过滤方法，支持用户查询的多样化模态（如文本、语义）。  <br/>4. **端到端推荐能力**：LLM同时完成意图理解、工具调用规划与结果生成，提升推荐系统的灵活性与效率。  <br/>5. **性能验证**：实验表明该框架在多种推荐场景中达到竞争力，为对话式推荐系统提供新范式。|
|2510.01698v1|[TalkPlay-Tools: Conversational Music Recommendation with LLM Tool   Calling](http://arxiv.org/abs/2510.01698v1)|总结：  <br/>提出一种基于LLMs的统一音乐推荐框架，融合多类检索方法和工具调用机制，实现端到端推荐系统与多数据库过滤整合，推动对话式推荐新范式。<br/><br/>贡献点：  <br/>1. **统一框架设计**：构建LLM驱动的音乐推荐系统，整合检索-重排序流程，通过工具调用实现多模块协同。  <br/>2. **多组件协同机制**：将LLM作为核心，集成布尔过滤（SQL）、稀疏检索（BM25）、密集检索（嵌入相似度）和生成式检索（语义ID）等模块。  <br/>3. **工具规划创新**：提出动态工具选择与执行顺序规划，根据用户查询精准匹配推荐策略，支持多模态交互。  <br/>4. **数据库过滤集成**：实现多种数据库过滤方法的无缝结合，提升推荐结果的精确性和多样性。  <br/>5. **性能验证**：在多样化推荐场景中验证框架有效性，证明其竞争力并开启对话式推荐新范式。|
|2510.01254v1|[Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of   Gender Bias in SpeechLLMs](http://arxiv.org/abs/2510.01254v1)|总结：  <br/>本文指出现有语音大模型偏见基准在跨任务泛化能力上的局限性，提出通过LoRA微调诱导特定MCQA行为，并验证其在不同任务中的迁移效果，设计可衡量行为可转移性的评估套件。  <br/><br/>贡献点：  <br/>1. **质疑现有假设**：挑战MCQA基准对SpeechLLMs跨任务泛化能力的默认假设，揭示其预测其他任务表现的不足。  <br/>2. **行为诱导实验**：利用LoRA适配器微调模型，分别诱导其偏好刻板印象、反刻板印象或中性/不确定的回答。  <br/>3. **跨任务验证**：系统评估诱导行为是否可迁移至其他MCQA基准及长文本生成任务，发现显著表现差异。  <br/>4. **提出评估工具**：设计新的评估套件，用于未来模型和基准的行为可转移性分析，推动公平性研究标准化。|
|2510.01157v1|[Backdoor Attacks Against Speech Language Models](http://arxiv.org/abs/2510.01157v1)|总结：首次系统研究语音模型的音频后门攻击，验证其在多任务和多模型上的高成功率，并提出基于微调的防御方法。  <br/><br/>贡献点：  <br/>1. **首次系统性研究**：针对语音语言模型提出首个音频后门攻击框架，覆盖ASR、情感识别、性别与年龄预测等任务。  <br/>2. **跨模型与数据集验证**：在四个语音编码器和三个数据集上实现90.76%-99.41%的高攻击成功率，证明攻击的广泛性。  <br/>3. **组件脆弱性分析**：定位后门传播的关键阶段，为理解攻击机制和优化防御策略提供依据。  <br/>4. **针对性防御方案**：提出基于微调的防御方法，有效缓解预训练编码器被污染的风险。|
|2509.26140v1|[OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models](http://arxiv.org/abs/2509.26140v1)|**贡献点总结（100字以内）：**  <br/>本文提出SAGE几何感知音频编码器，结合全景深度图像和房间脉冲响应提升空间推理精度；设计OWL模型集成SAGE与空间链式推理，实现高分辨率方向和距离估计；构建BiDepth数据集（百万级QA）支持室内外场景，推动语音模型训练与评估。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **SAGE编码器**：引入几何感知机制，通过全景深度图像和房间脉冲响应在训练阶段对齐双耳声学特征与3D空间结构，实现更精确的声学-空间映射。  <br/>2. **OWL模型**：基于SAGE构建空间接地的链式推理框架，支持方向到达（DoA）和距离估计，通过课程学习策略提升多步推理能力。  <br/>3. **BiDepth数据集**：创建首个含100万+问答对的跨场景数据集，融合双耳音频、全景深度图像及房间脉冲响应，用于大规模训练与评估。  <br/>4. **性能提升**：在SpatialSoundQA与BiDepth基准测试中，相比BAT的25%准确率提升，降低DoA误差11°，显著增强空间推理能力。  <br/>5. **跨模态训练方法**：首次将几何监督与音频模型结合，通过训练时使用视觉信息提升模型的鲁棒性与分辨率，推理时仅需音频输入。|
|2509.25694v2|[HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in   Music Modeling](http://arxiv.org/abs/2509.25694v2)|总结：  <br/>本研究提出HNote十六进制记谱系统，解决传统音乐格式的复杂性和结构性问题，通过数据转换与LoRA微调提升LLM在文化音乐建模中的应用效果，验证了其在语法正确性和风格连贯性上的优势。<br/><br/>贡献点：  <br/>1. **提出新型记谱系统**：设计HNote，基于十六进制编码框架，整合音高与时长信息，解决MIDI、ABC、MusicXML等格式的复杂性与结构不一致问题。  <br/>2. **优化LLM训练流程**：将12,300首江南风格歌曲从YNote转换为HNote，并采用参数高效的LoRA方法微调LLaMA-3.1模型。  <br/>3. **验证生成效果**：实验表明HNote在语法正确性上达到82.5%，BLEU与ROUGE指标显示其符号和结构相似性，生成风格连贯的音乐。  <br/>4. **探索文化音乐建模**：建立HNote作为LLM与文化音乐建模结合的有效框架，推动传统音乐领域的AI应用。|
|2509.25694v1|[HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in   Music Modeling](http://arxiv.org/abs/2509.25694v1)|总结（100字以内）:  <br/>提出HNote十六进制音乐表示系统，解决传统格式复杂和结构不一致问题，实现LLM兼容与文化音乐建模，实验验证其语法正确率和风格连贯性。  <br/><br/>**贡献点分点列出:**  <br/>1. **提出HNote新型音乐表示系统**：基于十六进制设计，扩展自YNote，整合音高与时长信息，解决MIDI等传统格式的复杂性和结构不一致问题。  <br/>2. **优化编码框架**：采用32-unit固定小节结构，确保对齐性并降低歧义，提升符号音乐生成的可学习性。  <br/>3. **适配LLM架构**：通过参数高效LoRA微调LLaMA-3.1(8B)，实现文化音乐建模的端到端训练，减少计算资源消耗。  <br/>4. **实验证明有效性**：在12,300首江南风格歌曲上验证，语法正确率达82.5%，BLEU和ROUGE评估显示高结构与风格相似性。  <br/>5. **文化音乐建模应用**：首次将LLM应用于传统音乐风格生成，为文化传承与符号音乐生成提供新方法。|
|2509.23435v1|[AudioRole: An Audio Dataset for Character Role-Playing in Large Language   Models](http://arxiv.org/abs/2509.23435v1)|总结：  <br/>本文提出AudioRole数据集与ARP-Eval评估框架，显著提高音频角色扮演模型的语音个性化和内容适配能力，实验证明其性能优于现有模型。<br/><br/>贡献点：  <br/>1. 构建首个语音角色扮演数据集AudioRole，包含13部电视剧、1K+小时同步音频-文本对及1M+角色标注对话。  <br/>2. 提出ARP-Eval双维度评估框架，分别衡量语音个性化（Acoustic Personalization）和内容适配（Content Personalization）。  <br/>3. 通过训练ARP-Model，在语音个性化得分（0.31）和内容适配得分（0.36）上均超越现有模型，提升38%。|
|2509.23350v1|[ABC-Eval: Benchmarking Large Language Models on Symbolic Music   Understanding and Instruction Following](http://arxiv.org/abs/2509.23350v1)|**贡献点（分点）：**  <br/>1. **提出首个专门针对文本化ABC记谱的开源基准ABC-Eval**，聚焦音乐理解与指令遵循能力评估。  <br/>2. **构建涵盖10个子任务的多样化测试集**（1,086个样本），范围从基础语法到复杂序列推理。  <br/>3. **系统评估七种SOTA语言模型**，揭示现有模型在符号音乐处理中的显著局限性。  <br/>4. **验证基准的可靠性**，证明不同基线在多子任务中的表现一致性。  <br/><br/>**总结（100字内）：**  <br/>本文提出首个文本化ABC记谱评估基准ABC-Eval，涵盖10个子任务及1,086个样本，系统测试七种SOTA模型，揭示其在音乐理解与推理中的局限性，同时验证基准的可靠性，为后续研究提供关键参考。|
|2509.23299v2|[MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow](http://arxiv.org/abs/2509.23299v2)|1. **提出一步生成语音增强框架（MeanFlowSE）**：通过MeanFlow技术实现单步潜在空间优化，替代传统多步采样（如扩散/流匹配），显著提升推理效率。  <br/>2. **基于自监督学习表征（SSL）而非VAE潜变量**：模型条件设计采用SSL提取的声学-语义特征，增强训练的鲁棒性和指导性。  <br/>3. **在感知质量与可懂度上达到SOTA水平**：在Interspeech 2020 DNS Challenge盲测与模拟测试集上，性能优于近期生成模型，且降低模型规模与实时因子（RTF）。  <br/>4. **开源代码促进实际应用与复现**：论文发表后公开代码，推动算法在现实场景中的部署与研究。|
|2509.23299v1|[MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow](http://arxiv.org/abs/2509.23299v1)|**贡献点：**  <br/>1. 提出MeanFlowSE框架，通过单步生成策略显著降低实时因子（RTF）和模型规模，克服传统多步采样（如扩散/流匹配）和大模型限制。  <br/>2. 首次将MeanFlow用于预测平均速度场，实现一步潜在变量优化，提升推理效率。  <br/>3. 引入自监督学习（SSL）表示作为条件输入，替代VAE潜变量，增强模型的声学-语义引导能力。  <br/>4. 在Interspeech 2020 DNS Challenge盲测集与模拟集上达成SOTA感知质量，同时保持竞争力的可懂度。  <br/>5. 开源代码以促进研究与实际应用。  <br/><br/>**总结（100字以内）：**  <br/>MeanFlowSE通过单步生成与自监督条件策略，显著提升语音增强实时性与效率，同时保持优质感知与可懂度，为实际部署提供高效解决方案。|
|2509.22651v1|[VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,   Speaking, and Viewing](http://arxiv.org/abs/2509.22651v1)|贡献点：  <br/>1. **提出首个多模态语音助手综合评估基准**：VoiceAssistant-Eval包含10,497个精心筛选的例子，涵盖听（自然声音、音乐、对话）、说（多轮对话、角色扮演）、看（异构图像）三大任务类型。  <br/>2. **揭示模型能力差异与挑战**：评估结果显示，专有模型未必优于开源模型；多数模型在口语任务表现优异，但音频理解能力较弱；小模型（如Step-Audio-2-mini）可与大模型竞争。  <br/>3. **识别关键技术瓶颈**：指出当前模型在多模态输入处理、角色扮演语音模仿、稳健性及安全性对齐方面存在显著不足。  <br/>4. **建立严谨评估框架**：为下一代AI助手的开发提供系统性指导，并公开代码与数据促进研究。|
|2509.22062v1|[Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling](http://arxiv.org/abs/2509.22062v1)|贡献点：<br/>1. 提出S3Codec语义编码器，通过语义蒸馏将语言特征注入主代码本，解决单代码本建模的信息损失问题<br/>2. 设计"理解-生成"双Transformer架构，分离文本理解与声学生成流程，建立跨模态语义表征<br/>3. 开发MAPI无参数推理策略，动态引导解码过程抑制局部误差，提升生成稳定性<br/><br/>总结：CaT-TTS通过语义增强编码器、双阶段Transformer架构和动态推理策略，实现零样本语音合成的语义对齐与生成稳定性提升。|
|2509.21990v1|[WAVE: Learning Unified & Versatile Audio-Visual Embeddings with   Multimodal LLM](http://arxiv.org/abs/2509.21990v1)|总结（100字以内）：  <br/>提出WAVE模型，首次实现文本-音频-视频统一嵌入空间，通过层次化特征融合与联合多模态多任务训练，显著提升跨模态检索与提示感知能力，在多个基准测试中取得SOTA，推动动态多模态学习应用。<br/><br/>贡献点：  <br/>1. **首个统一多模态嵌入**：WAVE是首个基于LLM的文本、音频、视频统一表示模型，打破模态间壁垒。  <br/>2. **创新融合策略**：提出层次化特征融合方法，增强跨模态信息交互与对齐能力。  <br/>3. **联合多任务训练**：通过多模态、多任务联合训练，实现any-to-any跨模态检索与提示感知嵌入生成。  <br/>4. **性能突破**：在MMEB-v2视频基准测试中取得SOTA，在音频/视频到音频检索和多模态问答任务中表现优异。  <br/>5. **消融实验验证**：证明联合训练策略的有效性，全面提升各模态性能。  <br/>6. **新基准引入**：发布首个支持动态音频-视觉学习的基准测试，拓展跨模态研究方向。  <br/>7. **开源资源**：提供代码、模型和数据，促进研究复现与应用。|
|2509.21144v1|[UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice](http://arxiv.org/abs/2509.21144v1)|总结：  <br/>本研究提出UniSS，一个单阶段表达性语音到语音翻译框架，结合语音语义和风格建模，开发跨模态链式提示过程，构建大型高质量UniST数据集，并在翻译准确性和语音质量上显著超越现有方法，为下一代系统提供更简单有效的范式。<br/><br/>贡献点：  <br/>1. 提出UniSS单阶段框架，解决多阶段处理复杂性，简化表达性S2ST系统构建流程。  <br/>2. 设计语音语义与风格建模模块，实现语音内容与情感风格的统一表征。  <br/>3. 开发跨模态链式思维提示过程（cross-modal chain-of-thought prompting），通过逐步对齐音频与文本语义确保风格迁移。  <br/>4. 构建并发布44.8k小时高质量表达性S2ST数据集UniST，填补数据稀缺问题。  <br/>5. 在翻译保真度、语音质量及语音/情绪/时长一致性方面实验验证性能显著提升。  <br/>6. 建立更有效、可扩展的表达性S2ST系统范式，推动语音生成与翻译技术融合。|
|2509.21033v1|[SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text   Contrastive Learning with Support Vector Regularization](http://arxiv.org/abs/2509.21033v1)|**总结（100字以内）:**  <br/>提出支持向量正则化（SVR）方法，通过控制负样本垂直分量解决对比学习中的优化漂移问题，并设计两种无监督策略优化语义半径，实验验证其在多项任务上优于现有基线。<br/><br/>**贡献点分点**：  <br/>1. **揭示负样本垂直分量的双重作用**：首次指出对比学习中负样本的垂直分量既含丰富补充信息，又易引发训练不稳定和优化漂移问题。  <br/>2. **提出支持向量正则化（SVR）方法**：设计辅助支持向量机制，精准控制垂直分量以平衡信息利用与优化稳定性。  <br/>3. **双策略优化语义半径**：开发直接参数化和带约束的自适应半径预测模块，提升半径建模的准确性与鲁棒性。  <br/>4. **验证方法有效性**：通过跨任务实验（分类、单语/多语检索）和理论分析，证明SVR在标准音频-文本数据集上显著优于InfoNCE和SigLIP等基线模型。|
|2509.20802v2|[SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS](http://arxiv.org/abs/2509.20802v2)|贡献点：  <br/>1. 提出SPADE框架，结合基于词错误率的结构化剪枝与多级知识蒸馏技术，实现LLM-TTS模型的高效压缩。  <br/>2. 通过剪枝显著降低Transformer深度，减少VRAM占用20%并提升实时生成速度1.7倍，同时保持近似原始模型的感知质量。  <br/>3. 在零样本基准测试中验证了压缩模型在语音自然性与说话人相似性方面的有效性，证明其具备实用化部署潜力。  <br/><br/>总结：  <br/>SPADE通过结构化剪枝与自适应蒸馏，实现LLM-TTS模型的高效压缩，显著降低资源占用与生成延迟，同时保持语音质量，推动实际应用。|
|2509.20802v1|[SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS](http://arxiv.org/abs/2509.20802v1)|总结：  <br/>SPADE提出结构化剪枝与多级知识蒸馏框架，显著压缩LLM-TTS模型规模，在保持感知质量的同时提升推理效率，推动高效语音生成技术的实际应用。<br/><br/>贡献点：  <br/>1. **结构化剪枝方法**：基于词错误率（WER）构建层重要性指标，高效移除非必要的Transformer层以降低参数量。  <br/>2. **多级知识蒸馏技术**：通过分层策略恢复自回归生成的连贯性，解决剪枝后语音质量下降问题。  <br/>3. **高效模型压缩效果**：在零样本任务中实现近似原模型的感知质量，同时将Transformer深度减半、VRAM占用降低20%、推理速度提升1.7倍（仅需5%训练数据）。  <br/>4. **实用化验证**：证明紧凑模型可兼顾自然语音生成与实时性能，为LLM-TTS的工程部署提供关键解决方案。|
|2509.20641v1|[Investigating Modality Contribution in Audio LLMs for Music](http://arxiv.org/abs/2509.20641v1)|**贡献点总结（分点）:**  <br/>1. **提出模态贡献量化方法**：首次将MM-SHAP框架应用于音频大语言模型（Audio LLMs），通过Shapley值量化音频和文本模态对模型输出的相对贡献。  <br/>2. **揭示音频与文本的协同作用**：实验证明，尽管高准确模型更依赖文本，但音频信息在关键事件定位中仍发挥重要作用，表明音频未被完全忽略。  <br/>3. **推动可解释AI研究**：为Audio LLMs的可解释性分析提供新工具，助力理解模型内部机制，为音频和AI领域未来研究奠定基础。  <br/><br/>**摘要总结（100字以内）:**  <br/>该研究首次利用MM-SHAP框架分析Audio LLMs的模态贡献，揭示文本与音频的协同作用，论证音频未被完全忽略，为可解释AI研究提供基础。|
|2509.19755v1|[Can Audio Large Language Models Verify Speaker Identity?](http://arxiv.org/abs/2509.19755v1)|总结：  <br/>本文提出将说话人验证转化为音频问答任务，设计硬对样本采样策略与轻量微调方法，提升ALLMs的零样本SV性能，并成功应用于文本相关SV任务，验证其作为统一鲁棒模型的潜力。<br/><br/>贡献点：  <br/>1. **任务重构**：首创将说话人验证（SV）框架重构为音频问答任务，拓展ALLMs在语音领域的应用范式。  <br/>2. **硬对采样策略**：提出基于规则的硬样本对生成方法，增强训练数据挑战性以提升模型鲁棒性。  <br/>3. **轻量微调优化**：通过监督微调显著改善ALLMs的零样本SV性能，但与传统模型仍存在性能差距。  <br/>4. **文本相关SV应用**：首次探索文本依赖SV，联合验证身份与语音内容，效果接近级联ASR-SV系统。  <br/>5. **统一模型潜力**：论证ALLMs经适配后可作为通用鲁棒SV模型，兼具音频理解与身份验证能力。|
|2509.19745v1|[PART: Progressive Alignment Representation Training for Multilingual   Speech-To-Text with LLMs](http://arxiv.org/abs/2509.19745v1)|**贡献点：**  <br/>1. **提出PART框架**：设计了一个多阶段、多任务的框架，首次将同语言和跨语言对齐分离，解决多语言语音与文本表示对齐的挑战。  <br/>2. **动态参数激活**：在跨语言训练阶段动态激活LLM参数，避免传统方法中冻结参数导致的跨语言收敛问题，提升模型灵活性。  <br/>3. **文本任务增强**：通过分阶段引入文本相关任务（如翻译、理解），进一步强化多语言能力，优化语音模态的跨语言泛化与语言特异性。  <br/>4. **实验验证有效性**：在CommonVoice 15、Fleurs、Wenetspeech和CoVoST2等多语言数据集上验证，证明PART在性能和平衡性上优于现有方法。  <br/>5. **通用性与扩展性**：方法适用于多种语音模态对齐任务，展示了其在多语言场景下的广泛应用潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出PART框架，通过动态激活参数和分阶段任务分离，有效解决多语言语音与文本对齐问题，实验验证其性能优于传统方法，具备语言特异性与跨语言泛化能力，适用于多种语音模态任务。|
|2509.19676v1|[Thinking While Listening: Simple Test Time Scaling For Audio   Classification](http://arxiv.org/abs/2509.19676v1)|总结（100字以内）：  <br/>提出"听中思"框架提升音频分类性能，解决思考机制集成和新架构设计两个核心问题，验证测试时扩展的有效性，并发现轻量级方法可超越大参数文本推理模型。<br/><br/>贡献点：  <br/>1. 提出"思考-听"（think-while-listening）框架，将推理能力引入音频分类流程以提升性能  <br/>2. 分析并解决两个核心问题：现有模型的推理机制改进与全新架构的端到端设计  <br/>3. 证明测试时扩展（test-time scaling）能带来持续准确率提升，样本数量增加时效果更显著  <br/>4. 通过对比实验发现：仅重训练小模型嵌入矩阵的轻量方法可超越大参数文本推理模型（如GPT-2 vs. GPT-OSS-20B/Qwen3-14B）的零样本推理表现|
|2509.19592v1|[Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech   Generation](http://arxiv.org/abs/2509.19592v1)|总结：  <br/>该研究提出两种基于局部Transformer的架构，支持高效帧堆叠解码，并系统分析了并行与迭代策略的权衡，为语音生成模型的部署提供实用选择指南。<br/><br/>贡献点：  <br/>1. 提出两种LT架构：自回归Transformer（逐帧生成代码本）与MaskGIT-based Transformer（迭代掩码预测），解决多代码本依赖问题。  <br/>2. 引入帧堆叠机制，使主Transformer预测多帧联合，结合LT解码代码本，在保持感知质量前提下提升生成速度。  <br/>3. 系统分析不同吞吐量和质量需求下，平行与迭代采样策略的性能权衡，揭示其适用场景。  <br/>4. 提出基于部署优先级（如计算效率与合成保真度）的实用解码策略选择指南，指导实际应用优化。|
|2509.19469v1|[MusiCRS: Benchmarking Audio-Centric Conversational Recommendation](http://arxiv.org/abs/2509.19469v1)|总结：  <br/>本文提出首个音乐领域对话推荐基准MusiCRS，整合音频与对话数据，支持多模态评估，揭示系统在音频推理上的局限，推动跨模态知识整合的进展。<br/><br/>贡献点：  <br/>1. **首个音频重心基准数据集**：MusiCRS是首个将真实Reddit用户对话与对应音频轨道关联的对话推荐数据集，填补音乐领域的研究空白。  <br/>2. **多模态多样性**：涵盖477个高质量对话（涉及古典、嘻哈、电子等多音乐流派）和3,589个音乐实体，提供YouTube音频链接以支持音频内容验证。  <br/>3. **系统性评估框架**：支持音频-only、query-only及音频+query（多模态）三种输入模态配置，便于对比音频大模型、检索框架与传统方法的性能。  <br/>4. **揭示核心挑战**：实验表明现有系统过度依赖文本信号，无法有效处理音频内容的抽象推理，指出现有跨模态知识整合的理论瓶颈。  <br/>5. **开放资源支持**：提供数据集、评估代码与基线，促进后续研究与模型开发。|
|2509.19001v1|[HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS](http://arxiv.org/abs/2509.19001v1)|**贡献点：**  <br/>1. 提出HD-PPT框架，将语音合成转化为结构化、分层的任务，解决单级文本指令与多级语音标记间的模态控制难题。  <br/>2. 设计新型语音编解码器，通过ASR和CLAP目标监督，提取细粒度的提示偏好和内容偏好语音标记。  <br/>3. 引入分层解码策略，按语义、细粒度风格、声学表示顺序生成语音标记，实现可控性与自然度的平衡。  <br/>4. 实验证明该方法在指令遵循度和语音自然度上达到SOTA，验证了精确可控语音合成的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HD-PPT框架，通过分层解码和新型语音编解码器解决TTS精确控制问题，提升指令遵循度与语音自然度，为可控语音合成提供新方法。|
|2509.18700v1|[Enhancing Automatic Chord Recognition through LLM Chain-of-Thought   Reasoning](http://arxiv.org/abs/2509.18700v1)|总结：  <br/>本文提出利用大语言模型整合多MIR工具的创新框架，通过文本化音频信息提升和弦识别性能，在多个数据集上实现1-2.77%的准确率提升，为音乐信息检索的多工具协作开辟新方向。<br/><br/>贡献点：  <br/>1. **提出LLM作为MIR工具整合桥梁**：首次将语言模型应用于连接音乐源分离、调识别、和弦识别等不同MIR模块，实现跨技术协同。  <br/>2. **文本化音频信息处理流程**：将音频衍生的音乐信息转化为文本，使LLM具备推理和修正能力，提升和弦识别任务的准确性。  <br/>3. **五阶段链式思维框架**：设计系统化的5阶段流程，利用音乐理论知识整合多MIR工具输出，优化和弦识别结果。  <br/>4. **实验证明有效性**：在三个数据集上验证方法，显示在MIREX等指标上实现1-2.77%的整体准确率提升。  <br/>5. **推动多工具协作研究**：为音乐信息检索任务中的多工具集成与协同提供新思路，拓展LLM在音频分析的应用场景。|
|2509.18606v1|[FlexSED: Towards Open-Vocabulary Sound Event Detection](http://arxiv.org/abs/2509.18606v1)|总结：  <br/>本文提出FlexSED，首个开放词汇声事件检测系统，结合SSL音频模型与CLAP文本编码器，通过编码器-解码器结构和自适应融合策略实现连续训练，利用LLM优化标签选择，有效解决零样本/少样本检测与文本查询问题，并开源代码模型支持研究应用。<br/><br/>贡献点：  <br/>1. **开放词汇SED系统**：首次构建支持自由文本查询的开放词汇声事件检测框架，突破传统多类分类的限制。  <br/>2. **跨模态融合架构**：集成预训练音频SSL模型与CLAP文本编码器，采用编码器-解码器结构及自适应融合策略，提升多模态信息处理能力。  <br/>3. **连续训练机制**：设计从预训练权重进行高效微调的连续训练框架，增强模型对未见类别的泛化能力。  <br/>4. **LLM辅助标签选择**：引入大语言模型解决训练数据标签缺失问题，优化事件查询生成与监督策略。  <br/>5. **强零样本/少样本能力**：在AudioSet-Strong数据集上验证，显著优于传统模型，同时具备灵活的文本查询与小样本适配性。  <br/>6. **开源促进应用**：公开代码与预训练模型，推动开放词汇SED技术在实际场景中的研究与落地。|
|2509.18570v1|[HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for   Multi-Task Speech Language Modeling](http://arxiv.org/abs/2509.18570v1)|**贡献点总结（100字以内）**：  <br/>提出HarmoniFuse框架，通过组件选择和提示适应机制，解决多任务语音模型的任务干扰问题，实现异构任务需求的协同处理，并引入批处理交错训练策略，提升ASR与SER性能，为真实数据约束下的多任务语音理解提供可扩展解决方案。<br/><br/>**分点贡献**：  <br/>1. **提出HarmoniFuse框架**：首次设计组件选择性和提示适应性机制，通过动态融合任务相关语音表征，有效缓解多任务间的干扰与性能退化问题。  <br/>2. **任务感知的组件选择**：采用门控语音编码器提取任务特定的声学特征，精准区分ASR与SER所需的不同信息类型（语言内容 vs. 语言情感线索）。  <br/>3. **动态融合模块**：基于任务特征自适应聚合Transformer层，实现模型对多任务需求的灵活响应与协同建模。  <br/>4. **批处理交错训练策略**：无需联合标注，通过分离ASR与SER数据集的训练方式提升模型泛化能力，降低数据标注成本。  <br/>5. **实验验证有效性**：在真实数据约束下验证框架的提升效果，证明其在多任务语音理解中的可扩展性与鲁棒性。|
|2509.18569v1|[Explore the Reinforcement Learning for the LLM based ASR and TTS system](http://arxiv.org/abs/2509.18569v1)|总结：本研究提出轻量级RL框架并验证其在语音模型中的效果，通过奖励函数设计与数据构建优化提升ASR和TTS性能，结合GRPO与DiffRO方法实现突破，证明RL在小数据场景下仍有显著优势。<br/><br/>贡献点：<br/>1. 提出首个针对音频处理的轻量级强化学习框架，适配音频输入-输出的LLMs<br/>2. 首次系统验证RL在ASR任务中的有效性，探索规则奖励函数与数据构建策略<br/>3. 对比GRPO与DiffRO方法，创新性融合两者优势提升TTS性能<br/>4. 验证RL在有限训练数据和优化步数下的泛化能力<br/>5. 提供音频领域RL应用的实践范式，填补该方向研究空白|
|2509.17901v1|[Does Audio Matter for Modern Video-LLMs and Their Benchmarks?](http://arxiv.org/abs/2509.17901v1)|贡献点总结（100字以内）:  <br/>揭示视频理解中音频的冗余性，提出音频编码器与轻量级压缩技术，设计新数据集推动评估改进，指出学术研究与实际需求的差距，并开源代码促进研究发展。  <br/><br/>分点贡献：  <br/>1. **音频冗余性分析**：发现多数现有视频理解任务的基准测试可通过单帧图像解决，音频信息在当前模型评价中作用有限。  <br/>2. **音频处理优化**：基于LLaVA-OneVision架构，集成Whisper音频编码器，并采用Mamba-based压缩技术解决音频token爆炸问题。  <br/>3. **新数据集与工具发布**：推出AVQA-Hard和Music-AVQA-Hard两个更具挑战性的数据集，并开源模型及代码，推动更可信的音频-视觉模型评估。  <br/>4. **学术与实际需求的差距**：揭示学术界对音频-视觉模型的评估方式与实际应用场景的不匹配，提供针对性改进工具。|
|2509.16990v1|[Advancing Speech Understanding in Speech-Aware Language Models with GRPO](http://arxiv.org/abs/2509.16990v1)|总结（100字以内）:  <br/>本文提出基于GRPO的SALLM训练方法，应用于开放格式语音理解任务。通过BLEU奖励信号优化模型，实验证明效果优于标准SFT，并探索离策略样本的整合以提升性能。<br/><br/>贡献点：  <br/>1. **方法创新**：提出Group Relative Policy Optimization (GRPO)框架用于训练Speech-Aware Large Language Models (SALLMs)。  <br/>2. **任务扩展**：将GRPO从多选任务延伸至开放格式任务（如语音问答、语音翻译），更贴合生成能力评估。  <br/>3. **奖励机制**：首次将BLEU作为奖励信号，优化模型生成质量。  <br/>4. **实证验证**：通过实验表明该方法在关键指标上显著优于标准监督微调（SFT）。  <br/>5. **研究方向**：探索离策略样本在GRPO中的潜在应用，为后续改进提供新思路。|
|2509.16975v1|[Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs](http://arxiv.org/abs/2509.16975v1)|总结：  <br/>提出首个基于自然语言的音频编辑评估框架，结合多模态大语言模型的微调与提示技术，实现更准确、可解释的评价，同时开源代码和演示。<br/><br/>贡献点：<br/>1. **首个自然语言驱动框架**：构建了首个利用自然语言描述进行音频编辑评估的自动化框架，替代传统客观指标，提升感知分析深度。  <br/>2. **双任务微调与推理增强**：引入两个微调任务结合Chain-of-Thought提示与轻量级指令调整，显著提升多音频理解与逐步推理能力。  <br/>3. **性能优势验证**：实验表明框架在准确性、可解释性上优于基线方法，且与主观评测和客观指标高度一致。  <br/>4. **开源实现**：提供代码和演示，便于复现与实际应用，推动相关领域的研究与技术落地。|
|2509.16971v2|[AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for   Coarse-to-Fine Audio Deep Reasoning](http://arxiv.org/abs/2509.16971v2)|总结：提出首个训练无多智能体系统AGR，通过文本化文档和主动迭代精炼机制提升音频推理性能，取得SOTA结果。<br/><br/>贡献点：<br/>1. 首创训练无（training-free）多智能体系统AGR，实现感知与推理的统一协调<br/>2. 引入"粗到细"认知范式，将音频深度推理转化为复杂文本理解任务<br/>3. 设计工具增强的主动迭代文档精炼循环，包含专用智能体和路径优化机制<br/>4. 建立完整的音频-文本证据链构建流程，实现信息缺失的持续检索与补充<br/>5. 在多个基准测试中取得SOTA性能，验证了方法的有效性和先进性|
|2509.16971v1|[AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for   Coarse-to-Fine Audio Deep Reasoning](http://arxiv.org/abs/2509.16971v1)|总结：提出首个无需训练的多智能体音频推理系统AGR，通过文本化文档与迭代优化机制提升性能，实现SOTA结果并开源代码。<br/><br/>贡献点：<br/>1. **首个统一训练-free多智能体系统**：构建了首个无需额外训练的音频深度推理框架AGR，实现感知与推理的协同处理。<br/>2. **范式转换**：创新性地将音频深度推理转化为复杂文本理解任务，挖掘大语言模型的潜力。<br/>3. **人类认知模拟**：采用"粗到细"分层推理范式，通过音频→文本文档的初步转化和迭代优化机制模拟人类认知过程。<br/>4. **主动探索机制**：设计工具增强的文档优化环路，结合专用智能体实现信息缺失检测与证据链增强。<br/>5. **性能突破**：在多个基准测试中取得SOTA效果，验证了方法的有效性和先进性。<br/>6. **开源贡献**：提供完整代码库促进研究复现与技术发展。|
|2509.16649v1|[AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval](http://arxiv.org/abs/2509.16649v1)|总结：本文提出基于双编码器和对比学习的音频-文本检索系统，结合知识蒸馏与大语言模型数据增强技术，引入聚类辅助任务提升性能，最终在Clotho数据集上取得46.62%和48.83%的mAP@16成绩。<br/><br/>贡献点：  <br/>1. 提出双编码器架构，通过对比学习对齐音频与文本表征。  <br/>2. 引入知识蒸馏与大语言模型（LLM）驱动的数据增强策略（包括回译与LLM mix）。  <br/>3. 设计聚类辅助分类任务，优化模型微调效果。  <br/>4. 在Clotho开发测试集上实现单系统46.62%、四系统集成48.83%的mAP@16性能指标。|
|2509.16622v2|[Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](http://arxiv.org/abs/2509.16622v2)|总结：  <br/>本研究探索了扩散模型在自动语音识别中的应用，提出基于LLaDA的级联系统与独立解码器方案，在提升识别准确率的同时，优化推理效率，为扩散模型在语音处理领域提供了实证基础。<br/><br/>贡献点：  <br/>1. **首次将扩散模型应用于ASR**：验证LLaDA在自动语音识别任务中的潜力，提出基于扩散模型的级联系统（Whisper-LLaDA）。  <br/>2. **提出改进策略**：通过随机掩码、低置信度掩码和半自回归方法，显著降低Whisper-LLaDA的词错误率（WER）。  <br/>3. **验证音频条件化重要性**：对比普通文本LLaDA，证明音频条件嵌入对提升ASR性能的关键作用。  <br/>4. **探索解码器效率优化**：评估LLaDA作为独立解码器，在保持较高准确率的同时实现更快的推理速度。|
|2509.16622v1|[Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](http://arxiv.org/abs/2509.16622v1)|总结：  <br/>本研究通过引入扩散模型LLaDA用于语音识别，提出三种掩码策略并验证音频嵌入的重要性，展示了其在LibriSpeech数据集上显著降低WER的效果，同时探讨了扩散模型在ASR中的优化潜力。<br/><br/>贡献点：  <br/>1. **比较DLLMs与传统方法**：验证扩散模型LLaDA在ASR任务中相比自回归解码器（如Whisper-LLaMA）的竞争力，尤其在WER指标上表现突出。  <br/>2. **提出三种掩码策略**：探索随机掩码、低置信度掩码和半自回归策略，证明LLaDA在提升Whisper-LLaMA转录质量方面的作用。  <br/>3. **强调音频条件嵌入关键性**：通过对比纯文本LLaDA与音频条件模型，证明声学特征对模型性能的直接影响，凸显音频嵌入的重要性。  <br/>4. **评估独立解码器性能**：测试LLaDA作为独立ASR解码器的效果，显示其在推理速度上的优势，尽管识别准确率略低于Whisper-LLaMA。  <br/>5. **提供实证改进方向**：为扩散模型在语音识别中的应用提供实验依据，揭示其潜在优化路径，如结合扩散与半自回归解码技术。|
|2509.15775v1|[EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large   Language Model](http://arxiv.org/abs/2509.15775v1)|总结：本文提出EmoQ框架，通过EmoQ-Former和MAL实现多模态情感融合与协同优化，结合软提示注入技术提升性能，并在IEMOCAP和MELD数据集上达到SOTA，为SER提供新范式。<br/><br/>贡献点：  <br/>1. 提出EmoQ框架：融合单模态情感信息与多模态特征对齐问题，构建端到端多模态情感识别系统。  <br/>2. 引入EmoQ-Former模块：生成跨模态查询嵌入，实现多模态信息的有效融合。  <br/>3. 设计多目标情感学习（MAL）策略：通过协同优化提升模型对复杂情感的判别能力。  <br/>4. 提出软提示注入机制：将多模态表征动态融入大语言模型，增强情感推理的准确性。  <br/>5. 验证有效性：在IEMOCAP和MELD数据集上取得当前最优性能（SOTA），证明框架优势。  <br/>6. 建立新范式：为语音情感识别领域的多模态融合提供理论与技术参考。|
|2509.15680v1|[Mamba-2 audio captioning: design space exploration and analysis](http://arxiv.org/abs/2509.15680v1)|总结：  <br/>本研究提出基于Mamba-2的音频描述模型，系统探索了LLM规模、LoRA秩和连接器设计，实现参数更少但性能优于大模型，并首次深入分析多个因素对音频captioning的影响。<br/><br/>贡献点：  <br/>1. 构建首个基于Mamba-2大语言模型的音频captioning系统，利用其SOTA状态空间模型结构；  <br/>2. 系统性研究LLM参数规模、LoRA秩和连接器设计，结合Mamba-2的线性时间复杂度优势；  <br/>3. 实现参数更少但性能更强的音频描述效果，超越同数据集上训练的更大模型；  <br/>4. 首次全面分析LLM参数数量、音频编码器微调策略、特征多样性及降/扩技术对性能的影响。|
|2509.15667v1|[VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](http://arxiv.org/abs/2509.15667v1)|**贡献点：**  <br/>1. 提出首个将预训练解码器型LLM与声学编码器-解码器架构（如Whisper）融合的框架，构建语音增强的大语言模型。  <br/>2. 引入**音频条件文本空间**作为中间对齐机制，替代直接使用音频嵌入，提高多模态表示对齐效果。  <br/>3. 实现完全基于连续文本表示空间的融合方法，通过跨模态注意力机制结合Whisper与LLM隐藏状态。  <br/>4. 支持**离线和实时流式模式**，扩展模型的应用场景。  <br/>5. 首次发布希腊语音LLM**VoxKrikri**，并在希腊ASR任务中达到SOTA性能，平均提升20%相对基准。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出多模态融合框架，通过音频条件文本空间实现跨模态对齐，支持离线与流式处理，构建首个希腊语音LLM（VoxKrikri），并在希腊ASR任务中取得SOTA性能，平均提升20%。|
|2509.15654v2|[EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced   Audio-Language Model for Generalized Speech Emotion Recognition](http://arxiv.org/abs/2509.15654v2)|总结：  <br/>本研究提出EMO-RL框架，通过创新的强化学习策略解决语音情感识别中的收敛不稳定和模型规模限制问题，实现情感推理能力的显著提升，并在多个数据集上取得SOTA结果，验证了其优秀的泛化能力。<br/><br/>贡献点：  <br/>1. **提出EMO-RL框架**：首次结合强化学习与语音情感识别，解决因情感边界模糊导致的收敛不稳定性和小模型推理能力不足的双重挑战。  <br/>2. **创新性模块设计**：引入**情感相似性加权奖励（ESWR）**和**显式结构化推理（ESR）**，通过情感约束优化提升模型对情感信号的敏感性和推理准确性。  <br/>3. **基于预训练模型的优化方法**：采用**组相对策略优化**，将情感约束融入策略更新过程，增强模型在复杂情感场景下的适应性。  <br/>4. **验证效果与泛化能力**：在MELD和IEMOCAP数据集上达到当前最优性能，并通过跨数据集实验证明了方法的强泛化能力。|
|2509.15654v1|[EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced   Audio-Language Model for Generalized Speech Emotion Recognition](http://arxiv.org/abs/2509.15654v1)|贡献点分点总结：<br/>1. 提出EMO-RL框架，将强化学习与情感建模结合，解决LALMs在情感计算中的性能瓶颈<br/>2. 创新设计情绪相似度加权奖励机制（ESWR）和显式结构化推理模块（ESR）<br/>3. 建立基于预训练LALMs的组相对策略优化方法，通过情绪约束提升模型稳定性<br/>4. 在MELD和IEMOCAP数据集取得SOTA效果，验证跨数据集泛化能力显著优于现有方法<br/><br/>总结（99字）：本研究提出EMO-RL框架，通过ESWR和ESR创新技术提升LALMs在情感识别与推理中的性能，解决收敛不稳和小模型局限问题，在多个数据集上取得最佳效果并展现强泛化能力。|
|2509.15253v1|[Emotion-Aware Speech Generation with Character-Specific Voices for   Comics](http://arxiv.org/abs/2509.15253v1)|总结：  <br/>本文提出了一种端到端的漫画语音生成系统，融合图像处理、大语言模型和TTS技术，实现角色特定且情感感知的语音合成，推动漫画的自动化语音生成与沉浸式阅读体验。<br/><br/>贡献点：  <br/>1. **端到端语音生成框架**：首次构建从漫画全卷到角色情感语音的完整生成流程，无需人工标注对话与情感信息。  <br/>2. **多模态信息整合**：通过图像处理模块提取角色身份和情感强度，结合大语言模型分析剧情上下文进行对话归属与情感分析。  <br/>3. **个性化语音合成**：采用定制化语音配置，根据角色特征及情感状态生成对应声音，增强语音表现力。  <br/>4. **自动化语音覆盖**：实现漫画语音的自动生成，为互动式阅读体验提供技术支撑。|
|2509.14804v1|[Towards Building Speech Large Language Models for Multitask   Understanding in Low-Resource Languages](http://arxiv.org/abs/2509.14804v1)|总结：本文提出首个针对泰语的自监督语音编码器XLSR-Thai、高效多任务对齐方法U-Align及跨语言数据生成管道Thai-SUP，有效解决泰语低资源下的语音语言模型性能问题，并开源相关成果以推动研究。<br/><br/>贡献点：<br/>1. XLSR-Thai：首个基于自监督学习的泰语语音编码器，通过36,000小时泰语数据训练，突破现有编码器在低资源语言上的局限。<br/>2. U-Align：提出更资源高效且支持多任务的语音-文本对齐方法，替代传统ASR对齐范式，降低训练成本。<br/>3. Thai-SUP：构建首个超1,000小时的泰语语音理解数据集，通过高资源语言数据生成实现低资源语言数据扩充。<br/>4. 综合验证：通过多项实验验证方法有效性，推动泰语语音语言模型的多任务理解能力提升。<br/>5. 开源贡献：公开XLSR-Thai和Thai-SUP，为低资源语言语音研究提供基准与工具。|
|2509.14666v1|[Spatial Audio Motion Understanding and Reasoning](http://arxiv.org/abs/2509.14666v1)|总结：  <br/>该论文提出基于空间音频编码器的多事件检测与定位方法，结合音频定位模型与大语言模型实现动态音频场景推理，并构建首个空间音频运动理解基准数据集，验证了框架的有效性。<br/><br/>贡献点：  <br/>1. **提出空间音频编码器**：实现多声源事件检测及帧级方向（DoA）与距离估计。  <br/>2. **设计跨模态音频定位模型**：通过交叉注意力机制对齐音频特征与语义文本嵌入，提升对未知事件的泛化能力。  <br/>3. **引入LLM空间推理框架**：利用结构化空间属性作为条件，支持复杂动态音频场景的查询解答。  <br/>4. **构建基准数据集**：首次提出用于评估空间音频运动理解与推理的专用数据集，并与基线模型对比验证性能。|
|2509.14270v2|[SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation   Pipeline for Training Text to Speech Models](http://arxiv.org/abs/2509.14270v2)|总结：  <br/>提出SpeechWeave合成语音数据生成框架，解决TTS训练数据多样性、文本归一化及语音一致性问题，实现高效高质量多语言领域数据生成。<br/><br/>贡献点：  <br/>1. **解决数据获取难题**：提出自动化合成多语言、领域特定TTS训练数据的框架，克服真实数据版权、可扩展性等限制。  <br/>2. **提升文本多样性**：生成的数据在语言和语音指标上比基准提升10-48%，避免LLMs生成重复性文本的问题。  <br/>3. **高精度文本归一化**：实现约97%正确率的文本归一化，减少工具引入异常或漏掉关键模式的影响。  <br/>4. **标准化语音一致性**：生成语音音频具备统一发音风格，提升商业TTS系统中语音一致性。  <br/>5. **可扩展的数据生成**：支持大规模数据生产，降低依赖人工语音录制的成本与难度。|
|2509.14270v1|[SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation   Pipeline for Training Text to Speech Models](http://arxiv.org/abs/2509.14270v1)|贡献点总结（100字以内）:<br/>提出SpeechWeave方法，解决TTS数据获取难题，提升数据多样性、文本规范化正确率及语音一致性，实现可扩展的高质量合成数据生成。<br/><br/>分点贡献：<br/>1. 构建多语言、领域特定的合成语音数据生成系统SpeechWeave，解决真实数据获取困难问题<br/>2. 通过创新生成机制提升数据多样性，实验显示在10-48%的 linguistic/phonetic指标上优于基准<br/>3. 实现97%的文本规范化正确率，避免现有工具异常和模式遗漏问题<br/>4. 保证语音标准化一致性，同时具备大规模数据生成的可扩展性能力，适用于商业TTS系统|
|2509.13927v2|[DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models](http://arxiv.org/abs/2509.13927v2)|**贡献点：**<br/><br/>1. 提出DSpAST，一种基于SpatialAST的新型音频编码器，用于空间音频处理。  <br/>2. 实现了对空间音频的解耦表征学习，同时保持参数数量仅增加0.2%。  <br/>3. 在SpatialSoundQA数据集上与BAT系统结合，显著提升了空间音频推理性能。  <br/><br/>**总结：**  <br/>DSpAST通过解耦表征学习提升空间音频推理性能，参数增加极少，效果优于SpatialAST。|
|2509.13927v1|[DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models](http://arxiv.org/abs/2509.13927v1)|总结：  <br/>提出DSpAST，通过解缠绕表示学习实现高效空间语音编码，仅增加0.2%参数，在空间音频推理任务中显著提升性能，优于SpatialAST。<br/><br/>贡献点：  <br/>1. **提出DSpAST模型**：基于SpatialAST，设计了能够学习解缠绕空间音频表示的编码器，提升多任务处理能力。  <br/>2. **参数效率优化**：仅需0.2%额外参数，显著降低计算资源需求，保持模型轻量化。  <br/>3. **多模态信息解耦**：有效分离声音事件类型、方向和距离等独立信息，增强任务针对性。  <br/>4. **实验验证性能提升**：在SpatialSoundQA数据集上与BAT系统结合，证明DSpAST在空间音频推理中的优越性。  <br/>5. **推动语音与语言模型融合**：为大语言模型处理空间音频任务提供高效、通用的前端编码器方案。|
|2509.13145v2|[UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System   Based on Multimodal Large Language Model](http://arxiv.org/abs/2509.13145v2)|**贡献点：**  <br/>1. **提出多模态语音康复系统**：结合超声舌成像与语音信号，实现实时、精准的发音运动反馈。  <br/>2. **构建高质量专业数据集**：创建包含超声-语音对话对的领域数据集，提升模型临床适配性。  <br/>3. **设计时空融合训练策略**：通过多模态数据融合，增强发音障碍的细粒度分析能力。  <br/>4. **验证系统有效性**：实验结果表明模型在发音分析与临床评估方面具有显著效果。  <br/><br/>**总结（100字以内）**：  <br/>本文提出基于多模态大语言模型的语音康复系统，融合超声舌成像与语音信号，构建专业数据集并设计时空训练策略，实现精准反馈与障碍分析，实验验证了其临床有效性。|
|2509.13145v1|[UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System   Based on Multimodal Large Language Model](http://arxiv.org/abs/2509.13145v1)|总结：<br/>该论文提出基于多模态大语言模型的语音康复系统，创新性整合超声舌成像与语音信号，构建高质量领域数据集，并设计时空融合训练策略，实现精准的发音障碍分析与可操作反馈生成。<br/><br/>贡献点：<br/>1. **提出多模态语音康复系统**：融合超声舌成像与语音信号，实现实时、交互式的发音运动反馈，突破传统方法在实时性与反馈精度的局限。<br/>2. **构建高质量领域数据集**：创建包含UTI-语音对话对的高精度数据集，支持模型的临床适配性优化与细粒度分析。<br/>3. **设计时空融合训练策略**：提出将超声视频与语音信号进行联合时空对齐的训练方法，提升模型对发音障碍的识别与反馈生成能力。|
|2509.12508v3|[Fun-ASR Technical Report](http://arxiv.org/abs/2509.12508v3)|**贡献点：**  <br/>1. 提出Fun-ASR系统，融合大规模数据、大模型容量、LLM集成与强化学习，实现复杂场景下的SOTA性能。  <br/>2. 针对实际部署需求优化，增强流式处理、抗噪声、代码切换及热词定制能力。  <br/>3. 在真实行业数据集上优于主流LLM-based ASR系统，验证其鲁棒性与实用性，解决LLM幻觉问题。  <br/><br/>**总结（100字以内）：**  <br/>Fun-ASR通过整合大规模数据、大模型、LLM与强化学习，优化实际部署能力，在真实行业数据集上实现SOTA性能，有效缓解LLM幻觉问题，提升语音识别在复杂场景下的鲁棒性与实用性。|
|2509.12508v2|[FunAudio-ASR Technical Report](http://arxiv.org/abs/2509.12508v2)|总结：  <br/>本文提出FunAudio-ASR系统，通过整合大规模数据、大模型容量、LLMs协同与强化学习，解决LLMs幻觉问题，实现跨复杂场景的SOTA性能，并针对实际部署优化流式处理、噪声鲁棒性、代码切换等能力。<br/><br/>贡献点：  <br/>1. **提出面向复杂场景的LLM-based ASR系统**：设计FunAudio-ASR，结合数据扩展、模型容量提升、LLMs深度整合与强化学习，突破传统ASR架构限制，实现多样化场景下的SOTA表现。  <br/>2. **针对性部署优化**：增强流式处理、噪声鲁棒性、代码切换、热词定制等能力，满足工业级应用需求（如实时性、抗干扰性、多语言支持）。  <br/>3. **解决LLMs幻觉问题**：通过系统性改进（如强化学习与数据协同），减少LLMs在ASR中的幻觉现象，提升实际场景的用户体验与可靠性。  <br/>4. **验证实际性能优势**：实验表明，FunAudio-ASR在真实行业数据集上表现显著优于开源基准上的LLM-based ASR系统，证明其在工业场景的适用性。|
|2509.12508v1|[FunAudio-ASR Technical Report](http://arxiv.org/abs/2509.12508v1)|**贡献点分点列表：**  <br/>1. **提出FunAudio-ASR系统**：首个大规模结合数据扩展、模型容量增强、LLM集成和强化学习的ASR框架，实现复杂场景下的SOTA性能。  <br/>2. **解决LLM幻觉问题**：通过强化学习优化，减少LLM在ASR中的幻觉现象，提升实际应用的用户体验。  <br/>3. **增强实际部署能力**：针对流式处理、噪声鲁棒性、代码切换、热词自定义等工业需求进行针对性优化，满足真实场景要求。  <br/>4. **验证行业适用性**：在真实应用数据集上表现优于开源基准，证明系统在实际场景中的有效性与鲁棒性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出FunAudio-ASR系统，结合大规模数据、LLM和强化学习，优化流式、噪声等工业需求，解决LLM幻觉问题，在真实应用数据集上实现SOTA性能，验证其在复杂场景下的有效性与实用性。|
|2509.11124v1|[STASE: A spatialized text-to-audio synthesis engine for music generation](http://arxiv.org/abs/2509.11124v1)|**贡献点：**  <br/>1. 提出STASE系统，支持用户自定义空间属性的文本到音频生成，突破传统单声道/固定立体声限制。  <br/>2. 引入LLM作为代理，直接解析文本中的空间线索（如方位、距离等），替代依赖潜在空间操控的方法。  <br/>3. 设计解耦架构：将语义理解与物理空间渲染分离，提升空间生成的可解释性和用户可控性。  <br/>4. 构建双路径处理机制：Description Prompts显式映射空间信息，Abstract Prompts结合RAG模块检索空间模板。  <br/>5. 系统化讨论STASE实现细节与当前生成空间音频评估中的挑战，推动领域研究。  <br/><br/>**总结（100字以内）：**  <br/>STASE通过LLM解析文本空间线索，解耦语义理解与物理渲染，支持用户自定义空间属性，结合RAG模块增强灵活性，为生成空间音频提供新方法并探讨评估挑战。|
|2509.09550v2|[Finite Scalar Quantization Enables Redundant and Transmission-Robust   Neural Audio Compression at Low Bit-rates](http://arxiv.org/abs/2509.09550v2)|**贡献点总结:**  <br/>1. 提出NeuCodec，首个基于Finite Scalar Quantization (FSQ)的神经音频编解码器，替代传统Residual Vector Quantization (RVQ)方法。  <br/>2. 通过编码器蒸馏实验证明，不同编码器可生成差异显著的码序列，同时保持相同的量化器、解码器和重建质量，展示编码鲁棒性。  <br/>3. 比较FSQ与RVQ在噪声信道下的抗扰动能力，证实FSQ在比特级扰动鲁棒性上具有显著优势。  <br/><br/>**研究摘要（100字以内）:**  <br/>该论文提出基于FSQ的NeuCodec，首次揭示编码器蒸馏中不同编码器的码序列差异性，同时证明FSQ在噪声传输下的抗扰动能力优于RVQ，为语音处理提供了更高效的编码方案。|
|2509.09174v1|[EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for   Speech-to-Speech LLMs](http://arxiv.org/abs/2509.09174v1)|总结：  <br/>提出EchoX模型，通过整合声学与语义学习，解决SLLMs的推理能力下降问题，验证其在知识问答任务上的优越性能，并开源项目促进应用。<br/><br/>贡献点：  <br/>1. **提出解决声学-语义差距的新方法**：通过动态生成语音训练目标，弥合现有SLLMs在特征表示空间中的声学与语义鸿沟。  <br/>2. **融合双模态学习机制**：结合声学和语义学习，使模型同时保留语音处理能力与强推理能力。  <br/>3. **验证性能优势**：在六千小时训练数据下，EchoX于多个知识型问答基准上达到先进水平。  <br/>4. **开源实践**：提供可复现实验的代码库，推动语音大模型研究与应用。|
|2509.07526v1|[Competitive Audio-Language Models with Data-Efficient Single-Stage   Training on Public Data](http://arxiv.org/abs/2509.07526v1)|总结：  <br/>Falcon3-Audio通过高效架构和少量数据实现语音-语言任务的高性能，挑战了传统依赖复杂训练策略和大规模数据的范式。<br/><br/>贡献点：  <br/>1. **创新架构**：提出Falcon3-Audio系列模型，整合指令调优的LLM与Whisper编码器，构建音频-语言统一框架。  <br/>2. **数据效率突破**：仅需<30K小时公开音频数据（5K个唯一语料）即可达到MMAU基准最优性能（64.14分），与R1-AQA持平。  <br/>3. **参数效率优胜**：1B参数模型性能可比肩2B-13B参数的主流模型，体现显著的压缩潜力与资源节约。  <br/>4. **简化训练流程**：采用单阶段训练方法，无需课程学习、多编码器或多注意力模块等复杂设计。  <br/>5. **透明性设计**：基于公开模型与标准数据集，提升模型可解释性与复现性。  <br/>6. **验证性能鲁棒性**：通过消融实验证明，简化的模型结构仍能实现与超大规模数据训练模型相当的性能。|
|2509.06452v1|[AudioBoost: Increasing Audiobook Retrievability in Spotify Search with   Synthetic Query Generation](http://arxiv.org/abs/2509.06452v1)|**贡献点总结：**  <br/>1. 提出AudioBoost系统，通过合成查询生成解决Spotify有声书冷启动检索问题。  <br/>2. 采用大语言模型（LLM）根据有声书元数据生成探索性查询，提升用户搜索意图匹配。  <br/>3. 将合成查询同步索引至查询自动补全（QAC）与检索引擎，实现查询优化与结果增强。  <br/>4. 离线实验验证合成查询有效提升可检索性，且质量较高。  <br/>5. 在线A/B测试显示：有声书曝光量+0.7%、点击率+1.22%、探索性查询完成率+1.82%。  <br/><br/>**摘要核心贡献（100字以内）：**  <br/>提出AudioBoost系统，利用LLM生成合成查询解决Spotify有声书冷启动检索问题，提升用户探索体验并通过实验验证显著效果。|