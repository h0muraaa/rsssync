|Source|Title|Summary|
|---|---|---|
|2508.04795v1|[Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a   Frozen LLM](http://arxiv.org/abs/2508.04795v1)|**贡献点总结：**  <br/>1. 提出通过添加元数据标签（如年龄、性别、情绪）丰富对话转录内容，而非仅优化语法标点。  <br/>2. 首创将冻结音频模型（Whisper/WavLM）与冻结LLM（LLAMA）结合，无需任务特定微调。  <br/>3. 设计轻量连接器实现跨模态表示融合，保持系统模块性和推理速度。  <br/>4. 验证冻结LLM可直接比较x-vectors，达到8.8%等错误率的语音分析性能。  <br/><br/>**摘要总结（100字内）：**  <br/>本文提出利用冻结音频与语言模型的组合，无需微调即可为对话转录添加元数据标签（如年龄、性别、情绪），并设计高效连接器实现跨模态融合，进一步验证冻结LLM在x-vector比较任务中的有效性，提升了对话分析的模块性与性能。|
|2508.04721v1|[Toward Low-Latency End-to-End Voice Agents for Telecommunications Using   Streaming ASR, Quantized LLMs, and Real-Time TTS](http://arxiv.org/abs/2508.04721v1)|总结：  <br/>提出低延迟电信AI语音代理管道，整合四专有模型，构建500条问答数据集，实现RTF<1.0的实时性能，推动下一代电信AI应用发展。<br/><br/>贡献点：  <br/>1. **提出低延迟语音代理管道**：专为电信实时交互场景设计，支持呼叫中心自动化、智能IVR与AI客服等应用。  <br/>2. **集成四专有模型**：结合TSLAM（4位量化LLM）、T-VEC（嵌入模型）、TTE（ASR）和T-Synth（TTS），实现端到端领域适配。  <br/>3. **构建电信专用数据集**：从RFCs提取500条真实语音问题，用于评估系统性能与场景模拟。  <br/>4. **实现实时性突破**：TSLAM、TTE、T-Synth的RTF均低于1.0，满足企业级低延迟部署需求。  <br/>5. **定义新基准**：通过流式ASR、对话智能、检索增强生成（RAG）与实时TTS的协同，提升电信语音助手效率。|
|2508.04141v1|[Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech](http://arxiv.org/abs/2508.04141v1)|**总结:**  <br/>本文提出融合AR与NAR模块的TTS框架，通过Parallel Tokenizer与Coupled NAR模型提升零样本语音合成的表达力和效率，实验验证在英汉数据集上均优于现有方法。<br/><br/>**贡献点:**  <br/>1. **混合框架设计**：首次结合自回归（AR）与非自回归（NAR）模块，协调语音与语义特征的独立及相互依赖关系。  <br/>2. **Parallel Tokenizer**：提出并行生成机制，同步合成顶层语义与音频标记，增强语义-语音关联性。  <br/>3. **Coupled NAR模型**：基于AR模型输出进行语义引导的详细预测，解决语义与音频的复杂依赖问题。  <br/>4. **Parallel GPT模型**：利用以上架构提升零样本TTS的质量与效率，实现更自然的语音生成。  <br/>5. **跨语言验证**：在英文和中文数据集上验证模型有效性，展示其语言通用性与性能优势。|
|2508.04096v1|[Efficient Scaling for LLM-based ASR](http://arxiv.org/abs/2508.04096v1)|总结：  <br/>提出EFIN多阶段训练策略，通过先预训练语音编码器提升LLM-ASR效率，显著降低计算成本并改进性能，建立ASR错误率与计算量的扩展定律。  <br/><br/>贡献点：  <br/>1. **提出EFIN策略**：设计Encoder First Integration新训练流程，对比传统联合后训练方法提升扩展效率。  <br/>2. **实验证明优势**：通过控制实验验证预训练编码器对LLM-ASR性能的提升效果（21.1% CERR改善），且计算成本降低49.9%。  <br/>3. **建立扩展定律**：推导ASR错误率与计算量的数学关系，为模型扩展提供量化指导。|
|2508.03983v1|[MiDashengLM: Efficient Audio Understanding with General Audio Captions](http://arxiv.org/abs/2508.03983v1)|**贡献点总结**  <br/>1. 提出首个开放的音频-语言模型MiDashengLM，突破闭源数据限制；  <br/>2. 引入新型ACAVCaps训练数据集，提升音频理解全面性；  <br/>3. 采用开源音频编码器Dasheng，增强模型透明度与可复现性；  <br/>4. 融合语音、声音与音乐信息，实现多模态统一文本表征；  <br/>5. 实现4倍TTFT加速和20倍吞吐量提升，显著优化计算效率。  <br/><br/>**总结（100字以内）**  <br/>本研究提出开放音频语言模型MiDashengLM，创新性地融合多模态音频信息并优化计算效率，为高效音频理解提供可复现、透明的解决方案。|
|2508.03365v1|[When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs](http://arxiv.org/abs/2508.03365v1)|**贡献点总结（100字以内）：**  <br/>提出WhisperInject两阶段框架，通过RL-PGD和PGD技术实现对音频语言模型的隐蔽攻击，成功突破多模型安全机制，揭示音频本源威胁的新类别，为AI语音交互安全性提供重要警示。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **提出新型对抗框架**：设计WhisperInject，首次构建针对音频语言模型的两阶段攻击方法，结合奖励机制与梯度优化，实现隐蔽且有效的有害内容生成。  <br/>2. **突破安全机制**：通过RL-PGD与PGD技术，成功绕过Qwen2.5-Omni与Phi-4-Multimodal等模型的安全协议，危害率超86%，证明语音模型存在实际漏洞。  <br/>3. **音频本源威胁研究**：首次将攻击聚焦于音频输入的自然属性（如天气查询、问候语），而非传统文本攻击，揭示语音交互场景中特有的安全风险。  <br/>4. **可感知扰动技术**：采用对人类不可察觉的音频微小扰动（如嵌入到正常语音中的噪声），确保攻击隐蔽性，拓展对抗攻击的隐蔽性研究方向。  <br/>5. **系统验证与评估**：在StrongREJECT、LlamaGuard及人工评估体系下验证攻击有效性，提供多维度实验结果，增强研究的可信度与实用性。|
|2508.02175v2|[Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through   Latent Acoustic Pattern Triggers](http://arxiv.org/abs/2508.02175v2)|总结：  <br/>该研究提出针对语音大模型的新型后门攻击框架HIN，并构建AudioSafe基准验证其安全性，揭示了语音模型在音频特征触发下的关键脆弱性。<br/><br/>贡献点：  <br/>1. 提出HIN框架，首次针对ALLM设计基于声学触发的后门攻击方法，利用时间动态调整与谱定制噪声实现隐蔽触发。  <br/>2. 构建AudioSafe基准，系统评估九类音频特征触发风险，填补语音安全领域研究空白。  <br/>3. 发现现有ALLMs存在三项核心漏洞：特定音频特征（如环境噪声、语速变化）攻击成功率超90%；跨特征敏感度差异显著（如对音量变化不敏感）；污染样本攻击隐蔽性强，仅引起微小损失波动。|
|2508.02175v1|[Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through   Latent Acoustic Pattern Triggers](http://arxiv.org/abs/2508.02175v1)|**贡献点：**  <br/>1. **提出HIN框架**：设计首个针对音频特征的后门攻击方法，通过时间动态修改和频谱噪声注入等音频特定操作，构建隐蔽且高效的触发器。  <br/>2. **开发AudioSafe基准**：构建首个评估音频特征引发风险的基准，涵盖9种风险类型，系统检验ALLM的安全性。  <br/>3. **揭示关键漏洞**：通过实验发现三类显著漏洞：（I）环境噪声与语速变化可达成超90%攻击成功率；（II）ALLM对不同特征的敏感性差异显著；（III）中毒样本导致的损失波动极小，攻击隐蔽性强。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HIN框架及AudioSafe基准，系统评估音频特征对ALLM的潜在风险，揭示了环境噪声、语速变化引发的高攻击成功率及模型对触发器敏感性差异等关键漏洞，强调音频安全的特殊挑战。|
|2508.01659v1|[From Contrast to Commonality: Audio Commonality Captioning for Enhanced   Audio-Text Cross-modal Understanding in Multimodal LLMs](http://arxiv.org/abs/2508.01659v1)|总结：本文提出Audio Commonality Captioning（ACC）方法，解决ADC存在的语义偏差和灾难性遗忘问题，验证其在提升跨模态理解与保持下游任务泛化能力方面更优。<br/><br/>贡献点：<br/>1. **提出ACC新范式**：首次引入 Audio Commonality Captioning（ACC），通过强调音频片段间的共享语义替代ADC的差异描述，弥补语义鸿沟问题。<br/>2. **缓解灾难性遗忘**：通过与预训练目标更一致的语义对齐策略，有效减少微调过程中由于任务偏移导致的模型能力退化。<br/>3. **验证多任务泛化性**：实验证明ACC在语音和音乐相关下游任务（如VSC、SER、MIC、MGC）中，相较于ADC能更好保持模型的通用能力。<br/>4. **平衡性能与泛化**：展示ACC在维持任务特定性能的同时，实现更优的跨模态理解与模型泛化能力的协同提升。|
|2508.01181v1|[Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion   Reasoning](http://arxiv.org/abs/2508.01181v1)|总结：  <br/>本研究提出CA-MER基准检验多模态情感模型在冲突场景的表现，发现现有模型过度依赖音频信号，设计MoSEAR框架通过模态专家与注意力重分配机制实现跨模态平衡，显著提升情感识别性能，尤其在冲突条件下表现优异。<br/><br/>贡献点：  <br/>1. **构建首个情感冲突基准CA-MER**：包含视频对齐、音频对齐和一致三类子集，系统性评估多模态大语言模型在情感冲突场景下的能力。  <br/>2. **揭示模型模态依赖偏差**：发现当前SOTA情感模型在冲突场景中系统性过度依赖音频信号，忽视视觉模态关键信息。  <br/>3. **提出参数高效框架MoSEAR**：  <br/>   - (1) **MoSE模块**：通过正则化门控机制降低细调头的模态偏倚  <br/>   - (2) **AR模块**：在推理阶段重新分配注意力权重，平衡冻结主干网络的模态贡献  <br/>4. **实现性能提升无模态权衡**：在缓解情感冲突的同时，保持音视频模态的均衡利用，提升一致样本的性能。  <br/>5. **验证方法有效性**：在多个基准（MER2023、EMER、DFEW、CA-MER）上证明MoSEAR在冲突条件下的SOTA性能。|
|2508.01178v1|[Advancing the Foundation Model for Music Understanding](http://arxiv.org/abs/2508.01178v1)|总结：  <br/>提出统一音乐理解模型MuFun，创新架构联合处理音源与歌词，构建多维度基准MuCUE，验证其在多种任务上的优越性能，推动音乐信息检索领域融合与泛化发展。  <br/><br/>贡献点：  <br/>1. 提出首个统一音乐理解基础模型MuFun，突破传统MIR领域碎片化模型的局限；  <br/>2. 设计能同时处理乐器与歌词内容的新型联合架构，提升多模态音乐分析能力；  <br/>3. 构建大规模多任务数据集，覆盖音乐分类、标签生成、问答等核心任务；  <br/>4. 发布多维度评估基准MuCUE，完善音乐理解任务的系统性评价体系；  <br/>5. 通过实验验证模型在MuCUE任务上显著优于现有音频大语言模型，展现SOTA性能与泛化能力。|
|2508.01166v1|[Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented   Conversational LLM-Based ASR](http://arxiv.org/abs/2508.01166v1)|总结：  <br/>本文提出MARS方法，通过多模态检索与选择优化对话ASR中的上下文利用，有效减少冗余信息并提升数据效率，实验证明其在较少训练数据下表现优于传统模型。<br/><br/>贡献点：  <br/>1. **提出MARS方法**：首次设计多模态检索-选择框架，解决对话场景中LLM-ASR对冗长历史上下文的混淆与计算效率问题。  <br/>2. **多模态上下文选择**：联合利用音频与文本特征，通过双相似性计算（声学/语义）筛选最相关的历史对话片段。  <br/>3. **近理想排序机制**：创新性地提出综合评估声学与文本相似性的排序策略，提升上下文匹配的准确性。  <br/>4. **数据效率提升**：在仅1.5K小时训练数据下，MARS显著优于训练数据量达179K小时的SOTA系统，验证了模型轻量化潜力。|
|2507.23511v2|[MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio   Understanding Tasks](http://arxiv.org/abs/2507.23511v2)|总结：  <br/>本研究提出MECAT基准与DATE评估指标，解决音频模型细粒度理解能力评估不足的问题，提供多视角数据和开放问答对，并通过全面评估揭示现存模型的优劣。<br/><br/>贡献点：  <br/>1. **提出新型基准MECAT**：构建多专家协作的细粒度音频理解任务基准，整合专家模型分析与Chain-of-Thought推理，生成多视角、高精度的细粒度描述及开放问答对。  <br/>2. **设计DATE评估指标**：创新性引入结合单样本语义相似性与跨样本可区分性的评估方法，有效区分通用与细节描述，提升对模型输出质量的判别能力。  <br/>3. **系统性评估模型能力**：对当前主流音频模型进行全面测试，揭示其在细粒度理解任务中的能力边界与改进方向。  <br/>4. **开源数据与代码**：公开MECAT数据集及实现代码，推动音频理解领域的研究与技术发展。|
|2507.23511v1|[MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio   Understanding Tasks](http://arxiv.org/abs/2507.23511v1)|总结：  <br/>本研究提出MECAT基准和DATE指标，解决现有音频模型评估体系的不足，提升细粒度音频理解能力，并展示全面评估结果。<br/><br/>贡献点：  <br/>1. **提出MECAT基准**：首个多专家构建的细粒度音频理解基准，整合专家模型分析与Chain-of-Thought推理，生成多视角、细粒度的音频描述和开放集问答对。  <br/>2. **设计DATE评估指标**：通过结合单样本语义相似度与跨样本可区分性，强化对具体细节的奖励机制，有效区分通用和高精度输出。  <br/>3. **系统评估最新模型**：提供全面的模型性能分析，揭示当前技术能力与局限性，为研究方向提供新的洞见。  <br/>4. **开源数据与代码**：通过公开的GitHub仓库，促进技术复现与社区验证，推动领域发展。|
|2507.23365v1|["I made this (sort of)": Negotiating authorship, confronting   fraudulence, and exploring new musical spaces with prompt-based AI music   generation](http://arxiv.org/abs/2507.23365v1)|总结：  <br/>通过创作实验探讨AI音乐生成与人类创作的互动，提出LLM作为自反工具的创新方法，挑战传统作者身份观念，揭示人机协作中的音乐身份转变与新创作空间的形成。<br/><br/>贡献点：  <br/>1. **实验方法创新**：提出基于提示式AI音乐生成平台的创作实验，通过"垃圾邮件"与AI的碰撞探索人类与技术的非典型互动模式。  <br/>2. **双专辑对比研究**：构建批判性对话框架，第一张专辑质疑AI的创作边界，第二张专辑通过"非练习/打磨"作品揭示人类创作的不可替代性。  <br/>3. **LLM介入的自反机制**：创新性地将大型语言模型作为研究工具，实现文本与音乐的跨模态对话，拓展AI参与艺术创作的研究维度。  <br/>4. **音乐身份重构理论**：提出人在与超越自身能力的AI协作中的主体性消解与身份转型问题，为人机共生时代艺术创作提供哲学反思。  <br/>5. **新创作空间定义**：揭示人机协作可能创造的元音乐空间，挑战传统创作主体与客体的二元对立，拓展AI艺术研究边界。|
|2507.20666v1|[MIMII-Agent: Leveraging LLMs with Function Calling for Relative   Evaluation of Anomalous Sound Detection](http://arxiv.org/abs/2507.20666v1)|总结：  <br/>作者提出一种基于大语言模型的合成方法，生成特定机器类型的异常声音，无需真实异常数据。该方法有效评估无监督系统在不同设备间的相对性能，实验证明合成数据与真实数据在检测难度趋势上一致，推动跨类型UASD研究。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的异常生成方法**：利用大语言模型解析故障文本，自动选择音频转换函数，生成多样且合理的机器类型异常声音。  <br/>2. **无需真实异常数据**：突破传统方法依赖手动标签和现有模型需要异常训练数据的限制，实现从正常声音到异常的合成。  <br/>3. **揭示跨机器类型的检测难度趋势**：通过合成与真实异常数据对比，验证不同机器类型在UASD中的相对检测难度一致性。  <br/>4. **支持无监督系统的相对评估**：证明该方法能有效用于评估仅利用正常数据训练的UASD系统在多种机器类型上的性能差异。  <br/>5. **推动大规模、跨类型研究**：为无监督异常声音检测的可扩展评估提供新工具，适应设备类型和异常模式多样化需求。|
|2507.19634v1|[MCIF: Multimodal Crosslingual Instruction-Following Benchmark from   Scientific Talks](http://arxiv.org/abs/2507.19634v1)|**贡献点总结**（100字以内）：  <br/>MCIF是首个多语言、人工标注的多模态指令跟随基准，覆盖语音、视觉、文本及四种语言，支持长短上下文评估，填补现有基准在跨语言和多模态能力测试上的空白，促进开放研究。<br/><br/>---<br/><br/>**详细贡献点分项**：  <br/>1. **首创性**：提出首个基于科学讲座的多语言、人工标注的多模态指令跟随基准（MCIF）。  <br/>2. **多模态整合**：涵盖语音、视觉和文本三种模态，系统评估MLLMs在多模态场景下的理解与生成能力。  <br/>3. **跨语言支持**：包含英语、德语、意大利语和中文四种语言，打破单一语言限制，验证模型语言泛化能力。  <br/>4. **上下文覆盖全面**：支持短文本和长文本输入，评估模型在不同上下文长度下的指令遵循表现。  <br/>5. **开放共享**：以CC-BY 4.0协议开放发布，推动多模态大模型研究的开放协作与技术进步。|
|2507.19361v1|[SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice   Understanding Large Language Models](http://arxiv.org/abs/2507.19361v1)|贡献点：  <br/>1. 提出SIQ框架：首次将人类认知启发的评价体系（基于布卢姆分类法）引入语音理解大模型评估，突破传统WER等单一指标限制。  <br/>2. 多级认知评估：从记忆、理解、应用三个维度系统性评估语音理解能力，覆盖基础准确性到下游任务模拟的复杂性。  <br/>3. 统一模型比较：建立跨级联方法（如ASR+LLM）与端到端模型的统一评估基准，揭示不同架构的优劣差异。  <br/>4. 问题检测能力：识别现有语音基准中的标注错误，检测大模型在语音理解中的幻觉问题。  <br/>5. 认知-技术桥梁：首创将认知科学原理与语音评估结合，暴露多模态训练中被忽视的技术挑战。  <br/><br/>总结（100字内）：  <br/>本研究提出SIQ框架，通过布卢姆分类法构建多级语音理解评估体系，突破传统指标局限，实现对级联与端到端模型的统一比较，揭露标注错误与模型幻觉问题，为多模态训练提供认知启发的新视角。|
|2507.19037v1|[MLLM-based Speech Recognition: When and How is Multimodality Beneficial?](http://arxiv.org/abs/2507.19037v1)|**贡献点：**  <br/>1. 验证多模态融合可提升噪声环境下的ASR准确率，但效果与噪声强度相关。  <br/>2. 明确同步模态（如唇部动作）在高噪声下的显著优势，异步模态（如图像上下文）在中等噪声下更有效。  <br/>3. 强调高质量视觉表示对ASR的持续提升，推动视觉编码器的优化方向。  <br/>4. 比较Mamba与Transformer在多模态建模中的表现，发现两者存在相似的性能趋势。  <br/>5. 提出输入模态顺序及损失函数中权重分配对ASR准确率有显著影响，为模型设计提供新视角。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过实验揭示多模态对ASR在噪声环境中的提升机制，指出同步与异步模态在不同噪声水平下的差异化作用，强调视觉编码质量的重要性，并比较Mamba与Transformer的性能趋势，为多模态语音识别的优化奠定基础。|
|2507.18750v1|[CatchPhrase: EXPrompt-Guided Encoder Adaptation for Audio-to-Image   Generation](http://arxiv.org/abs/2507.18750v1)|总结（100字以内）:  <br/>提出CatchPhrase框架，通过生成富语义的跨模态提示、改进提示选择机制和轻量映射网络，解决音频与图像生成的语义对齐问题，实验验证其提升生成质量的效果。<br/><br/>**贡献点分点列出：**  <br/>1. **提出新型音频-图像生成框架**  <br/>   - 设计了CatchPhrase，专注于解决音频与生成图像之间的语义对齐问题。<br/><br/>2. **引入跨模态语义提示生成方法**  <br/>   - 利用LLMs和ACMs，从弱类标签中挖掘高语义的跨模态提示（EXPrompt Mining），缓解同音词和听觉错觉导致的歧义。<br/><br/>3. **提出分层语义对齐选择机制**  <br/>   - 通过多模态过滤与检索技术（EXPrompt Selector），分别应对类别级和实例级的语义偏差，选择最优对齐提示。<br/><br/>4. **轻量化适配方案**  <br/>   - 构建映射网络，将预训练文本-图像模型高效适配到音频输入，降低计算成本。<br/><br/>5. **验证有效性与泛化能力**  <br/>   - 在多音频分类数据集上进行实验，证明框架显著提升生成质量与语义对齐效果。|
|2507.18452v1|[DIFFA: Large Language Diffusion Models Can Listen and Understand](http://arxiv.org/abs/2507.18452v1)|总结：  <br/>提出首个基于扩散模型的语音理解大模型DIFFA，通过创新架构和训练方法实现高效音频语言处理，超越传统自回归模型。<br/><br/>贡献点：  <br/>1. **首个扩散模型语音理解框架**：构建了首个融合扩散模型与语音理解的大型音频-语言模型DIFFA，填补了扩散模型在音频模态应用的空白。  <br/>2. **双适配器架构设计**：采用轻量级双适配器结构，有效连接语音理解与自然语言推理任务，提升模型多模态处理能力。  <br/>3. **两阶段训练策略**：提出分阶段训练方法（ASR对齐+合成数据指令学习），通过自监督和合成数据优化模型性能。  <br/>4. **高效训练数据利用**：仅需960小时ASR数据和127小时合成数据即可达到竞争力表现，证明扩散模型在资源效率上的优势。  <br/>5. **推动语音驱动AI新方向**：验证扩散模型在音频理解任务中的可扩展性，为语音相关AI应用提供新范式和研究路径。|
|2507.18181v2|[SpecASR: Accelerating LLM-based Automatic Speech Recognition via   Speculative Decoding](http://arxiv.org/abs/2507.18181v2)|总结（100字以内）:  <br/>本研究提出SpecASR，针对ASR任务优化speculative decoding，通过自适应draft生成、序列回收和双通道稀疏树算法，在保持识别准确率的同时实现显著加速效果。<br/><br/>贡献点:  <br/>1. 提出核心观察：ASR解码具有音频条件特性，导致小/大模型输出高度对齐，为专用解码框架提供理论依据。  <br/>2. 设计自适应draft序列生成机制：动态调节draft长度以最大化token接受长度，提升解码效率。  <br/>3. 引入draft序列回收策略：复用历史生成序列降低draft模型延迟。  <br/>4. 提出双通道稀疏token树生成算法：平衡draft与目标模型的延迟，优化整体性能。  <br/>5. 实验证明：在保持准确率前提下，相较传统方法实现3.04-3.79倍和1.25-1.84倍的加速效果。|
|2507.18181v1|[SpecASR: Accelerating LLM-based Automatic Speech Recognition via   Speculative Decoding](http://arxiv.org/abs/2507.18181v1)|总结：  <br/>本文提出SpecASR，一种针对ASR任务的专用投机解码框架，通过音频条件特性优化生成过程，实现高效低延迟的语音识别。<br/><br/>贡献点：  <br/>1. 提出SpecASR框架，专门针对ASR任务设计，解决传统投机解码忽略音频条件导致的效率瓶颈。  <br/>2. 引入自适应草稿序列生成机制，动态调整序列长度以最大化token接受长度。  <br/>3. 设计草稿序列回收策略，复用历史生成结果降低草稿模型延迟。  <br/>4. 提出两阶段稀疏token树生成算法，平衡草稿与目标模型的延迟开销。  <br/>5. 实验验证SpecASR在保持识别准确率的前提下，较基线方法和传统投机解码实现显著速度提升（3.04x-3.79x和1.25x-1.84x）。|
|2507.18161v1|[Recent Trends in Distant Conversational Speech Recognition: A Review of   CHiME-7 and 8 DASR Challenges](http://arxiv.org/abs/2507.18161v1)|总结：  <br/>该论文分析了CHiME-7/8挑战的系统设计与趋势，揭示了端到端ASR的普及、神经SSE技术局限、说话人分割优化的重要性、会议摘要评估与转录质量的弱相关性，以及自发性语音转录在复杂环境下的持续挑战。<br/><br/>贡献点：  <br/>1. **系统设计与趋势分析**：总结了挑战的核心目标（多通道联合ASR与说话人分割）及参与系统多样性和技术演进。  <br/>2. **端到端ASR的优势**：指出e2e系统取代混合系统的趋势，归因于预训练模型降低了数据需求。  <br/>3. **神经SSE技术的局限性**：强调当前神经语音分离增强技术仍依赖传统引导分离方法，难以应对复杂场景。  <br/>4. **说话人分割优化的关键作用**：揭示准确说话人计数对减少误差的重要性，并分析CHiME-8的优化策略。  <br/>5. **评估指标的弱相关性**：提出会议摘要等下游任务与转录质量的弱关联，归功于大语言模型的容错能力。  <br/>6. **挑战性语音环境的困难**：指出即使使用复杂系统，自发性语音在恶劣环境下的准确转录仍具难度。|
|2507.18061v1|[TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in   Chinese Interactive Scenarios](http://arxiv.org/abs/2507.18061v1)|总结：本文提出TELEVAL基准，针对中文对话场景定义三维度评估体系，强调隐性线索理解能力，揭示现有SLM在自然对话中的不足。<br/><br/>贡献点：<br/>1. 提出首个面向真实中文交互场景的动态语音模型评估基准TELEVAL<br/>2. 构建包含显性语义、非语言语义、隐性语义与系统能力的四维评估框架<br/>3. 设计符合实际对话流程的测评方式，分别评估文本和音频输出质量<br/>4. 突出评估模型自主理解隐性语义线索并作出自然回应的能力<br/>5. 通过实验证明当前SLM在对话任务中存在显著提升空间|
|2507.16632v2|[Step-Audio 2 Technical Report](http://arxiv.org/abs/2507.16632v2)|总结：<br/>Step-Audio 2 提出多模态大模型结合音频编码器与推理强化学习，实现端到端音频理解与对话，通过离散音频标记生成、RAG技术及外部工具集成提升表现，达到行业领先性能。<br/><br/>贡献点：<br/>1. 提出工业级多模态大语言模型 Step-Audio 2，整合音频编码与语言建模实现端到端音频理解及对话  <br/>2. 创新性引入推理中心化强化学习框架，提升ASR与音频理解性能  <br/>3. 首次将离散音频标记生成融入语言模型，增强对语调、情感等非语言特征的捕捉能力  <br/>4. 集成检索增强生成（RAG）技术，支持外部工具调用以减少幻觉和提升音色切换能力  <br/>5. 基于数百万小时语音-音频数据训练，实现跨场景的智能与表达能力  <br/>6. 在多个音频理解与对话基准测试中取得SOTA性能，超越现有开源及商业方案|
|2507.16632v1|[Step-Audio 2 Technical Report](http://arxiv.org/abs/2507.16632v1)|总结：  <br/>Step-Audio 2是首个融合音频编码器与推理强化学习的端到端多模态语音对话模型，具备生成离散音频标记、调用外部工具、大规模数据训练和SOTA性能等优势。<br/><br/>贡献点：  <br/>1. **多模态架构设计**：集成潜在音频编码器与推理中心化强化学习（RL），实现工业级音频理解和语音对话能力。  <br/>2. **对话增强机制**：通过生成离散音频标记，显著提升对语音中的非语言信息（如语调、情绪）的响应能力。  <br/>3. **知识整合能力**：引入检索增强生成（RAG）技术，可调用外部工具（如网络搜索、音频搜索）以减少幻觉并支持音色切换。  <br/>4. **大规模训练数据**：基于数百万小时语音与音频数据，提升模型在复杂场景下的表达力与智能化水平。  <br/>5. **性能突破**：在多项音频理解及对话基准测试中达到最先进水平，优于现有开源和商业解决方案。|
|2507.16564v1|[TTMBA: Towards Text To Multiple Sources Binaural Audio Generation](http://arxiv.org/abs/2507.16564v1)|总结：  <br/>本文提出一种级联方法，通过时间与空间控制生成多源双耳音频，解决了现有文本转音频方法忽视空间信息的不足，提升了沉浸式听觉体验的质量和空间感知准确性。<br/><br/>贡献点：  <br/>1. **多源双耳音频生成框架**：首次提出文本到多源双耳音频的生成方法，整合时空控制以增强沉浸感。  <br/>2. **结构化文本处理**：利用预训练LLM对文本进行分段，提取每个声事件的时间与空间细节。  <br/>3. **多模态生成策略**：结合预训练单音频生成网络，创建不同持续时间的单声道音频。  <br/>4. **空间感知渲染技术**：基于LLM的空间数据，通过双耳渲染神经网络将单声道转换为双耳音频。  <br/>5. **时空同步排列机制**：按声事件的起始时间对双耳音频进行排列，实现多源音频的时序一致性。  <br/>6. **实验验证有效性**：在生成质量和空间感知准确性上超越现有方法，证明了方法的优越性。|
|2507.16456v1|[An approach to measuring the performance of Automatic Speech Recognition   (ASR) models in the context of Large Language Model (LLM) powered   applications](http://arxiv.org/abs/2507.16456v1)|**贡献点**：<br/>1. 分析LLMs在纠正ASR错误（如插入、删除、替换）中的能力，揭示其对下游任务的影响。<br/>2. 提出针对LLM驱动应用的新型ASR性能评估指标，突破传统WER的局限性。<br/><br/>**总结（100字以内）**：  <br/>该研究分析了LLMs对ASR错误的修正能力，并提出适用于LLM增强系统的新型评估指标，旨在更精准地衡量ASR性能对下游任务的实际影响。|
|2507.12951v1|[UniSLU: Unified Spoken Language Understanding from Heterogeneous   Cross-Task Datasets](http://arxiv.org/abs/2507.12951v1)|**贡献点（分点）：**  <br/>1. **提出统一框架UniSLU**：整合ASR、spoken NER和SA任务，解决传统方法依赖独立架构导致的系统复杂性和跨任务交互受限问题。  <br/>2. **统一表示方法**：设计适用于多任务的通用表示，实现对异构数据集的高效利用。  <br/>3. **联合生成建模**：开发统一生成方法，增强任务间信息交互并无缝对接大语言模型，提升生成能力。  <br/>4. **实验验证有效性**：在公开数据集上取得优于基准方法的SLU性能，证明其对实际场景的适用性。  <br/>5. **开源促进研究**：公开代码和模型，推动未来研究与应用拓展。  <br/><br/>**总结（100字以内）：**  <br/>提出UniSLU统一框架，整合多SLU任务，解决异构数据利用不足和系统复杂度问题，显著提升性能并兼容大语言模型，开源模型促进技术发展。|
|2507.12808v1|[Large Language Models' Internal Perception of Symbolic Music](http://arxiv.org/abs/2507.12808v1)|总结：  <br/>本研究通过文本提示生成无显式音乐训练的MIDI数据集，验证LLMs在音乐生成中的潜力与局限，揭示其隐含编码音乐模式的能力。<br/><br/>贡献点：  <br/>1. **构建新型音乐数据集**：提出无需显式音乐训练的LLM生成MIDI文件数据集，填补音乐领域LLMs研究空白。  <br/>2. **探索隐含音乐建模**：分析LLMs通过文本描述生成音乐结构和时间关系的能力，验证其隐式编码音乐模式的潜力。  <br/>3. **基准测试与比较**：基于LLM生成数据训练神经网络，对比现有模型在音乐分类和旋律生成任务中的性能。  <br/>4. **揭示能力边界**：系统评估LLMs在音乐生成中的局限性，明确其因缺乏显式音乐上下文而存在的表现瓶颈。  <br/>5. **跨领域应用启示**：为LLMs在符号领域的泛化能力研究提供音乐场景的实证案例与方向参考。|
|2507.12015v1|[EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis](http://arxiv.org/abs/2507.12015v1)|总结：  <br/>本文提出EME-TTS框架，结合弱监督学习与EPE模块，解决情感TTS中强调与情感的协同问题，实现自然情感语音合成并保持强调稳定性，实验验证其有效性。  <br/><br/>贡献点：  <br/>1. **提出新框架**：设计EME-TTS，系统性解决情感表达与强调控制的协同问题，首次探索两者交互机制。  <br/>2. **弱监督学习方法**：引入基于方差的强调特征和伪标签，提升对情感语音中强调信息的建模效率。  <br/>3. **EPE模块创新**：开发Emphasis Perception Enhancement（EPE）模块，增强情感信号与强调位置的交互能力。  <br/>4. **大模型集成**：结合大语言模型预测强调位置，显著提升合成语音的自然度与情感表达的稳定性。  <br/>5. **实验验证与公开数据**：通过实验验证框架有效性，并提供合成样本供在线查阅，推动领域研究。|
|2507.10016v1|[The Man Behind the Sound: Demystifying Audio Private Attribute Profiling   via Multimodal Large Language Model Agents](http://arxiv.org/abs/2507.10016v1)|**贡献点总结：**  <br/>1. **提出新隐私风险**：发现MLLMs可通过音频数据推断敏感属性（音频隐私属性分析）。  <br/>2. **构建基准数据集**：创建AP^2数据集，包含真实世界音频及敏感属性标注，填补研究空白。  <br/>3. **设计混合框架**：提出Gifts框架，结合ALM与LLM优势，提升音频敏感属性推理性能并减少幻觉。  <br/>4. **防御策略研究**：探讨模型与数据级别的防御方法，验证攻击可行性，推动隐私安全研究。|
|2507.09834v1|[Generative Audio Language Modeling with Continuous-valued Tokens and   Masked Next-Token Prediction](http://arxiv.org/abs/2507.09834v1)|**贡献点总结：**  <br/>1. 提出因果语言模型与扩散模型结合，生成连续音频无需离散token。  <br/>2. 引入token-wise扩散建模连续值分布，显著提升音频生成质量（FAD/KL）。  <br/>3. 设计新型masked next-token预测任务，与因果LM框架兼容并达到SOTA水平。  <br/>4. 实现更小参数规模（193M/462M），优于AudioGen的285M/1B参数。  <br/><br/>**100字以内总结：**  <br/>创新方法结合因果语言模型与扩散模型，显著提升音频生成质量（FAD/KL），参数规模更小，达到SOTA水平。|
|2507.09499v1|[The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM   Challenge](http://arxiv.org/abs/2507.09499v1)|**贡献点：**  <br/>1. 提出DKU系统，首次实现无需Oracle说话人标签和时间边界直接进行多说话人语音识别。  <br/>2. 构建跨模态框架，将说话人嵌入与时间边界信息无缝整合至Qwen2.5基底LLM，提升多说话人场景下的建模能力。  <br/>3. 引入多语言适配技术，通过语言特异性adapter和LoRA模块优化LLM解码器，增强跨语言性能。  <br/>4. 在MLC-SLM数据集上取得显著成果，开发集和测试集tcpWER分别达23.56%和18.08%，优于官方基线。  <br/><br/>**总结（100字以内）：**  <br/>本文提出DKU系统，通过整合说话人嵌入与时间边界信息至Qwen2.5 LLM，并引入多语言适配模块，实现了无需Oracle标签的多说话人语音识别，大幅优于官方基线。|
|2507.09195v1|[Towards Spatial Audio Understanding via Question Answering](http://arxiv.org/abs/2507.09195v1)|总结：  <br/>本研究提出一种基于问答范式的空间音频理解框架，通过构建细粒度时空文本描述数据集和语言引导模型，在仅使用场景级监督下实现与帧级监督模型相当的性能，推动空间场景分析与语言监督的结合。<br/><br/>贡献点：  <br/>1. **提出语言引导的空间音频理解框架**：首次将问答范式引入空间音频处理，旨在拓展声事件定位与检测（SEL）至空间场景理解与推理。  <br/>2. **构建细粒度时空文本描述数据集**：基于规则方法对STARSS23数据集生成高质量的文本描述，并通过大语言模型（LLM）增强语言多样性和表达能力。  <br/>3. **设计多模态问答数据集**：创建与STARSS23场景对齐的问答数据集，涵盖事件存在、定位、时空关系等多维度，并利用LLM生成多版本问题以提升数据丰富性。  <br/>4. **开发语言引导的基线模型**：提出将FOA信号与自然语言问题作为输入的空间音频问答模型，通过分类任务实现对声事件时空关系的理解。  <br/>5. **验证语言监督的有效性**：在仅使用场景级问答监督的情况下，模型性能与基于帧级标注的完全监督模型相当，证明了语言引导方法在空间音频分析中的潜力。|
|2507.09161v1|[Large Language Models and Non-Negative Matrix Factorization for   Bioacoustic Signal Decomposition](http://arxiv.org/abs/2507.09161v1)|**总结（100字以内）:**  <br/>本文提出结合大语言模型与矩阵分解的生物声学信号分析框架，实现无需标注数据的信号分离与医学条件关联，为临床决策支持提供更可解释的方法，具有智能诊断工具集成潜力。<br/><br/>**贡献点分点列出:**  <br/>1. **提出混合框架**：首次将大语言模型与矩阵分解方法结合，用于生物声学信号分析，突破传统数值方法的局限。  <br/>2. **信号分离创新**：采用矩阵分解技术分离临床录制中重叠的生物声学信号，实现对混合信号的可解释性分解。  <br/>3. **疾病关联建模**：利用大语言模型将分离的声学特征与潜在医学条件（如心律失常、呼吸异常）建立关联。  <br/>4. **高质量数据采集**：通过数字听诊器和临床假人构建受控实验环境，确保声学信号的高保真度与可靠性。  <br/>5. **无监督自动化**：方法无需依赖标注数据或源类型先验知识，提升临床数据分析的自动化和实用性。  <br/>6. **临床应用潜力**：为智能诊断工具提供可解释的框架，推动生物声学信号在医疗领域的实际应用。|
|2507.09116v3|[Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition](http://arxiv.org/abs/2507.09116v3)|**贡献点：**  <br/>1. 提出多模态GER（结合语音模态发音信息）与多粒度GER（引入音素级细粒度信息）方法，解决口音场景下的发音与语义建模不足问题。  <br/>2. 设计HDMoLE混合专家框架，通过层级路由与动态阈值机制融合单口音LoRA专家，应对口音多样性挑战。  <br/>3. 验证方法有效性：在多口音英语数据集上，较基线Whisper-large-v3模型将词错误率降低67.35%。  <br/><br/>**总结（100字以内）：**  <br/>本文通过多模态与多粒度生成式错误纠正方法，结合HDMoLE混合专家模型，有效提升多口音语音识别性能，显著降低词错误率。|
|2507.08603v1|[Unlocking Speech Instruction Data Potential with Query Rewriting](http://arxiv.org/abs/2507.08603v1)|总结：  <br/>提出多LLM知识融合的查询重写框架，通过代理协作构建高质量语音指令数据集，提升TTS模型数据适配性与复杂任务处理能力。<br/><br/>贡献点：  <br/>1. **问题定位**：指出现有语音指令数据集构建方法存在LLM生成与人类响应偏差大、TTS模型处理分布外文本不足的局限性。  <br/>2. **方法创新**：设计基于多LLM知识融合的查询重写框架，引入多代理协同注释与验证机制，实现无需人工标注的高质量数据集生成。  <br/>3. **技术突破**：通过零样本重写技术，将文本指令转化为TTS模型更易处理的分布，显著提升数据可用性（72%→93%）。  <br/>4. **应用扩展**：验证框架在复杂知识和上下文相关任务中的独特优势，推动语音指令数据集构建的通用性和鲁棒性。|
|2507.07799v1|[SecureSpeech: Prompt-based Speaker and Content Protection](http://arxiv.org/abs/2507.07799v1)|总结：  <br/>该论文提出了一种基于提示的语音生成框架，实现说话者身份与内容的双重匿名化，通过描述符控制身份隐藏和敏感内容替换，证明其在隐私保护与语音质量之间的平衡效果。<br/><br/>贡献点：  <br/>1. **提出双重匿名化框架**：首次结合说话者身份匿名化和语音内容匿名化，系统性解决语音领域隐私泄露问题。  <br/>2. **身份匿名化技术**：利用描述符控制语音生成过程，确保生成语音与源说话者身份无关联。  <br/>3. **内容敏感信息处理**：集成实体识别模型与大语言模型，实现对文本中敏感内容的自动替换。  <br/>4. **高保真语音生成**：在保持语音质量的前提下，通过TTS模型生成隐私友好的合成语音。  <br/>5. **实验验证有效性**：通过实验证明方法在隐私保护与内容保留/音频质量之间的显著效果。  <br/>6. **描述符影响分析**：研究不同描述对生成语音隐私性和实用性的潜在偏差，指导实际应用优化。|
|2507.06256v1|[Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World](http://arxiv.org/abs/2507.06256v1)|总结：  <br/>本研究揭示了音基大语言模型在真实场景中存在可被隐蔽音频扰动操控的漏洞，并发现背景噪声可严重影响用户交互，进一步证明攻击的可扩展性和迁移性，为安全防护提供新视角。<br/><br/>贡献点：  <br/>1. **发现隐蔽音频攻击的可能性**：首次证明攻击者可构造隐蔽的音频扰动，诱导ALLMs执行目标行为（如唤醒词响应或有害操作）。  <br/>2. **揭示噪声干扰的隐蔽性与危害性**：证明在用户交互过程中播放对抗性噪声会对响应质量造成显著损害，且噪声传播方式（如通过空气）可能影响其他无辜用户。  <br/>3. **验证攻击的可扩展性**：系统性地说明所提攻击方法可扩展至真实场景，对多方用户造成潜在威胁，拓展了对抗攻击的研究边界。  <br/>4. **探讨攻击迁移性与防御方向**：分析攻击方法在不同环境下的迁移性，提出针对性防御措施的研究路径，为提升语音模型安全提供了理论支持。|
|2507.05727v2|[ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark](http://arxiv.org/abs/2507.05727v2)|总结：  <br/>本文提出ContextASR-Bench，一个大规模多领域基准数据集，用于评估ASR系统的语言能力，通过引入上下文信息（如领域和专有名词）和三种评估模式，验证了大语言模型在提升语音识别准确率方面的显著优势，并公开数据集与代码。<br/><br/>贡献点：  <br/>1. **构建多领域基准数据集**：提出ContextASR-Bench，覆盖10个以上领域，包含超300,000个专有名词及40,000+数据条目，丰富上下文标注（领域与实体列表）。  <br/>2. **设计创新评估模式**：引入三种评价方式，系统评估模型在利用上下文信息提升语音识别准确率方面的表现。  <br/>3. **验证大模型优势**：实验证明Large Audio Language Models（LALMs）在语言能力上显著优于传统ASR模型，揭示其强大的世界知识与上下文建模能力。  <br/>4. **开放数据与代码**：提供完整数据集及评估代码，推动语音识别语言能力研究的可复现性与进一步探索。|
|2507.05727v1|[ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark](http://arxiv.org/abs/2507.05727v1)|总结：  <br/>该论文提出ContextASR-Bench，首个综合评估上下文语音识别的基准，涵盖多场景和命名实体识别，验证了LALMs在通用AI能力上的优势，并公开数据集与代码。<br/><br/>贡献点：  <br/>1. **提出首个上下文语音识别基准**：构建了ContextASR-Bench，突破传统无上下文评估框架，系统性检验ASR模型在情境理解与适用性上的表现。  <br/>2. **大规模多领域数据集**：包含40,000+条数据，覆盖10+领域，支持对用户、场景、上下文信息（粗粒度/细粒度）的全面评估。  <br/>3. **引入命名实体识别分析**：首次将名实识别能力（如人名、地名等）纳入ASR评估，增强对模型语言理解与推理能力的测试。  <br/>4. **验证LALMs优势**：通过实验表明，LALMs凭借世界知识和上下文学习能力，在复杂场景下显著优于传统ASR模型。  <br/>5. **开源资源支持**：提供数据集与评测代码，推动研究复现与扩展，促进该领域技术发展。|
|2507.05177v1|[OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech   Language Model](http://arxiv.org/abs/2507.05177v1)|**贡献点总结：**  <br/>1. 提出OpenS2S：首个完全开源、透明且端到端的情感语言模型（LSLM），支持情感语音交互。  <br/>2. 流式交错解码：实现低延迟语音生成，优化实时性。  <br/>3. 自动化数据构建：低成本生成高质量、多样化的语音对话数据。  <br/>4. 可控TTS系统：引入说话者和情感多样性，减少人工标注。  <br/>5. 开源资源发布：提供数据集、模型权重及训练代码，推动研究与创新。  <br/><br/>**100字内摘要：**  <br/>OpenS2S是一个开源情感语言模型，结合流式解码与自动化数据构建，实现低延迟语音生成并减少人工监督，促进情感语音系统研究与创新。|
|2507.03147v2|[DeepGesture: A conversational gesture synthesis system based on emotions   and semantics](http://arxiv.org/abs/2507.03147v2)|**贡献点：**  <br/>1. 提出DeepGesture框架，基于扩散模型生成与多模态信号（文本、语音、情感、初始动作）对齐的表达性共语手势。  <br/>2. 引入新颖的架构改进，提升生成手势的语义对齐与情感表达能力。  <br/>3. 集成快速文本转录作为语义条件，并实现情感引导的无分类器扩散方法，支持跨情感状态下可控手势生成。  <br/>4. 开发基于BVH的Unity全渲染管线，实现手势可视化。  <br/>5. 在ZeroEGGS数据集上验证有效性，表明生成手势具有更高的自然度与情境适配性。  <br/>6. 支持情感状态插值及对非分布语音（含合成语音）的泛化能力，推动多模态情感感知数字人类技术发展。  <br/><br/>**总结：**  <br/>本文提出DeepGesture框架，通过多模态信号驱动生成自然且情感丰富的共语手势，实现了高人类相似度与跨情感控制，推进了情感感知数字人类的构建。|
|2507.03147v1|[DeepGesture: A conversational gesture synthesis system based on emotions   and semantics](http://arxiv.org/abs/2507.03147v1)|**贡献点**  <br/>1. 提出DeepGesture框架，基于扩散模型生成与语音输入自然匹配的表达性伴随手势，支持多模态信号（文本、语音、情绪、种子动作）条件控制。  <br/>2. 引入快速文本转录作为语义条件，结合情感引导的无分类器扩散策略，实现情绪状态下的可控手势生成。  <br/>3. 设计轻量级Transformer架构，融合全自注意力与跨局部自注意力，提升异构模态特征的集成效率。  <br/>4. 构建基于BVH的Unity完整渲染管线，实现高质量手势可视化与数字人类交互效果。  <br/>5. 在ZeroEGGS数据集上验证，显著优于基线模型（MOS与FGD指标），且支持情绪插值与分布外语音（含合成声音）的泛化。  <br/><br/>**总结**  <br/>DeepGesture通过多模态条件控制与创新架构提升手势生成质量，实现情感化、跨模态的数字人类交互，推动全多模态数字人技术发展。|
|2507.02927v1|[A Unified Speech LLM for Diarization and Speech Recognition in   Multilingual Conversations](http://arxiv.org/abs/2507.02927v1)|**总结（100字以内）:**  <br/>本文提出一种统一的语音LLM，端到端实现对话分割与识别，通过优化数据格式和推理流程，在Task II中相对基线提升54.87%，并在Task I中展示微调模型效果，同时采用较小模型 backbone 实现高效性能。<br/><br/>**贡献点分点:**  <br/>1. **端到端联合建模方案**：首次提出统一的Speech LLM架构，同时完成对话分割（diarization）和语音识别（ASR），无需依赖oracle信息，解决传统方法中分割与识别分离的瓶颈。  <br/>2. **数据格式优化与推理改进**：通过改革训练数据格式和调整推理流程，有效缓解预分割音频的语义歧义问题，提升模型对自然对话场景的适应性。  <br/>3. **性能突破与效率优势**：在Task II中实现54.87%的相对性能提升（tcpWER/tcpCER），且在模型规模较小的条件下达到第8名，验证了高效模型设计的有效性。  <br/>4. **任务I的补充验证**：在Task I中展示基于微调的Speech LLM在oracle分割下的性能表现，为实际应用提供参考。（注：若需严格区分“贡献”与“补充结果”，可仅保留前三点）|
|2507.02768v1|[DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with   Self-Generated Cross-Modal Alignment](http://arxiv.org/abs/2507.02768v1)|总结（100字以内）:  <br/>提出DeSTA2.5-Audio模型，通过自生成策略解决LALM语言能力退化问题，构建跨模态对齐任务无关数据集，实现零样本泛化，在多个音频语言基准测试中达到SOTA性能。<br/><br/>贡献点分点列表:  <br/>1. **模型创新**  <br/>   提出DeSTA2.5-Audio，采用自生成的跨模态对齐策略，无需任务特定音频指令微调，有效缓解LLM语言能力退化（灾难性遗忘）问题。  <br/><br/>2. **数据集构建**  <br/>   构建DeSTA-AQA5M数据集，包含500万个样本（来自7000小时音频），涵盖语音、环境声、音乐等50类多元数据，实现任务无关的广泛适用性。  <br/><br/>3. **性能突破**  <br/>   在Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval、VoiceBench等音频语言基准测试中达到SOTA或竞争力水平。  <br/><br/>4. **方法验证**  <br/>   通过系统对比实验表明，自生成策略比传统方法在音频感知与指令跟随能力上表现更优，验证了其有效性。  <br/><br/>5. **理论价值**  <br/>   强调数据构建设计对LALM发展的关键作用，为构建通用型、鲁棒的音频-语言模型提供实践指导。|
|2507.02380v1|[JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](http://arxiv.org/abs/2507.02380v1)|**贡献点总结：**  <br/>1. **系统开发**：提出JoyTTS，首个融合大语言模型与TTS的端到端语音聊天机器人，具备语音克隆功能。  <br/>2. **模型整合**：基于MiniCPM-o和CosyVoice2开源模型构建，实现高效语音生成。  <br/>3. **大规模数据训练**：使用2000小时对话数据训练，提升对话自然度和语音质量。  <br/>4. **开源贡献**：提供完整训练代码及模型，推动技术社区的开发与优化。  <br/>5. **性能指标**：在seed-tts-zh测试中达成SS 0.73和WER 5.09，验证系统有效性。  <br/><br/>**总结（100字以内）：**  <br/>JoyTTS结合LLM与TTS技术，实现语音克隆功能；基于开源模型训练，提供完整代码及模型，促进社区开发。其在2000小时对话数据上取得SS 0.73和WER 5.09的优异表现，推动语音交互领域创新。|
|2507.01348v2|[SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and   Text to Speech](http://arxiv.org/abs/2507.01348v2)|**贡献点：**  <br/>1. 提出SpeechCodeVAE，首度将CTC嵌入代码本分词流程，实现语音内容分词的“locality”属性，平衡内容忠实度、时间连续性与结构恢复性。  <br/>2. 通过多任务学习策略联合训练FAC与TTS模块，提升收敛速率与生成质量，缓解数据稀缺问题。  <br/>3. 设计SpeechRestorer后处理架构，优化LLM生成结果，降低随机误差并增强韵律试验连续性，经消融实验验证有效性。  <br/><br/>**总结：**  <br/>本研究提出SpeechAccentLLM框架，通过创新性分词模型与多任务学习策略解决FAC难题，并引入后处理模块提升生成效果。|
|2507.01348v1|[SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and   Text to Speech](http://arxiv.org/abs/2507.01348v1)|总结：  <br/>该研究提出SpeechAccentLLM框架，通过SpeechCodeVAE首次将CTC集成到语音离散化中，并引入多任务学习和SpeechRestorer模块，解决了FAC数据稀缺问题，提升生成语音质量与韵律连续性。<br/><br/>贡献点：  <br/>1. **提出SpeechAccentLLM框架**：基于LLM的TTS技术，创新性地将其应用于语音外语音调转换（FAC），构建统一模型架构。  <br/>2. **设计SpeechCodeVAE模型**：首次将连接主义时序分类（CTC）直接引入codebook离散化，生成具有“局部性”特性的语音内容token，实现内容忠实性、时序一致性和结构恢复性的平衡。  <br/>3. **多任务学习策略**：联合训练FAC与TTS模块，缓解数据稀缺问题，并提升模型收敛速度和语音生成质量。  <br/>4. **开发SpeechRestorer后处理模块**：针对性地修正LLM生成的随机误差，增强韵律连续性，通过消融实验验证其有效性。|
|2507.00693v1|[Leveraging Large Language Models for Spontaneous Speech-Based Suicide   Risk Detection](http://arxiv.org/abs/2507.00693v1)|总结：  <br/>本研究提出利用语音分析预测青少年自杀风险，创新性融合大语言模型与传统特征，取得74%准确率并获挑战赛第一名，验证了LLM在心理健康评估中的潜力。<br/><br/>贡献点：  <br/>1. 首次将大语言模型（LLM）作为主要特征提取工具应用于自杀风险语音分析。  <br/>2. 融合LLM与传统声学、语义特征，提升模型对语音数据的多维度解析能力。  <br/>3. 在SW1挑战赛中实现74%测试集准确率，排名第一，证明方法有效性。  <br/>4. 展示语音作为非侵入性心理健康指标在青少年群体中的应用前景。  <br/>5. 推动LLM在心理健康领域，尤其是自杀风险评估中的创新性研究进展。|
|2506.23325v2|[XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate   Speech Codecs](http://arxiv.org/abs/2506.23325v2)|总结：  <br/>提出XY-Tokenizer，通过多阶段多任务学习平衡语义与音色，实现与SOTA编解码器相当的性能，并开源代码。<br/><br/>贡献点：  <br/>1. **问题分析**：揭示现有语音编解码器在语义丰富性与音色保真度之间的平衡不足。  <br/>2. **创新方法**：设计XY-Tokenizer，采用多阶段、多任务学习策略，有效缓解语义与音色能力的冲突。  <br/>3. **性能突破**：在相同比特率下，XY-Tokenizer的语义任务表现优于SpeechTokenizer和Mimi，音色重建性能与BigCodec相当（说话人相似度0.83 vs. 0.84）。  <br/>4. **开源共享**：提供代码与模型，便于研究复现与应用推广。|
|2506.23325v1|[XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate   Speech Codecs](http://arxiv.org/abs/2506.23325v1)|**贡献点总结（分点）：**  <br/>1. **提出新型语音编解码器**：设计XY-Tokenizer，通过多阶段、多任务学习解决语义与声学信息平衡问题。  <br/>2. **性能突破**：在相似码率下，XY-Tokenizer的语义文本对齐能力超越现有语义模型（如SpeechTokenizer、Mimi），声学重建表现接近顶级声学编解码器BigCodec。  <br/>3. **开源共享**：提供代码和模型供研究复现，促进技术发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出XY-Tokenizer，通过多阶段多任务学习实现语音语义与声学信息的平衡，兼顾文本对齐和高保真音频重建，在相似码率下性能优于现有方法，代码开源推动领域发展。|
|2506.22846v1|[Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](http://arxiv.org/abs/2506.22846v1)|**总结（100字以内）:**  <br/>提出Language-Aware Intermediate Loss（LAIL）框架，结合大语言模型（LLM）的语言知识，通过连接层和因果语言建模损失提升CTC-based ASR的准确性，实现在LibriSpeech、TEDLIUM2和WSJ等数据集上达到SOTA性能，且保持高效解码能力。<br/><br/>**贡献点分点列出:**  <br/>1. **提出LAIL框架**：设计一种新的辅助损失机制，通过整合大语言模型（LLM）的语言知识提升CTC-based ASR系统的性能。  <br/>2. **非自回归解码增强**：在中间编码器层添加连接层，将输出映射到LLM的嵌入空间，并引入因果语言建模损失，强化语言依赖建模。  <br/>3. **高效性与准确性的平衡**：保持CTC非自回归解码的计算效率，同时显著降低Word Error Rate（WER），实现性能突破。  <br/>4. **跨数据集验证**：在LibriSpeech、TEDLIUM2和WSJ等主流语音数据集上验证LAIL框架的有效性，证明其普适性。  <br/>5. **模型架构适配**：基于Conformer和LLaMA系列模型的实验，展示LAIL框架对不同编码器结构的兼容性与提升效果。|
|2506.21448v2|[ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing](http://arxiv.org/abs/2506.21448v2)|**贡献点总结：**  <br/>1. 提出ThinkSound框架，集成多阶段CoT推理实现视频音频生成与编辑。  <br/>2. 构建AudioCoT数据集，关联视觉、文本与声音合成的结构化注释。  <br/>3. 在音频质量与跨模态一致性指标上达到SOTA，并在Movie Gen Audio基准中表现优异。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出ThinkSound框架，通过分阶段CoT推理实现视频到音频的高保真生成与交互编辑，并构建AudioCoT数据集以促进跨模态对齐。实验表明其在音频指标与CoT评估中均领先，且在电影音频生成基准中表现卓越。|
|2506.21448v1|[ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing](http://arxiv.org/abs/2506.21448v1)|总结：  <br/>该论文提出ThinkSound框架，通过多阶段CoT推理实现高保真视频到音频生成，引入AudioCoT数据集，兼具语音生成与多模态理解，实验验证其在音频质量和推理任务上的SOTA表现。<br/><br/>贡献点：  <br/>1. **提出ThinkSound框架**：首次将Chain-of-Thought（CoT）推理引入视频到音频生成，通过分阶段（基础 Foley、对象中心交互精修、自然语言引导编辑）实现音频生成与编辑的协同。  <br/>2. **构建AudioCoT数据集**：提供结构化推理标注，建立视觉、文本与声音合成的关联，支持多模态学习与评估。  <br/>3. **统一多模态模型架构**：在生成各阶段整合多模态大语言模型与音频基础模型，实现上下文对齐的推理引导。  <br/>4. **验证跨模态性能**：在音频质量指标及CoT推理任务中达到SOTA，并在Movie Gen Audio基准测试中表现出色。|
|2506.21191v2|[Prompt-Guided Turn-Taking Prediction](http://arxiv.org/abs/2506.21191v2)|总结：  <br/>本研究提出基于文本提示的动态turn-taking预测模型，通过融合提示嵌入提升预测准确性，并验证了该方法在对话场景中的有效性。<br/><br/>贡献点：  <br/>1. **动态控制机制**：首次将文本提示（如“更快”“更冷静”）引入turn-taking预测，实现对对话行为的直观控制。  <br/>2. **模型结构创新**：在Transformer语音活动预测框架中，结合文本提示嵌入至通道级与跨通道级注意力机制。  <br/>3. **数据生成方法**：因缺乏真实提示数据，提出利用大语言模型生成合成提示句子以支持实验验证。  <br/>4. **实证效果验证**：通过950小时真实对话数据，证明模型能有效调整预测行为并提升准确性。|
|2506.21191v1|[Prompt-Guided Turn-Taking Prediction](http://arxiv.org/abs/2506.21191v1)|**贡献点：**  <br/>1. **提出动态控制的turn-taking预测模型**：首次通过文本提示（如"faster"或"calmer"）实现对对话行为的实时、动态调控，增强系统适应对话伙伴和情境的能力。  <br/>2. **创新性融合提示嵌入机制**：将文本提示嵌入至通道内Transformer和跨通道Transformer，实现上下文感知与多通道信息协同优化。  <br/>3. **构建合成数据生成方案**：利用大语言模型（LLM）生成符合实际场景的文本提示数据，填补现有数据集的空白。  <br/>4. **验证模型有效性**：在950小时真实对话数据上实验，证明模型能显著提升预测准确性并灵活调整对话节奏行为。  <br/><br/>**总结（100字以内）：**  <br/>本文提出通过文本提示动态控制turn-taking预测的模型，结合LLM生成合成数据，验证了其提升预测精度和适应对话行为的能力，为对话系统提供更灵活的交互调控方法。|
|2506.18510v1|[Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich   Transcripts](http://arxiv.org/abs/2506.18510v1)|总结：  <br/>本研究提出利用大语言模型处理不流畅语音的新方法，通过整合声学特征与包含时间戳的文本输入，验证了即使文本不完美，LLMs也能鲁棒地生成高质量标注转录，推动更包容的语音技术发展。<br/><br/>贡献点：  <br/>1. **提出语音不流畅转录新框架**：首次将大语言模型（LLMs）用于将语音中的不流畅现象显式标注为带时间戳的独立符号。  <br/>2. **验证文本输入鲁棒性**：证明LLMs在处理包含时间戳线索的文本输入时，无需依赖完美转录，可有效消除文本缺陷。  <br/>3. **多模态信息融合**：结合音频编码器提取的声学特征与多种文本输入（如对齐转录、音素ASR输出），实现跨模态协同标注。  <br/>4. **推动应用落地**：为生成全注释的不流畅语音转录提供可行方案，提升ASR和语言处理系统的鲁棒性与实用性。|
|2506.17611v1|[OpusLM: A Family of Open Unified Speech Language Models](http://arxiv.org/abs/2506.17611v1)|总结：本研究提出开源语音语言模型OpusLMs，通过大规模训练和技术创新实现多任务性能突破，并全面开源代码与数据提升研究透明度。<br/><br/>贡献点：  <br/>1. **多模态基础模型架构**：首次构建7B参数规模的开放语音语言模型家族（OpusLMs），融合语音-文本对与纯文本数据训练。  <br/>2. **多阶段训练策略**：提出多流语言模型、多阶段训练方法，优化语音与文本任务的联合学习。  <br/>3. **性能验证突破**：在语音识别、语音合成及纯文本任务中，表现优于或持平于现有SpeechLMs。  <br/>4. **开源与透明实践**：完全基于公开材料构建，公开代码、数据、训练日志和检查点，推动开放研究。  <br/>5. **数据选择机制研究**：实验证明模型规模扩展与数据选择（如annealing策略）对性能的关键影响。|
|2506.15556v1|[PredGen: Accelerated Inference of Large Language Models through   Input-Time Speculation for Real-Time Speech Interaction](http://arxiv.org/abs/2506.15556v1)|**Summary (within 100 characters):**  <br/>提出PredGen框架，显著降低实时语音聊天中的延迟，提升用户体验。<br/><br/>---<br/><br/>**Contributions:**  <br/>1. **提出PredGen框架**：设计了一种基于推测解码的新型方法，通过在用户输入过程中生成候选响应，突破传统LLM在语音聊天中的延迟瓶颈。  <br/>2. **实验验证有效性**：在Lmsys和MT-Bench数据集上验证，方法可将延迟降低约2倍，且计算成本增加微乎其微。  <br/>3. **揭示延迟根源**：发现LLM生成首句话是导致延迟的核心因素，针对这一问题提出优化方案。  <br/>4. **适应低资源场景**：在消费级硬件上实现高效部署，改善单用户语音助手的实时性与用户体验。|
|2506.15220v2|[video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](http://arxiv.org/abs/2506.15220v2)|总结（100字以内）:  <br/>该论文提出video-SALMONN 2，结合LoRA与DPO技术优化视频音频描述生成，引入新评估指标并设计多轮DPO训练方法，显著降低28%错误率，在70亿参数规模下超越GPT-4o和Gemini-1.5-Pro，保持视频问答任务的竞争力。<br/><br/>贡献点分点列出:  <br/>1. **提出video-SALMONN 2模型**：首个集成低秩适应（LoRA）和定向偏好优化（DPO）的视听大语言模型，专为视频（含配对音频）的精准描述生成设计。  <br/>2. **创新评估指标**：开发新的指标用于量化视频描述的完整性和准确性，并通过DPO进行优化。  <br/>3. **多轮DPO训练方法（MrDPO）**：  <br/>   - 定期更新DPO参考模型  <br/>   - 每轮训练后合并/重置LoRA模块（每1000步）  <br/>   - 引入真实视频字幕指导以提升稳定性  <br/>4. **性能突破**：在70亿参数规模下，视频-SALMONN 2的视频描述错误率降低28%，文本生成能力超越GPT-4o和Gemini-1.5-Pro，并与同规模SOTA模型在视频问答任务中竞争。  <br/>5. **开源实现**：提供完整代码，便于复现和进一步研究。|
|2506.15220v1|[video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](http://arxiv.org/abs/2506.15220v1)|总结:  <br/>本研究提出视频-SALMONN 2，结合LoRA与DPO技术提升视频描述生成，并引入新评估指标和多轮优化策略，显著提高准确性且保持高效参数规模，优于主流模型。<br/><br/>贡献点:  <br/>1. **提出视频-SALMONN 2模型**：集成低秩适应（LoRA）和定向偏好优化（DPO），专为音频-视频配对的描述生成任务设计。  <br/>2. **定义新评估指标**：开发衡量视频描述完整性与准确性的新指标，并通过DPO进行优化。  <br/>3. **创新多轮DPO训练框架（MrDPO）**：包含周期性更新参考模型、合并/重置LoRA模块、整合真实字幕指导等策略，提升训练稳定性。  <br/>4. **验证模型有效性**：实验表明MrDPO使错误率降低28%，且在70亿参数下超越GPT-4o和Gemini-1.5-Pro等主流模型。  <br/>5. **开源实现**：提供代码供研究和应用，促进技术复现与推广。|
|2506.15154v1|[SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](http://arxiv.org/abs/2506.15154v1)|**贡献点：**<br/>1. 提出多任务音乐字幕生成模型SonicVerse，整合音乐特征检测（调识别、人声检测等）与字幕生成任务。  <br/>2. 设计投影式架构，将音频输入转化为语言标记，实现低级声学细节与高级音乐属性的联合建模。  <br/>3. 引入专用辅助头检测音乐特征，并将检测结果投影为语言标记以增强字幕生成输入。  <br/>4. 利用大语言模型链接分段输出，生成长音乐作品的时间轴相关详细描述。  <br/>5. 扩展MusicBench数据集，通过MIRFLEX标注音乐特征，构建配对数据用于模型训练。  <br/>6. 实验验证：融合音乐特征显著提升字幕生成的质量与细节表达。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出SonicVerse多任务模型，结合音频标注与音乐特征检测，通过投影架构及大语言模型实现长音乐作品的时序描述生成，扩展数据集并验证了特征融合对生成质量的提升效果。|
|2506.14767v1|[A Variational Framework for Improving Naturalness in Generative Spoken   Language Models](http://arxiv.org/abs/2506.14767v1)|**贡献点总结（100字以内）**:  <br/>提出端到端变分方法，自动编码语音的连续属性以提升语义tokens，无需人工特征工程，增强语音自然度，并依据人类评分优化语音延续效果，提供代码和模型便于复现。<br/><br/>**分点贡献**:  <br/>1. **方法创新**：首次提出基于变分自编码器的端到端框架，自动学习编码连续语音属性（包括非语言特征）。  <br/>2. **问题解决**：解决现有语义tokens仅关注语言信息、忽视语调等非语言特征导致语音不自然的问题。  <br/>3. **评估优化**：生成的语音延续符合人类评分者偏好，提升生成质量与实用性。  <br/>4. **简化流程**：消除手动特征提取和选择的依赖，降低模型设计复杂度。  <br/>5. **开源共享**：提供完整代码、样本和模型，推动研究与应用。|
|2506.13642v2|[Stream-Omni: Simultaneous Multimodal Interactions with Large   Language-Vision-Speech Model](http://arxiv.org/abs/2506.13642v2)|**贡献点：**  <br/>1. **提出Stream-Omni架构**：首次设计支持文本、视觉、语音多模态协同的模型，通过灵活的模态对齐策略实现高效交互。  <br/>2. **差异化模态对齐方法**：  <br/>   - 对视觉模态采用序列维度拼接，实现语义互补的视觉-文本对齐；  <br/>   - 对语音模态引入CTC-based层维度映射，实现语义一致的语音-文本对齐。  <br/>3. **降低数据依赖**：通过针对性对齐策略，显著减少训练数据需求（尤其语音数据），提升跨模态迁移能力。  <br/>4. **多任务性能验证**：在视觉理解、语音交互及视觉-语音联合任务中均表现出色，证明模型的通用性与有效性。  <br/>5. **实时中间输出支持**：利用层维度映射技术，在语音交互过程中同步生成ASR转录和模型响应，增强用户交互体验。  <br/><br/>**总结：** 本研究提出Stream-Omni模型，通过差异化模态对齐策略与CTC层映射技术，降低数据依赖并提升跨模态迁移能力，验证了其在多模态任务中的高性能与实时交互优势。|
|2506.13642v1|[Stream-Omni: Simultaneous Multimodal Interactions with Large   Language-Vision-Speech Model](http://arxiv.org/abs/2506.13642v1)|**总结**：  <br/>本文提出Stream-Omni模型，通过序列和层维度对齐策略实现多模态高效整合，减少语音数据依赖，提升文本迁移能力，并在多个任务中验证其有效性，支持实时中间文本输出。<br/><br/>---<br/><br/>**贡献点**：  <br/>1. **提出Stream-Omni模型**：首次设计支持文本、视觉、语音多模态整合的架构，实现更高效且灵活的跨模态对齐。  <br/>2. **差异化对齐策略**：针对视觉模态采用序列维度拼接，语音模态引入CTC-based层维度映射，分别优化模态间的语义关联。  <br/>3. **数据效率提升**：通过减少对语音数据的依赖，显著降低训练成本，同时增强文本能力向其他模态的迁移能力。  <br/>4. **多任务性能验证**：在视觉理解、语音交互及视觉-语音结合任务上取得优异表现，证明模型的泛化性和有效性。  <br/>5. **实时中间输出支持**：利用层维度映射技术，在语音交互过程中同步提供ASR转录和模型响应等中间文本结果，提升用户体验。|
|2506.13596v2|[Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems](http://arxiv.org/abs/2506.13596v2)|**总结（100字以内）**  <br/>本文提出面向多语言语音识别的系统，结合Whisper-large-v3编码器与高效投影器架构，通过三阶段训练优化模型组件，采用Gemma3-12B和Qwen2.5-7B作为解码器，实现16.63%和18.6%的WER/CER性能，展示多语言建模的有效性。<br/><br/>**贡献点分点**  <br/>1. **系统设计**：提出多语言语音识别与语言建模的联合系统，集成微调后的Whisper-large-v3编码器与高效投影器架构。  <br/>2. **三阶段训练**：开发逐阶段优化编码器、投影器和LLM组件的训练方法，提升模型整体性能。  <br/>3. **解码器配置**：采用Gemma3-12B和Qwen2.5-7B作为解码器，验证不同模型在多语言任务中的效果。  <br/>4. **性能表现**：在MLC-SLM Challenge 2025中取得16.63%和18.6%的WER/CER结果，体现竞争力与实用性。|
|2506.13596v1|[Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems](http://arxiv.org/abs/2506.13596v1)|**贡献点：**  <br/>1. **系统架构创新**：融合Whisper-large-v3编码器与高效投影器架构，优化多语言语音识别流程。  <br/>2. **分阶段训练方法**：提出逐阶段优化编码器、投影器和LLM的三阶段训练策略，提升模型性能。  <br/>3. **解码器验证**：采用Gemma3-12B和Qwen2.5-7B作为解码器-only模型，实验证明其在多语言任务中的竞争力。  <br/><br/>**总结**：  <br/>该论文提出多语言语音识别系统的创新架构与分阶段训练方法，结合高效模型组件，验证了不同解码器在挑战赛中的性能表现。|
|2506.12935v1|[SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models](http://arxiv.org/abs/2506.12935v1)|**贡献点总结（100字以内）:**  <br/>本文提出ALR数据集及SoundMind算法，显著提升音频逻辑推理性能，推动听觉智能发展，代码与数据集已开源。<br/><br/>**分点贡献:**  <br/>1. **构建高质量音频-语言推理数据集**：创建包含6,446个文本-音频标注样本的ALR数据集，专为复杂推理任务设计。  <br/>2. **提出SoundMind算法**：开发基于规则的强化学习（RL）方法，专门增强音频语言模型（ALMs）的双模态推理能力。  <br/>3. **实现SOTA性能**：通过在ALR数据集上训练Qwen2.5-Omni-7B，使模型在音频逻辑推理任务中达到当前最优水平。  <br/>4. **强调数据与算法协同作用**：验证高质量推理数据与专用RL技术结合对 auditory intelligence 的推动价值。  <br/>5. **开源资源**：公开代码及数据集，促进研究复现与社区发展。|
|2506.12570v1|[StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous   Autoregressive Modeling](http://arxiv.org/abs/2506.12570v1)|总结：  <br/>本文提出StreamMel，首个单阶段流式TTS框架，通过建模连续mel频谱图和文本-声学交织机制，实现低延迟高质量语音生成，性能媲美离线系统，为实时语音大语言模型集成提供新方案。<br/><br/>贡献点：  <br/>1. **首次提出单阶段流式TTS框架**：突破传统多阶段流水线设计，简化系统结构。  <br/>2. **建模连续mel频谱图**：替代离散表示方法，直接生成连续时域音频信号。  <br/>3. **文本-声学帧交织机制**：实现低延迟自回归合成，同时维持高说话人相似度和自然度。  <br/>4. **优化实时性能**：在LibriSpeech实验中，质量与延迟均优于现有流式TTS基线。  <br/>5. **接近离线系统表现**：在实时生成场景下达到离线系统的语音质量水平。  <br/>6. **推动实时应用落地**：展示与实时语音大语言模型集成的可行性与广泛前景。|
|2506.12481v1|[Exploring Audio Cues for Enhanced Test-Time Video Model Adaptation](http://arxiv.org/abs/2506.12481v1)|**贡献点：**  <br/>1. **提出音频辅助的视频TTA方法**：首次将音频信息引入视频TTA框架，弥补现有方法仅依赖视觉信号的不足。  <br/>2. **引入音频-视频伪标签生成机制**：通过预训练音频模型与大语言模型结合，实现音频到视频标签的语义映射，生成新型伪标签。  <br/>3. **动态适应迭代策略**：基于损失与多视角一致性变化，设计灵活的适应周期以优化每样本的迭代次数。  <br/>4. **构建新数据集与验证有效性**：提出AVE-C与AVMIT-C两个新型音频-视频TTA数据集，覆盖多类噪声干扰，在多个基准数据集上验证方法优势。  <br/><br/>**总结**（100字以内）：  <br/>本研究首次将音频信息与视频TTA结合，提出音频辅助伪标签生成与动态适应策略，构建新数据集验证方法有效性，显著提升了视频分类模型的测试时适应性能。|
|2506.12285v1|[CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction   Following](http://arxiv.org/abs/2506.12285v1)|**贡献点总结（100字以内）:**  <br/>提出CMI-Bench，统一评估音频-文本LLMs在音乐信息检索任务中的表现，涵盖13项核心任务，采用标准化指标与工具包，揭示模型性能差距及文化、性别等偏差，推动音乐感知LLMs发展。<br/><br/>**分点贡献:**  <br/>1. **构建新型基准**：将传统MIR注释重新设计为指令跟随格式，推出CMI-Bench，覆盖音乐理解与生成的多维度任务（如分类、回归、标注等）。  <br/>2. **任务多样性**：整合13项MIR核心挑战任务（包括体裁、情感、乐器、节奏、歌词等），全面评估音频-文本LLMs能力。  <br/>3. **标准化评估**：采用与SOTA MIR模型一致的评价指标，确保与监督方法的直接可比性，提升测评客观性。  <br/>4. **开放工具支持**：提供兼容主流开源音频-文本LLMs（如LTU、Qwen-audio等）的评估工具包，促进模型通用性测试。  <br/>5. **揭示性能与偏见**：实验发现LLMs与监督模型存在显著性能差异，并暴露文化、时间、性别等潜在偏见。  <br/>6. **推动领域进展**：为音乐感知LLMs建立统一测评框架，加速研究方向与技术改进。|
|2506.12222v1|[SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for   Polyphonic Soundscapes](http://arxiv.org/abs/2506.12222v1)|**贡献点总结（100字以内）**  <br/>提出SSLAM方法，针对性解决多声部音频泛化问题，显著提升多声部数据性能（最高9.1%），同时保持单声道基准表现（AS-2M达50.2 mAP），推动自监督音频学习向更复杂现实场景发展。<br/><br/>**分点贡献**  <br/>1. **提出SSLAM框架**：首次探索自监督学习在多声部音频混音数据上的应用，突破传统单声道数据集的限制。  <br/>2. **解决泛化能力不足**：通过设计多声部训练机制，提升模型在自然场景中处理多源重叠音频的能力。  <br/>3. **双任务性能保障**：在标准单声道音频SSL基准（如AudioSet-2M）上保持或超越现有方法，同时大幅优化多声部任务表现。  <br/>4. **实验验证优势**：在高质量多声部数据集上实现最高9.1%的性能提升，在单声道任务中达到50.2 mAP（提升3.9%），刷新SOTA。|
|2506.12083v1|[TuneGenie: Reasoning-based LLM agents for preferential music generation](http://arxiv.org/abs/2506.12083v1)|总结：  <br/>该论文提出基于LLM的音乐偏好分析框架TuneGenie，并开发评估方法，探索AI在音乐生成艺术中的潜力，推动相关技术研究与争议讨论。<br/><br/>贡献点：  <br/>1. **创新模型提出**：首次构建TuneGenie，基于LLM的文本表示方法，专用于音乐模型与偏好分析。  <br/>2. **音乐偏好分析**：利用播放列表元数据、用户评论等多源数据，开发LLMs在分析个体音乐偏好的新方法。  <br/>3. **生成音乐提示**：设计基于偏好分析的提示生成机制，优化Suno AI等音乐生成工具的输入效果。  <br/>4. **评估基准开发**：建立多种模型评估与基准测试方法，为AI音乐生成研究提供量化分析工具。  <br/>5. **促进领域发展**：通过实验与讨论，拓展AI生成艺术的探索边界，引发对技术伦理与效果的争议性思考。|
|2506.11091v1|[Customizing Speech Recognition Model with Large Language Model Feedback](http://arxiv.org/abs/2506.11091v1)|**总结**：提出基于大语言模型的强化学习方法，通过无监督领域自适应提升语音识别的命名实体识别性能，实现21%的实体词错误率改善。<br/><br/>**贡献点**：<br/>1. **提出新型自适应策略**：开发基于强化学习的无监督领域自适应方法，解决ASR系统在罕见命名实体和领域不匹配场景下的识别难题。<br/>2. **引入LLM作为奖励模型**：将大语言模型作为奖励函数，利用其对ASR假设的评分能力，优化传统自训练方法的局限性。<br/>3. **有效利用未标记数据**：通过LLM反馈信号实现对ASR模型的微调，无需依赖人工标注数据，在未监督场景下提升性能。<br/>4. **显著性能提升**：在实体词错误率（WER）上较传统方法提升21%，验证了该方法在应对领域适应问题上的有效性。|
|2506.11079v1|[Improving Child Speech Recognition and Reading Mistake Detection by   Using Prompts](http://arxiv.org/abs/2506.11079v1)|总结：  <br/>本研究提出了一种结合Whisper与指令调优大语言模型的多模态方法，显著提升儿童语音识别准确率与阅读错误检测性能，在荷兰语儿童朗读数据集上达到SOTA水平。<br/><br/>贡献点：  <br/>1. **提出多模态方法**：首次融合音频信号与文本资源知识，构建阅读评估系统的创新框架。  <br/>2. **优化儿童语音识别**：通过提示机制改进Whisper模型，将荷兰儿童朗读的词错误率（WER）从9.4%降至5.1%。  <br/>3. **提升阅读错误检测**：验证该方法在阅读错误识别任务中的有效性，F1分数由0.39提升至0.73。  <br/>4. **指令调优LLMs的应用**：探索指令调优大语言模型与提示的协同作用，推动语音评估技术的实用性。  <br/>5. **验证基线对比优势**：证明提示优化后的模型在关键指标上优于传统基线方法，具有实际推广价值。|
|2506.10423v1|[PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer   from Audio Encoders to LLMs](http://arxiv.org/abs/2506.10423v1)|总结：  <br/>该论文提出优化音频-LLM跨模态信息传输的三个关键策略，验证了延迟音频整合、注意力子模块主导信息提取及多编码器集成的有效性，显著提升模型性能。<br/><br/>贡献点：  <br/>1. 提出延迟音频整合至LLM初始层以增强语境建模的机制，提升音频信息探查能力。  <br/>2. 验证LLM仅需通过注意力子模块即可有效提取音频表征，无需依赖Feed-Forward Network。  <br/>3. 设计多音频编码器集成架构，实现互补表征融合，拓宽音频信息探查范围。|
|2506.10416v1|[Can Sound Replace Vision in LLaVA With Token Substitution?](http://arxiv.org/abs/2506.10416v1)|总结：  <br/>本文提出SoundCLIP框架，通过直接音频-视觉融合替代传统文本对齐方式，设计两种音频嵌入策略，构建AVE-2数据集，并揭示跨模态对齐与生成质量间的权衡关系，为多模态任务提供新思路。<br/><br/>贡献点：  <br/>1. **提出直接音频-视觉融合方法**：取代CLIP文本对齐表示，通过替换视觉token为音频表示，实现更直接的音视频信息整合。  <br/>2. **设计两种音频嵌入配置**：  <br/>   - 音频特征通过InfoNCE训练的MLP投影到CLIP视觉空间；  <br/>   - 保留原始音频嵌入并仅做少量维度调整。  <br/>3. **构建AVE-2数据集**：包含580,147个三秒音视频片段及细粒度对齐标注，为多模态研究提供高质量基准。  <br/>4. **揭示关键权衡关系**：发现音频投影提升检索性能的同时会损害文本生成质量，强调语言预训练的重要性。  <br/>5. **提出帕累托最优平衡观点**：指出多模态任务需在跨模态对齐与生成能力间取舍，而非追求单一性能最大化。|
|2506.10312v1|[AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](http://arxiv.org/abs/2506.10312v1)|总结：  <br/>本文提出一种基于对话延续能力的音频理解模型，通过训练生成对话式回复缓解描述变化问题，实现零样本指令跟随，无需多任务训练，并在多个基准数据集上验证其有效性。<br/><br/>贡献点：  <br/>1. **方法创新**：首次将大语言模型（LLM）的对话延续能力引入音频理解任务，构建指令跟随型音频语义模型。  <br/>2. **训练策略优化**：采用对话延续训练替代传统目标描述生成，降低模型对多样化描述的依赖，提升语义一致性。  <br/>3. **零样本能力**：无需多任务指令调优，仅基于音频描述数据集即可实现零样本指令遵循，扩展模型适用性。  <br/>4. **跨任务验证**：在AudioCaps、WavCaps、Clotho数据集及AudioBench评测中验证模型对未见指令的泛化能力。|
|2506.10299v1|[Scheduled Interleaved Speech-Text Training for Speech-to-Speech   Translation with LLMs](http://arxiv.org/abs/2506.10299v1)|**贡献点：**  <br/>1. **提出新训练方法**：设计"Schedule Interleaved Speech-Text Training"，通过交替使用语音和文本单元（文本词元在单词层面对齐），解决LLMs从文本到语音模态的迁移难题。  <br/>2. **渐进式模态适应**：动态调整训练中语音与文本的比例，逐步减少文本依赖以促进语音模态的适应，提升模型对语音数据的建模能力。  <br/>3. **实验验证有效性**：在CVSS数据集上对LLaMA3.2-1B进行微调，证明该方法显著提升S2ST性能，尤其在低资源语言场景下效果更优。  <br/><br/>**总结（100字内）：**  <br/>本研究提出一种渐进式语音-文本联合训练方法，通过动态调整模态比例解决大语言模型在语音翻译中的迁移难题，实验表明其在低资源语言上显著提升翻译性能。|
|2506.10274v2|[Discrete Audio Tokens: More Than a Survey!](http://arxiv.org/abs/2506.10274v2)|**总结**：  <br/>论文系统性地综述了离散音频标记方法，提出多维度分类框架，并在跨领域基准上评估性能，揭示关键挑战与实用方向，为语音与音频处理研究提供指导和资源。<br/><br/>**贡献点**：  <br/>1. **离散音频标记优势与应用场景**：  <br/>   提出离散音频标记作为紧凑表示，同时保留感知质量、语音内容、说话人特征，并具备高效存储、推理能力及下游任务竞争力，为语音和音频整合到LLMs提供替代方案。  <br/><br/>2. **跨领域系统性综述与基准测试**：  <br/>   首次对语音、音乐、一般音频三领域的离散标记器进行系统性回顾与统一基准评估，填补现有研究缺乏跨基准对比的空白。  <br/><br/>3. **多维度分类框架**：  <br/>   基于编码器-解码器结构、量化技术、训练范式、流式处理和应用领域，提出统一的分类体系，便于方法比较与研究方向归纳。  <br/><br/>4. **多基准综合评估与消融分析**：  <br/>   在重建、下游任务和声学语言建模等基准上对标记器进行评估，并通过控制实验分析技术间的权衡与性能影响。  <br/><br/>5. **关键挑战与研究方向探索**：  <br/>   总结离散标记器的局限性与实用问题，提出未来研究的开放挑战，为技术发展提供理论指导。  <br/><br/>6. **开放资源与数据库**：  <br/>   提供离散音频标记器数据库及详细实验结果，通过网站公开研究成果，促进领域共享与复用。|
|2506.10274v1|[Discrete Audio Tokens: More Than a Survey!](http://arxiv.org/abs/2506.10274v1)|总结：  <br/>该论文系统分析了离散音频标记器在语音、音乐和通用音频领域的表现，提出统一分类框架并进行多维度基准测试，揭示技术局限与挑战，为后续研究提供指导。<br/><br/>贡献点：  <br/>1. **首次跨领域综述与基准测试**：系统性评估了离散音频标记器在语音、音乐和通用音频三大领域的性能，填补了现有研究单一化的空白。  <br/>2. **提出统一分类框架**：基于编码器-解码器结构、量化技术、训练范式、流式处理及应用场景，构建离散音频标记方法的分类学体系。  <br/>3. **多任务综合评估**：在重建任务、下游任务表现和声学语言建模等维度设计基准测试，量化不同标记器的优劣与适用性。  <br/>4. **控制消融实验分析权衡**：通过结构化消融研究，揭示标记器设计中的关键性能权衡与优化方向。  <br/>5. **识别关键挑战与研究方向**：总结技术局限与实际应用问题，为离散音频标记领域的未来研究提供系统性洞见。|
|2506.09175v1|[PHRASED: Phrase Dictionary Biasing for Speech Translation](http://arxiv.org/abs/2506.09175v1)|**贡献点分点列出**:<br/><br/>1. **提出新的方法**  <br/>   首次提出"短语词典偏差方法"（Phrase Dictionary Biasing），用于解决语音翻译中短语翻译困难的问题，通过利用源语言与目标语言之间的短语映射关系提升翻译质量。<br/><br/>2. **方法适用性验证**  <br/>   将该方法应用于两类主流模型：基于transducer的流式语音翻译模型和多模态大语言模型，证明其跨模型的通用性与有效性。<br/><br/>3. **性能提升显著**  <br/>   在流式语音翻译模型中，该方法相较传统"短语列表偏差"方法（Phrase List Biasing）实现**21%相对提升**，在多模态模型中提升**85%短语召回率**。<br/><br/>**总结**:  <br/>提出短语词典偏差方法，通过利用语言对映射关系显著提升语音翻译模型的短语翻译性能与召回率。|
|2506.08524v2|[Teaching Physical Awareness to LLMs through Sounds](http://arxiv.org/abs/2506.08524v2)|总结：  <br/>提出ACORN框架，通过物理声学模拟器与音频编码器提升LLMs对物理现象的理解能力，构建AQA-PHY数据集，在多普勒效应、空间关系等任务中实现有效应用。<br/><br/>贡献点：  <br/>1. **ACORN框架**：首次将物理感知与LLMs结合，通过声音信号教授基础物理现象（如多普勒效应、多径效应、空间关系）。  <br/>2. **物理驱动的模拟器**：设计结合真实声源与可控物理信道的模拟器，解决数据稀缺问题并生成多样性训练数据。  <br/>3. **AQA-PHY数据集**：构建专注物理相关音频任务的高质量数据集，涵盖声学现象与空间感知标注。  <br/>4. **音频编码器优化**：提出并行处理音频幅度与相位信息的编码器，增强对声学特征的捕捉能力。  <br/>5. **跨模态应用验证**：在模拟与真实场景中验证框架有效性，实现视线检测、多普勒估计等物理任务的可迁移能力。|
|2506.08524v1|[Teaching Physical Awareness to LLMs through Sounds](http://arxiv.org/abs/2506.08524v1)|**贡献点：**  <br/>1. **提出ACORN框架**：通过声音教学，赋予LLMs对物理现象（如多普勒效应、多径效应、空间关系）的认知能力。  <br/>2. **开发物理基模拟器**：结合真实声源与可控物理信道，生成多样化的训练数据以解决数据稀缺问题。  <br/>3. **构建AQA-PHY数据集**：创建首个针对物理现象的音频问答数据集，覆盖多种真实场景任务。  <br/>4. **设计音频编码器**：首次同时处理音频的幅度与相位信息，提升对物理信号的建模精度。  <br/>5. **实验验证有效性**：在模拟与真实任务中（如视线检测、到达方向估计）取得合理结果，推动LLMs理解物理世界。  <br/><br/>**总结（100字以内）：**  <br/>本文提出ACORN框架，通过物理模拟和音频编码器增强LLMs对声学物理现象的理解，并构建专用数据集验证方法的有效性，为LLMs在物理场景中的应用奠定基础。|
|2506.08400v2|[mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text   Tasks](http://arxiv.org/abs/2506.08400v2)|总结：  <br/>本文提出mSTEB基准，评估LLMs在多语言多模态任务中的表现，揭示高资源与低资源语言间的性能差距，并呼吁更多资源投入以提升低资源语言覆盖。<br/><br/>贡献点：  <br/>1. **提出多语言多模态评估基准**：mSTEB是首个针对低资源语言的语音与文本任务统一评估框架，覆盖语言识别、文本分类、问答和翻译等多任务。  <br/>2. **系统性评估主流模型**：对Gemini 2.0 Flash、GPT-4o（Audio）等主流LLM及开源模型进行跨语言、跨模态的性能对比实验。  <br/>3. **揭示性能差距**：通过实验发现，高资源语言（如英语）与低资源语言（如非洲、美洲/大洋洲语言）在模型表现上存在显著差异，凸显语言覆盖不均问题。  <br/>4. **提出改进方向**：强调需加强低资源语言在LLMs中的训练与覆盖，以解决其在语音处理中的代表性不足问题。|
|2506.08400v1|[mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text   Tasks](http://arxiv.org/abs/2506.08400v1)|总结：  <br/>本文提出mSTEB基准，评估LLMs在语音与文本任务中的表现，揭示高资源与低资源语言间的显著差距，呼吁更多投入以提升后者在LLMs中的覆盖。<br/><br/>贡献点：  <br/>1. **构建首个多语言语音任务评估基准**：提出mSTEB，覆盖语言识别、文本分类、问答和翻译任务，填补低资源语言无统一评估标准的空白。  <br/>2. **多模态任务对比评估**：系统测试Gemini 2.0 Flash、GPT-4o (Audio)、Qwen 2 Audio和Gemma 3 27B等主流模型在语音和文本模态下的性能差异。  <br/>3. **揭示语言资源不均衡问题**：发现高资源语言与低资源语言（尤其非洲及美洲/大洋洲地区）在任务表现上存在显著差距，凸显欠发达语言的覆盖不足。  <br/>4. **提出针对性改进方向**：通过实证分析强调需加大资源投入，推动低资源语言在LLMs中的更广泛覆盖与性能提升。|
|2506.07520v2|[LeVo: High-Quality Song Generation with Multi-Preference Alignment](http://arxiv.org/abs/2506.07520v2)|总结：  <br/>本文提出LeVo框架，通过双标记建模与DPO优化解决音乐生成的复杂编排和数据稀缺问题，显著提升音质与音乐性，实验验证效果优越。<br/><br/>贡献点：  <br/>1. **提出LeVo框架**：结合语言模型（LeLM）和音乐编解码器，首次针对歌词到歌曲生成的复杂编排和数据稀缺问题进行系统性解决。  <br/>2. **双标记并行建模**：引入混合标记（融合声乐与伴奏）与双轨标记（分离声乐与伴奏），分别解决声乐-乐器和谐与高质量生成的挑战。  <br/>3. **解耦训练策略**：采用两个解码器-only transformer模型和模块化扩展训练，有效抑制不同标记类型间的干扰。  <br/>4. **多偏好对齐优化**：基于DPO设计半自动数据构建与后训练方法，提升指令遵循能力和音乐性，适配多样化的用户偏好。  <br/>5. **实验证明有效性**：在客观与主观指标上超越现有方法，并通过消融实验验证设计贡献，同时公开音频示例与代码。|
|2506.07520v1|[LeVo: High-Quality Song Generation with Multi-Preference Alignment](http://arxiv.org/abs/2506.07520v1)|总结:  <br/>本文提出LeVo框架，通过混合与双轨token建模提升音乐生成质量，结合多偏好对齐与双重解码器策略优化训练，显著改善音质、音乐性及人声伴奏和谐表现。<br/><br/>贡献点:  <br/>1. **LeVo框架创新**：首次集成语言模型（LeLM）与音乐编码器，解决音乐生成中复杂编曲与数据稀缺问题。  <br/>2. **双token建模机制**：引入混合token（整合人声与伴奏）与双轨token（分离编码），双重优化声乐与伴奏的和谐性与生成质量。  <br/>3. **双重解码器与模块化策略**：采用两个解码器-only Transformer及模块化扩展训练，有效减少不同token类型间的干扰。  <br/>4. **多偏好对齐方法**：基于DPO设计半自动数据构建与偏好对齐流程，提升指令遵循能力和音乐多样性。  <br/>5. **系统性验证**：通过消融实验与多维度评估（客观/主观指标）验证方法有效性，并提供公开音频示例。|
|2506.07323v1|[Speech Recognition on TV Series with Video-guided Post-Correction](http://arxiv.org/abs/2506.07323v1)|**总结（100字以内）：**  <br/>本研究提出结合视频与语言模型的多模态后修正框架，通过提取视频上下文信息提升ASR在复杂环境中的准确性，尤其在电视剧转录任务中显著改善转录效果。<br/><br/>---<br/><br/>**贡献点分点列出：**  <br/>1. **提出新框架**：设计针对复杂音频环境（如电视剧）的两阶段多模态后修正框架，解决重叠语音、领域术语和长程上下文依赖等问题。  <br/>2. **融合视频与语言模型**：创新性结合Video-Large Multimodal Model (VLMM)与Large Language Model (LLM)，通过视频上下文信息增强ASR的精炼能力。  <br/>3. **定制提示提取上下文**：引入Tailored Prompts优化VLMM，高效提取视频中的关键语境信息以辅助ASR错误修正。  <br/>4. **实证效果验证**：在TV系列ASR多模态基准上验证方法有效性，证明视频上下文显著提升转录准确率与鲁棒性。|
|2506.07081v2|[Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and   Label-Delayed Training](http://arxiv.org/abs/2506.07081v2)|总结：  <br/>该论文提出基于低比特率神经音频编码器的实时端点检测方法，通过标签延迟训练减少截断误差，并成功整合至语音大语言模型，显著提升效率与性能。<br/><br/>贡献点：  <br/>1. **提出新型特征**：采用流式、低比特率的Neural Audio Codec (NAC) 特征，替代传统频谱特征实现多轮对话的实时端点检测。  <br/>2. **创新训练方案**：设计新型标签延迟训练策略，有效降低截断误差。  <br/>3. **实验效果提升**：在固定160ms中位延迟下，单流端点器截断误差减少42.7%，双流配置减少37.5%。  <br/>4. **系统集成优化**：实现与编码器预训练语音大语言模型的高效整合，响应时间缩短1200ms，截断误差降低35%。|
|2506.07081v1|[Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and   Label-Delayed Training](http://arxiv.org/abs/2506.07081v1)|总结：  <br/>提出基于低比特率NAC流式特征的多轮对话实时端点检测方法，结合新颖的标签延迟训练显著降低截止错误，并优化与编码器预训练语音大语言模型的集成，提升响应效率。<br/><br/>贡献点：  <br/>1. **提出新型特征方法**：采用流式、低比特率Neural Audio Codec (NAC) 特征替代传统频谱特征，实现多轮对话的实时端点检测。  <br/>2. **创新训练策略**：设计标签延迟训练方案，进一步减少端点检测的截止错误。  <br/>3. **性能提升验证**：在固定160ms中位延迟下，单流端点器截止错误降低42.7%，双流配置降低37.5%。  <br/>4. **高效模型集成**：实现与基于编码器的预训练语音大语言模型的高效结合，响应时间缩短1200ms，截止错误减少35%。|
|2506.06862v1|[Multimodal Spatial Language Maps for Robot Navigation and Manipulation](http://arxiv.org/abs/2506.06862v1)|总结：  <br/>本文提出多模态空间语言地图（VLMaps和AVLMaps），通过融合多模态特征与3D重建实现环境感知与语言目标的精准关联，显著提升机器人在复杂环境中的导航和目标消歧能力。<br/><br/>贡献点：  <br/>1. **提出新颖的多模态空间语言地图框架**：首次将预训练多模态模型与3D环境重建结合，解决传统方法与环境映射脱节、缺乏几何精度的问题。  <br/>2. **构建自主生成的视觉-语言地图（VLMaps）**：通过标准探索生成开放词汇空间目标，支持自然语言指令的直接定位与跨机器人共享的障碍地图生成。  <br/>3. **扩展至音频-视觉-语言地图（AVLMaps）**：引入音频信息，实现统一的3D空间表示，增强多模态目标（文本、图像、音频）的消歧能力。  <br/>4. **实验证明零样本导航能力**：在仿真和现实场景中验证方法的有效性，提升50%召回率，并适用于移动机器人和桌面操作器。|
|2506.05688v2|[Voice Impression Control in Zero-Shot TTS](http://arxiv.org/abs/2506.05688v2)|**贡献点分点总结：**  <br/>1. 提出一种基于低维向量的零样本语音合成方法，可精确控制语音的para-/non-linguistic特性（如暗/明亮）。  <br/>2. 首次将大语言模型与语音印象生成结合，实现从自然语言描述直接生成目标印象向量。  <br/>3. 消除传统方法需手动优化的步骤，提升自动化水平与效率。  <br/>4. 通过客观指标与主观评估验证方法在语音印象控制上的显著有效性。  <br/>5. 提供公开的音频示例与演示页面，直观展示技术成果与应用效果。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种零样本语音合成方法，利用低维向量结合大语言模型实现语音印象的自动化控制，无需手动优化，通过实验验证其有效性，并提供公开音频示例以展示技术应用。|
|2506.05688v1|[Voice Impression Control in Zero-Shot TTS](http://arxiv.org/abs/2506.05688v1)|总结（100字以内）:  <br/>该研究提出一种基于低维向量的zero-shot TTS语音印象控制方法，通过大语言模型实现自然语言描述到目标声音特征的自动化生成，克服了传统需手动优化的局限，验证了在客观和主观评估中的有效性。<br/><br/>贡献点:  <br/>1. **提出零样本TTS语音印象控制框架**：首次在零样本TTS系统中引入对para-/non-linguistic信息（如dark-bright）的调控机制，解决声音特征控制的难点。  <br/>2. **低维向量表征声音印象对**：设计低维向量以量化不同声音印象（如暗/亮）的强度，提升模型对语音印象的操控效率与可解释性。  <br/>3. **结合大语言模型实现自动化生成**：通过大语言模型将自然语言描述转化为目标印象向量，替代传统手动优化流程，增强系统灵活性与实用性。  <br/>4. **验证方法有效性**：通过客观和主观实验验证了该方法对语音印象的精准控制能力，证明其在实际应用中的可行性与优势。|
|2506.02012v1|[Leveraging Large Language Models in Visual Speech Recognition: Model   Scaling, Context-Aware Decoding, and Iterative Polishing](http://arxiv.org/abs/2506.02012v1)|总结：  <br/>本论文提出三种方法提升视觉语音识别性能，包括LLM规模评估、上下文感知解码和迭代优化，验证了LLMs在VSR任务中的巨大潜力。<br/><br/>贡献点：  <br/>1. **规模效应验证**：系统研究LLM参数量对VSR性能的影响，首次在VSR任务中验证了模型规模与性能的缩放定律。  <br/>2. **上下文感知解码**：引入上下文文本引导LLM解码过程，显著提升语音识别准确率。  <br/>3. **迭代优化策略**：提出通过多轮迭代修正LLM输出，逐步降低识别错误率，实现更精准的转录结果。|
|2506.01845v1|[On-device Streaming Discrete Speech Units](http://arxiv.org/abs/2506.01845v1)|**贡献点总结（100字以内）**:  <br/>提出一种集成注意力窗口缩减和模型压缩的DSU方法，降低计算成本的同时保持语音单元有效性，并通过实验验证其在资源受限环境下的实时语音处理可行性。<br/><br/>**分点贡献**:<br/>1. **方法创新**：提出降低注意力窗口大小与模型参数规模的策略，首次实现DSUs在保持性能的同时的高效压缩。  <br/>2. **效率提升**：在ML-SUPERB 1h数据集上，将FLOPs降低50%，仅带来6.5%的CER上升，显著优化资源消耗。  <br/>3. **应用场景扩展**：证明DSUs适用于轻量级流式语音处理，为资源受限设备的实时语音交互提供可行性方案。|
|2506.00975v4|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v4)|**贡献点：**  <br/>1. 提出**Next-Token-Pair Prediction (NTPP)**，一种基于双通道语音数据的新型生成建模范式，首次用于解码器-only架构实现说话人无关的对话学习。  <br/>2. 系统性探索双通道语音数据在现代大语言模型中的应用，揭示其在捕捉对话结构与动态中的潜在优势。  <br/>3. 实验证明NTPP在**回合切换预测、响应连贯性、对话自然性**等关键指标上显著优于现有方法。  <br/>4. 实现**更低的推理延迟**，突出其在实时语音交互场景中的高效性与实用性。  <br/><br/>**总结：**  <br/>本文首次提出NTPP方法，利用双通道语音数据提升说话人无关对话能力，显著改善对话质量并降低延迟，为高效实时语音交互提供了新思路。|
|2506.00975v3|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v3)|**总结**（100字以内）：  <br/>本文提出NTPP新方法，首次采用解码器-only架构实现说话人独立的双通道语音对话学习，显著提升对话系统的轮流预测、连贯性与自然度，同时降低推理延迟，为实时语音交互提供高效解决方案。<br/><br/>**贡献点**：  <br/>1. **提出NTPP生成建模范式**：首次将"下一个词对预测"（Next-Token-Pair Prediction）引入语音领域，基于解码器-only架构实现说话人独立的双通道对话学习。  <br/>2. **有效利用双通道数据**：系统性探索双通道语音数据对对话建模的价值，充分挖掘对话结构与动态信息。  <br/>3. **提升对话表现能力**：在标准基准测试中，NTPP显著优于现有方法，改善对话系统的轮流预测、响应连贯性和自然度。  <br/>4. **优化推理效率**：实现比传统方法更低的推理延迟，为实时语音交互应用提供更高效的模型支持。|
|2506.00267v3|[CASPER: A Large Scale Spontaneous Speech Dataset](http://arxiv.org/abs/2506.00267v3)|**贡献点：**  <br/>1. 提出了一种新型自发语音数据收集管道，实现自然对话的激发与记录。  <br/>2. 发布了首个包含100+小时高质量自发语音数据的公开语料库。  <br/>3. 通过鼓励多样化主题与真实交互，提升数据的语境多样性与真实性。  <br/>4. 设计可复现的数据采集框架，推动领域研究标准化与可重复性。  <br/>5. 规划数据集的持续扩展，形成长期可用的研究资源。  <br/><br/>**总结（100字以内）：**  <br/>本文提出新型自发语音数据收集方法，构建并发布大规模自然对话语料库，填补现有数据不足，优化交互真实性与多样性，为语音研究提供可复现框架及持续扩展的资源。|
|2506.00267v2|[CASPER: A Large Scale Spontaneous Speech Dataset](http://arxiv.org/abs/2506.00267v2)|**总结（100字以内）**：  <br/>提出自然对话数据收集新流程，发布100+小时自发语音数据集，支持真实互动与多样化话题，提供可复制框架，并计划持续扩展数据集以促进研究社区发展。<br/><br/>---<br/><br/>**贡献点分点**：  <br/>1. **提出新数据收集方法**：设计可激发自然对话的流程，突破传统剧本对话数据局限。  <br/>2. **构建大规模自发语音数据集**：提供100+小时高质量自发语音数据，填补领域数据缺口。  <br/>3. **促进真实互动与多样性**：通过鼓励开放话题和互动交流，增强数据的实用性和真实性。  <br/>4. **提供可复现框架**：建立标准化流程，支持未来研究的重复实验与数据扩展。  <br/>5. **开放持续扩展计划**：承诺未来数据集的更新与扩展，形成长期可用的研究资源。|
|2506.00267v1|[CASPER: A Large Scale Spontaneous Speech Dataset](http://arxiv.org/abs/2506.00267v1)|**贡献点：**<br/>1. **提出新型数据采集流程**：设计并实现了一种能够激发自然对话、鼓励多样化话题和互动交流的记录方法，突破传统剧本对话数据的局限性。<br/>2. **发布大规模自发语音数据集**：公开了包含200+小时真实自发语音的Stage 1数据集，为语音处理研究提供高质量基准数据。<br/>3. **构建可复现研究框架**：通过框架化设计，为后续自发性语音数据收集提供标准化和可重复的解决方案，推动领域发展。|
|2505.22995v1|[LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable   Data for Custom Keyword Spotting](http://arxiv.org/abs/2505.22995v1)|**贡献点**：<br/>1. 提出利用大语言模型（LLMs）生成混淆关键词音频，并通过文本到语音（TTS）引擎合成多样化的说话风格，增强混淆场景下的训练数据。<br/>2. 设计新型北星指标c-AUC，基于混淆关键词组的平均DET曲线面积，更精准地衡量用户在混淆场景下的体验。<br/>3. 实现零人工成本和高可扩展性的训练增强策略，提升语音命令测试集的AUC和c-AUC性能（分别提升3.7%和11.3%）。<br/>4. 针对传统对比训练在混淆关键词识别中的局限性，提出通过语义生成和语音合成构建更真实的对抗样本。<br/>5. 验证了混淆关键词组的训练策略对模型鲁棒性和识别准确性的显著提升效果。<br/><br/>**总结**：该论文提出基于LLMs生成混淆关键词并结合TTS合成多样风格的方法，设计了c-AUC新指标，有效提升了KWS性能。|
|2505.22943v1|[Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of   Pre-trained Multimodal Representation via Text Updates](http://arxiv.org/abs/2505.22943v1)|总结：  <br/>本文提出MAC基准测试，利用LLMs生成欺骗性文本暴露多模态模型组合漏洞，并通过自训练方法提升攻击效果与多样性，验证了小模型在跨模态评估中的有效性。<br/><br/>贡献点：  <br/>1. **提出Multimodal Adversarial Compositionality（MAC）基准测试**  <br/>   - 通过大语言模型（LLMs）生成欺骗性文本样本，系统评估多模态模型在图像、视频、音频等不同模态中的组合漏洞。  <br/>   - 引入样本级攻击成功率和群体级熵评估指标，量化漏洞的多样性和攻击效果。<br/><br/>2. **改进零样本方法的自训练框架**  <br/>   - 设计基于拒绝采样微调的自训练策略，结合多样性促进过滤机制，同时提升攻击成功率和样本多样性。  <br/>   - 有效解决传统方法在跨模态攻击中稳定性不足的问题。<br/><br/>3. **验证小语言模型的高效性**  <br/>   - 采用轻量级语言模型（如Llama-3.1-8B）实现更优的漏洞检测能力，表明小模型在跨模态任务中具有成本效益和实用性。  <br/>   - 展示了该方法在多种多模态表示（图像、视频、音频）上的广泛适用性。|
|2505.22063v1|[Weakly Supervised Data Refinement and Flexible Sequence Compression for   Efficient Thai LLM-based ASR](http://arxiv.org/abs/2505.22063v1)|**贡献点（分点）:**  <br/>1. 提出**EThai-ASR**系统，首次将大型语言模型（LLMs）应用于泰语自动语音识别（ASR），构建高效且完整的LLM-based ASR框架。  <br/>2. 设计**自进化数据精炼策略**，通过优化弱标签提升语音编码器质量，解决低资源场景下的数据稀缺问题。  <br/>3. 引入**可插拔的序列压缩模块**，包含三种模式以减少序列长度，显著降低计算需求同时保持性能。  <br/>4. 在多语种数据集上实现**SOTA精度**，并通过公开精炼文本转录数据促进后续研究。  <br/><br/>**总结（100字以内）:**  <br/>本文提出EThai-ASR，首次将LLMs应用于泰语ASR，通过数据精炼和序列压缩模块缓解低资源问题并提升效率，实验验证其性能领先，同时开放数据以推动研究。|
|2505.21148v1|[Assessment of L2 Oral Proficiency using Speech Large Language Models](http://arxiv.org/abs/2505.21148v1)|**贡献点**  <br/>1. **提出多模态LLM作为L2口试评分新范式**：首次将多模态大语言模型应用于L2英语口语评估，突破传统级联系统和端到端模型的信息丢失与性能局限。  <br/>2. **训练策略对比与优化**：系统比较基于回归与分类目标的训练方法，验证语音LLM在不同任务设置下的有效性。  <br/>3. **性能提升**：在两个标准数据集上实现优于现有最佳基线模型的评估效果。  <br/>4. **泛化能力验证**：通过跨部分/跨任务测试，证明预训练阶段获取的语音理解知识可显著提升模型在未见过场景中的适应性。  <br/><br/>**总结**（100字以内）：  <br/>本文提出多模态LLM用于L2口语评分，对比回归/分类训练策略，实验证明其性能优于传统方法，并展现跨任务泛化能力，为自动语音评估提供了新思路。|
|2505.21138v2|[Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis](http://arxiv.org/abs/2505.21138v2)|总结：  <br/>提出基于自监督预训练与LLM的中文方言ASR方法，在大规模未标记数据和监督数据训练下取得SOTA性能，并开源以推动研究。<br/><br/>贡献点：  <br/>1. **方法创新**：首次将自监督预训练（Data2vec2）与大语言模型（LLM）结合，用于中文方言和口音识别的低资源场景优化。  <br/>2. **数据规模**：构建300,000小时未标记方言/口音数据的预训练语料，叠加40,000小时监督数据进行对齐训练，提升模型泛化能力。  <br/>3. **系统评估**：系统分析不同投影器与LLM对普通话、方言、口音识别性能的影响，揭示多模态融合的关键性。  <br/>4. **性能突破**：在Kespeech等中文方言数据集上取得SOTA结果，验证该范式在方言识别中的有效性。  <br/>5. **开源贡献**：开放源代码与模型，促进可复现研究及社区协作，推动语音领域方言处理技术发展。|
|2505.21138v1|[Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis](http://arxiv.org/abs/2505.21138v1)|总结：  <br/>该研究提出基于自监督学习和大型语言模型的框架，解决中文方言识别的低资源问题，实现多数据集SOTA性能，并开放源码促进研究可重复性。<br/><br/>贡献点：  <br/>1. **提出应用自监督预训练与LLM的跨语言范式**：首次将Data2vec2模型与LLM结合应用于中文方言识别，为低资源场景提供新思路。  <br/>2. **构建大规模未标注方言及口音数据集**：基于30万小时无标签方言和口音语音数据进行预训练，提升模型对语音特征的建模能力。  <br/>3. **设计对齐训练策略**：在40万小时监督数据集上进行对齐训练，优化方言与标准汉语之间的表征关联。  <br/>4. **系统评估不同组件影响**：分析不同投影器和LLM对多语言（普通话、方言、口音）识别性能的作用机制。  <br/>5. **实现SOTA结果**：在Kespeech等多方言数据集上取得当前最佳性能，验证方法有效性。  <br/>6. **开放源码推动研究**：开源代码与模型，促进学术界对方言ASR的可复现研究与技术共享。|
|2505.20770v2|[Can Large Language Models Predict Audio Effects Parameters from Natural   Language?](http://arxiv.org/abs/2505.20770v2)|**贡献点分点总结：**<br/>1. 提出LLM2Fx框架：无需任务专用训练或微调，直接通过自然语言生成音频效果（Fx）参数。  <br/>2. 解决Text2Fx任务：将自然语言描述映射到均衡和混响等具体效果参数。  <br/>3. 引入三种上下文示例：包括DSP特征、DSP代码和少量示例，提升模型性能。  <br/>4. 零样本生成能力：展示LLM在零样本场景下能揭示音色语义与音频效果的关系。  <br/>5. 推动音乐生产工具发展：实现文本驱动的音频界面，提升工具的可用性与直观性。  <br/><br/>**总结（100字以内）：**  <br/>LLM2Fx通过自然语言生成音频效果参数，无需专用训练，引入三种上下文示例提升性能，为音乐制作提供更直观的文本驱动工具。|
|2505.20770v1|[Can Large Language Models Predict Audio Effects Parameters from Natural   Language?](http://arxiv.org/abs/2505.20770v1)|贡献点总结（100字以内）:  <br/>LLM2Fx首次实现零样本文本到音频效果参数生成，引入DSP特征、代码及少量样本示例提升性能，在音乐制作中提供自然语言驱动的直观接口。<br/><br/>分点贡献:<br/>1. **无需任务特定训练**：直接利用LLMs生成音频效果（如均衡、混响）参数，突破传统需要微调的局限。  <br/>2. **Text2Fx任务解决**：创新性地建立自然语言描述与音频效果参数的映射关系，揭示音色语义与效果的关联。  <br/>3. **上下文增强机制**：提出三种类型上下文示例（DSP特征、代码、少量样本）提升模型性能与生成准确性。  <br/>4. **性能超越传统方法**：实验验证LLM生成效果参数优于现有优化方法，具有竞争力。  <br/>5. **应用创新**：开创文本驱动的音频生产界面，推动音乐制作工具向更直观、易用方向发展。|
|2505.20638v1|[Music's Multimodal Complexity in AVQA: Why We Need More than General   Multimodal LLMs](http://arxiv.org/abs/2505.20638v1)|**贡献点：**<br/>1. 指出音乐领域需定制化多模态处理方法，而非通用大语言模型  <br/>2. 系统分析Music AVQA数据集与方法，揭示该领域的独特挑战  <br/>3. 提出三类关键要素：专用输入处理、时空架构设计、音乐特定建模策略  <br/>4. 总结有效设计模式，并为整合音乐先验知识提出未来研究方向  <br/>5. 建立多模态音乐理解的理论基础，提供开源研究资源支持  <br/><br/>**总结：**  <br/>本研究聚焦音乐领域多模态处理，提出定制化方法的关键要素，为音乐AVQA研究提供理论指导与实践方向。|
|2505.20491v1|[In-context learning capabilities of Large Language Models to detect   suicide risk among adolescents from speech transcripts](http://arxiv.org/abs/2505.20491v1)|总结：  <br/>该研究提出基于大语言模型和系统提示工程的语音分析方法，解决了青少年自杀风险检测的可扩展性难题，并在SpeechWellness Challenge中取得优异性能，验证了LLMs在心理健康评估中的潜力。<br/><br/>贡献点：  <br/>1. **提出系统性方法应对挑战**：针对SpeechWellness Challenge（SW1）设计基于转录文本的自动 Suicide Risk 评估框架，解决语音匿名化下的实际应用难题。  <br/>2. **创新性迁移LLM模型**：将大语言模型（LLMs）应用于语言特征分类，突破传统语音分析方法的限制，首次在中文青少年语料中实现高效风险识别。  <br/>3. **优化提示工程技术**：通过DSPy框架进行系统性提示设计，开发出优于传统微调的上下文学习方法，显著提升分类性能。  <br/>4. **量化提示效果机制**：通过消融实验验证提示样本数量对性能的提升作用（p=0.003），揭示不同模型规模与类型的效果差异。  <br/>5. **验证LLMs应用价值**：在实际比赛中取得第三、第四名的优秀成绩（0.68准确率，F1=0.7），体现LLMs在心理健康领域的重要应用潜力。|
|2505.20237v2|[Efficient Speech Translation through Model Compression and Knowledge   Distillation](http://arxiv.org/abs/2505.20237v2)|**贡献点：**  <br/>1. 提出结合**迭代层剪枝**、**低秩适配（QLoRA）**与**知识蒸馏**的综合模型压缩方法，用于高效部署大音频-语言模型。  <br/>2. 在**语音翻译任务**中实现高达50%的模型参数与存储空间减少，同时保持97-100%的翻译质量。  <br/>3. 验证了压缩模型在**德语和中文翻译**中的有效性，展示了压缩技术的跨语言适用性。  <br/>4. 通过**系统提交至IWSLT 2025模型压缩赛道**，推动了语音领域大模型的实用化研究。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出综合模型压缩方案，通过迭代剪枝、QLoRA及知识蒸馏显著降低语音翻译模型的参数和存储需求，保持高翻译质量，并在IWSLT 2025竞赛中验证其有效性。|
|2505.20166v2|[From Alignment to Advancement: Bootstrapping Audio-Language Alignment   with Synthetic Data](http://arxiv.org/abs/2505.20166v2)|总结：  <br/>本文提出BALSa框架，通过合成数据生成解决音频模型的灾难性遗忘与资源消耗问题，提升多音频场景下的语音-语言对齐能力。<br/><br/>贡献点：  <br/>1. 提出对比学习风格的合成数据生成框架，有效缓解音频训练导致的文本能力退化和幻觉问题。  <br/>2. 实现跨模态对齐优化，减少对海量标注数据的依赖，提升音频-语言理解效率。  <br/>3. 扩展至多音频场景，支持音频差异解释与统一描述生成，增强模型多模态处理能力。  <br/>4. 实验验证该方法在音频理解、推理及指令遵循任务中均表现出优异性能，且具备高效可扩展性。|
|2505.20166v1|[From Alignment to Advancement: Bootstrapping Audio-Language Alignment   with Synthetic Data](http://arxiv.org/abs/2505.20166v1)|总结：  <br/>该论文提出BALSa方法，通过合成数据缓解ALLMs的灾难性遗忘和跨模态对齐问题，并引入LISTEN和多音频训练策略，显著提升音频理解与多模态对齐能力，实现高效、可扩展的音频-语言模型开发。<br/><br/>贡献点：  <br/>1. **提出BALSa方法**：利用基础LLM生成通用caption-style对齐数据，解决音频任务训练导致的文本能力流失问题。  <br/>2. **设计LISTEN训练机制**：通过生成扩展负样本，增强ALLMs区分音频存在与缺失的能力，减少声音幻觉。  <br/>3. **扩展至多音频场景**：使模型能处理多音频输入（如描述差异或生成统一caption），提升跨模态对齐效率。  <br/>4. **实验验证有效性**：证明方法在音频理解、推理和指令遵循任务中保持高性能，同时降低对特定数据集的依赖。  <br/>5. **提出高效开发路径**：为ALLMs构建提供可扩展、低成本的训练框架，推动音频-语言模型的实用性发展。|
|2505.19937v2|[ALAS: Measuring Latent Speech-Text Alignment For Spoken Language   Understanding In Multimodal LLMs](http://arxiv.org/abs/2505.19937v2)|总结（100字以内）:  <br/>本研究提出ALAS，填补了语音理解中音频-文本对齐评估的空白，通过跨transformer层的相关性分析实现有效度量，并在问答与情感识别任务中验证其跨任务适应性。<br/><br/>贡献点:  <br/>1. **提出新评估指标**：设计ALAS，首次系统评估音频与文本表示的对齐质量，解决现有融合方法缺乏统一度量的问题。  <br/>2. **跨层分析机制**：通过测量不同transformer层中音频-文本表示的相关性，捕捉多尺度对齐特征，揭示模型内部对齐动态。  <br/>3. **任务泛化验证**：在口语问答与情感识别任务中验证ALAS的实用性，证明其跨任务有效性及对模型性能的指导意义。|
|2505.16211v2|[AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models](http://arxiv.org/abs/2505.16211v2)|**贡献点总结（100字以内）：**  <br/>提出AudioTrust，首个针对音频大语言模型的多维度信任度评估框架，涵盖公平性、幻觉、安全、隐私、鲁棒性和认证六大维度，设计18个实验场景与9个专用评估指标，构建4420个真实场景音频文本数据集，并开发自动化评分系统，揭示现有模型的信任度边界，为安全部署提供依据。  <br/><br/>**分点贡献：**  <br/>1. **首个专用评估框架**：AudioTrust是首个系统性评估音频大语言模型（ALLMs）信任度的框架，填补了音频模态安全研究的空白。  <br/>2. **多维度覆盖**：全面评估公平性、幻觉、安全、隐私、鲁棒性和认证六大关键维度，突破传统文本模态评估的局限。  <br/>3. **真实场景数据集**：构建包含4420个音频/文本样本的高质量数据集，源自日常对话、应急呼叫等现实场景，针对性测试模型能力。  <br/>4. **专用评估指标**：设计9个与音频特性紧密相关的评估指标，增强评价的准确性和适用性。  <br/>5. **自动化评分系统**：开发大规模自动化流水线，实现客观、可扩展的模型输出评分，提升评估效率。  <br/>6. **风险分析与指导**：通过实验揭示当前主流ALLMs在高风险场景下的信任度边界，为后续安全应用提供理论支持。|
|2505.14518v2|[Teaching Audio-Aware Large Language Models What Does Not Hear:   Mitigating Hallucinations through Synthesized Negative Samples](http://arxiv.org/abs/2505.14518v2)|总结：  <br/>提出LISTEN方法，通过合成数据与轻量级适配器解决ALLMs声音幻觉问题，提升可靠性与效率，保持音频任务性能。<br/><br/>贡献点：  <br/>1. **原创方法**：提出基于合成数据的对比学习框架LISTEN，通过扩展负样本增强模型对真实与不存在声音的区分能力。  <br/>2. **无需参数修改**：在不调整LLM核心参数的前提下，通过轻量级适配器整合音频信息，保持模型结构原生性。  <br/>3. **有效性验证**：在音频问答与推理基准测试中，显著减少幻觉现象且性能表现优异。  <br/>4. **高效性优势**：在数据与计算资源消耗上均优于现有方法，提升实际应用可行性。|
|2505.13880v3|[U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding](http://arxiv.org/abs/2505.13880v3)|总结:  <br/>本研究提出U-SAM模型，通过集成专用编码器与大语言模型、引入MoE特征融合机制和语义感知对比损失模块，显著提升跨模态对齐效果和音频任务泛化能力。<br/><br/>贡献点:  <br/>1. **多模态统一框架设计**：首次将语音、音频事件、音乐的专用编码器与预训练大语言模型（LLM）结合，实现跨音频类型的一体化理解。  <br/>2. **任务感知特征融合**：提出基于Mixture of Experts（MoE）的动态路由投影器，根据不同任务需求高效整合领域专属编码器输出。  <br/>3. **语义对比损失优化**：设计结合语言监督的语义-感知对比损失模块，主动识别并修正冗余音频特征的语义与频谱表示，增强跨模态对齐。  <br/>4. **强大泛化能力验证**：在多种基准测试中超越现有模型，并展现对未见任务的涌现能力，证明其通用性与鲁棒性。  <br/>5. **开源实现支持**：提供代码公开，便于复现与进一步研究（GitHub链接）。|
|2505.13577v2|[VocalAgent: Large Language Models for Vocal Health Diagnostics with   Safety-Aware Evaluation](http://arxiv.org/abs/2505.13577v2)|总结：  <br/>本文提出VocalAgent音频大语言模型，通过医院真实数据训练和多维度评估框架（含安全评估、跨语言分析、模态消融）提升语音疾病分类准确性，强调伦理验证与可扩展性，为声障诊断提供便捷解决方案。<br/><br/>贡献点：  <br/>1. **提出VocalAgent模型**：开发首个专门针对语音健康诊断的音频大语言模型（LLM），通过微调Qwen-Audio-Chat解决声障诊断可及性问题。  <br/>2. **多维度评估框架**：构建包含安全评估（减少诊断偏见）、跨语言性能分析和模态消融实验的综合评估体系，确保模型可靠性。  <br/>3. **性能提升**：在语音疾病分类任务中超越现有基线模型，验证其高准确率和有效性。  <br/>4. **可扩展性与伦理考量**：强调LLM方法的可扩展性，推动健康诊断技术普及，并突出伦理和技术双重验证的重要性。|
|2505.13062v3|[Hearing from Silence: Reasoning Audio Descriptions from Silent Videos   via Vision-Language Model](http://arxiv.org/abs/2505.13062v3)|总结：  <br/>该论文提出SVAD任务，构建CoT-AudioCaps数据集并设计思维链监督微调策略，有效提升VLMs在无声视频到音频推理及视频到音频生成中的模态不匹配处理能力。<br/><br/>贡献点：  <br/>1. **提出新任务**：定义"Reasoning Audio Descriptions from Silent Videos (SVAD)"任务，填补多模态模型在模态不匹配场景下音频推理研究的空白。  <br/>2. **构建专用数据集**：创建包含思维链引导的CoT-AudioCaps数据集，为SVAD任务提供高质量训练与评估资源。  <br/>3. **创新训练策略**：提出基于思维链（Chain-of-Thought）的监督微调方法，增强模型对目标模态的推理能力。  <br/>4. **验证方法有效性**：通过实验证明所提方法在SVAD任务中显著提升模态不匹配推理效果，并有效改进VT2A任务的音频描述生成能力。|
|2505.11079v2|[ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for   Audio Deepfake Detection](http://arxiv.org/abs/2505.11079v2)|总结：  <br/>本研究提出ALLM4ADD框架，通过将音频深度伪造检测转化为问答任务，并结合监督微调提升性能，在数据稀缺场景下实现优于现有方法的检测效果，为语音领域提供新思路。<br/><br/>贡献点：  <br/>1. **首次系统评估**：对音频大语言模型（ALLMs）在音频深度伪造检测（ADD）任务中的零样本表现进行全面评估，揭示其局限性。  <br/>2. **框架创新**：提出ALLM4ADD框架，通过ALLM驱动的范式解决ADD问题，开创性地将检测任务转化为问答形式。  <br/>3. **监督微调方法**：设计基于监督微调的训练策略，提升ALLM对音频真实性的判断能力。  <br/>4. **数据稀缺优势**：在数据稀缺场景下验证了所提方法的优越性能，填补了现有研究的空白。  <br/>5. **开源实现**：开放实验代码，推动社区进一步研究和应用ALLMs在ADD领域的潜力。|