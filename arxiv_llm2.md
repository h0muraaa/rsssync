|Source|Title|Summary|
|---|---|---|
|2506.23325v1|[XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate   Speech Codecs](http://arxiv.org/abs/2506.23325v1)|总结：  <br/>本研究提出XY-Tokenizer，通过多阶段多任务学习平衡语音编解码器的语义和声学能力，在相似码率下实现与SOTA编解码器相当的性能，并开源代码促进研究复现。<br/><br/>贡献点：  <br/>1. **提出XY-Tokenizer新编解码器**：设计了一种新型语音编解码器，兼顾声学信息保真与语义信息捕捉的双重需求。  <br/>2. **多阶段多任务学习框架**：创新性地采用分阶段、多任务训练策略，有效缓解语义与声学能力之间的冲突。  <br/>3. **语义与声学性能平衡**：在保持较高说话人相似度（0.83）的同时，文本对齐效果优于SpeechTokenizer等基于蒸馏的语义模型方法。  <br/>4. **与纯声学SOTA模型竞争力对比**：重建性能接近当前声学编解码器SOTA（BigCodec，0.84相似度），且同时支持语义建模。  <br/>5. **开源实现**：提供代码和模型，便于社区验证与进一步研究。|
|2506.22846v1|[Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](http://arxiv.org/abs/2506.22846v1)|**Summary** (in Chinese):  <br/>提出语言感知中间损失（LAIL）框架，将大语言模型的语言知识与CTC目标结合，提升语音识别性能同时保持高效推理，实验证明在多个数据集上显著降低词错误率（WER）并达到CTC方法的最先进水平。<br/><br/>**Contribution Points** (in English):  <br/>1. **Proposed a novel auxiliary loss framework (Language-Aware Intermediate Loss, LAIL)** to integrate linguistic knowledge from large language models (LLMs) into CTC-based automatic speech recognition (ASR).  <br/>2. **Introduced connector layers** attached to intermediate encoder layers to map encoder outputs to LLMs' embedding space and compute causal language modeling loss during training.  <br/>3. **Demonstrated significant performance improvements** in Word Error Rate (WER) on LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art results for CTC-based ASR systems with minimal computational overhead.  <br/>4. **Preserved the computational efficiency of CTC decoding** while enhancing linguistic dependency modeling through the auxiliary loss design.|
|2506.21613v1|[ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate   Speech](http://arxiv.org/abs/2506.21613v1)|1. **提出首个专门针对儿童的仇恨言论数据集**：构建ChildGuard1，通过整合现有语料库并添加儿童特定标注，填补了儿童针对性仇恨言论研究领域缺乏专用数据集的空白。  <br/>2. **涵盖多年龄段和多样化语境**：数据集覆盖不同年龄群体，全面记录儿童针对性仇恨言论的多种场景和语境，提升对复杂情境的捕捉能力。  <br/>3. **评估现有方法在儿童场景中的表现**：系统验证了当前主流的仇恨言论检测模型（包括大语言模型）在儿童针对性内容识别和语境理解上的有效性与局限性。  <br/>4. **公开数据集以促进研究**：向学术界公开ChildGuard1，为后续开发更精准的儿童针对性仇恨言论检测与干预技术提供基准和资源。  <br/>5. **强调儿童情感影响的特殊性**：首次在数据集中明确标注仇恨言论对儿童群体的情感冲击，推动针对儿童的伦理与社会影响研究。|
|2506.21448v2|[ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing](http://arxiv.org/abs/2506.21448v2)|贡献点：  <br/>1. **提出ThinkSound框架**：首次将Chain-of-Thought（CoT）推理引入视频到音频生成，通过分步交互式流程实现高质量音频生成与编辑。  <br/>2. **三阶段方法设计**：  <br/>   - 基础音效生成：构建语义连贯的环境音景；  <br/>   - 交互式物体中心优化：通过用户精准操作实现细节调整；  <br/>   - 自然语言引导编辑：支持语义化的音频内容修改。  <br/>3. **构建AudioCoT数据集**：提供结构化推理标注，建立视觉内容、文本描述与声音合成之间的关联，推动跨模态研究。  <br/>4. **实验验证有效性**：在音频质量指标和CoT指标均达到SOTA水平，且优于Movie Gen Audio基准测试。<br/><br/>总结：  <br/>本文提出ThinkSound框架，通过CoT推理分阶段实现视频音频生成与编辑，并构建AudioCoT数据集提升跨模态研究。实验表明其在生成质量和推理一致性上均达到当前最优，显著优于现有基准。|
|2506.21448v1|[ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing](http://arxiv.org/abs/2506.21448v1)|总结：  <br/>该论文提出ThinkSound框架，通过多阶段CoT推理实现高保真视频到音频生成，引入AudioCoT数据集，兼具语音生成与多模态理解，实验验证其在音频质量和推理任务上的SOTA表现。<br/><br/>贡献点：  <br/>1. **提出ThinkSound框架**：首次将Chain-of-Thought（CoT）推理引入视频到音频生成，通过分阶段（基础 Foley、对象中心交互精修、自然语言引导编辑）实现音频生成与编辑的协同。  <br/>2. **构建AudioCoT数据集**：提供结构化推理标注，建立视觉、文本与声音合成的关联，支持多模态学习与评估。  <br/>3. **统一多模态模型架构**：在生成各阶段整合多模态大语言模型与音频基础模型，实现上下文对齐的推理引导。  <br/>4. **验证跨模态性能**：在音频质量指标及CoT推理任务中达到SOTA，并在Movie Gen Audio基准测试中表现出色。|
|2506.21191v1|[Prompt-Guided Turn-Taking Prediction](http://arxiv.org/abs/2506.21191v1)|**贡献点：**  <br/>1. **提出动态控制的turn-taking预测模型**：首次通过文本提示（如"faster"或"calmer"）实现对对话行为的实时、动态调控，增强系统适应对话伙伴和情境的能力。  <br/>2. **创新性融合提示嵌入机制**：将文本提示嵌入至通道内Transformer和跨通道Transformer，实现上下文感知与多通道信息协同优化。  <br/>3. **构建合成数据生成方案**：利用大语言模型（LLM）生成符合实际场景的文本提示数据，填补现有数据集的空白。  <br/>4. **验证模型有效性**：在950小时真实对话数据上实验，证明模型能显著提升预测准确性并灵活调整对话节奏行为。  <br/><br/>**总结（100字以内）：**  <br/>本文提出通过文本提示动态控制turn-taking预测的模型，结合LLM生成合成数据，验证了其提升预测精度和适应对话行为的能力，为对话系统提供更灵活的交互调控方法。|
|2506.18510v1|[Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich   Transcripts](http://arxiv.org/abs/2506.18510v1)|总结：  <br/>提出基于大语言模型的语音口吃检测方法，结合声学特征与文本输入，利用时间戳线索实现鲁棒的不完美信息处理，推动更包容的语音技术发展。<br/><br/>贡献点：  <br/>1. **新型口吃转录方法**：首次将语音中的口吃作为显式标记（如重复、停顿、自我修正）进行转录，并附加精确的时间戳，生成结构化的、富有口吃信息的语音转录。  <br/>2. **多模态输入融合**：创新性整合声学表示（来自音频编码器）与文本输入（包含不完美信息，如无口吃转录、对齐转录或音素级ASR输出），提升鲁棒性。  <br/>3. **鲁棒性验证**：实验证明LLMs在文本输入存在缺陷的情况下，仍能通过时间戳相关线索有效生成完整的口吃注释，突破传统对输入质量的依赖。  <br/>4. **应用场景拓展**：为开发更包容的语音技术（如处理口吃、改善语言模型对口语的适应性）提供方法支持，推动实际应用中的语音处理能力提升。|
|2506.17611v1|[OpusLM: A Family of Open Unified Speech Language Models](http://arxiv.org/abs/2506.17611v1)|**贡献点总结**（100字以内）:  <br/>提出7B参数开源SpeechLM家族OpusLMs，结合语音-文本对与大规模文本数据训练，展示语音识别、合成及纯文本任务的性能提升，并通过模型结构设计与训练策略优化验证扩展性，实现完全透明的开放研究。<br/><br/>**分点贡献**:  <br/>1. **开源大模型**：首次发布7B参数规模的开放SpeechLM家族OpusLMs，基于公开材料构建。  <br/>2. **多模态预训练**：采用213K小时语音-文本对及292B文本-only token进行持续训练，增强跨模态理解能力。  <br/>3. **性能提升**：在语音识别、合成及纯文本任务中达到或超越现有SpeechLMs的性能。  <br/>4. **结构设计**：提出tokenization、多流语言模型及多阶段训练策略，优化模型架构。  <br/>5. **扩展性验证**：实验证明模型规模扩展与数据选择退火对性能的关键影响。  <br/>6. **开放研究支持**：提供代码、数据、检查点及训练日志，推动SpeechLM领域开放协作。|
|2506.15556v1|[PredGen: Accelerated Inference of Large Language Models through   Input-Time Speculation for Real-Time Speech Interaction](http://arxiv.org/abs/2506.15556v1)|总结：  <br/>本文提出PredGen框架，通过推测解码在用户输入过程中生成候选响应，显著降低实时语音聊天中的延迟，同时保持计算成本可控，适用于消费级硬件。<br/><br/>贡献点：  <br/>1. **识别延迟关键因素**：明确指出LLM生成首句的时间是延迟的主要瓶颈，为优化提供理论依据。  <br/>2. **提出PredGen框架**：创新性引入推测解码技术，在用户输入时并行生成候选响应，实现TTS提前启动。  <br/>3. **实验证明有效性**：在多个数据集上验证，延迟降低约2倍，且额外计算成本极低，有效利用闲置资源。|
|2506.15220v1|[video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](http://arxiv.org/abs/2506.15220v1)|**贡献点总结：**  <br/>1. 提出video-SALMONN 2，结合LoRA与DPO优化视频-音频联合描述生成。  <br/>2. 设计新评估指标，量化视频描述的完整性和准确性。  <br/>3. 引入多轮DPO（MrDPO）方法，通过周期性更新参考模型、LoRA模块合并重置及真实字幕指导提升训练稳定性。  <br/>4. 实验显示MrDPO将描述错误率降低28%，且70亿参数模型超越GPT-4o和Gemini-1.5-Pro，保持与SOTA相近的性能。  <br/><br/>（99字）|
|2506.15154v1|[SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](http://arxiv.org/abs/2506.15154v1)|总结（100字以内）:  <br/>本研究提出SonicVerse多任务音乐描述生成模型，通过投影架构融合音频特征检测与文本生成，结合大语言模型实现长音频时序描述，创新性地提升音乐描述的准确性和细节水平。<br/><br/>贡献点：<br/>1. **多任务集成框架**：首次将音乐描述生成与关键特征检测（调识别、人声检测等）结合，统一模型架构以同步捕捉低层音频细节和高层音乐属性。<br/>2. **投影式架构设计**：创新采用音频到语言标记的投影机制，通过专用辅助头实现特征检测，并将特征结果进一步投射为语言标记以增强描述输入。<br/>3. **时间信息生成能力**：利用大语言模型链接多任务输出，支持对长音频片段生成详尽的时间戳描述，突破传统模型在长时序描述上的局限。<br/>4. **数据集扩展与标注**：基于MusicBench扩展出包含音频、描述和多维特征的混合数据集，通过MIRFLEX模块化工具实现高效特征提取与标注。<br/>5. **特征增强效果验证**：实验证明融合音乐特征显著提升描述生成质量，推进音乐AI在内容理解与数据库构建中的应用。|
|2506.14767v1|[A Variational Framework for Improving Naturalness in Generative Spoken   Language Models](http://arxiv.org/abs/2506.14767v1)|**总结**：  <br/>提出端到端变分方法，自动学习编码连续语音属性以提升语义token，无需手动特征工程，并通过人类评分验证生成语音的自然度与偏好性，同时开源代码与模型。<br/><br/>**贡献点**：  <br/>1. **首次提出端到端变分学习方法**：通过自动编码连续语音属性（如韵律信息），替代传统需人工提取的paralinguistic特征，突破了现有Speech Tokenizer的局限。  <br/>2. **无需手动特征工程**：消除了对音高、语速等特征的手动选择与设计，简化模型训练流程并增强泛化能力。  <br/>3. **提升语音生成自然度**：通过编码连续属性，增强语音的连贯性与语境适应性，解决现有模型生成语音缺乏自然性的缺陷。  <br/>4. **基于人类评价验证有效性**：生成的语音延续被验证更符合人类偏好，提供客观的性能评估依据。  <br/>5. **开源实现与复现**：提供代码、样例和模型，便于研究者对比实验与应用。|
|2506.13642v2|[Stream-Omni: Simultaneous Multimodal Interactions with Large   Language-Vision-Speech Model](http://arxiv.org/abs/2506.13642v2)|**贡献点**:  <br/>1. 提出Stream-Omni模型，首次结合文本、视觉和语音模态并实现高效跨模态对齐，无需依赖超大规模数据。  <br/>2. 创新性地采用序列维度拼接（视觉-text）和CTC-based层维度映射（语音-text）的混合对齐策略。  <br/>3. 显著降低语音数据依赖，通过层维度映射实现文本能力向其他模态的迁移。  <br/>4. 在视觉理解、语音交互及视觉-语音交互任务中取得强性能，验证了模型的多模态兼容性。  <br/>5. 支持语音交互中实时生成中间文本输出（如ASR转录和模型响应），提升用户沉浸式体验。  <br/><br/>**总结（100字以内）**:  <br/>本文提出Stream-Omni，通过序列与层维度映射实现文本、视觉、语音高效对齐，减少数据依赖并支持跨模态迁移。实验表明其在多任务中表现优异，可提供交互式文本输出，优化多模态交互体验。|
|2506.13642v1|[Stream-Omni: Simultaneous Multimodal Interactions with Large   Language-Vision-Speech Model](http://arxiv.org/abs/2506.13642v1)|**总结**：  <br/>本文提出Stream-Omni模型，通过序列和层维度对齐策略实现多模态高效整合，减少语音数据依赖，提升文本迁移能力，并在多个任务中验证其有效性，支持实时中间文本输出。<br/><br/>---<br/><br/>**贡献点**：  <br/>1. **提出Stream-Omni模型**：首次设计支持文本、视觉、语音多模态整合的架构，实现更高效且灵活的跨模态对齐。  <br/>2. **差异化对齐策略**：针对视觉模态采用序列维度拼接，语音模态引入CTC-based层维度映射，分别优化模态间的语义关联。  <br/>3. **数据效率提升**：通过减少对语音数据的依赖，显著降低训练成本，同时增强文本能力向其他模态的迁移能力。  <br/>4. **多任务性能验证**：在视觉理解、语音交互及视觉-语音结合任务上取得优异表现，证明模型的泛化性和有效性。  <br/>5. **实时中间输出支持**：利用层维度映射技术，在语音交互过程中同步提供ASR转录和模型响应等中间文本结果，提升用户体验。|
|2506.13596v1|[Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems](http://arxiv.org/abs/2506.13596v1)|**贡献点：**  <br/>1. **系统架构创新**：融合Whisper-large-v3编码器与高效投影器架构，优化多语言语音识别流程。  <br/>2. **分阶段训练方法**：提出逐阶段优化编码器、投影器和LLM的三阶段训练策略，提升模型性能。  <br/>3. **解码器验证**：采用Gemma3-12B和Qwen2.5-7B作为解码器-only模型，实验证明其在多语言任务中的竞争力。  <br/><br/>**总结**：  <br/>该论文提出多语言语音识别系统的创新架构与分阶段训练方法，结合高效模型组件，验证了不同解码器在挑战赛中的性能表现。|
|2506.12935v1|[SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models](http://arxiv.org/abs/2506.12935v1)|总结：  <br/>该研究提出Audio Logical Reasoning数据集与SoundMind算法，通过高质量音频-文本数据与规则强化学习技术提升ALMs的双模态推理能力，并实现SOTA性能，推动语音领域智能发展。<br/><br/>贡献点：  <br/>1. **构建首个专门针对音频复杂推理任务的标注数据集**（ALR），包含6,446个文本-音频对，填补了音频模态推理数据的空白。  <br/>2. **设计SoundMind算法**：基于规则的强化学习框架，专为增强ALMs的深度双模态推理能力。  <br/>3. **验证效果**：在Qwen2.5-Omni-7B模型上使用ALR数据集和SoundMind算法，达到音频逻辑推理领域的最先进性能。  <br/>4. **理论创新**：强调高质数据与定制化RL技术结合对模型推理能力的提升作用，推动语音与语言模型的融合发展。  <br/>5. **开源贡献**：公开数据集和代码，便于后续研究复现与扩展（GitHub链接）。|
|2506.12570v1|[StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous   Autoregressive Modeling](http://arxiv.org/abs/2506.12570v1)|**贡献点：**  <br/>1. 提出首个单阶段流式TTS框架StreamMel，解决现有多阶段设计导致的高计算成本问题；  <br/>2. 首次直接建模连续mel-spectrograms，而非传统离散表示，提升生成质量与自然度；  <br/>3. 通过交织文本token与声学帧实现低延迟自回归合成，兼顾实时性与高说话人相似性；  <br/>4. 实验证明在LibriSpeech数据集上，StreamMel在质量和延迟均优于现有流式TTS基线，接近离线系统性能；  <br/>5. 展示了与实时语音大语言模型整合的潜力，推动实时语音生成技术发展。  <br/><br/>**总结（100字以内）：**  <br/>本文提出StreamMel，首个单阶段流式TTS框架，通过连续mel建模与文本-声学交织技术实现低延迟、高质量语音合成，性能接近离线系统，为实时语音大语言模型应用提供新方向。|
|2506.12481v1|[Exploring Audio Cues for Enhanced Test-Time Video Model Adaptation](http://arxiv.org/abs/2506.12481v1)|**贡献点总结：**  <br/>1. 提出音频辅助伪标签概念，首次将音频信息引入视频TTA框架。  <br/>2. 设计音频到视频标签映射方法，结合预训练音频模型与大语言模型。  <br/>3. 引入灵活适应周期，动态调整样本的适应迭代次数以优化性能。  <br/>4. 构建两个新型音频-视频TTA数据集（AVE-C, AVMIT-C），支持多类噪声验证。  <br/>5. 实验验证方法在多种视频分类模型上的通用性与有效性，代码开源。  <br/><br/>**摘要贡献点：**  <br/>- **创新方法**：提出音频信息辅助的视频TTA框架，突破传统仅依赖视觉监督的局限。  <br/>- **跨模态映射**：通过预训练音频模型和大语言模型实现音频-视频标签空间的关联。  <br/>- **自适应训练机制**：基于损失和一致性动态调整样本的适应迭代次数，提升效率。  <br/>- **数据集贡献**：创建新型音频-视频TTA数据集，扩展实验场景覆盖多种噪声类型。  <br/>- **泛化能力验证**：在主流视频分类模型（如UCF101, Kinetics）及新数据集上展示性能优势。|
|2506.12285v2|[CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction   Following](http://arxiv.org/abs/2506.12285v2)|**总结（100字以内）:**  <br/>提出了CMI-Bench，将传统MIR任务转化为指令跟随格式，推动音乐感知LLMs的统一评估与改进，揭示当前模型的性能差距及偏见问题。<br/><br/>**贡献点分点列出:**  <br/>1. **构建全面音乐指令基准**：将传统MIR注释重新诠释为指令跟随格式，覆盖音乐分类、情感分析、节奏检测等13类核心任务，突破现有简化评估的局限。  <br/>2. **标准化评估体系**：采用与SOTA MIR模型一致的评估指标，确保音频-文本LLMs与监督方法的直接可比性。  <br/>3. **开源工具支持**：提供兼容LTU、Qwen-audio、SALMONN等主流模型的评估工具包，促进技术复用与公平比较。  <br/>4. **揭示模型缺陷**：实验发现LLMs在音乐理解任务中存在显著性能差距及文化、时间、性别偏见，明确其应用潜力与限制。  <br/>5. **推动领域发展**：为音乐感知LLMs建立统一评估框架，加速该方向的研究与技术优化。|
|2506.12285v1|[CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction   Following](http://arxiv.org/abs/2506.12285v1)|**贡献点总结（100字以内）:**  <br/>提出CMI-Bench，统一评估音频-文本LLMs在音乐信息检索任务中的表现，涵盖13项核心任务，采用标准化指标与工具包，揭示模型性能差距及文化、性别等偏差，推动音乐感知LLMs发展。<br/><br/>**分点贡献:**  <br/>1. **构建新型基准**：将传统MIR注释重新设计为指令跟随格式，推出CMI-Bench，覆盖音乐理解与生成的多维度任务（如分类、回归、标注等）。  <br/>2. **任务多样性**：整合13项MIR核心挑战任务（包括体裁、情感、乐器、节奏、歌词等），全面评估音频-文本LLMs能力。  <br/>3. **标准化评估**：采用与SOTA MIR模型一致的评价指标，确保与监督方法的直接可比性，提升测评客观性。  <br/>4. **开放工具支持**：提供兼容主流开源音频-文本LLMs（如LTU、Qwen-audio等）的评估工具包，促进模型通用性测试。  <br/>5. **揭示性能与偏见**：实验发现LLMs与监督模型存在显著性能差异，并暴露文化、时间、性别等潜在偏见。  <br/>6. **推动领域进展**：为音乐感知LLMs建立统一测评框架，加速研究方向与技术改进。|
|2506.12222v1|[SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for   Polyphonic Soundscapes](http://arxiv.org/abs/2506.12222v1)|**贡献点总结**  <br/>1. 提出SSLAM方法，专门针对多音轨音频的自监督学习，弥补现有方法对复杂自然场景的适应性不足。  <br/>2. 在标准单音轨基准数据集和多种多音轨数据集上验证方法有效性，展现跨任务泛化能力。  <br/>3. 实现多音轨音频任务性能显著提升（AudioSet-2M达50.2 mAP），并在Linear和Fine-tuning场景中均创新SOTA。  <br/>4. 保持对单音轨数据的高表现（提升3.9% mAP），确保实用性与鲁棒性。  <br/><br/>**摘要总结（100字内）**  <br/>本研究提出SSLAM方法，针对多音轨音频优化自监督学习，通过全面实验验证其在单音轨和多音轨任务中的有效性，显著提升性能并创新SOTA，为自然场景音频处理提供更鲁棒的解决方案。|
|2506.12083v1|[TuneGenie: Reasoning-based LLM agents for preferential music generation](http://arxiv.org/abs/2506.12083v1)|总结：  <br/>该论文提出基于LLM的音乐偏好分析框架TuneGenie，并开发评估方法，探索AI在音乐生成艺术中的潜力，推动相关技术研究与争议讨论。<br/><br/>贡献点：  <br/>1. **创新模型提出**：首次构建TuneGenie，基于LLM的文本表示方法，专用于音乐模型与偏好分析。  <br/>2. **音乐偏好分析**：利用播放列表元数据、用户评论等多源数据，开发LLMs在分析个体音乐偏好的新方法。  <br/>3. **生成音乐提示**：设计基于偏好分析的提示生成机制，优化Suno AI等音乐生成工具的输入效果。  <br/>4. **评估基准开发**：建立多种模型评估与基准测试方法，为AI音乐生成研究提供量化分析工具。  <br/>5. **促进领域发展**：通过实验与讨论，拓展AI生成艺术的探索边界，引发对技术伦理与效果的争议性思考。|
|2506.11091v1|[Customizing Speech Recognition Model with Large Language Model Feedback](http://arxiv.org/abs/2506.11091v1)|**总结**：提出基于大语言模型的强化学习方法，通过无监督领域自适应提升语音识别的命名实体识别性能，实现21%的实体词错误率改善。<br/><br/>**贡献点**：<br/>1. **提出新型自适应策略**：开发基于强化学习的无监督领域自适应方法，解决ASR系统在罕见命名实体和领域不匹配场景下的识别难题。<br/>2. **引入LLM作为奖励模型**：将大语言模型作为奖励函数，利用其对ASR假设的评分能力，优化传统自训练方法的局限性。<br/>3. **有效利用未标记数据**：通过LLM反馈信号实现对ASR模型的微调，无需依赖人工标注数据，在未监督场景下提升性能。<br/>4. **显著性能提升**：在实体词错误率（WER）上较传统方法提升21%，验证了该方法在应对领域适应问题上的有效性。|
|2506.11079v1|[Improving Child Speech Recognition and Reading Mistake Detection by   Using Prompts](http://arxiv.org/abs/2506.11079v1)|总结：  <br/>本研究提出了一种结合Whisper与指令调优大语言模型的多模态方法，显著提升儿童语音识别准确率与阅读错误检测性能，在荷兰语儿童朗读数据集上达到SOTA水平。<br/><br/>贡献点：  <br/>1. **提出多模态方法**：首次融合音频信号与文本资源知识，构建阅读评估系统的创新框架。  <br/>2. **优化儿童语音识别**：通过提示机制改进Whisper模型，将荷兰儿童朗读的词错误率（WER）从9.4%降至5.1%。  <br/>3. **提升阅读错误检测**：验证该方法在阅读错误识别任务中的有效性，F1分数由0.39提升至0.73。  <br/>4. **指令调优LLMs的应用**：探索指令调优大语言模型与提示的协同作用，推动语音评估技术的实用性。  <br/>5. **验证基线对比优势**：证明提示优化后的模型在关键指标上优于传统基线方法，具有实际推广价值。|
|2506.10574v1|[DanceChat: Large Language Model-Guided Music-to-Dance Generation](http://arxiv.org/abs/2506.10574v1)|**总结（100字以内）**：  <br/>提出DanceChat方法，通过LLM生成文本动指令并设计多模态融合与扩散合成模块，显著提升音乐到舞蹈生成的多样性与对齐度，并在实验中验证其优越性。<br/><br/>**贡献点**：  <br/>1. **提出LLM引导的音乐到舞蹈生成框架**：首次将大语言模型（LLM）作为编舞者，通过文本指令提供显式的高层指导，弥补音乐抽象信息的不足。  <br/>2. **设计三阶段核心组件**：  <br/>   - (1) LLM驱动的伪指令生成模块，基于音乐风格与结构生成文本舞蹈指导；  <br/>   - (2) 多模态特征提取与融合模块，整合音乐、节奏和文本指令为共享表征；  <br/>   - (3) 扩散模型运动合成模块及多模态对齐损失，确保生成结果与音乐和文本的高度匹配。  <br/>3. **验证方法有效性**：在AIST++数据集上的实验及人类评估结果表明，DanceChat在多样性与风格对齐度上优于现有SOTA方法。|
|2506.10423v1|[PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer   from Audio Encoders to LLMs](http://arxiv.org/abs/2506.10423v1)|总结:  <br/>本文提出并通过实验验证三种音频-LLM架构优化方法：延迟音频集成、专注注意力模块探查、多编码器集成，显著提升跨模态信息传递效率，实现性能最高提升60%。<br/><br/>贡献点:  <br/>1. **提出跨模态信息传递机制的理论框架**：将音频-LLM交互定义为LLM通过注意力模块有效探查音频编码器表示，而非依赖FFN，揭示了关键路径。  <br/>2. **设计可解释的架构优化策略**：基于机制可解释性研究，提出延迟音频集成（提升文本上下文建模能力）和多编码器集成（增强表示多样性）两个关键修改方案。  <br/>3. **验证优化效果**：通过统一的三阶段训练流程在560万音频-文本数据集上系统测试，证实所提方法可使模型性能在10%至60%范围内提升。|
|2506.10416v1|[Can Sound Replace Vision in LLaVA With Token Substitution?](http://arxiv.org/abs/2506.10416v1)|**贡献总结（100字以内）**:  <br/>本文提出SoundCLIP框架，直接整合音频与视觉输入，设计两种音频-视觉融合方法，并构建AVE-2数据集。通过实验揭示了多模态任务中跨模态对齐与生成质量的权衡关系，强调语言监督的重要性。  <br/><br/>**分点贡献**:  <br/><br/>1. **方法创新**：提出直接音频-视觉整合框架SoundCLIP，替代传统文本对齐方式，通过替换CLIP的视觉token为音频表示并选择音频相关patch token优化多模态模型。  <br/>2. **融合策略**：设计两种配置：(1) 用MLP将音频特征投影至CLIP视觉空间（基于InfoNCE训练）；(2) 保留原始音频嵌入仅做轻微维度调整，验证不同融合路径对性能的影响。  <br/>3. **数据集构建**：开发AVE-2数据集（580,147条3秒音频视觉片段），提供细粒度时间对齐标注，支持更精确的多模态研究。  <br/>4. **实验证据**：对比五种音频编码器，发现音频投影显著提升视频检索性能（Top-1准确率提升44个百分点），但降低文本生成质量，揭示语言监督对生成任务的关键作用。  <br/>5. **理论启示**：挑战“更强跨模态对齐必然提升性能”的假设，提出帕累托前沿概念，证明需平衡检索准确率与文本生成质量以实现最优效果。|
|2506.10312v1|[AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](http://arxiv.org/abs/2506.10312v1)|**总结**：本文提出一种基于对话延续的音频理解模型，通过生成对话回应缓解字幕多样性问题，实现零样本指令跟随能力。  <br/><br/>**贡献点**：  <br/>1. **方法创新**：首次将大语言模型（LLMs）的对话延续能力引入音频理解任务，替代传统“直接生成字幕”的训练方式。  <br/>2. **解决关键问题**：通过对话延续训练有效缓解音频字幕的多样性问题，提升模型对语义的理解能力。  <br/>3. **零样本能力**：模型无需多任务指令调优，仅基于音频字幕数据集即可实现对未见过指令的遵循，具备强泛化性。  <br/>4. **实验验证**：在AudioCaps、WavCaps、Clotho等数据集及AudioBench测试中验证了模型的指令遵循性能，证明其优越性。|
|2506.10299v1|[Scheduled Interleaved Speech-Text Training for Speech-to-Speech   Translation with LLMs](http://arxiv.org/abs/2506.10299v1)|总结：  <br/>本文提出一种渐进式交织语音-文本训练方法，通过逐步减少文本比例提升S2ST性能，尤其在小语种数据稀缺场景下表现突出，并验证了该方法在LLaMA3.2-1B模型上的有效性。<br/><br/>贡献点：  <br/>1. **提出新训练范式**：设计"schedule interleaved speech-text training"方法，通过在训练阶段融合语音-文本单元（而非纯语音）实现模态迁移。  <br/>2. **渐进式模态适应策略**：引入训练过程中文本比例动态调整机制，从文本主导逐步过渡到语音主导，促进模型适应语音模态。  <br/>3. **提升小语种翻译效果**：在CVSS数据集验证中，证明该方法对低资源语言的翻译性能具有显著提升效果。  <br/>4. **模型适配验证**：基于LLaMA3.2-1B模型进行S2ST任务实验，验证了该方法的可迁移性与有效性。|
|2506.10274v2|[Discrete Audio Tokens: More Than a Survey!](http://arxiv.org/abs/2506.10274v2)|总结：  <br/>本文系统性地综述和评估了离散音频标记器在语音、音乐和通用音频领域的表现，构建了分类学框架，分析了不同方法的优劣，并为未来研究提供了指导。<br/><br/>贡献点：  <br/>1. **提出离散音频标记的实用价值**：强调其在保留感知质量、语音内容和说话人特征的同时，实现高效存储与推理，适用于多类下游任务。  <br/>2. **构建统一的分类框架**：基于编码器-解码器、量化技术、训练范式、流式性及应用领域，对现有标记方法进行系统分类。  <br/>3. **跨领域基准测试**：首次在语音、音乐和通用音频三个领域对标记器进行多维度评估，涵盖重建性能、下游任务表现及声学语言建模。  <br/>4. **量化分析权衡**：通过控制变量消融实验，揭示不同标记方法在性能与效率之间的关键权衡因素。  <br/>5. **揭示研究挑战**：总结当前方法的关键局限性与未解决的问题，为后续研究提供方向性指导。  <br/>6. **公开数据资源**：提供标记器数据库和实验结果，便于学术界进一步验证与应用。|
|2506.10274v1|[Discrete Audio Tokens: More Than a Survey!](http://arxiv.org/abs/2506.10274v1)|总结：  <br/>该论文系统分析了离散音频标记器在语音、音乐和通用音频领域的表现，提出统一分类框架并进行多维度基准测试，揭示技术局限与挑战，为后续研究提供指导。<br/><br/>贡献点：  <br/>1. **首次跨领域综述与基准测试**：系统性评估了离散音频标记器在语音、音乐和通用音频三大领域的性能，填补了现有研究单一化的空白。  <br/>2. **提出统一分类框架**：基于编码器-解码器结构、量化技术、训练范式、流式处理及应用场景，构建离散音频标记方法的分类学体系。  <br/>3. **多任务综合评估**：在重建任务、下游任务表现和声学语言建模等维度设计基准测试，量化不同标记器的优劣与适用性。  <br/>4. **控制消融实验分析权衡**：通过结构化消融研究，揭示标记器设计中的关键性能权衡与优化方向。  <br/>5. **识别关键挑战与研究方向**：总结技术局限与实际应用问题，为离散音频标记领域的未来研究提供系统性洞见。|
|2506.09175v1|[PHRASED: Phrase Dictionary Biasing for Speech Translation](http://arxiv.org/abs/2506.09175v1)|**贡献点分点列出**:<br/><br/>1. **提出新的方法**  <br/>   首次提出"短语词典偏差方法"（Phrase Dictionary Biasing），用于解决语音翻译中短语翻译困难的问题，通过利用源语言与目标语言之间的短语映射关系提升翻译质量。<br/><br/>2. **方法适用性验证**  <br/>   将该方法应用于两类主流模型：基于transducer的流式语音翻译模型和多模态大语言模型，证明其跨模型的通用性与有效性。<br/><br/>3. **性能提升显著**  <br/>   在流式语音翻译模型中，该方法相较传统"短语列表偏差"方法（Phrase List Biasing）实现**21%相对提升**，在多模态模型中提升**85%短语召回率**。<br/><br/>**总结**:  <br/>提出短语词典偏差方法，通过利用语言对映射关系显著提升语音翻译模型的短语翻译性能与召回率。|
|2506.08524v2|[Teaching Physical Awareness to LLMs through Sounds](http://arxiv.org/abs/2506.08524v2)|**总结（100字以内）**:  <br/>提出ACORN框架，通过基于物理的模拟器和专为音频设计的编码器，扩展LLMs在语音相关物理任务中的能力，验证其在多普勒效应、空间关系等领域的应用效果。<br/><br/>**贡献点**:<br/>1. **提出ACORN框架**：首次设计框架通过声音信息（如多普勒效应、多径效应、空间关系）赋予LLMs物理感知能力，弥补其对现实物理现象的理解缺陷。  <br/>2. **开发物理模拟器**：构建结合真实声源与可控物理信道的模拟器，解决数据稀缺问题，生成多样化的训练数据。  <br/>3. **创建AQA-PHY数据集**：设计全面的音频问答数据集，涵盖物理现象相关的问答任务，为研究提供基准。  <br/>4. **提出音频编码器**：开发处理音频幅度与相位信息的编码器，增强模型对声学特征的表征能力。  <br/>5. **验证实际效果**：将编码器与先进LLMs结合，在模拟与真实任务中（如视线检测、方向估计）取得合理性能，推动LLMs在物理世界的理解应用。|
|2506.08524v1|[Teaching Physical Awareness to LLMs through Sounds](http://arxiv.org/abs/2506.08524v1)|**贡献点：**  <br/>1. **提出ACORN框架**：通过声音教学，赋予LLMs对物理现象（如多普勒效应、多径效应、空间关系）的认知能力。  <br/>2. **开发物理基模拟器**：结合真实声源与可控物理信道，生成多样化的训练数据以解决数据稀缺问题。  <br/>3. **构建AQA-PHY数据集**：创建首个针对物理现象的音频问答数据集，覆盖多种真实场景任务。  <br/>4. **设计音频编码器**：首次同时处理音频的幅度与相位信息，提升对物理信号的建模精度。  <br/>5. **实验验证有效性**：在模拟与真实任务中（如视线检测、到达方向估计）取得合理结果，推动LLMs理解物理世界。  <br/><br/>**总结（100字以内）：**  <br/>本文提出ACORN框架，通过物理模拟和音频编码器增强LLMs对声学物理现象的理解，并构建专用数据集验证方法的有效性，为LLMs在物理场景中的应用奠定基础。|
|2506.08400v2|[mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text   Tasks](http://arxiv.org/abs/2506.08400v2)|**贡献点：**  <br/>1. **提出mSTEB基准**：填补低资源语言在语音领域LLM评估的空白，成为首个覆盖多模态和多任务的标准化评测体系。  <br/>2. **多任务与模态覆盖**：涵盖语言识别、文本分类、问答、翻译等任务，并同时支持语音与文本输入输出。  <br/>3. **全面模型评估**：测试Gemini 2.0 Flash、GPT-4o（Audio）等闭源模型及Qwen 2 Audio、Gemma 3 27B等开源模型，对比分析性能差异。  <br/>4. **揭示资源差距**：量化高资源与低资源语言在LLM表现上的显著差异，尤其突出非洲及美洲/大洋洲语言的不足。  <br/>5. **呼吁公平发展**：通过实证结果强调需加大投入以解决低资源语言在LLM覆盖中的系统性缺失。  <br/><br/>**总结（100字以内）：**  <br/>本文提出首个针对低资源语言的语音-文本多模态评测基准mSTEB，系统评估主流LLM性能差异，并揭示非洲及美洲/大洋洲语言的显著不足，呼吁加强资源投入以促进语言公平性。|
|2506.08400v1|[mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text   Tasks](http://arxiv.org/abs/2506.08400v1)|总结：  <br/>本文提出mSTEB基准，评估LLMs在语音与文本任务中的表现，揭示高资源与低资源语言间的显著差距，呼吁更多投入以提升后者在LLMs中的覆盖。<br/><br/>贡献点：  <br/>1. **构建首个多语言语音任务评估基准**：提出mSTEB，覆盖语言识别、文本分类、问答和翻译任务，填补低资源语言无统一评估标准的空白。  <br/>2. **多模态任务对比评估**：系统测试Gemini 2.0 Flash、GPT-4o (Audio)、Qwen 2 Audio和Gemma 3 27B等主流模型在语音和文本模态下的性能差异。  <br/>3. **揭示语言资源不均衡问题**：发现高资源语言与低资源语言（尤其非洲及美洲/大洋洲地区）在任务表现上存在显著差距，凸显欠发达语言的覆盖不足。  <br/>4. **提出针对性改进方向**：通过实证分析强调需加大资源投入，推动低资源语言在LLMs中的更广泛覆盖与性能提升。|
|2506.08346v1|[SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on   Speech Classification Models](http://arxiv.org/abs/2506.08346v1)|**贡献点总结（100字以内）:**  <br/>本研究提出Speech Prompt Backdoor Attack（SPBA）利用SLLM生成多样化触发器，针对语音元素如音色和情感，突破传统方法限制。并引入MGDA策略有效缓解攻击，通过实验验证SPBA在攻击指标上的显著效果，推动语音安全性研究。  <br/><br/>**分点贡献:**  <br/>1. **提出新型语音后门攻击方法SPBA**：利用Speech Large Language Model（SLLM）生成面向语音元素（如音色、情感）的多样触发器，突破传统触发器功能的局限性。  <br/>2. **引入MGDA防御策略**：通过多梯度下降算法有效降低攻击成功率，提供针对语音后门问题的缓解方案。  <br/>3. **实验验证攻击有效性**：在关键词识别和说话人验证任务中，实验证明SPBA具有高触发效果和优异的攻击性能，揭示其对语音安全性的威胁。|
|2506.07520v2|[LeVo: High-Quality Song Generation with Multi-Preference Alignment](http://arxiv.org/abs/2506.07520v2)|**贡献点**：<br/>1. 提出LeVo框架，集成LeLM和音乐编解码器，实现歌词至歌曲生成的多任务处理。<br/>2. 设计混合token（融合人声与伴奏）与双轨token（分离编码人声/伴奏）并行建模机制，提升音乐性与和谐度。<br/>3. 引入双解码器-only Transformer和模块化扩展训练策略，避免token类型干扰，增强模型稳定性。<br/>4. 开发基于DPO的多偏好对齐方法，通过半自动数据构建与后训练优化指令遵循和用户偏好适配。<br/>5. 通过实验验证（包括消融研究）和开放资源（音频示例、代码）证明方法有效性。<br/><br/>**总结**（100字以内）：  <br/>本文提出LeVo框架，融合LeLM与音乐编解码器，创新性地设计混合与双轨token建模机制，结合DPO优化偏好适配，有效提升音乐生成质量，并通过实验和开源资源验证成果。|
|2506.07520v1|[LeVo: High-Quality Song Generation with Multi-Preference Alignment](http://arxiv.org/abs/2506.07520v1)|总结:  <br/>本文提出LeVo框架，通过混合与双轨token建模提升音乐生成质量，结合多偏好对齐与双重解码器策略优化训练，显著改善音质、音乐性及人声伴奏和谐表现。<br/><br/>贡献点:  <br/>1. **LeVo框架创新**：首次集成语言模型（LeLM）与音乐编码器，解决音乐生成中复杂编曲与数据稀缺问题。  <br/>2. **双token建模机制**：引入混合token（整合人声与伴奏）与双轨token（分离编码），双重优化声乐与伴奏的和谐性与生成质量。  <br/>3. **双重解码器与模块化策略**：采用两个解码器-only Transformer及模块化扩展训练，有效减少不同token类型间的干扰。  <br/>4. **多偏好对齐方法**：基于DPO设计半自动数据构建与偏好对齐流程，提升指令遵循能力和音乐多样性。  <br/>5. **系统性验证**：通过消融实验与多维度评估（客观/主观指标）验证方法有效性，并提供公开音频示例。|
|2506.07323v1|[Speech Recognition on TV Series with Video-guided Post-Correction](http://arxiv.org/abs/2506.07323v1)|**总结（100字以内）：**  <br/>本研究提出结合视频与语言模型的多模态后修正框架，通过提取视频上下文信息提升ASR在复杂环境中的准确性，尤其在电视剧转录任务中显著改善转录效果。<br/><br/>---<br/><br/>**贡献点分点列出：**  <br/>1. **提出新框架**：设计针对复杂音频环境（如电视剧）的两阶段多模态后修正框架，解决重叠语音、领域术语和长程上下文依赖等问题。  <br/>2. **融合视频与语言模型**：创新性结合Video-Large Multimodal Model (VLMM)与Large Language Model (LLM)，通过视频上下文信息增强ASR的精炼能力。  <br/>3. **定制提示提取上下文**：引入Tailored Prompts优化VLMM，高效提取视频中的关键语境信息以辅助ASR错误修正。  <br/>4. **实证效果验证**：在TV系列ASR多模态基准上验证方法有效性，证明视频上下文显著提升转录准确率与鲁棒性。|
|2506.07081v2|[Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and   Label-Delayed Training](http://arxiv.org/abs/2506.07081v2)|总结：  <br/>本文提出基于神经音频编解码器的实时端点检测方法，通过标签延迟训练显著降低截止误差，并实现与预训练语音大语言模型的高效整合，提升系统响应速度和性能。<br/><br/>贡献点：  <br/>1. **引入NAC特征**：采用低比特率、流式神经音频编解码器（NAC）特征，替代传统频谱特征，提升端点检测效率。  <br/>2. **标签延迟训练方案**：设计新颖的标签延迟训练机制，有效减少截止误差。  <br/>3. **性能提升**：在固定160ms延迟下，单流/双流端点检测分别降低截止误差42.7%和37.5%。  <br/>4. **模型集成优化**：实现与基于编解码器的预训练语音大语言模型的高效对接，缩短响应时间1200ms并降低截止误差35%。|
|2506.07081v1|[Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and   Label-Delayed Training](http://arxiv.org/abs/2506.07081v1)|总结：  <br/>提出基于低比特率NAC流式特征的多轮对话实时端点检测方法，结合新颖的标签延迟训练显著降低截止错误，并优化与编码器预训练语音大语言模型的集成，提升响应效率。<br/><br/>贡献点：  <br/>1. **提出新型特征方法**：采用流式、低比特率Neural Audio Codec (NAC) 特征替代传统频谱特征，实现多轮对话的实时端点检测。  <br/>2. **创新训练策略**：设计标签延迟训练方案，进一步减少端点检测的截止错误。  <br/>3. **性能提升验证**：在固定160ms中位延迟下，单流端点器截止错误降低42.7%，双流配置降低37.5%。  <br/>4. **高效模型集成**：实现与基于编码器的预训练语音大语言模型的高效结合，响应时间缩短1200ms，截止错误减少35%。|
|2506.06862v1|[Multimodal Spatial Language Maps for Robot Navigation and Manipulation](http://arxiv.org/abs/2506.06862v1)|总结：  <br/>本文提出多模态空间语言地图（VLMaps和AVLMaps），通过融合多模态特征与3D重建实现环境感知与语言目标的精准关联，显著提升机器人在复杂环境中的导航和目标消歧能力。<br/><br/>贡献点：  <br/>1. **提出新颖的多模态空间语言地图框架**：首次将预训练多模态模型与3D环境重建结合，解决传统方法与环境映射脱节、缺乏几何精度的问题。  <br/>2. **构建自主生成的视觉-语言地图（VLMaps）**：通过标准探索生成开放词汇空间目标，支持自然语言指令的直接定位与跨机器人共享的障碍地图生成。  <br/>3. **扩展至音频-视觉-语言地图（AVLMaps）**：引入音频信息，实现统一的3D空间表示，增强多模态目标（文本、图像、音频）的消歧能力。  <br/>4. **实验证明零样本导航能力**：在仿真和现实场景中验证方法的有效性，提升50%召回率，并适用于移动机器人和桌面操作器。|
|2506.06820v1|[Beyond Classification: Towards Speech Emotion Reasoning with Multitask   AudioLLMs](http://arxiv.org/abs/2506.06820v1)|**贡献点：**  <br/>1. 提出情感推理（Emotion Reasoning）新策略：将情感理解从传统分类任务转向生成语义对齐、证据支持的解释，增强模型对情感的深层理解。  <br/>2. 设计统一框架：结合推理增强数据监督、双编码器架构和任务交替训练，整合多任务学习与情感推理能力。  <br/>3. 实验验证有效性：在IEMOCAP和MELD数据集上证明，该方法提升情感预测准确率，同时增强生成响应的连贯性与证据基础。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出基于生成能力的情感推理框架，通过多任务学习和结构创新提升情感识别的准确性与解释可靠性，验证了其在语音数据集上的有效性。|
|2506.05688v2|[Voice Impression Control in Zero-Shot TTS](http://arxiv.org/abs/2506.05688v2)|**贡献点分点总结：**  <br/>1. 提出一种基于低维向量的零样本语音合成方法，可精确控制语音的para-/non-linguistic特性（如暗/明亮）。  <br/>2. 首次将大语言模型与语音印象生成结合，实现从自然语言描述直接生成目标印象向量。  <br/>3. 消除传统方法需手动优化的步骤，提升自动化水平与效率。  <br/>4. 通过客观指标与主观评估验证方法在语音印象控制上的显著有效性。  <br/>5. 提供公开的音频示例与演示页面，直观展示技术成果与应用效果。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种零样本语音合成方法，利用低维向量结合大语言模型实现语音印象的自动化控制，无需手动优化，通过实验验证其有效性，并提供公开音频示例以展示技术应用。|
|2506.05688v1|[Voice Impression Control in Zero-Shot TTS](http://arxiv.org/abs/2506.05688v1)|总结（100字以内）:  <br/>该研究提出一种基于低维向量的zero-shot TTS语音印象控制方法，通过大语言模型实现自然语言描述到目标声音特征的自动化生成，克服了传统需手动优化的局限，验证了在客观和主观评估中的有效性。<br/><br/>贡献点:  <br/>1. **提出零样本TTS语音印象控制框架**：首次在零样本TTS系统中引入对para-/non-linguistic信息（如dark-bright）的调控机制，解决声音特征控制的难点。  <br/>2. **低维向量表征声音印象对**：设计低维向量以量化不同声音印象（如暗/亮）的强度，提升模型对语音印象的操控效率与可解释性。  <br/>3. **结合大语言模型实现自动化生成**：通过大语言模型将自然语言描述转化为目标印象向量，替代传统手动优化流程，增强系统灵活性与实用性。  <br/>4. **验证方法有效性**：通过客观和主观实验验证了该方法对语音印象的精准控制能力，证明其在实际应用中的可行性与优势。|
|2506.02012v1|[Leveraging Large Language Models in Visual Speech Recognition: Model   Scaling, Context-Aware Decoding, and Iterative Polishing](http://arxiv.org/abs/2506.02012v1)|总结：  <br/>本论文提出三种方法提升视觉语音识别性能，包括LLM规模评估、上下文感知解码和迭代优化，验证了LLMs在VSR任务中的巨大潜力。<br/><br/>贡献点：  <br/>1. **规模效应验证**：系统研究LLM参数量对VSR性能的影响，首次在VSR任务中验证了模型规模与性能的缩放定律。  <br/>2. **上下文感知解码**：引入上下文文本引导LLM解码过程，显著提升语音识别准确率。  <br/>3. **迭代优化策略**：提出通过多轮迭代修正LLM输出，逐步降低识别错误率，实现更精准的转录结果。|
|2506.01845v1|[On-device Streaming Discrete Speech Units](http://arxiv.org/abs/2506.01845v1)|**贡献点总结（100字以内）**:  <br/>提出一种集成注意力窗口缩减和模型压缩的DSU方法，降低计算成本的同时保持语音单元有效性，并通过实验验证其在资源受限环境下的实时语音处理可行性。<br/><br/>**分点贡献**:<br/>1. **方法创新**：提出降低注意力窗口大小与模型参数规模的策略，首次实现DSUs在保持性能的同时的高效压缩。  <br/>2. **效率提升**：在ML-SUPERB 1h数据集上，将FLOPs降低50%，仅带来6.5%的CER上升，显著优化资源消耗。  <br/>3. **应用场景扩展**：证明DSUs适用于轻量级流式语音处理，为资源受限设备的实时语音交互提供可行性方案。|
|2506.00975v4|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v4)|**贡献点：**  <br/>1. **提出新型生成范式**：首次引入基于解码器-only架构的**Next-Token-Pair Prediction (NTPP)** 方法，实现说话人无关的双通道口语对话学习。  <br/>2. **高效利用双通道数据**：系统性地探索双通道语音数据在现代大语言模型中的应用，挖掘其对捕捉对话结构与动态的潜力。  <br/>3. **提升对话性能**：在回合转换预测、响应连贯性及自然性等关键指标上，显著优于现有方法。  <br/>4. **优化实时效率**：相比传统方法，NTPP降低推理延迟，适合实时语音交互场景。  <br/><br/>**总结（100字以内）**:  <br/>本文首次提出基于解码器-only架构的Next-Token-Pair Prediction方法，通过双通道语音数据提升SLMs的对话能力与自然度，同时显著降低推理延迟，适用于实时互动场景。|
|2506.00975v3|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v3)|**总结**（100字以内）：  <br/>本文提出NTPP新方法，首次采用解码器-only架构实现说话人独立的双通道语音对话学习，显著提升对话系统的轮流预测、连贯性与自然度，同时降低推理延迟，为实时语音交互提供高效解决方案。<br/><br/>**贡献点**：  <br/>1. **提出NTPP生成建模范式**：首次将"下一个词对预测"（Next-Token-Pair Prediction）引入语音领域，基于解码器-only架构实现说话人独立的双通道对话学习。  <br/>2. **有效利用双通道数据**：系统性探索双通道语音数据对对话建模的价值，充分挖掘对话结构与动态信息。  <br/>3. **提升对话表现能力**：在标准基准测试中，NTPP显著优于现有方法，改善对话系统的轮流预测、响应连贯性和自然度。  <br/>4. **优化推理效率**：实现比传统方法更低的推理延迟，为实时语音交互应用提供更高效的模型支持。|
|2506.00267v3|[CASPER: A Large Scale Spontaneous Speech Dataset](http://arxiv.org/abs/2506.00267v3)|总结（100字以内）:  <br/>本文提出首个可复现的自然对话采集框架，发布包含100+小时高质量自发语音数据集，解决语音领域数据稀缺问题，并计划持续扩展数据资源以促进研究。<br/><br/>贡献点:  <br/>1. **提出新型对话采集方法**：设计可引导自然对话、鼓励多样化互动的主题框架，突破传统剧本对话限制。  <br/>2. **构建大规模自发语音数据集**：提供100+小时真实、无脚本的口语数据，填补语音研究中高质量数据不足的空白。  <br/>3. **强化交互真实性**：通过结构化流程确保对话的真实性与可重复性，为后续研究提供可靠基础。  <br/>4. **开放数据扩展机制**：规划持续扩展数据集的路径，形成长期可用的研究资源。|
|2506.00267v2|[CASPER: A Large Scale Spontaneous Speech Dataset](http://arxiv.org/abs/2506.00267v2)|**总结（100字以内）**：  <br/>提出自然对话数据收集新流程，发布100+小时自发语音数据集，支持真实互动与多样化话题，提供可复制框架，并计划持续扩展数据集以促进研究社区发展。<br/><br/>---<br/><br/>**贡献点分点**：  <br/>1. **提出新数据收集方法**：设计可激发自然对话的流程，突破传统剧本对话数据局限。  <br/>2. **构建大规模自发语音数据集**：提供100+小时高质量自发语音数据，填补领域数据缺口。  <br/>3. **促进真实互动与多样性**：通过鼓励开放话题和互动交流，增强数据的实用性和真实性。  <br/>4. **提供可复现框架**：建立标准化流程，支持未来研究的重复实验与数据扩展。  <br/>5. **开放持续扩展计划**：承诺未来数据集的更新与扩展，形成长期可用的研究资源。|
|2506.00267v1|[CASPER: A Large Scale Spontaneous Speech Dataset](http://arxiv.org/abs/2506.00267v1)|**贡献点：**<br/>1. **提出新型数据采集流程**：设计并实现了一种能够激发自然对话、鼓励多样化话题和互动交流的记录方法，突破传统剧本对话数据的局限性。<br/>2. **发布大规模自发语音数据集**：公开了包含200+小时真实自发语音的Stage 1数据集，为语音处理研究提供高质量基准数据。<br/>3. **构建可复现研究框架**：通过框架化设计，为后续自发性语音数据收集提供标准化和可重复的解决方案，推动领域发展。|
|2505.22995v1|[LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable   Data for Custom Keyword Spotting](http://arxiv.org/abs/2505.22995v1)|**贡献点**：<br/>1. 提出利用大语言模型（LLMs）生成混淆关键词音频，并通过文本到语音（TTS）引擎合成多样化的说话风格，增强混淆场景下的训练数据。<br/>2. 设计新型北星指标c-AUC，基于混淆关键词组的平均DET曲线面积，更精准地衡量用户在混淆场景下的体验。<br/>3. 实现零人工成本和高可扩展性的训练增强策略，提升语音命令测试集的AUC和c-AUC性能（分别提升3.7%和11.3%）。<br/>4. 针对传统对比训练在混淆关键词识别中的局限性，提出通过语义生成和语音合成构建更真实的对抗样本。<br/>5. 验证了混淆关键词组的训练策略对模型鲁棒性和识别准确性的显著提升效果。<br/><br/>**总结**：该论文提出基于LLMs生成混淆关键词并结合TTS合成多样风格的方法，设计了c-AUC新指标，有效提升了KWS性能。|
|2505.22943v1|[Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of   Pre-trained Multimodal Representation via Text Updates](http://arxiv.org/abs/2505.22943v1)|总结：  <br/>本文提出MAC基准测试，利用LLMs生成欺骗性文本暴露多模态模型组合漏洞，并通过自训练方法提升攻击效果与多样性，验证了小模型在跨模态评估中的有效性。<br/><br/>贡献点：  <br/>1. **提出Multimodal Adversarial Compositionality（MAC）基准测试**  <br/>   - 通过大语言模型（LLMs）生成欺骗性文本样本，系统评估多模态模型在图像、视频、音频等不同模态中的组合漏洞。  <br/>   - 引入样本级攻击成功率和群体级熵评估指标，量化漏洞的多样性和攻击效果。<br/><br/>2. **改进零样本方法的自训练框架**  <br/>   - 设计基于拒绝采样微调的自训练策略，结合多样性促进过滤机制，同时提升攻击成功率和样本多样性。  <br/>   - 有效解决传统方法在跨模态攻击中稳定性不足的问题。<br/><br/>3. **验证小语言模型的高效性**  <br/>   - 采用轻量级语言模型（如Llama-3.1-8B）实现更优的漏洞检测能力，表明小模型在跨模态任务中具有成本效益和实用性。  <br/>   - 展示了该方法在多种多模态表示（图像、视频、音频）上的广泛适用性。|
|2505.22063v1|[Weakly Supervised Data Refinement and Flexible Sequence Compression for   Efficient Thai LLM-based ASR](http://arxiv.org/abs/2505.22063v1)|**贡献点（分点）:**  <br/>1. 提出**EThai-ASR**系统，首次将大型语言模型（LLMs）应用于泰语自动语音识别（ASR），构建高效且完整的LLM-based ASR框架。  <br/>2. 设计**自进化数据精炼策略**，通过优化弱标签提升语音编码器质量，解决低资源场景下的数据稀缺问题。  <br/>3. 引入**可插拔的序列压缩模块**，包含三种模式以减少序列长度，显著降低计算需求同时保持性能。  <br/>4. 在多语种数据集上实现**SOTA精度**，并通过公开精炼文本转录数据促进后续研究。  <br/><br/>**总结（100字以内）:**  <br/>本文提出EThai-ASR，首次将LLMs应用于泰语ASR，通过数据精炼和序列压缩模块缓解低资源问题并提升效率，实验验证其性能领先，同时开放数据以推动研究。|
|2505.21148v1|[Assessment of L2 Oral Proficiency using Speech Large Language Models](http://arxiv.org/abs/2505.21148v1)|**贡献点**  <br/>1. **提出多模态LLM作为L2口试评分新范式**：首次将多模态大语言模型应用于L2英语口语评估，突破传统级联系统和端到端模型的信息丢失与性能局限。  <br/>2. **训练策略对比与优化**：系统比较基于回归与分类目标的训练方法，验证语音LLM在不同任务设置下的有效性。  <br/>3. **性能提升**：在两个标准数据集上实现优于现有最佳基线模型的评估效果。  <br/>4. **泛化能力验证**：通过跨部分/跨任务测试，证明预训练阶段获取的语音理解知识可显著提升模型在未见过场景中的适应性。  <br/><br/>**总结**（100字以内）：  <br/>本文提出多模态LLM用于L2口语评分，对比回归/分类训练策略，实验证明其性能优于传统方法，并展现跨任务泛化能力，为自动语音评估提供了新思路。|
|2505.21138v2|[Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis](http://arxiv.org/abs/2505.21138v2)|**贡献点（分点）:**<br/>1. 构建了300,000小时的无标注中文方言与口音语音数据集，用于Data2vec2模型的自监督预训练。  <br/>2. 设计了监督数据集上的对齐训练策略，增强低资源场景下方言与口音的ASR性能。  <br/>3. 系统分析了不同投影器与LLM对普通话、方言及口音识别任务的影响，提供优化方向。  <br/>4. 在多个中文方言数据集（如Kespeech）上达到SOTA性能，验证了方法有效性。  <br/>5. 开源实验成果以促进语音领域可重复研究与技术推广。  <br/><br/>**总结（100字以内）:**  <br/>该研究通过大规模自监督预训练结合对齐训练与LLM，显著提升中文方言和口音的ASR性能，取得SOTA结果，并开源数据与代码以推动相关研究的可复现性和发展。|
|2505.21138v1|[Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis](http://arxiv.org/abs/2505.21138v1)|总结：  <br/>该研究提出基于自监督学习和大型语言模型的框架，解决中文方言识别的低资源问题，实现多数据集SOTA性能，并开放源码促进研究可重复性。<br/><br/>贡献点：  <br/>1. **提出应用自监督预训练与LLM的跨语言范式**：首次将Data2vec2模型与LLM结合应用于中文方言识别，为低资源场景提供新思路。  <br/>2. **构建大规模未标注方言及口音数据集**：基于30万小时无标签方言和口音语音数据进行预训练，提升模型对语音特征的建模能力。  <br/>3. **设计对齐训练策略**：在40万小时监督数据集上进行对齐训练，优化方言与标准汉语之间的表征关联。  <br/>4. **系统评估不同组件影响**：分析不同投影器和LLM对多语言（普通话、方言、口音）识别性能的作用机制。  <br/>5. **实现SOTA结果**：在Kespeech等多方言数据集上取得当前最佳性能，验证方法有效性。  <br/>6. **开放源码推动研究**：开源代码与模型，促进学术界对方言ASR的可复现研究与技术共享。|
|2505.20770v1|[Can Large Language Models Predict Audio Effects Parameters from Natural   Language?](http://arxiv.org/abs/2505.20770v1)|贡献点总结（100字以内）:  <br/>LLM2Fx首次实现零样本文本到音频效果参数生成，引入DSP特征、代码及少量样本示例提升性能，在音乐制作中提供自然语言驱动的直观接口。<br/><br/>分点贡献:<br/>1. **无需任务特定训练**：直接利用LLMs生成音频效果（如均衡、混响）参数，突破传统需要微调的局限。  <br/>2. **Text2Fx任务解决**：创新性地建立自然语言描述与音频效果参数的映射关系，揭示音色语义与效果的关联。  <br/>3. **上下文增强机制**：提出三种类型上下文示例（DSP特征、代码、少量样本）提升模型性能与生成准确性。  <br/>4. **性能超越传统方法**：实验验证LLM生成效果参数优于现有优化方法，具有竞争力。  <br/>5. **应用创新**：开创文本驱动的音频生产界面，推动音乐制作工具向更直观、易用方向发展。|
|2505.20638v1|[Music's Multimodal Complexity in AVQA: Why We Need More than General   Multimodal LLMs](http://arxiv.org/abs/2505.20638v1)|**贡献点：**<br/>1. 指出音乐领域需定制化多模态处理方法，而非通用大语言模型  <br/>2. 系统分析Music AVQA数据集与方法，揭示该领域的独特挑战  <br/>3. 提出三类关键要素：专用输入处理、时空架构设计、音乐特定建模策略  <br/>4. 总结有效设计模式，并为整合音乐先验知识提出未来研究方向  <br/>5. 建立多模态音乐理解的理论基础，提供开源研究资源支持  <br/><br/>**总结：**  <br/>本研究聚焦音乐领域多模态处理，提出定制化方法的关键要素，为音乐AVQA研究提供理论指导与实践方向。|
|2505.20491v1|[In-context learning capabilities of Large Language Models to detect   suicide risk among adolescents from speech transcripts](http://arxiv.org/abs/2505.20491v1)|总结：  <br/>该研究提出基于大语言模型和系统提示工程的语音分析方法，解决了青少年自杀风险检测的可扩展性难题，并在SpeechWellness Challenge中取得优异性能，验证了LLMs在心理健康评估中的潜力。<br/><br/>贡献点：  <br/>1. **提出系统性方法应对挑战**：针对SpeechWellness Challenge（SW1）设计基于转录文本的自动 Suicide Risk 评估框架，解决语音匿名化下的实际应用难题。  <br/>2. **创新性迁移LLM模型**：将大语言模型（LLMs）应用于语言特征分类，突破传统语音分析方法的限制，首次在中文青少年语料中实现高效风险识别。  <br/>3. **优化提示工程技术**：通过DSPy框架进行系统性提示设计，开发出优于传统微调的上下文学习方法，显著提升分类性能。  <br/>4. **量化提示效果机制**：通过消融实验验证提示样本数量对性能的提升作用（p=0.003），揭示不同模型规模与类型的效果差异。  <br/>5. **验证LLMs应用价值**：在实际比赛中取得第三、第四名的优秀成绩（0.68准确率，F1=0.7），体现LLMs在心理健康领域的重要应用潜力。|
|2505.20237v2|[Efficient Speech Translation through Model Compression and Knowledge   Distillation](http://arxiv.org/abs/2505.20237v2)|**贡献点：**  <br/>1. 提出结合**迭代层剪枝**、**低秩适配（QLoRA）**与**知识蒸馏**的综合模型压缩方法，用于高效部署大音频-语言模型。  <br/>2. 在**语音翻译任务**中实现高达50%的模型参数与存储空间减少，同时保持97-100%的翻译质量。  <br/>3. 验证了压缩模型在**德语和中文翻译**中的有效性，展示了压缩技术的跨语言适用性。  <br/>4. 通过**系统提交至IWSLT 2025模型压缩赛道**，推动了语音领域大模型的实用化研究。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出综合模型压缩方案，通过迭代剪枝、QLoRA及知识蒸馏显著降低语音翻译模型的参数和存储需求，保持高翻译质量，并在IWSLT 2025竞赛中验证其有效性。|
|2505.20166v2|[From Alignment to Advancement: Bootstrapping Audio-Language Alignment   with Synthetic Data](http://arxiv.org/abs/2505.20166v2)|**贡献点：**  <br/>1. 提出BALSa框架，通过合成对比学习数据缓解ALLMs的灾难性遗忘问题，保留文本理解和推理能力。  <br/>2. 创新性设计多音频场景下的训练策略，支持模型对音频差异的解释或统一描述，增强跨模态对齐。  <br/>3. 有效降低音频幻觉现象，同时保持音频理解与指令跟随任务的高性能表现。  <br/>4. 提供高效、可扩展的合成数据生成方法，减少对大规模任务特定数据集的依赖。  <br/><br/>**总结：**  <br/>本论文提出BALSa框架，通过合成对比数据解决ALLMs的灾难性遗忘与跨模态对齐资源消耗问题，提升音频理解与多音频处理能力，实现高效可扩展的模型训练。|
|2505.20166v1|[From Alignment to Advancement: Bootstrapping Audio-Language Alignment   with Synthetic Data](http://arxiv.org/abs/2505.20166v1)|总结：  <br/>该论文提出BALSa方法，通过合成数据缓解ALLMs的灾难性遗忘和跨模态对齐问题，并引入LISTEN和多音频训练策略，显著提升音频理解与多模态对齐能力，实现高效、可扩展的音频-语言模型开发。<br/><br/>贡献点：  <br/>1. **提出BALSa方法**：利用基础LLM生成通用caption-style对齐数据，解决音频任务训练导致的文本能力流失问题。  <br/>2. **设计LISTEN训练机制**：通过生成扩展负样本，增强ALLMs区分音频存在与缺失的能力，减少声音幻觉。  <br/>3. **扩展至多音频场景**：使模型能处理多音频输入（如描述差异或生成统一caption），提升跨模态对齐效率。  <br/>4. **实验验证有效性**：证明方法在音频理解、推理和指令遵循任务中保持高性能，同时降低对特定数据集的依赖。  <br/>5. **提出高效开发路径**：为ALLMs构建提供可扩展、低成本的训练框架，推动音频-语言模型的实用性发展。|
|2505.16211v2|[AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models](http://arxiv.org/abs/2505.16211v2)|总结：  <br/>提出首个针对音频大语言模型的多维度可信性评估框架AudioTrust，涵盖公平性、幻觉、安全、隐私、鲁棒性和认证六大维度，构建包含4420个真实场景样本的数据集，并设计专用评估指标与自动化评分流程，揭示当前模型在高风险音频场景中的可信性边界。<br/><br/>贡献点：  <br/>1. **首个专用评估框架**：提出AudioTrust，专门针对音频大语言模型（ALLMs）的可信性评估，填补该领域的系统性研究空白。  <br/>2. **多维度评估体系**：构建涵盖公平性、幻觉、安全、隐私、鲁棒性和认证的六大核心评估维度，全面覆盖音频模态的特殊风险。  <br/>3. **真实场景数据集**：创建包含4420个音频/文本样本的高质量数据集，来源覆盖日常对话、紧急呼叫等实际应用场景。  <br/>4. **音频专用评估指标**：设计9项针对音频模态的专属评价指标，解决传统文本评估框架在音频任务中的适配不足。  <br/>5. **自动化评分流程**：开发大规模自动化评分管道，实现模型输出的客观、可扩展评估，提升评估效率与一致性。  <br/>6. **实证分析与启示**：通过实验揭示当前主流ALLMs在高风险场景下的可信性边界与局限，为未来模型的安全部署提供指导。  <br/>7. **开源平台**：提供公开的评估平台与基准数据，促进学术界与工业界对音频模型可信性的研究与改进。|
|2505.14518v2|[Teaching Audio-Aware Large Language Models What Does Not Hear:   Mitigating Hallucinations through Synthesized Negative Samples](http://arxiv.org/abs/2505.14518v2)|**贡献点总结（100字以内）**  <br/>本文提出LISTEN方法，通过合成数据和轻量级适配器改进ALLMs的音频处理能力，有效缓解幻觉问题并提升数据与计算效率。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **提出对比学习框架**：引入对比学习（contrastive-like）方法，利用合成数据训练ALLMs以更好区分存在和不存在的音频事件。  <br/>2. **轻量级适配器设计**：无需修改LLM参数，通过轻量级适配器高效整合音频表示，保持模型原生结构。  <br/>3. **有效性验证**：实验表明LISTEN显著减少幻觉现象，在音频问答与推理任务中保持高性能，并提升数据和计算效率。|
|2505.13880v3|[U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding](http://arxiv.org/abs/2505.13880v3)|总结:  <br/>本研究提出U-SAM模型，通过集成专用编码器与大语言模型、引入MoE特征融合机制和语义感知对比损失模块，显著提升跨模态对齐效果和音频任务泛化能力。<br/><br/>贡献点:  <br/>1. **多模态统一框架设计**：首次将语音、音频事件、音乐的专用编码器与预训练大语言模型（LLM）结合，实现跨音频类型的一体化理解。  <br/>2. **任务感知特征融合**：提出基于Mixture of Experts（MoE）的动态路由投影器，根据不同任务需求高效整合领域专属编码器输出。  <br/>3. **语义对比损失优化**：设计结合语言监督的语义-感知对比损失模块，主动识别并修正冗余音频特征的语义与频谱表示，增强跨模态对齐。  <br/>4. **强大泛化能力验证**：在多种基准测试中超越现有模型，并展现对未见任务的涌现能力，证明其通用性与鲁棒性。  <br/>5. **开源实现支持**：提供代码公开，便于复现与进一步研究（GitHub链接）。|
|2505.13577v2|[VocalAgent: Large Language Models for Vocal Health Diagnostics with   Safety-Aware Evaluation](http://arxiv.org/abs/2505.13577v2)|总结：  <br/>本文提出VocalAgent音频大语言模型，通过医院真实数据训练和多维度评估框架（含安全评估、跨语言分析、模态消融）提升语音疾病分类准确性，强调伦理验证与可扩展性，为声障诊断提供便捷解决方案。<br/><br/>贡献点：  <br/>1. **提出VocalAgent模型**：开发首个专门针对语音健康诊断的音频大语言模型（LLM），通过微调Qwen-Audio-Chat解决声障诊断可及性问题。  <br/>2. **多维度评估框架**：构建包含安全评估（减少诊断偏见）、跨语言性能分析和模态消融实验的综合评估体系，确保模型可靠性。  <br/>3. **性能提升**：在语音疾病分类任务中超越现有基线模型，验证其高准确率和有效性。  <br/>4. **可扩展性与伦理考量**：强调LLM方法的可扩展性，推动健康诊断技术普及，并突出伦理和技术双重验证的重要性。|
|2505.13062v3|[Hearing from Silence: Reasoning Audio Descriptions from Silent Videos   via Vision-Language Model](http://arxiv.org/abs/2505.13062v3)|总结：  <br/>该论文提出SVAD任务，构建CoT-AudioCaps数据集并设计思维链监督微调策略，有效提升VLMs在无声视频到音频推理及视频到音频生成中的模态不匹配处理能力。<br/><br/>贡献点：  <br/>1. **提出新任务**：定义"Reasoning Audio Descriptions from Silent Videos (SVAD)"任务，填补多模态模型在模态不匹配场景下音频推理研究的空白。  <br/>2. **构建专用数据集**：创建包含思维链引导的CoT-AudioCaps数据集，为SVAD任务提供高质量训练与评估资源。  <br/>3. **创新训练策略**：提出基于思维链（Chain-of-Thought）的监督微调方法，增强模型对目标模态的推理能力。  <br/>4. **验证方法有效性**：通过实验证明所提方法在SVAD任务中显著提升模态不匹配推理效果，并有效改进VT2A任务的音频描述生成能力。|