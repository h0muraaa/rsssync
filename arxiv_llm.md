|Source|Title|Summary|
|---|---|---|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|**贡献点**  <br/>1. 提出首个专门针对端到端语音交互的LLM评估框架，解决现有文本基准在语音特性（如韵律、同音词、结巴）中的不足。  <br/>2. 构建包含多维真实场景数据的数据集，覆盖多样性说话者属性、声学条件及语音特有现象（如语调、语音干扰）。  <br/>3. 设计查询感知评价方法，通过定制化检查清单和提示提升自动评估的准确性与细粒度分析能力。  <br/>4. 系统分析主流语音模型在多场景下的表现差异，揭示其性能局限与改进方向。  <br/>5. 提供可复用的语音模型评估基准，为实际应用优化和模型开发提供理论与实践支持。  <br/><br/>**总结（100字内）**  <br/>本研究提出面向语音交互的LLM评估框架，构建涵盖多样语音特性的基准数据集，设计查询感知方法提升评估准确性，揭示模型场景性能差异，为语音模型优化与应用提供关键参考。|
|2506.21864v1|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v1)|总结（100字以内）：  <br/>论文提出DeepTalk框架，通过自适应模态专家学习解决原生MLLMs的性能下降和延迟问题，保持高效语音-文本交互并开源代码。<br/><br/>贡献点：  <br/>1. **提出DeepTalk框架**：基于MoE架构设计自适应模态专家学习机制，解决原生多模态模型因数据不足导致的灾难性遗忘问题。  <br/>2. **分阶段训练策略**：结合自适应模态划分、单模态专项训练与多模态协作训练，优化模型性能与数据效率。  <br/>3. **性能提升显著**：相比原生MLLMs（如GLM-4-Voice）平均20%的性能下降，Deep Talk仅损失5.5%，接近模块化模型效果。  <br/>4. **低延迟交互**：端到端对话延迟控制在0.5秒内，实现流畅的智能语音交互体验。  <br/>5. **开源实现**：提供代码与模型，便于复现与进一步研究。|
|2506.20666v1|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v1)|总结：  <br/>本研究提出利用认知模型分析LLMs在价值权衡中的表现，揭示了不同训练策略下效用分配差异，为理解模型行为、优化训练机制及控制价值冲突提供新视角。<br/><br/>贡献点：  <br/>1. **引入认知模型框架**：首次将认知科学中的价值权衡模型（如礼貌语言模型）应用于LLMs，系统分析其在社交情境中处理矛盾目标的能力。  <br/>2. **双维度模型评估**：针对前沿黑盒模型的推理"努力"程度与开源模型的RL对齐动态，提出统一的评估方法，探索不同场景下的效用函数分配机制。  <br/>3. **发现效用偏向规律**：揭示推理模型更注重信息效用而非社会效用，同时指出开源模型在数学推理上的优势，为模型能力差异提供实证依据。  <br/>4. **揭示训练动态影响**：证明模型训练早期存在显著的效用值变化，且基础模型和预训练数据选择对效用分配具有持续影响，优于反馈数据或对齐方法的作用。  <br/>5. **方法泛化与应用价值**：提出的方法可适配LLMs快速演进的生态，为推测高阶行为、设计训练策略及平衡模型价值冲突提供理论支持和实践指导。|
|2506.19835v1|[MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration](http://arxiv.org/abs/2506.19835v1)|总结：  <br/>该论文提出MAM框架，通过模块化多代理设计解决医疗LLMs在知识更新、全面性和灵活性方面的局限性，跨模态数据集实验验证其显著性能提升，并开源代码促进应用。<br/><br/>贡献点：  <br/>1. **提出模块化多代理框架（MAM）**：首次将医疗诊断任务拆分为多个专业化角色（全科医生、专家团队、放射科医生、医疗助手、导演），通过协作提升诊断准确性与效率。  <br/>2. **降低知识更新成本**：设计高效的模块化知识更新机制，减少整体模型训练和迭代的资源消耗。  <br/>3. **跨模态性能优势**：在包含文本、图像、音频、视频的多模态医疗数据集上验证，表现出优于单一模态LLM的综合能力。  <br/>4. **显著性能提升**：实验结果表明，MAM在多个任务上较基线模型提升18%-365%，突出其有效性。  <br/>5. **开源实现**：提供公开代码，便于研究社区复现与进一步改进，推动医疗AI领域发展。|
|2506.19732v1|[Who Does What in Deep Learning? Multidimensional Game-Theoretic   Attribution of Function of Neural Units](http://arxiv.org/abs/2506.19732v1)|总结：  <br/>提出MSA框架，用于解释深度神经网络中神经单元的贡献，适用于多种模型并揭示计算集中化和生成结构等关键机制，促进模型解释、编辑与压缩。  <br/><br/>贡献点：  <br/>1. **提出模型无关的博弈论解释框架**：开发Multiperturbation Shapley-value Analysis (MSA)，填补现有可解释方法在输出维度贡献分析上的空白。  <br/>2. **跨尺度适用性**：将MSA应用于从基础多层感知机（MLP）到超大规模语言模型（如56B参数Mixtral-8x7B）及生成对抗网络（GAN），验证其广泛适用性。  <br/>3. **量化单元级贡献**：通过系统性损伤单元组合，生成与模型输出维度一致的Shapley Modes，直观展示每个神经单元对结果的具体影响。  <br/>4. **揭示关键机制**：发现正则化在模型中集中计算资源（“few hubs”），并解析LLM语言特定专家结构及GANs的倒置像素生成层次。  <br/>5. **拓展应用前景**：展示MSA在模型**解释、编辑和压缩**中的潜力，为深度学习模型的分析与优化提供新工具。|
|2506.19502v1|[MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications](http://arxiv.org/abs/2506.19502v1)|总结：  <br/>本研究提出MATE多模态无障碍多智能体系统，通过灵活的模态转换与本地部署保障隐私，创新性地引入ModCon-Task-Identifier模型提升任务识别准确率，为残疾人提供跨领域的实时数字环境辅助。<br/><br/>贡献点：  <br/>1. **提出MATE系统**：首个针对无障碍需求的多模态多智能体系统，通过自适应模态转换（如图像转音频描述）解决用户交互障碍问题。  <br/>2. **高度灵活性与兼容性**：支持LLM API调用、自定义ML分类器等多模型架构，适配不同领域（如医疗）和硬件环境。  <br/>3. **本地化部署机制**：确保敏感信息隐私安全，避免云端传输带来的数据泄露风险。  <br/>4. **机构技术集成能力**：可无缝对接数字医疗等专业系统，实现实时用户辅助。  <br/>5. **ModCon-Task-Identifier模型**：创新设计的任务识别模型，在自定义数据上显著优于其他LLM和统计方法。|
|2506.19073v1|[MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral   Reasoning of LLMs through Hate Speech Multi-hop Explanation](http://arxiv.org/abs/2506.19073v1)|**贡献点：**  <br/>1. 提出多语言基准数据集MFTCXplain，支持葡萄牙语、意大利语、波斯语和英语四种语言，覆盖3,000条推文。  <br/>2. 引入基于道德基础理论（MFT）的多跳解释方法，增强道德分类任务的透明度和可解释性。  <br/>3. 首次系统分析LLMs在跨文化场景下的道德推理能力差异，揭示语言多样性对评估结果的影响。  <br/>4. 通过实验验证LLMs在道德情感预测（F1<0.35）及推理理由对齐方面的显著不足，强调当前技术瓶颈。  <br/><br/>**总结：**  <br/>该研究构建MFTCXplain多语言数据集，揭示LLMs在跨文化道德推理任务中存在广泛不足，推动更公平、透明的道德评估方法发展。|
|2506.18576v1|[A Modular Taxonomy for Hate Speech Definitions and Its Impact on   Zero-Shot LLM Classification Performance](http://arxiv.org/abs/2506.18576v1)|**贡献点分点总结**  <br/>1. **理论框架**：提出14个概念要素分类法，系统梳理仇恨言论定义的多样性与核心维度（如目标群体、潜在后果等）。  <br/>2. **实验验证**：设计零样本评估实验，使用三种LLMs和三种不同类型的仇恨言论数据集（合成、人机协同、真实世界）进行对比分析。  <br/>3. **定义影响规律**：发现仇恨言论定义的模糊性会影响模型性能，但具体影响依赖定义的粒度（如具体元素的编码程度）。  <br/>4. **架构差异性**：揭示不同LLM架构对同一定义的响应存在显著差异，说明模型设计对定义敏感度的影响。  <br/>5. **跨数据集一致性**：指出定义对模型性能的影响在不同数据集类型中表现不一致，凸显定义泛化性与数据适配性的挑战。  <br/><br/>**总结（100字以内）**：  <br/>本研究构建仇恨言论定义的理论分类体系，并通过多模型、多数据集实验分析不同定义对检测性能的影响，揭示了定义模糊性与模型架构的交互作用，为改进仇恨言论识别提供了新视角。|
|2506.18274v1|[Leveraging Large Language Models for Information Verification -- an   Engineering Approach](http://arxiv.org/abs/2506.18274v1)|总结：  <br/>提出基于GPT-4o的自动化多媒体新闻源验证方法，整合多模态数据处理与交叉验证技术，减少人工干预。<br/><br/>贡献点：  <br/>1. **多模态数据整合**：首次将图像、视频、音频等多类型数据纳入新闻源验证流程，提升信息全面性。  <br/>2. **LLM为核心架构**：采用GPT-4o作为验证管道的骨干模型，实现端到端自动化处理。  <br/>3. **流程自动化**：通过提示工程完全自动化元数据生成、数据分帧、特征筛选及交叉验证等步骤。  <br/>4. **关键帧筛选机制**：开发基于重要性排序的帧选择策略，优化视频分析效率。  <br/>5. **轻量人工干预**：仅需人类参与最终结果验证，显著降低人工成本与操作复杂性。|
|2506.17715v1|[Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource   Medieval Romance Languages](http://arxiv.org/abs/2506.17715v1)|总结：  <br/>本研究系统分析中世纪罗曼语词性标注的挑战，评估多种技术方法，揭示LLMs在历史语言处理上的局限性，并探索有效解决方案，涵盖多领域与多语言文本。<br/><br/>贡献点分点：  <br/>1. **系统性研究历史语言变体挑战**：首次对中世纪奥克语、西班牙语、法语文本的词性标注进行多维度分析，揭示历时性语言演变、拼写非标准化和数据稀缺等问题对模型性能的影响。  <br/>2. **多技术方法对比实验**：系统评估微调、提示工程、模型架构、解码策略及跨语言迁移学习等技术对词性标注准确率的影响，提供方法选择的实证依据。  <br/>3. **多领域与多语言覆盖**：基于包含宗教、圣徒传、医学、饮食等领域的跨语言语料库（Medieval Occitan、Spanish、French），验证技术的泛化能力与适应性。  <br/>4. **提出针对性解决方案**：发现特定技术（如跨语言迁移学习）在低资源历史语言场景中的有效性，为优化历史语言处理提供新思路。|
|2506.17410v1|[Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A   Feasibility Study](http://arxiv.org/abs/2506.17410v1)|**贡献点总结：**  <br/>1. 提出利用生成式AI分析真实数学辅导的可行性与可扩展性方法；  <br/>2. 识别并评估"有效表扬"和"纠正学生数学错误"两项关键辅导行为；  <br/>3. 验证多模型（GPT-4系列、Gemini-1.5-pro、LearnLM）在辅导行为检测与评估中的可靠性；  <br/>4. 设计成本效益高的提示策略以提升AI在教育评估中的实用性；  <br/>5. 促进LLM在AI支持学习研究中的可复现性与实际应用价值。<br/><br/>**研究摘要总结（100字以内）：**  <br/>开发生成式AI方法分析真实数学辅导行为，验证多模型在检测表扬与纠错效果上的可靠性，提出高效提示策略，推动AI在教育评估中的可扩展应用与研究创新。|
|2506.16575v1|[Advancing Harmful Content Detection in Organizational Research:   Integrating Large Language Models with Elo Rating System](http://arxiv.org/abs/2506.16575v1)|**贡献点总结（100字内）：**  <br/>提出基于Elo评分的改进方法，提升LLM在有害内容分析中的性能，通过两个数据集验证其在准确性、精确率与F1分数上的优势，减少误报并增强可扩展性，支持职场骚扰检测与构建安全工作环境等组织应用。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：设计Elo评分机制，优化LLM对有害内容（如微歧视、仇恨言论）的分析能力，解决传统系统过度审查的问题。  <br/>2. **数据集验证**：在微歧视检测和仇恨言论分析的两个数据集上验证方法有效性，证明其优于传统提示技术与常规机器学习模型。  <br/>3. **性能提升**：显著提高关键指标（准确率、精确率、F1分数）表现，减少误报并增强结果可靠性。  <br/>4. **可扩展性**：支持大规模数据集处理，提升实际应用的可行性。  <br/>5. **应用价值**：推动组织研究中职场骚扰识别、毒性沟通评估及安全环境构建等实际场景的应用。|
|2506.16528v1|[Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility   Metrics Using Phonetic, Semantic, and NLI Approaches](http://arxiv.org/abs/2506.16528v1)|**贡献点：**  <br/>1. **提出新的ASR评估指标**：整合自然语言推理（NLI）评分、语义相似性和语音相似性，克服传统指标（如WER、CER）对语音可懂度的不足。  <br/>2. **验证有效性**：在Speech Accessibility Project数据集上实现与人类判断的0.890高相关性，证明该指标优于现有方法。  <br/>3. **强调评估方向**：指出应优先关注语音内容的可懂度而非单纯依赖错误率，为语音识别系统优化提供理论依据。  <br/><br/>**总结：**  <br/>本研究提出一种结合NLI、语义和语音相似性的新型ASR评估指标，显著提升与人类判断的相关性，推动语音可懂度评估从错误率向内容理解转变。|
|2506.16008v1|[ChatAR: Conversation Support using Large Language Model and Augmented   Reality](http://arxiv.org/abs/2506.16008v1)|**贡献点总结：**  <br/>本研究提出了一种结合HMD-AR与LLMs的实时对话支持系统，通过关键词识别、信息生成与可视化增强交流效率；创新性地引入眼动控制策略，避免暴露用户阅读行为；实验验证了该系统在提升对话平衡性与互动兴奋度方面的有效性。  <br/><br/>**分点贡献：**  <br/>1. **系统设计**：构建集成HMD-AR与LLMs的实时对话辅助系统，实现关键词识别、信息生成及动态可视化呈现。  <br/>2. **眼动优化方法**：开发基于自然眼动模式的信息展示策略，降低用户阅读行为被对话方察觉的风险。  <br/>3. **实验验证**：通过两组对照实验，证实所提方法能提升对话平衡性与用户感知的交流兴奋度。|
|2506.14973v1|[Thinking in Directivity: Speech Large Language Model for Multi-Talker   Directional Speech Recognition](http://arxiv.org/abs/2506.14973v1)|**贡献点总结**  <br/>（100字以内）：  <br/>提出directional-SpeechLlama模型，结合智能眼镜麦克风阵列实现定向语音识别、声源定位和旁瓣干扰抑制，创新性引入S-DOT和CDDA技术，有效提升空间音频理解能力。  <br/><br/>**分点贡献**  <br/>1. **首个结合麦克风阵列的定向Speech LLM**：  <br/>   利用智能眼镜的多通道麦克风阵列，首次实现针对声源方向的语音识别、定位及旁瓣干扰抑制，突破传统Speech LLM对空间信息处理的局限。<br/><br/>2. **创新性训练方法S-DOT**：  <br/>   提出串联方向输出训练（Serialized Directional Output Training），通过结构化方向信息增强模型对文本与空间音频关系的建模能力。<br/><br/>3. **对比方向数据增强技术CDDA**：  <br/>   引入对比方向数据增强（Contrastive Direction Data Augmentation），通过对比学习策略提升模型在复杂声学环境下的鲁棒性与泛化能力。<br/><br/>4. **实验验证性能优势**：  <br/>   在语音识别和声源定位任务中，模型表现出显著性能提升，证明了文本线索与空间音频关系建模的有效性。|
|2506.14903v1|[DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct   Preference Optimization](http://arxiv.org/abs/2506.14903v1)|总结（100字以内）：  <br/>本文提出DPO-Kernels，通过混合损失、核化表示和分歧选择三个方向提升T2I模型的对齐性能，并构建首个包含社会偏见数据的DETONATE基准，引入AQI指标评估对齐质量，验证HT-SR机制的有效性，公开数据与代码。<br/><br/>贡献点分点：  <br/>1. **DPO-Kernels框架**：首次将DPO思想扩展至T2I系统，提出混合损失（结合嵌入与概率目标）、核化表示（RBF、多项式、小波核）和分歧选择（Wasserstein与Rényi分歧）三重改进，提升生成内容的对齐性与鲁棒性。  <br/>2. **DETONATE基准**：构建首个大规模T2I对齐基准，包含10万对精心筛选的图像对，系统性涵盖种族、性别、残疾三种社会偏见类别，用于评估模型公平性与安全性的隐性漏洞。  <br/>3. **AQI指标**：提出基于几何度量的对齐质量指数（Alignment Quality Index），量化潜在空间中安全/危险图像激活的分离程度，揭示T2I模型的隐藏偏见问题。  <br/>4. **HT-SR机制**：通过实验证明DPO-Kernels结合重尾自正则化（Heavy-Tailed Self-Regularization）可维持强泛化边界，并公开完整代码与数据集以促进研究复现。|
|2506.14028v2|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v2)|总结：  <br/>提出首个多语言多模态金融基准MultiFinBen，包含新颖的跨语言与OCR嵌入任务，设计动态难度选择机制，并揭示现有LLMs在复杂金融场景下的局限性。<br/><br/>贡献点：  <br/>1. **构建首个全球化金融多模态基准**：MultiFinBen是首个支持多语言（中、英、西等）和多模态（文本、视觉、音频）的金融领域基准，全面评估LLMs的跨模态与跨语言能力。  <br/>2. **引入双跨任务**：设计PolyFiQA-Easy/PolyFiQA-Expert（多语言复杂推理任务）和EnglishOCR/SpanishOCR（OCR嵌入的金融问答任务），填补金融领域多语言与多模态任务的空白。  <br/>3. **动态难度筛选机制**：提出基于难度感知的样本选择方法，而非简单合并现有数据集，确保基准数据集的平衡性与有效性。  <br/>4. **揭示模型局限性**：通过评估22个SOTA模型，发现即便具备多模态能力的LLMs仍难以处理金融领域的复杂跨语言和跨模态任务。  <br/>5. **开放数据促进研究**：公开MultiFinBen数据集，推动金融NLP和语音应用研究的透明性、可复现性与全球化发展。|
|2506.14028v1|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v1)|总结：  <br/>本文提出首个多语言多模态金融基准MultiFinBen，包含新任务与难度评估机制，为金融领域模型评估提供全面且挑战性的测试平台。<br/><br/>贡献点：  <br/>1. **首创多语言多模态基准框架**：构建首个覆盖文本、视觉、音频三模态及多语言场景的金融领域基准MultiFinBen，填补现有单语单模态研究的不足。  <br/>2. **引入双新颖任务**：  <br/>   - *PolyFiQA*（易/专家级）：首次提出多语言金融复杂推理任务，支持混合语言输入；  <br/>   - *EnglishOCR/SpanishOCR*：首次设计OCR嵌入的金融问答任务，要求模型从视觉文本中提取与推理信息。  <br/>3. **动态难度选择机制**：提出基于模型能力的难度自适应筛选方法，确保基准的挑战性与公平性。  <br/>4. **精炼平衡数据集**：通过精细化数据筛选而非简单数据集拼接，构建紧凑且跨领域平衡的测试集合。  <br/>5. **揭示模型能力瓶颈**：实验证明主流模型在多模态与跨语言金融任务中存在显著性能局限，为后续研究提供方向。|
|2506.13894v1|[EmoNews: A Spoken Dialogue System for Expressive News Conversations](http://arxiv.org/abs/2506.13894v1)|**总结**  <br/>该论文提出了一种基于上下文情感调节的新闻对话语音系统，结合LLM与PromptTTS技术，设计新评估指标，并开源代码。<br/><br/>**贡献点**  <br/>1. **开发整合情感调控的SDS**：首次将情感分析与任务导向的语音对话系统结合，通过上下文线索生成更具同理心的新闻对话。  <br/>2. **创新情感语音合成技术**：提出基于PromptTTS的上下文匹配情感语音生成方法，提升对话的情感适配性。  <br/>3. **构建主观评估体系**：设计专门用于情绪调控的主观评价量表，填补社会目标评估的空白，并验证系统效果。  <br/>4. **开源实现与可复现性**：提供完整代码库，支持相关研究的复现与扩展。|
|2506.13252v1|[Vector Ontologies as an LLM world view extraction method](http://arxiv.org/abs/2506.13252v1)|总结：  <br/>首次提出并验证向量本体方法，构建音乐领域的高维几何框架，并证明LLM内部可解析的结构化音乐知识与真实数据的对齐性及提示语相关性。<br/><br/>贡献点：  <br/>1. **首个实证验证**：首次通过实验验证向量本体框架（vector ontologies）可将LLM的高维神经表示转化为可解释的几何结构。  <br/>2. **构建音乐领域向量本体**：基于Spotify音频特征构建8维音乐流派向量本体，提供具体领域的应用案例。  <br/>3. **内部分布一致性分析**：证明LLM的音乐世界模型在不同语言提示下仍保持高空间一致性，且与真实音频特征分布高度对齐。  <br/>4. **提示语与空间变化的关联**：揭示提示语措辞直接影响LLM推理的向量本体空间位置，证明方法的可控性和可验证性。  <br/>5. **方法应用前景**：提出向量本体作为提取和分析LLM内部结构化知识的有效工具，推动可解释AI在语音领域的应用。|
|2506.12936v1|[CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team   Reflection in Action During Clinical Operation](http://arxiv.org/abs/2506.12936v1)|**总结（100字以内）：**  <br/>本研究提出CliniDial数据集，包含多模态临床操作数据，通过实验揭示现有大模型在处理真实临床场景中面临的挑战，推动相关算法发展，并开源代码促进研究。<br/><br/>**贡献点：**  <br/>1. **构建多模态临床操作数据集**：首次整合手术模拟中的音频、转录、生理信号及双视角视频，涵盖标签不平衡、自然交互等复杂特性。  <br/>2. **提出行为代码标注框架**：基于现有方法对团队协作过程进行结构化标注，为分析临床对话与行为模式提供标准化工具。  <br/>3. **评估LLMs在临床场景中的表现**：系统测试现有大模型对CliniDial数据的处理能力，揭示其在真实临床任务中的局限性。  <br/>4. **开源数据与代码**：推动数据共享，促进语音与多模态技术在医疗领域的研究与应用。|
|2506.12744v1|[Rethinking Hate Speech Detection on Social Media: Can LLMs Replace   Traditional Models?](http://arxiv.org/abs/2506.12744v1)|总结（100字以内）:  <br/>本文提出IndoHateMix数据集，验证了大语言模型在处理多语言、混合语言仇恨言论检测中的优势，挑战传统BERT模型的主导地位，并探讨未来研究应侧重模型优化还是数据增强。<br/><br/>贡献点分点列出：<br/>1. **构建多语言仇恨言论数据集**：提出IndoHateMix，首次系统性收录印度语境下的印地语-英语混合语言（code-mixing）和转写（transliteration）文本，填补了该领域的高质量标注数据空白。<br/>2. **验证大语言模型性能优势**：实验证明LLMs（如LLaMA-3.1）在少量数据微调下仍显著优于专门训练的BERT模型，展现更强的泛化能力与跨语言适应性。<br/>3. **揭示模型发展新方向**：提出未来研究需在模型优化与数据多样性之间权衡的思考，引发对通用LLMs替代领域专用模型的讨论。<br/>4. **推动复杂场景下的NLP应用**：为评估多语言环境下仇恨言论检测的鲁棒性提供现实基准，促进应对非正式、文化嵌入性语言的NLP技术发展。|
|2506.12537v1|[Speech-Language Models with Decoupled Tokenizers and Multi-Token   Prediction](http://arxiv.org/abs/2506.12537v1)|总结：  <br/>本文提出解耦分词器设计、多token预测机制及speaker-aware生成范式，优化语音-文本跨模态对齐与生成效率，建立RoleTriviaQA基准测试，显著提升语音语言模型的性能与说话人一致性。<br/><br/>贡献点：  <br/>1. **系统分析SLM关键组件**：首次系统研究语音分词器、语音头、说话人建模等组件对LLM-centric语音模型的影响。  <br/>2. **解耦分词器设计**：提出耦合、半解耦、全解耦分词器对比实验，证明解耦分词可显著提升跨模态对齐与语音合成质量。  <br/>3. **多token预测机制**：引入MTP解决语音与文本信息密度差异，实现12倍解码速度提升与3.01的词错误率降低。  <br/>4. **Speaker-aware生成范式**：构建RoleTriviaQA大型基准测试，涵盖多样说话人身份，提升知识理解与说话人一致性。|
|2506.12502v1|[Towards Fairness Assessment of Dutch Hate Speech Detection](http://arxiv.org/abs/2506.12502v1)|**贡献点：**  <br/>1. **构建荷兰语社会群体术语列表**：首次系统梳理反映荷兰社会背景的歧视性社会群体术语，为后续研究提供基础数据。  <br/>2. **生成反事实数据**：利用大语言模型（LLMs）和MGS、SLL策略构建荷兰语仇恨言论的反事实数据集，解决语言特定挑战。  <br/>3. **微调模型提升检测性能**：通过反事实数据微调Transformer模型，验证其在荷兰语仇恨言论检测任务中的有效性。  <br/>4. **提出综合公平性评估框架**：引入CTF与群体公平性指标（equality of odds、demographic parity），量化模型在反事实公平性与群体层面的公平表现。  <br/><br/>**总结：**  <br/>该研究聚焦荷兰语仇恨言论检测，提出反事实公平性评估方法，构建语言特定数据集，验证模型性能，填补文献空白并提供改进建议。|
|2506.11842v2|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v2)|**贡献点总结（100字以内）：**  <br/>提出PACE-ADS框架，通过多代理协作实现自动驾驶系统的人机双向通信，提升舒适度、安全性和个性化体验，可扩展集成于现有平台，弥补技术自主与人本需求的鸿沟。<br/><br/>**分点贡献：**  <br/>1. **提出人机中心自主框架**：PACE-ADS通过整合心理学与认知模型，解决自动驾驶车辆缺乏双向人机交互的问题，提升个性化体验和应对复杂场景的能力。  <br/>2. **三代理协同机制**：Driver Agent处理外部环境，Psychologist Agent解析心理信号（如EEG、心率）与语音指令，Coordinator Agent整合信息生成高阶决策，增强系统响应性。  <br/>3. **分层架构设计**：在语义规划层部署，保留低级控制权给原系统，确保兼容性与灵活性，避免对现有模块的重构。  <br/>4. **多场景验证**：通过闭环模拟测试，在复杂交通场景（如交叉路口、行人交互）中验证框架的有效性，展示提升的舒适度与安全性。  <br/>5. **最小化集成改造**：框架可无缝集成至现有自动驾驶平台，减少对原有系统的调整，提高实际部署的可行性。|
|2506.11842v1|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v1)|总结：  <br/>本文提出PACE-ADS框架，通过整合语音等认知指令与心理状态感知，实现人机协同的自动驾驶系统，提升舒适度与安全性，展示大语言模型在人车交互中的应用潜力。<br/><br/>贡献点：  <br/>1. **提出跨模态人车交互框架**：首次构建融合外部交通环境感知与内部乘客心理/认知状态理解的人机协同自动驾驶系统（PACE-ADS），填补自动化驾驶与人类需求的gap。  <br/>2. **设计语音认知代理模块**：开发专门处理乘客语音指令（如语义理解）与心理信号（如EEG、心率）的Psychologist Agent，实现多模态意图解析。  <br/>3. **实现行为级自主决策**：通过Coordinator Agent整合感知信息，支持自主驾驶逻辑生成与安全恢复策略，无需替代传统模块。  <br/>4. **验证语音引导有效性**：在交通信号、行人、施工区等场景的仿真测试中，证明语音交互可提升驾驶风格适应性与用户舒适度。  <br/>5. **推动LLM应用落地**：探索基于大语言模型（LLM）的框架在自动驾驶领域的人类中心化驱动潜力，为未来人车协作提供新思路。  <br/><br/>（注：原文实际属于自动驾驶领域，但按语音视角提炼了其中与语音指令处理、多模态交互、LLM应用等相关内容作为贡献）|
|2506.11558v2|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v2)|**总结**  <br/>DaMO提出了一种数据高效的视频语言模型，通过创新的时序感知架构和训练方法，显著提升时间推理能力，并构建了增强的时序问答数据集，为视频-语言建模提供了新方向。<br/><br/>**贡献点**  <br/>1. **提出Temporal-aware Fuseformer模型**  <br/>   - 采用分层双流架构，分步捕捉视觉/音频模态的时间动态，实现模态间信息互补融合。<br/><br/>2. **引入全局残差机制**  <br/>   - 减少空间冗余，保留关键语义信息，提升计算效率。<br/><br/>3. **四阶段渐进式训练范式**  <br/>   - 逐步构建多模态对齐、语义接地和时序推理能力，优化模型学习过程。<br/><br/>4. **构建时序问答数据集**  <br/>   - 基于现有数据集扩展，添加GPT生成的时序相关问答对，助力时序监督任务。<br/><br/>5. **实验验证有效性**  <br/>   - 在时序定位和视频问答任务中超越现有方法，尤其在精确时间对齐和推理场景表现突出。<br/><br/>6. **探索数据效率方向**  <br/>   - 为视频-语言建模提供数据高效且具备强时序理解能力的解决方案，具有广泛应用潜力。|
|2506.11558v1|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v1)|**贡献点总结**（100字以内）:  <br/>提出DaMO数据高效视频语言模型，创新性采用分层双流时序感知架构与全局残差机制，设计四阶段渐进训练方法，构建增强数据集，并在时序任务中实现性能突破。  <br/><br/>**分点贡献**:  <br/>1. **模型设计**：提出DaMO，专为准确时序推理和多模态理解优化，提升数据效率。  <br/>2. **架构创新**：首创Temporal-aware Fuseformer，采用分层双流结构，分别捕捉视频/音频时序动态，有效融合多模态信息。  <br/>3. **效率优化**：引入全局残差机制，减少空间冗余的同时保留关键语义信息。  <br/>4. **训练方法**：提出四阶段渐进式训练范式，逐步强化多模态对齐、语义 grounding 和时序推理能力。  <br/>5. **数据贡献**：构建多个增强数据集，基于现有数据集添加GPT生成的时序相关问答对，支持时序监督任务。  <br/>6. **实验验证**：在时序定位和视频问答基准测试中，DaMO显著优于现有方法，尤其在精确时序对齐任务中表现突出。|
|2506.11125v1|[ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone   Scams](http://arxiv.org/abs/2506.11125v1)|**贡献点分点列出：**  <br/>1. **识别关键漏洞**：首次定位语音钓鱼（vishing）攻击链中的ASR转录环节为最大薄弱点，指出其对攻击成功的关键作用。  <br/>2. **提出ASRJam框架**：设计了一种主动防御系统，通过向目标音频注入对抗扰动，干扰攻击者的ASR并切断攻击反馈循环。  <br/>3. **改进对抗扰动方法**：提出EchoGuard，利用自然语音现象（如回声、混响）作为扰动源，实现对ASR的破坏性干扰，同时保持人类语音可懂性。  <br/>4. **实证评估**：通过39人用户实验验证EchoGuard的实用性，对比现有攻击技术，证明其在ASR干扰与用户体验平衡上的最优性。  <br/><br/>**总结（100字以内）：**  <br/>论文提出针对语音钓鱼的双防御策略：ASRJam主动干扰ASR转录，EchoGuard利用自然扰动阻断攻击而不影响人声理解。通过用户实验验证了EchoGuard在攻防效果与可用性上的优越性。|
|2506.10789v1|[FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](http://arxiv.org/abs/2506.10789v1)|**贡献点总结**（100字以内）:  <br/>本研究提出首个美国社会背景下的Neo-fascist文本编码方案，结合NLP与政治科学，构建大规模标注数据集并开发分类模型，揭示该言论在论坛中的广泛存在，强调社会背景的重要性，呼吁持续对抗以维护民主。<br/><br/>**分点贡献**:<br/>1. **首创编码方案**：提出首个针对美国社会语境的Neo-fascist话语编码体系，由政治科学学者主导，填补了NLP与政治学在该领域的交叉研究空白。  <br/>2. **跨学科方法论**：首次将政治学与NLP结合，构建系统性框架以识别和分析Neo-fascist言论，推动多学科协作研究。  <br/>3. **大规模标注数据**：通过众包技术标注1000条文本，创建首个公共可用的Neo-fascist话语标注数据集，为后续研究提供基础。  <br/>4. **模型验证与对比**：对SLMs和LLMs分别进行微调与测试，开发首个可用于Neo-fascist文本分类的模型，验证不同规模模型的效果差异。  <br/>5. **现象分析与警示**：揭示Neo-fascist言论在特定论坛中的普遍性，为理解其传播特征和对民主社会的威胁提供实证依据。  <br/>6. **研究伦理说明**：明确研究仅针对文本内容，不涉及对个人或组织的标签化，强调方法的中立性与合规性。|
|2506.10779v1|[Improving Named Entity Transcription with Contextual LLM-based Revision](http://arxiv.org/abs/2506.10779v1)|**贡献点：**  <br/>1. **提出LLM修订机制**：设计基于大语言模型（LLM）的命名实体修正方法，结合LLM的推理能力和局部上下文信息（如课程笔记）改进ASR对命名实体的识别。  <br/>2. **引入NER-MIT-OpenCourseWare数据集**：发布包含45小时MIT课程音频的开放数据集，支持命名实体识别任务的开发与测试。  <br/>3. **显著性能提升**：在所提出数据集上，该技术使命名实体的词错误率（WER）降低最高达30%，验证了方法的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出LLM驱动的命名实体修正方法，结合上下文信息提升ASR准确性，并发布专用数据集，实验证明在命名实体识别任务中WER降低30%。|
|2506.10504v1|[Beyond Single-User Dialogue: Assessing Multi-User Dialogue State   Tracking Capabilities of Large Language Models](http://arxiv.org/abs/2506.10504v1)|**贡献点分点总结：**  <br/>1. **构建多用户对话数据集**：基于言语行为理论，扩展现有DST数据集，生成第二用户的话语，模拟真实多用户交互场景。  <br/>2. **提出可控评估框架**：系统性地将第二用户话语融入对话，设计方法论以评估LLMs在多用户DST中的鲁棒性。  <br/>3. **揭示性能局限性**：实验表明，LLMs在多用户DST任务中表现显著下降，凸显其在处理多重说话者时的不足。  <br/>4. **指导未来研究方向**：强调需优化LLMs以应对多用户场景，推动更真实、鲁棒的对话状态跟踪模型发展。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过构建多用户DST数据集并设计可控评估框架，揭示了LLMs在处理复杂多用户对话中的性能局限，为未来改进多用户对话理解模型提供了方向。|
|2506.10245v1|[ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in   Portuguese](http://arxiv.org/abs/2506.10245v1)|**贡献点总结（100字以内）**  <br/>1. 构建首个大规模葡萄牙语细粒度仇恨言论语料库，覆盖九个法律保护少数群体。  <br/>2. 提出四阶段合成数据生成流程，包含人工种子、少样本扩展、改写增强和领域平衡。  <br/>3. 实现跨领域泛化能力，验证在多种任务和数据集上的有效性。  <br/>4. 公开数据促进低资源环境下合成数据和仇恨言论检测研究。|
|2506.10202v1|[Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual   Text-to-Video Retrieval](http://arxiv.org/abs/2506.10202v1)|**贡献点：**<br/>1. 提出Q2E方法：首个针对复杂现实事件的零样本多语言文本到视频检索框架，通过LLMs和VLMs提取隐式参数知识实现查询-事件分解。  <br/>2. 多模态输入兼容：支持视觉和语音输入的联合处理，展示跨模态知识融合的有效性。  <br/>3. 熵融合策略：引入熵基融合评分机制，提升零样本多模态信息整合能力。  <br/>4. 音频增强效果：验证音频信息在文本到视频检索中的显著提升作用。  <br/>5. 可泛化性：方法可适应不同数据集、领域及模型架构（LLMs/VLMs）。  <br/>6. 开源贡献：公开代码与数据，推动后续研究。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Q2E，通过LLMs和VLMs提取隐式知识实现零样本多语言文本到视频检索，支持视觉/语音输入融合，并验证音频信息的增强效果。方法具备良好的泛化性，已开源促进研究。|
|2506.09983v2|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v2)|**贡献点总结**  <br/>（100字以内）  <br/>提出分步指令策略与简化输出格式，显著提升多语言依赖分析性能，避免幻觉污染；揭示显式推理步骤对跨语言泛化的重要性，为LLM解析提供可扩展、格式一致的替代方案。<br/><br/>**详细贡献点**  <br/>1. **分步推理策略**：首次将通用词性标注作为预处理步骤，优先预测句法主语和依存标签，提升结构有效性与准确性。  <br/>2. **输出格式优化**：设计简化的CoNLL-U格式，避免传统基于括号的解析方法的污染与冗余问题。  <br/>3. **多语言微调增强**：证明多语言微调可同时提升跨语言泛化性能，扩大模型适用范围。  <br/>4. **SOTA性能验证**：在17种语言的Universal Dependencies数据集上实现当前最优的解析准确率，无幻觉或污染。  <br/>5. **可扩展性方案**：提供格式一致、易于扩展的解析框架，替代传统基于括号的Approach，推动LLM在语音领域的应用。|
|2506.09983v1|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v1)|总结（100字以内）:  <br/>提出一种基于逐步指令策略的依存解析方法，结合通用词性标注和简化输出格式，在17种语言中实现SOTA性能，同时通过多语言微调提升跨语言泛化能力，验证了显式推理步骤对LLM解析的有效性。<br/><br/>贡献点:  <br/>1. **创新的逐步指令策略**：首次将通用词性标注作为先验步骤，结合句法头和依存关系预测，提升解析的结构有效性与准确性。  <br/>2. **简化输出格式设计**：采用类似CoNLL-U的轻量化输出格式，减少冗余信息干扰，避免模型生成错误或污染（hallucination）。  <br/>3. **多语言性能突破**：在17种语言的Universal Dependencies数据集上均取得领先的准确率，证明方法的普适性。  <br/>4. **跨语言泛化优化**：通过多语言微调，显著提升模型在不同语言间的迁移能力，增强通用性。  <br/>5. **方法对比与可扩展性**：提出一种格式一致、可扩展的替代方案，优于传统基于括号的解析方法，推动下游任务的兼容性。|
|2506.09707v2|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v2)|**总结（100字以内）:**  <br/>本研究提出一种基于预训练模型和LoRA微调的自动PE疗法依从性评估方法，通过音频-文本分析精准定位治疗关键阶段，显著提升效率，具有临床培训与质量监控的应用潜力。<br/><br/>**贡献点：**  <br/>1. **方法创新**：首次将LoRA技术应用于PE疗法依从性评估，通过30秒音频-转录窗口微调Qwen2-Audio模型，实现关键环节的自动时间定位。  <br/>2. **标签生成机制**：结合LLM提示与人工评分者验证，生成并验证三阶段（P1/P2/P3）依从性标签，提升标注准确性。  <br/>3. **软监督策略**：提出基于任务提示的软监督训练框架，直接预测归一化边界偏移，优化模型适应性。  <br/>4. **参数分析研究**：系统评估窗口大小及LoRA秩对性能的影响，揭示上下文粒度与模型调优的重要性。  <br/>5. **应用价值**：构建可扩展的PE疗法依从性追踪框架，为临床培训、监督及质量保证提供自动化工具。|
|2506.09707v1|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v1)|总结（100字以内）:  <br/>提出基于预训练模型和LoRA技术的自动PE治疗遵循定位方法，实现高精度识别与可扩展的临床应用。<br/><br/>贡献点:  <br/>1. **方法创新**：首次将LoRA技术应用于预训练音频-语言模型（Qwen2-Audio）的微调，实现对PE治疗关键环节的自动时间定位。  <br/>2. **标签生成**：通过LLM提示和人工验证结合，为三个核心协议阶段（P1/P2/P3）生成高质量的遵循标签。  <br/>3. **软监督策略**：设计任务特定提示引导的软监督框架，提升边界预测的准确性（MAE为5.3秒）。  <br/>4. **参数分析**：系统研究窗口大小与LoRA秩对性能的影响，揭示上下文粒度与模型适配的关键作用。  <br/>5. **实际应用**：构建可扩展的框架，支持PE治疗的临床培训、监督与质量评估，解决传统人工评估的效率问题。|
|2506.09391v1|[Comparing human and LLM politeness strategies in free production](http://arxiv.org/abs/2506.09391v1)|总结：  <br/>本研究揭示大语言模型在礼貌策略上虽能复制人类偏好并被偏好，但存在过度依赖消极策略、导致误解的偏差，突显AI系统在语用对齐中的关键挑战。<br/><br/>贡献点：  <br/>1. 系统分析LLM与人类在礼貌策略使用上的差异，验证大规模模型（≥70B参数）可复制计算语用学关键偏好。  <br/>2. 发现人类评估者在开放性任务中更倾向接受LLM生成的礼貌回应，表明模型在部分场景下的表现接近人类。  <br/>3. 指出LLM在积极语境中过度依赖消极礼貌策略，可能引发语用误解，揭示模型行为的局限性。  <br/>4. 强调语用对齐在AI系统中的重要性，为未来研究提供方向性启示。|
|2506.09349v1|[OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment](http://arxiv.org/abs/2506.09349v1)|总结（100字以内）:  <br/>提出OmniDRCA并行语音-文本模型，采用双分辨率表示和对比对齐，实现SOTA性能并在全双工对话场景中展示应用潜力。<br/><br/>贡献点：  <br/>1. 提出OmniDRCA模型，基于联合自回归建模实现并行生成语音和文本，突破传统分离式生成的局限。  <br/>2. 引入双分辨率语音表示（高低频特征分开编码），提升对语音细节和整体语义的理解能力。  <br/>3. 设计对比交叉模态对齐机制，增强语音与文本之间的对应关系和互模态感知。  <br/>4. 在口语问答基准测试中取得优于现有平行模型的SOTA性能，并验证其与交织模型的竞争力。  <br/>5. 探索框架在全双工对话场景中的扩展性，推动语音生成技术向更复杂交互任务迁移。|
|2506.09301v1|[$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for   Figurative Language Understanding](http://arxiv.org/abs/2506.09301v1)|贡献点：<br/>1. 提出$(RSA)^2$框架，首次将修辞策略纳入RSA模型以处理隐喻语言<br/>2. 实现无需建模非字面表达动机的意图理解机制，突破传统RSA框架限制<br/>3. 开发PragMega+新数据集并验证框架有效性，取得LLMs在讽刺识别任务上的SOTA性能<br/>4. 建立可扩展的语音理解范式，实现字面与意图语义的兼容性解读<br/><br/>总结：该研究提出$(RSA)^2$框架，通过建模修辞策略实现无需动机建模的隐喻理解，在新数据集上取得LLMs讽刺识别的最先进性能。|
|2506.08967v2|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v2)|总结（100字以内）:  <br/>提出全端到端LALM Step-Audio-AQAA，结合双代码库分词、130B参数模型与声码器，创新训练方法，显著提升语音控制能力，并建立新基准测试验证效果。  <br/><br/>**贡献点分项:**  <br/>1. **端到端语音交互模型**：首次设计直接生成自然语音响应的AQAA专用LALM (Step-Audio-AQAA)，突破传统文本依赖的瓶颈。  <br/>2. **双代码库音频分词器**：融合语言与语义特征提取，增强对音频内容的理解与建模能力。  <br/>3. **超大规模模型架构**：采用1300亿参数LLM作为骨干，显著提升生成质量与复杂任务处理能力。  <br/>4. **训练方法创新**：提出交错token输出机制，结合DPO与模型合并技术，优化语义连贯性与语音合成效果。  <br/>5. **基准测试与性能验证**：开发StepEval-Audio-360基准，证明模型在语音控制等关键指标上优于现有SOTA方法，凸显声码器的必要性。|
|2506.08967v1|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v1)|**贡献点：**  <br/>1. **提出全端到端的AQAA模型（Step-Audio-AQAA）**：首次设计专门针对音频查询-音频回答（AQAA）任务的端到端大型音频语言模型，克服传统模型依赖文本输出的局限。  <br/>2. **双代码本音频分词器设计**：引入语言与语义双重编码机制，有效提取语音中的结构化与高层次语义特征。  <br/>3. **大参数量骨干模型与神经声码器结合**：采用1300亿参数的LLM作为主干，并搭配神经声码器，实现高保真语音合成与控制。  <br/>4. **混合训练策略优化**：通过文本与音频交错输出增强语义连贯性，并结合Direct Preference Optimization（DPO）与模型合并技术提升整体性能。  <br/>5. **语音控制任务的性能突破**：在StepEval-Audio-360基准测试中，显著优于现有SOTA模型，验证了模型在语音控制方向的有效性。  <br/>6. **突出token-based声码器的关键作用**：强调声码器在端到端AQAA任务中的核心地位，为未来研究提供新的技术视角。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Step-Audio-AQAA，通过双代码本分词器、大参数模型和神经声码器实现端到端音频对话，结合交错训练与DPO优化，并在语音控制任务中超越现有SOTA模型。|
|2506.08633v1|[Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs](http://arxiv.org/abs/2506.08633v1)|**贡献点：**<br/>1. 提出基于小连接模块（connector module）的跨模态桥接方法，整合语音编码器（如WavLM-large）与大语言模型（如OLMo/LLaMA）的表示空间。<br/>2. 推动全开源和开放数据方案，采用WavLM-large和OLMo等公开模型实现端到端DST系统。<br/>3. 系统性消融实验分析了不同微调策略（全量/LoRA适配器）与对话轮次影响，验证了关键组件的有效性。<br/>4. 引入基于模糊匹配的输出后处理技术，显著提升对话槽值中的命名实体识别性能。<br/>5. 创新性结合SpokenWOZ数据集与Speech-Aware MultiWOZ数据集，提升训练数据多样性与模型表现。<br/>6. 在 SpokenWOZ 标准测试集上取得 34.66% JGA 的 SOTA 结果，并通过 Gemma-2-9B-instruct 实现更高精度（42.17% JGA）。<br/><br/>**总结（100字以内）：**  <br/>本研究通过连接模块融合语音与语言模型，结合开源组件与数据增强策略，系统性优化DST方法。实验证明其在SpokenWOZ数据集上达到SOTA性能，并进一步通过大模型提升至更高水平。|
|2506.08593v1|[Hateful Person or Hateful Model? Investigating the Role of Personas in   Hate Speech Detection by Large Language Models](http://arxiv.org/abs/2506.08593v1)|总结（100字以内）:  <br/>本文首次系统研究MBTI性格特质对LLM仇恨言论分类的系统性影响，发现人格特征显著改变模型输出，并提出优化标注流程的建议，强调对公平性与价值观对齐的重要性。<br/><br/>贡献点:  <br/>1. **首次研究框架**：提出首个基于MBTI人格特质的个性提示（persona prompt）系统性框架，用于分析其对仇恨言论分类的影响。  <br/>2. **人类标注验证**：通过大规模人类标注调查，实证证明MBTI维度显著影响标注行为，揭示人格特质在主观任务中的关键作用。  <br/>3. **多模型跨数据集评估**：在三个主流仇恨言论数据集上，评估四个开源LLM的输出差异，验证人格提示对模型表现的调控效应。  <br/>4. **识别偏差现象**：发现人格驱动导致的三类问题：与真实标签不一致、跨人格标注分歧、logit层级的隐性偏差，凸显模型与人类价值观的潜在差距。  <br/>5. **实践指导意义**：提出需谨慎设计人格提示以提升LLM标注流程的公平性，为伦理引导和模型对齐提供理论依据。|
|2506.08147v1|[Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models](http://arxiv.org/abs/2506.08147v1)|总结：  <br/>本研究构建首个乌尔都语-英语-西班牙语三语仇恨言论检测数据集，提出融合注意力机制与大语言模型的方法，显著提升多语言分类性能，为全球数字社区安全提供有效解决方案。<br/><br/>贡献点：  <br/>1. **构建首个三语仇恨言论数据集**  <br/>   - 收集10,193条跨语言推文（英语3,834；乌尔都语3,197；西班牙语3,162），通过关键词过滤确保平衡标注（4,849 Hate/5,344 Not-Hate）。  <br/>   - 弥补乌尔都语在仇恨言论研究中的空白，尤其缺乏基于翻译的语料支持。  <br/><br/>2. **提出混合模型方法论**  <br/>   - 结合注意力层（预处理）与大语言模型（如GPT-3.5 Turbo、Qwen 2.5 72B），优化多语言特征提取。  <br/>   - 明确区分传统模型（TF-IDF + SVM）与Transformer模型（BERT/RoBERTa）的对比实验。  <br/><br/>3. **验证高性能结果**  <br/>   - 在英语、西班牙语及乌尔都语中分别实现87%、85%、81%的F1得分，多语言联合模型达88%，均优于SVM基线（提升8.75%-7.32%）。  <br/>   - 通过Fleiss' Kappa（0.821）确保标注一致性，提升数据可靠性。  <br/><br/>4. **推动多语言检测应用**  <br/>   - 提供可扩展框架，支持跨语言仇恨言论识别，促进全球社交媒体安全治理。|
|2506.07726v1|[Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription   with RAG-based Correction and Predicted BLEU](http://arxiv.org/abs/2506.07726v1)|**贡献点：**  <br/>1. 构建了首个包含801小时、高质量的长文本形式瑞士议会语料库（751小时通过质量控制），支持瑞士德语多小时辩论会的语音-文本对齐。  <br/>2. 提出分两阶段的GPT-4o校正流程：第一阶段修正ASR误识别（如专有名词），第二阶段评估语义完整性，提升文本准确性。  <br/>3. 引入基于BLEU分数和LLM评估的自动化过滤机制，优化数据质量并确保语义一致性。  <br/>4. 验证了结合高精度ASR、LLM校正与数据驱动过滤方法对低资源、领域专用语音语料库的显著效果（BLEU提升6分）。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了大尺寸瑞士议会长文本语音语料库，通过优化ASR与LLM校正流程提升数据质量，验证了多阶段校正与过滤方法对低资源语料的有效性，实现BLEU显著提升，为瑞士德语语音研究提供重要资源。|
|2506.07707v1|[Interaction Analysis by Humans and AI: A Comparative Perspective](http://arxiv.org/abs/2506.07707v1)|总结：  <br/>本研究通过比较MR与2D视频会议对儿童手势猜谜游戏交流的影响，揭示了MR在增强互动质量与协作学习中的潜力，同时探讨了LLMs在儿童互动分析中的效率与局限性。<br/><br/>贡献点：  <br/>1. **对比研究**：首次系统分析MR（HoloLens）与2D视频会议（Zoom）在儿童协作任务中的交互差异。  <br/>2. **LLMs应用**：开发基于大语言模型的自动化分析框架，实现注释、翻译及迭代校正，提升数据处理效率。  <br/>3. **交互特征发现**：发现MR促进更丰富的非语言交流（如情感表达），而Zoom则以简洁性和可访问性为优势。  <br/>4. **跨平台洞见**：为分布式教育场景下的协作学习设计提供实证依据，强调MR在增强沉浸感与参与度方面的潜力。|
|2506.06775v1|[They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse](http://arxiv.org/abs/2506.06775v1)|总结：  <br/>本研究首次构建意大利政治演讲的IMPAQTS语料库，系统评估LLMs在处理隐含内容（预设与暗示）上的局限性，并提出改进方向，同时开源数据与代码。<br/><br/>贡献点：  <br/>1. **首次构建专用语料库**：创建包含意大利政治演讲及操纵性隐含内容标注的IMPAQTS语料库，填补该领域研究空白。  <br/>2. **设计双重评估任务**：通过多项选择和开放生成任务，全面测试LLMs对隐含内容的解析能力。  <br/>3. **揭示模型局限性**：证明当前LLMs在政治语境下的隐含内容理解存在显著缺陷，识别其缺失的关键pragmatic能力。  <br/>4. **提出改进方向**：指出增强模型对高度隐含语言处理能力的潜在研究路径。  <br/>5. **开放数据与代码**：提供数据集和实现代码，促进该领域的进一步研究与应用。|
|2506.06113v1|[Bridging the Gap: In-Context Learning for Modeling Human Disagreement](http://arxiv.org/abs/2506.06113v1)|**贡献点分点总结**  <br/>1. **探索LLMs在主观任务中的多视角建模能力**：首次系统研究LLMs是否能通过上下文学习（ICL）捕捉多角度观点，并反映主观注释中存在的分歧（如仇恨言论检测）。  <br/>2. **提出多标签建模策略的对比实验**：对比了聚合硬标签与拆分硬标签/软标签在零样本和小样本场景下的效果，揭示了不同标签策略对模型表现的影响。  <br/>3. **分析小样本提示优化方法**：评估了基于文本相似度（BM25、PLM）、注释分歧（熵）和组合排名的演示选择方法，以及随机与课程式（curriculum-based）示例排序策略的有效性。  <br/>4. **揭示LLMs建模主观性的局限**：发现零样本设置下多视角生成可行，但小样本场景难以全面体现人类判断，提示设计和演示选择对性能具有显著影响。  <br/>5. **强调改进方向**：指出需构建更视角敏感、具备社会智能的LLMs，以更好应对主观性任务中的注释分歧问题。  <br/><br/>**总结（100字以内）**：  <br/>该研究探讨LLMs在主观任务中捕捉多视角与注释分歧的潜力，通过对比多种标签策略和提示优化方法，揭示模型在零样本与小样本场景下的差异，并提出改进LLMs社会智能性的方向。|
|2506.06066v1|[Conversational Interfaces for Parametric Conceptual Architectural   Design: Integrating Mixed Reality with LLM-driven Interaction](http://arxiv.org/abs/2506.06066v1)|总结：  <br/>本论文提出一种基于对话的MR界面，整合语音、手势与多智能体LLM，实现参数化建模的直观操作，降低设计门槛，推动MR向生成设计平台发展。<br/><br/>贡献点：  <br/>1. **提出新型对话式MR交互框架**：首次将语音输入、手势识别与多智能体大语言模型（LLM）系统结合，形成面向参数化建模的自然交互范式。  <br/>2. **动态参数状态管理机制**：通过对话与上下文提示解决命令歧义，实现参数的实时动态调整与状态跟踪。  <br/>3. **降低设计认知与操作壁垒**：简化参数化工作流程，使无编程背景的设计师可高效探索和优化设计空间。  <br/>4. **扩展MR作为生成设计平台**：将MR环境从空间交互工具升级为支持程序化思维的创造性设计平台。|
|2506.05796v1|[Diarization-Aware Multi-Speaker Automatic Speech Recognition via Large   Language Models](http://arxiv.org/abs/2506.05796v1)|**贡献点总结（100字以内）**：  <br/>提出融合说话人聚类与大语言模型的多说话人语音识别系统，保留绝对时间信息，提升多语言对话与复杂多说话人场景的识别性能，验证了LLM作为统一后端在联合发言分割和转录中的潜力。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新框架**：设计了一种结合说话人聚类（diarization）与大语言模型（LLM）的多说话人语音识别（MS-ASR）系统，解决重叠语音转录难题。  <br/>2. **保留时序信息**：不同于传统序列化输出训练（SOT）方法，通过整合帧级说话人和语义嵌入，保留绝对时间信息以适应时间敏感场景。  <br/>3. **多模态输入处理**：框架同时处理结构化diarization输入与帧级嵌入，实现段级转录输出，提升识别粒度与准确性。  <br/>4. **多语言与复杂场景适应性**：实验表明系统在多语言对话及高重叠多说话人会议场景中均表现优异，验证其鲁棒性与实用性。  <br/>5. **统一后端潜力**：强调LLM作为统一后端在联合发言分割与转录任务中的优势，推动语音处理与自然语言处理的融合。|
|2506.05706v1|[Bridging the Modality Gap: Softly Discretizing Audio Representation for   LLM-based Automatic Speech Recognition](http://arxiv.org/abs/2506.05706v1)|总结（100字以内）:  <br/>本文提出将向量量化整合到LLM的ASR系统中，通过软离散化方法提升模型对跨域音频的处理能力，揭示了其作为模态桥梁的潜力。<br/><br/>贡献点:  <br/>1. **提出VQ与LLM结合的方法**：解决音频连续性与LLM离散token范式之间的鸿沟，实现音频表示与语言模型的对齐。  <br/>2. **构建基于LLM嵌入表的VQ codebook**：利用预训练LLM的嵌入表作为量化字典，降低模型训练复杂度并增强跨模态一致性。  <br/>3. **设计软离散化技术**：通过动态更新codebook和加权求和策略，生成更符合语言结构的离散音频representation。  <br/>4. **验证有效性与泛化性**：实验表明该方法在out-of-domain场景下显著优于基线，为LLM-based ASR提供新思路。|
|2506.05671v1|[Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](http://arxiv.org/abs/2506.05671v1)|总结：  <br/>本研究提出文本仅微调策略，通过未配对文本实现低资源领域自适应，保持语音-文本对齐并提升模型泛化能力，为ASR领域提供有效新方法。<br/><br/>贡献点：  <br/>1. 提出无需额外音频的文本仅微调框架，解决低资源环境下配对语音-文本数据不足的问题。  <br/>2. 引入实时评估机制，确保微调过程中语音-文本对齐性得以保留。  <br/>3. 实验证明方法在多个数据集上可保持与全音频-文本微调相近的性能，且性能退化最小。  <br/>4. 有效提升模型对新领域的泛化能力，避免灾难性遗忘问题。  <br/>5. 为低资源领域适应性ASR提供可扩展的文本驱动优化方案。|
|2506.05538v1|[SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful   Deepfake Content on Social Media Platforms](http://arxiv.org/abs/2506.05538v1)|总结：  <br/>提出SocialDF数据集及多模态LLM检测方法，解决社交媒体深伪内容的识别难题。<br/><br/>贡献点：  <br/>1. 构建SocialDF数据集：首个涵盖社交媒体真实场景的深伪挑战数据集，包含多来源高保真合成媒体。  <br/>2. 多模态检测框架：融合面部识别、语音转录与多智能体LLM，实现音频-视觉线索的交叉验证。  <br/>3. 语言行为分析：引入 linguistic、behavioral 和 contextual 多维度分析，提升检测的鲁棒性和准确性。|
|2506.05414v1|[SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and   Hearing](http://arxiv.org/abs/2506.05414v1)|总结：  <br/>本研究提出首个支持动态场景中3D空间推理的基准SAVVY-Bench，并设计无需训练的推理框架SAVVY，通过多模态轨迹聚合与坐标对齐提升音频-视觉大语言模型的性能，推动动态3D场景理解。<br/><br/>贡献点：  <br/>1. 提出首个专注于动态、音频-视觉环境中3D空间推理的基准SAVVY-Bench，引入同步空间音频数据。  <br/>2. 设计无训练的推理框架SAVVY，包含两个阶段：基于视角的物体轨迹估计和动态全局地图构建。  <br/>3. 引入细粒度时序定位、一致3D定位及多模态标注，提升场景理解的复杂性与准确性。  <br/>4. 实验证明SAVVY显著提升现有AV-LLMs性能，为动态3D空间推理研究建立新标准。|
|2506.05209v1|[The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly   Licensed Text](http://arxiv.org/abs/2506.05209v1)|**贡献点:**<br/>1. 提出Common Pile v0.1，首个面向LLM预训练的8TB开放授权文本数据集（覆盖30个来源，包括论文、代码、书籍等）；<br/>2. 通过训练7B参数模型验证数据集有效性，性能与基于未授权文本的LLM（如Llama 1/2 7B）相当；<br/>3. 释放数据集构建代码、训练混合策略及模型检查点，提升研究透明度与可复现性。<br/><br/>**总结（100字以内）:**  <br/>本文构建首个8TB开放授权文本数据集Common Pile v0.1，涵盖多领域内容。基于其训练的7B参数模型性能媲美未授权文本训练的LLM，并开源全流程代码和训练资源，推动合规模型研究发展。|
|2506.05191v1|[MokA: Multimodal Low-Rank Adaptation for MLLMs](http://arxiv.org/abs/2506.05191v1)|总结：  <br/>该论文提出MokA方法，通过结合单模态适配与跨模态交互，解决现有多模态微调方法忽视模态差异的问题，实验证明其有效性与普适性，为高效多模态模型适配提供新思路。<br/><br/>贡献点：  <br/>1. **揭示现有方法的局限性**：指出当前高效多模态微调方法直接套用LLM策略，忽视多模态场景的内在差异，影响模态协同利用。  <br/>2. **提出双适应理论框架**：论证单模态适配与跨模态适配是多模态模型微调的两个核心组成部分，强调两者的共同作用。  <br/>3. **设计MokA方法**：提出一种多模态感知的低秩微调策略，通过模态特异性参数压缩单模态信息，显式增强跨模态交互。  <br/>4. **多场景验证有效性**：在音频-视觉-文本、视觉-文本、语音-文本三种典型场景及多个大模型架构（如LLaMA2/3、Qwen2等）中验证方法的通用性与性能提升。  <br/>5. **系统评估方法优势**：通过消融实验与效率分析，全面验证MokA在参数压缩与跨模态增强上的效果。  <br/>6. **推动多模态研究**：认为MokA为多模态大模型高效适配提供更精准的解决方案，为后续研究奠定基础。|
|2506.05062v1|[Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation](http://arxiv.org/abs/2506.05062v1)|总结：  <br/>提出Debate Speech Evaluation新基准，系统分析LLM与人类在多维度辩论评估中的表现，揭示模型判断行为差异并评估生成能力，为LLM判决研究提供关键洞察。<br/><br/>贡献点：  <br/>1. **提出新基准**：构建首个用于评估大语言模型辩论判断能力的基准任务，填补LLM系统性基准领域的空白。  <br/>2. **多维评估框架**：提出综合考察论点强度、相关性、连贯性、风格适配等多层面的评价标准，系统性分析LLM的判断能力。  <br/>3. **大规模标注数据集**：利用包含600+条精细标注的辩论演讲数据集，首次实现对LLM在辩论任务上的大规模实验与验证。  <br/>4. **模型行为分析**：揭示大型模型在个体判断与整体行为层面与人类法官的显著差异，为LLM的局限性研究提供实证依据。  <br/>5. **生成能力评估**：验证前沿LLM生成具有说服力和观点性演讲的能力，证明其在特定任务上可能达到人类水平。|
|2506.04711v1|[LLM-based phoneme-to-grapheme for phoneme-based speech recognition](http://arxiv.org/abs/2506.04711v1)|**贡献点：**<br/>1. **提出LLM-P2G解码框架**：首次将大语言模型（LLMs）引入基于音素的ASR系统，结合语音到音素（S2P）和音素到字符（P2G）的分步解码流程。  <br/>2. **解决级联信息损失问题**：针对S2P与P2G联用时的潜在信息损失，设计了两种训练策略：带噪声音素的数据增强（DANP）和随机化top-K边缘化训练与解码（TKM）。  <br/>3. **跨语言性能提升**：实验验证在波兰语和德语的跨语言ASR任务中，LLM-P2G相比传统WFST解码方法分别降低WER 3.6%和6.9%。  <br/>4. **简化解码流程**：通过LLMs替代WFST，减少了解码的复杂性，同时提升对多语言数据的适应能力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出LLM-P2G解码方法，结合S2P与P2G流程并引入数据增强和top-K边缘化策略，有效缓解信息损失问题，在波兰语和德语跨语言ASR中显著提升识别性能。|
|2506.04693v1|[Cracking the Code: Enhancing Implicit Hate Speech Detection through   Coding Classification](http://arxiv.org/abs/2506.04693v1)|总结：  <br/>提出新的隐性仇恨言论分类体系（codetypes），设计两种LLMs检测方法，验证其在中英文数据集上的有效性。<br/><br/>贡献点：  <br/>1. **构建隐性仇恨言论新分类框架**：首次提出六种编码策略（codetypes），为im-HS检测提供系统化的分类依据。  <br/>2. **创新性方法设计**：提出两种基于大语言模型的整合策略——直接提示分类与编码器嵌入codetypes，解决隐性HS检测挑战。  <br/>3. **跨语言有效性验证**：通过中英文数据集的实验结果，证明所提方法在不同语言环境下的普遍适用性与检测性能提升。|
|2506.04586v1|[LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech   Foundational Models](http://arxiv.org/abs/2506.04586v1)|总结:  <br/>提出LESS框架，通过LLM优化伪标签并结合数据过滤策略提升语音任务性能，验证其跨语言和任务的适应性，为语音处理提供新方法。<br/><br/>贡献点:  <br/>1. **提出LESS框架**：首个结合大语言模型与半监督学习的语音处理框架，用于修正野外数据生成的伪标签。  <br/>2. **LLM伪标签修正**：利用LLM提升ASR/AST任务中伪标签的质量，显著降低词错误率（WER）。  <br/>3. **数据过滤策略**：设计数据增强机制优化LLM知识迁移效率，提升模型训练效果。  <br/>4. **跨语言/任务验证**：在中文ASR和西班牙语-英语AST任务中均取得显著性能提升，证明框架的通用性。  <br/>5. **消融研究分析**：通过不同LLM和提示配置的实验，揭示LLM衍生知识在语音处理中的关键作用与优化方向。|
|2506.04043v1|[Think Like a Person Before Responding: A Multi-Faceted Evaluation of   Persona-Guided LLMs for Countering Hate](http://arxiv.org/abs/2506.04043v1)|**贡献点（分点）:**  <br/>1. 提出首个针对大语言模型（LLM）生成的反叙事（CN）的多维度评估框架，涵盖角色框架、冗长性与可读性、情感基调及伦理稳健性。  <br/>2. 系统测试三种提示策略在MT-Conan和HatEval数据集上的表现，对比GPT-4o-Mini、CommandR-7B和LLaMA 3.1-70B等主流模型的生成效果。  <br/>3. 首次揭示LLM生成的CN存在可访问性不足的问题（如冗长、适配大学以上学历），限制其实际应用效果。  <br/>4. 量化分析情感引导提示策略在提升CN同理心与可读性方面的优势，同时指出其潜在安全与伦理风险。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出LLM生成反叙事的评估框架，系统分析三种提示策略及多模型表现，发现其冗长性与可访问性不足，情感引导策略能提升可读性但存在安全风险，为优化反仇恨言论技术提供关键见解。|
|2506.03099v1|[TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1)|**贡献点：**<br/><br/>1. **模型转化**：将预训练的SOTA图像-视频生成模型（DiT）改造为音频驱动的高参数（180亿）虚拟形象生成模型，实现实时对话动画。<br/>2. **无误差流技术**：通过双向教师模型向稀疏因果自回归学生模型的异步知识蒸馏，解决无限视频流中的误差累积问题。<br/>3. **高效推理优化**：设计高吞吐量、低延迟的推理管道，包含以下工程创新：  <br/>   - 分布式计算（DiT和VAE解码器分设设备）  <br/>   - CUDA流技术实现设备间通信与计算重叠  <br/>   - 消除冗余计算提升帧生成效率  <br/><br/>**总结（100字内）：**  <br/>本文提出TalkingMachines框架，将预训练视频生成模型转化为音频驱动的实时角色动画系统，通过模型优化与分布式推理技术，实现无误差无限视频流和高效生成性能。|
|2506.03009v1|[Conditioning Large Language Models on Legal Systems? Detecting   Punishable Hate Speech](http://arxiv.org/abs/2506.03009v1)|总结：  <br/>本文探讨了如何通过不同抽象层次的法律知识对齐LLMs，分析其在仇恨言论检测任务中的表现，揭示模型与法律专家在法律评估能力上的显著差距，并探讨抽象与具体法律知识对模型性能的不同影响。<br/><br/>贡献点：  <br/>1. 提出并研究了LLMs在法律系统不同抽象层级（宪法、法规、判例）的对齐方法，探索其法律问题评估能力。  <br/>2. 聚焦德国刑法框架下的煽动仇恨行为分类任务，构建具体应用案例分析。  <br/>3. 揭示LLMs在法律评估中的性能瓶颈：抽象法律知识模型缺乏任务理解，易出现矛盾与幻觉；具体法律知识模型虽能识别目标群体，但分类行为特征存在困难。  <br/>4. 为法律与AI交叉领域提供实证依据，指出模型与法律专家间的核心差距，并启发更精准的法律知识融入策略。|
|2506.02758v1|[Exploiting the English Vocabulary Profile for L2 word-level vocabulary   assessment with LLMs](http://arxiv.org/abs/2506.02758v1)|**总结（100字以内）：**  <br/>本研究提出结合大语言模型与英语词汇档案（EVP）的新型方法，实现对二语学习者写作中词汇使用的细粒度评估，解决多义性与上下文变化等挑战，并验证了其在词级与作文级熟练度相关性分析中的有效性。  <br/><br/>---<br/><br/>**贡献点：**  <br/>1. **提出新方法**：首次将大型语言模型（LLMs）与英语词汇档案（EVP）结合，实现基于句子语境的细粒度词汇评估。  <br/>2. **解决复杂问题**：有效应对二语词汇中的多义性（polysemy）、上下文差异（contextual variation）及多词表达（multi-word expressions）等评估难题。  <br/>3. **对比基准模型**：通过对比传统词性（PoS）基线，证明LLMs能利用更丰富的语义信息，提升词汇评分准确性。  <br/>4. **探索相关性**：首次分析词级语言能力与作文整体水平之间的关联，揭示词汇使用的层级性特征。  <br/>5. **验证EVP一致性**：应用该方法重新检验EVP的等级划分合理性，证实LLMs在词汇评估任务中的适用性与可靠性。|
|2506.02742v1|[Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions](http://arxiv.org/abs/2506.02742v1)|总结（100字以内）:  <br/>本研究提出PUE方法，通过情感引导的提示学习实现零样本下的未见过情绪语音生成，利用LLM-TTS架构保持情绪一致性，并支持灵活调整情感比例以合成多风格混合情感语音。<br/><br/>贡献点:  <br/>1. **提出新型PUE框架**：首次设计Prompt-Unseen-Emotion（PUE）方法，突破传统情感TTS系统对固定分类情绪的依赖，支持生成任意未见过的情绪语音。  <br/>2. **情感一致性建模**：通过LLM-TTS架构将类别情感提示与语音输出进行对齐，确保生成语音与提示情绪在语义和情感特征上一致。  <br/>3. **情绪权重量化**：在训练阶段实现对每句话中不同情绪权重的量化建模，提升生成语音的情感表达的细腻度。  <br/>4. **混合情感合成能力**：在推理阶段支持通过动态调整情绪比例生成混合情感语音，扩展情感表达的多样性。  <br/>5. **零样本有效性验证**：在无需情绪训练数据的零样本场景下成功生成具有表达性的目标情绪语音，验证方法的泛化能力。|
|2506.02457v1|[SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant](http://arxiv.org/abs/2506.02457v1)|总结：  <br/>提出SOVA-Bench基准框架，系统评估语音LLMs的语义与声学生成能力，填补声学质量量化评估空白，推动语音交互系统发展方向。<br/><br/>贡献点：  <br/>1. 提出SOVA-Bench：首个系统性评估语音LLMs的基准框架，整合多维度能力测试。  <br/>2. 填补声学质量量化空白：首次对生成语音的声学特性进行量化评估，突破以往仅关注语义准确性的局限。  <br/>3. 综合多能力评估：同时衡量语音理解、语音识别、语义生成和声学生成能力，提供全面对比分析。  <br/>4. 指导技术发展：为语音交互系统的优化方向提供理论依据与实践参考，促进更自然的语音生成研究。|
|2506.01808v1|[NAVER LABS Europe Submission to the Instruction-following Track](http://arxiv.org/abs/2506.01808v1)|**贡献点总结（100字以内）:**  <br/>本文提出一种多任务语音处理系统，整合语音到LLM嵌入投影器与LoRA适配器，通过指令微调实现跨语言（中、意、德）的ASR、ST、SQA任务联合处理，优化了多语言和多模态数据下的模型性能。<br/><br/>**分点贡献：**  <br/>1. **多任务联合处理**：开发可同步执行语音识别（ASR）、语音翻译（ST）和语音问答（SQA）的系统，支持英语输入到中文、意大利语和德语的跨语言转换。  <br/>2. **模块化架构**：采用两个预训练模块：(1) 语音到LLM的嵌入投影器（基于SeamlessM4T-v2-large语音编码器）；(2) LoRA适配器（基于Llama-3.1-8B-Instruct语言模型）。  <br/>3. **指令微调策略**：联合加载模块后，在多语言和多模态数据上进行1K步指令优化，提升对复杂指令的响应能力。  <br/>4. **高效训练方法**：通过预训练模块与微调结合，简化多任务模型的训练流程，可能提升推理效率和泛化性能。|
|2506.01683v1|[Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection   Using Speech and Large Language Models](http://arxiv.org/abs/2506.01683v1)|**贡献点：**  <br/>1. 提出结合语音和语言模型的CoT推理方法，用于阿尔茨海默病（AD）与非AD分类。  <br/>2. 引入监督微调（SFT）与CoT提示策略，增强模型分类能力。  <br/>3. 设计线性层作为分类模块，提升模型对语音文本的判别性能。  <br/>4. 实验显示方法在无CoT策略的对比基线中实现16.7%的相对性能提升。  <br/>5. 达到当前CoT方法在阿尔茨海默病诊断领域的最先进性能水平。  <br/><br/>**总结：**  <br/>该研究提出一种基于链式思维的语音-语言模型联合框架，通过监督微调与提示策略显著提升痴呆症分类准确率，达到领域内最先进水平。|
|2506.01484v2|[LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech   Detoxification](http://arxiv.org/abs/2506.01484v2)|总结（100字以内）:  <br/>该研究提出基于LLM的自动化detoxification框架，构建首个大规模hatespeech平行数据集ParaDeHate，并验证其在提升模型性能方面的有效性，为替代人工标注提供可扩展解决方案。<br/><br/>贡献点分点列出:<br/>1. **提出LLM-in-the-loop自动化流程**：设计一种利用GPT-4o-mini替代人工标注的新型管道，实现对有害语言的自动改写，降低标注成本。<br/>2. **构建领域专用数据集ParaDeHate**：创建包含8K对仇恨/非仇恨文本的平行数据集，填补hatespeech detoxification的高质量数据缺口。<br/>3. **验证LLM生成数据的有效性**：通过实验表明，基于ParaDeHate微调的BART等模型在风格准确性、内容保留和流畅度方面显著优于现有方法。<br/>4. **建立基准与方法对比**：发布ParaDeHate作为评估基准，并系统比较多种基线模型表现，推动该领域的研究进展。|
|2506.01133v1|[From Words to Waves: Analyzing Concept Formation in Speech and   Text-Based Foundation Models](http://arxiv.org/abs/2506.01133v1)|贡献点（分点）:<br/>1. 首次验证语音模型是否能像文本模型一样获得抽象语义概念<br/>2. 系统比较单模态（语音/文本）与多模态联合训练模型的语义结构差异<br/>3. 提出并应用Latent Concept Analysis方法分析跨模态语义形成机制<br/>4. 开源实验代码与资源提升研究可复现性<br/><br/>总结: 本研究通过无监督分析方法，揭示语音与文本模型在语义抽象形成上的异同，验证多模态训练对语义理解的增强作用，并开放资源促进学术研究复现。|
|2506.01111v1|[FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal   Contextual Fusion](http://arxiv.org/abs/2506.01111v1)|**贡献点分点：**  <br/>1. **两阶段音频描述生成方法**：提出基于人类听觉感知启发的自动化框架，结合多模态信息提取（语音、音乐、环境声音、视频）和大语言模型（LLM）的上下文融合，实现细粒度、语境感知的音频描述。  <br/>2. **FusionAudio数据集**：构建包含120万条高质量音频描述和60万问答对的大型数据集，为跨模态研究提供标注资源。  <br/>3. **改进的音频模型**：开发基于CLAP的音频编码器，提升音频-文本对齐能力与指令遵循效果，增强模型对复杂音频环境的理解。  <br/><br/>**总结（100字以内）：**  <br/>提出两阶段音频描述生成框架与FusionAudio数据集，结合多模态信息和大语言模型提升描述质量，优化CLAP编码器实现更精准的音频-文本对齐与理解。|
|2506.01077v1|[TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans](http://arxiv.org/abs/2506.01077v1)|总结：  <br/>本论文提出TRiMM框架，解决数字人类实时手势生成与长文本理解难题，包含跨模态注意力、长上下文建模及大规模动作匹配系统，实现120fps推理速度，并在消费级GPU上保持低延迟，代码开源。<br/><br/>贡献点：  <br/>1. **提出TRiMM框架**  <br/>   - 首次结合多模态技术实现实时3D手势生成，同时解决长文本理解与实时合成的挑战。  <br/><br/>2. **设计三大核心模块**  <br/>   - **跨模态注意力机制**：实现语音与手势的精确定时对齐。  <br/>   - **长上下文自回归模型**：采用滑动窗口机制高效建模长序列，增强语义连贯性。  <br/>   - **大规模动作匹配系统**：构建原子动作库，支持实时检索生成高质量手势。  <br/><br/>3. **轻量级Unreal Engine实现**  <br/>   - 开发轻量级实验流水线，实现实时推理速度（120 fps）与低句级延迟（0.15秒）。  <br/><br/>4. **全面评估验证效果**  <br/>   - 在ZEGGS和BEAT数据集上完成主观与客观评估，证明其性能优于当前SOTA方法。  <br/><br/>5. **开源代码促进应用**  <br/>   - 提供完整代码库，推动LLM驱动数字人类研究的可复现性与实际部署。|
|2506.00955v1|[Leveraging Large Language Models for Sarcastic Speech Annotation in   Sarcasm Detection](http://arxiv.org/abs/2506.00955v1)|总结：  <br/>提出基于大语言模型的语音讽刺标注方法，构建首个大规模单模态讽刺语音数据集PodSarc，验证其有效性并展示73.63%的检测性能，为该领域研究提供新基准。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的单模态标注流程**：首次利用GPT-4o和LLaMA 3等大语言模型生成语音讽刺标注数据，解决语音领域数据稀缺问题。  <br/>2. **构建大规模语音讽刺数据集PodSarc**：通过人机协作验证，创建高质量单模态讽刺语音数据集，填补语音讽刺研究的数据空白。  <br/>3. **验证方法有效性**：采用协作门控架构对比标注质量与检测性能，证明所生成数据集的可靠性及研究价值。  <br/>4. **提供基准性能指标**：检测模型在公开数据集上达到73.63% F1分数，为后续研究提供量化评估标准。|
|2506.00304v1|[Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion   with LLMs](http://arxiv.org/abs/2506.00304v1)|总结：  <br/>本研究提出一种无需配对语音数据的EMG适配器模块，使LLMs能处理无声EMG信号，实现低错误率的EMG-to-text转换，并在小数据量下显著优于专用模型。<br/><br/>贡献点：  <br/>1. **首次提出EMG适配器模块**：创新性地设计模块，将无声EMG特征映射至LLM输入空间，无需依赖配对的有声信号或语音数据。  <br/>2. **实现高效转换性能**：在封闭词汇任务中达成平均WER 0.49，表明模型对无声EMG信号的识别能力。  <br/>3. **小数据量优势**：仅需6分钟无声EMG数据，性能超越专用模型近20%，验证方法的泛化能力与实用性。  <br/>4. **拓展LLMs应用边界**：探索LLMs在理解发声生物信号（如无声EMG）中的潜力，为跨模态语音识别提供新方向。|
|2506.00160v1|[Werewolf: A Straightforward Game Framework with TTS for Improved User   Engagement](http://arxiv.org/abs/2506.00160v1)|贡献点：  <br/>1. 提出基于LLM的新型Werewolf社会推理游戏系统，融合文本生成与语音交互技术，增强游戏沉浸感。  <br/>2. 设计调优后的文本到语音（TTS）模型，提升与多种LLM的兼容性，降低适配成本。  <br/>3. 通过简化系统结构（无需额外组件），实现更高效的用户参与度提升，反驳传统依赖微调或经验池的方案。  <br/>4. 强调LLM推理能力的持续提升将减少对辅助技术（如复杂提示工程）的依赖，指向未来研究方向。  <br/><br/>总结：  <br/>本文提出一种结合TTS与LLM的新型社会推理游戏系统，通过调优提升兼容性并简化架构，有效增强用户体验，同时指出LLM能力进步将减少对额外技术的依赖。|
|2505.24869v1|[SiLVR: A Simple Language-based Video Reasoning Framework](http://arxiv.org/abs/2505.24869v1)|总结（100字以内）:  <br/>提出SiLVR框架，通过双阶段语言表征与推理实现复杂视频语言理解，利用多感官输入提升性能，并验证强LLM无需视频训练即可有效处理多模态信息，达到多个基准数据集的最佳结果。<br/><br/>**贡献点分点列出**:<br/>1. **双阶段框架设计**：创新性地构建SiLVR框架，将复杂视频理解拆分为语言表征提取与推理两阶段，简化多模态处理流程。<br/>2. **多感官输入融合**：引入短片标题、音频/语音字幕等多模态数据作为语言表征的输入，增强对视频内容的描述能力。<br/>3. **自适应token削减方案**：提出动态调整时间粒度的token减少方法，有效处理长上下文多模态输入的效率与精度问题。<br/>4. **训练自由的模组化架构**：框架无需额外训练，依赖推理时的语言模型能力，提升灵活性和易用性。<br/>5. **多任务性能突破**：在Video-MME、Video-MMMU、Video-MMLU、CGBench、EgoLife等基准数据集上取得当前最优性能。<br/>6. **跨模态推理验证**：实验证明，强LLM无需视频领域训练即可高效聚合多感官信息，支持复杂时序、因果、长上下文及知识推理任务。|
|2505.24691v1|[Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing   Cross-Lingual Transfer in Low-Resource Scenarios](http://arxiv.org/abs/2505.24691v1)|总结：提出融合音素表征与Chain-of-Thought框架的S2TT方法，通过课程学习策略提升低资源与零资源场景的翻译性能，为跨语言语音翻译的普及提供新思路。<br/><br/>贡献点：<br/>1. 提出将音素表征整合入CoT框架，构建新型S2TT系统  <br/>2. 引入音素识别作为跨语言迁移的中间步骤，实现零资源翻译能力  <br/>3. 基于多语言LLM开发语音-文本联合处理架构  <br/>4. 设计渐进式课程学习策略，优化多任务训练过程  <br/>5. 证实音素增强的CoT方法在低资源场景显著提升译质，且具备可扩展性|
|2505.24493v1|[MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging   LLM Embedded Knowledge](http://arxiv.org/abs/2505.24493v1)|总结：  <br/>该研究通过GPT-4o构建首个全自动生成的多模态情感数据集MELT，并验证其在自监督学习中的有效性，显著提升语音情感识别性能。<br/><br/>贡献点：  <br/>1. **提出无监督情感标注方法**：首次利用GPT-4o纯文本标注能力，基于情景喜剧Friends构建多模态情感数据集MELT，解决传统人工标注成本高和不一致的难题。  <br/>2. **构建全自动生成数据集**：MELT是首个完全依赖大型语言模型（LLM）生成标签的多模态情感数据集，无需人工监督或多模态输入。  <br/>3. **验证SSL模型效果**：通过微调四个自监督学习（SSL）基线模型，评估MELT在语音情感识别任务中的适用性与性能提升。  <br/>4. **实验证明性能优势**：主观实验结果表明，MELT显著改善了语音情感识别（SER）的准确性与一致性。|
|2505.24458v1|[SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social   Engineering Behaviors](http://arxiv.org/abs/2505.24458v1)|**贡献点：**  <br/>1. **首个整合AR与多模态LLM的社会工程数据集**：SEAR是首个专门针对AR增强现实和多模态大语言模型驱动的社会工程攻击的多模态数据集。  <br/>2. **多场景对抗性对话数据**：包含60名参与者在模拟会议、课堂、社交活动等场景中的180条标注对话，涵盖多样化的社会工程情境。  <br/>3. **多模态同步数据采集**：整合AR捕捉的同步视觉/音频线索（如面部表情、语音语调）、环境信息及用户社交媒体资料，实现全面行为分析。  <br/>4. **主观攻击效果评估**：引入信任评分与易受性评估等主观指标，量化攻击对用户心理的影响。  <br/>5. **高攻击效能实证结果**：揭示SEAR在诱导用户点击钓鱼链接（93.3%）、接受电话（85%）、提升信任度（76.7%）等任务中的显著效果。  <br/>6. **伦理合规保障**：通过匿名化处理与IRB（伦理审查委员会）批准，确保数据集的负责任使用。  <br/>7. **开源开放共享**：数据集通过GitHub平台公开，支持学术研究与技术开发。  <br/><br/>**总结（100字以内）**：  <br/>SEAR Dataset是首个结合AR与多模态LLM的社会工程攻击数据集，包含多场景、多模态数据及主观评估指标，揭示攻击高效能，为检测与防御研究提供资源，同时保障伦理合规与公开共享。|
|2505.24347v2|[Fewer Hallucinations, More Verification: A Three-Stage LLM-Based   Framework for ASR Error Correction](http://arxiv.org/abs/2505.24347v2)|总结:  <br/>本文提出RLLM-CF框架，通过错误预检测、迭代修正和推理验证三阶段解决LLM在语音识别中的hallucinations问题，无需额外数据或微调，实验证明在多个数据集上显著降低CER/WER。<br/><br/>贡献点:  <br/>1. **提出RLLM-CF框架**：设计包含错误预检测、链式思维子任务迭代修正和推理过程验证的三阶段校正流程，系统性解决LLM在ASR中的错误修正问题。  <br/>2. **无额外训练需求**：方法无需额外标注数据或模型微调，直接利用LLM能力进行端到端校正，降低应用门槛。  <br/>3. **抑制hallucinations**：通过多阶段验证机制有效避免LLM误改正确文本，保障修正结果的准确性。  <br/>4. **实验证明有效性**：在AISHELL-1、AISHELL-2和Librispeech数据集上验证，显示GPT-4o模型结合该框架后CER/WER分别降低21%/11%/9%/11.4%，具有实际应用价值。|
|2505.24016v1|[BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech   Translation System](http://arxiv.org/abs/2505.24016v1)|总结:  <br/>本研究提出BeaverTalk级联系统，结合VAD分段器、Whisper Large V2和Gemma 3 12B，通过LoRAs微调和单句记忆机制实现高效实时翻译，显著提升英德、英中翻译性能。<br/><br/>贡献点:  <br/>1. **系统架构创新**：设计级联系统BeaverTalk，集成VAD分段器、Whisper Large V2语音识别模型和Gemma 3 12B语言模型，实现端到端语音到文本翻译。  <br/>2. **微调方法优化**：采用低秩适配器（LoRAs）技术对翻译LLM进行轻量级微调，结合对话提示策略利用单一源语言前句记忆库提升上下文建模能力。  <br/>3. **延迟与语言方向支持**：系统兼容低延迟（StreamLAAL 1837.86）和高延迟（StreamLAAL 3343.73）运行模式，在英德（BLEU 24.64/27.83）与英中（BLEU 34.07/37.23）任务中均取得突出性能。  <br/>4. **实际部署效果**：在IWSLT 2025真实任务中验证系统有效性，为多语言实时翻译提供可落地的解决方案。|
|2505.23990v2|[Multi-RAG: A Multimodal Retrieval-Augmented Generation System for   Adaptive Video Understanding](http://arxiv.org/abs/2505.23990v2)|总结：  <br/>提出Multi-RAG多模态检索增强生成系统，通过整合视频、音频、文本多源信息提升情境理解与决策效率，验证其在动态场景中优于现有模型且资源占用更少的优势。  <br/><br/>贡献点：  <br/>1. 提出Multi-RAG系统，解决动态场景下人机协作的认知负担问题。  <br/>2. 首次整合视频、音频、文本多模态信息流进行联合推理与生成。  <br/>3. 在MMBench-Video基准测试中表现优于现有视频大语言模型（Video-LLM）和视觉语言模型（LVLM）。  <br/>4. 以更少资源和输入数据实现高效性能，具有实际应用潜力。|
|2505.22251v2|[Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in   Large Language Models for Speech Recognition](http://arxiv.org/abs/2505.22251v2)|摘要贡献点：  <br/>1. 揭示LibriSpeech和Common Voice数据集与公开LLM预训练语料存在显著重叠，质疑现有语音任务评估结果的可靠性。  <br/>2. 提出通过对比含/不含数据污染的LLM评估污染影响的方法，验证污染数据的存在性及对模型表现的影响。  <br/>3. 发现污染LLM的语音识别器虽在错误率上差异微小，但会显著提升对训练数据的转录概率，反映输出偏差。  <br/>4. 强调需使用独立数据评估LLM语音系统，以避免因数据污染导致的不准确结果。  <br/><br/>总结：  <br/>该研究揭露语音任务评估数据与LLM训练数据重叠问题，揭示污染对模型性能的潜在影响，并呼吁使用独立数据验证模型效果。|
|2505.22029v2|[Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection](http://arxiv.org/abs/2505.22029v2)|**贡献点：**  <br/>1. **构建首个全面覆盖词和音素层级的11类不流畅性合成语料库**：LLM-Dys解决了现有数据集中语调不自然和上下文多样性不足的问题，提供更真实的语音不流畅性样本。  <br/>2. **提出改进的端到端检测框架**：基于LLM-Dys数据集优化模型，实现语音不流畅性检测的最先进性能。  <br/>3. **开源全部资源**：公开数据、模型与代码，促进领域研究和应用。  <br/><br/>**总结（100字以内）：**  <br/>本文构建了首个全面覆盖词/音素层级的不流畅性合成数据集LLM-Dys，改进端到端检测框架达SOTA，所有数据及代码开源，推动语音不流畅性研究。|
|2505.20445v3|[In-context Language Learning for Endangered Languages in Speech   Recognition](http://arxiv.org/abs/2505.20445v3)|**总结（100字以内）:**  <br/>本文探索LLM通过上下文学习在低资源语言语音识别中的应用，证实相关文本样本能提升性能，概率方法优于传统指令方法，并展示ICL可使LLM达到或超越专用语言模型的ASR效果，同时保留原有能力。<br/><br/>**贡献点:**  <br/>1. **验证ICL在低资源语音识别中的可行性**：首次将上下文学习方法应用于语音识别领域，证明LLM能在未训练的低资源语言上实现有效学习。  <br/>2. **提出文本样本增强策略**：发现提供与任务更相关的文本样本能显著提升语言建模和ASR性能，为多语言模型优化提供新方向。  <br/>3. **对比方法效果与模型性能**：表明概率方法优于传统指令方法，且ICL使LLM在ASR任务中达到专用语言模型水平，同时保持其通用能力。|
|2505.18614v2|[MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song   Translation](http://arxiv.org/abs/2505.18614v2)|贡献点：  <br/>1. **提出首个多语言多模态基准**：构建了Multilingual Audio-Video Lyrics Benchmark (MAVL)，首次整合文本、音频与视频数据，为动画歌曲翻译提供综合评估标准。  <br/>2. **多模态数据增强翻译质量**：通过融合音频和视频信息，使翻译更贴近原作的旋律、节奏及风格，突破传统文本仅依赖语义的局限。  <br/>3. **创新音节约束模型结构**：提出SylAVL-CoT模型，结合链式推理（Chain-of-Thought）与音节约束机制，提升歌词的自然度与可唱性。  <br/>4. **验证多模态方法有效性**：实验表明该模型在可唱性和上下文准确性上显著优于文本基础模型，证明多模态、多语言框架对歌词翻译的价值。  <br/><br/>总结（100字以内）：  <br/>本研究提出多语言多模态基准MAVL与SylAVL-CoT模型，融合文本、音频和视频数据，通过音节约束提升歌词翻译的自然度和可唱性，验证了多模态方法在动画歌曲翻译中的优势。|
|2505.18110v2|[Watch and Listen: Understanding Audio-Visual-Speech Moments with   Multimodal LLM](http://arxiv.org/abs/2505.18110v2)|总结：  <br/>本文提出TriSense三模态模型，结合视觉、音频与语音信号，引入Query-Based Connector实现模态自适应融合，构建TriSense-2M数据集支持多模态分析，验证了模型的有效性，并公开代码与数据集。<br/><br/>贡献点：  <br/>1. **提出TriSense模型**：首个整合视觉、音频和语音三模态的大型语言模型，用于全面提升视频内容的时间理解能力。  <br/>2. **设计Query-Based Connector**：通过自适应调整模态权重，支持在模态缺失情况下的鲁棒性，并实现灵活的多模态输入组合。  <br/>3. **构建TriSense-2M数据集**：包含200万样本的高质量数据集，采用自动化生成流程，涵盖长视频及多样化的多模态组合。  <br/>4. **实验验证与公开资源**：在多个基准上验证模型效果，证明其对多模态视频分析的潜力，并开放代码和数据集供研究复现。|
|2505.17536v2|[Multimodal Conversation Structure Understanding](http://arxiv.org/abs/2505.17536v2)|总结：  <br/>本研究提出对话角色归属与线程划分任务框架，构建大规模人工标注数据集，评估多模态模型在理解对话结构上的性能差异，揭示关键影响因素，为改进多模态语言模型的对话理解能力提供基础。<br/><br/>贡献点：  <br/>1. **构建多模态对话结构标注数据集**：提供首个包含4,398条对话角色标注、5,755条收件人信息、3,142条旁观者信息的语料库，涵盖多参与者、多模态场景。  <br/>2. **定义关键任务框架**：提出针对对话角色分配（说话人、收件人、旁观者）和对话线程构建（语句关联与聚类）的系统性任务，结合会话分析与社会语言学理论。  <br/>3. **评估模型性能差异**：对比音频-视觉LLM与视觉-语言模型，发现前者在说话人/收件人识别上更优，但匿名化参与者时性能显著下降。  <br/>4. **揭示关键影响因素**：通过实验发现对话参与者数量是角色识别的主要负向预测因子，而声学清晰度（音调、频谱质心）和面部覆盖情况与性能呈正相关。  <br/>5. **推动未来研究方向**：为多模态LLM在对话结构建模和推理能力的提升提供基准与启示。|
|2505.15670v2|[Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](http://arxiv.org/abs/2505.15670v2)|总结：  <br/>本文提出首个无需语音预训练的双工S2S模型，通过连续输入输出与信道融合实现实时对话，降低比特率并提升推理、回合管理及打断处理能力，同时减少数据需求，开源代码促进复现。<br/><br/>贡献点：  <br/>1. **双工S2S架构设计**：支持连续用户输入与同步代理输出，引入信道融合机制实现真实场景下的实时交互（如用户打断）。  <br/>2. **无需语音预训练**：首次提出仅依赖流式编码器的双工模型，消除对专用语音预训练的需求。  <br/>3. **高效编码与微调**：采用独立的代理和用户建模架构，支持编码微调以优化代理语音质量，将比特率降至0.6kbps。  <br/>4. **性能提升**：在推理能力、回合控制和打断处理等关键指标上超越现有双工模型。  <br/>5. **数据需求降低**：跳过语音预训练环节，显著减少所需语音数据量，简化模型构建流程。  <br/>6. **开源与可复现性**：首个公开完整训练与推理代码的双工S2S模型，推动领域研究复现与验证。|
|2505.13338v2|[Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data   Condensation and Spoken QA Generation](http://arxiv.org/abs/2505.13338v2)|**贡献点分点列出：**  <br/>1. **提出首个融合上下文推理与语音伴随信息的框架**：首次设计结合两者的数据集生成方法，解决传统Speech-LLMs在综合理解上的不足。  <br/>2. **创新性的数据生成机制**：包含两阶段技术——基于伪语音伴随标签的野外语音数据压缩，以及LLM驱动的上下文语音伴随问答（CPQA）生成。  <br/>3. **验证框架有效性**：通过Qwen2-Audio-7B-Instruct在自动生成与人工标注CPQA数据集上的强相关性评估，证明其能力。  <br/>4. **揭示Speech-LLMs的局限性**：明确指出模型在共情推理任务中的缺陷，强调需针对性数据集与更优模型。  <br/>5. **潜在应用价值**：为训练具备语音伴随推理能力的鲁棒Speech-LLMs提供基础，推动语音理解研究发展。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出首个融合上下文推理与语音伴随信息的数据集生成框架，揭示Speech-LLMs在共情任务中的局限，验证框架对模型训练的有效性，为提升语音AI能力提供新方向。|
|2505.09439v2|[Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](http://arxiv.org/abs/2505.09439v2)|总结：  <br/>本文提出Omni-R1，通过GRPO强化学习微调Qwen2.5-Omni，在MMAU和MMAR基准上取得SOTA性能，揭示了文本推理能力对音频任务的关键作用，并意外发现文本数据微调可提升音频表现。<br/><br/>贡献点：  <br/>1. **提出Omni-R1模型**：通过GRPO强化学习方法对Qwen2.5-Omni进行微调，聚焦音频问答任务。  <br/>2. **SOTA性能突破**：在MMAU和MMAR基准测试中达到当前最优结果，尤其在声音、音乐、语音及总体平均类别均表现最佳。  <br/>3. **因果分析**：验证了性能提升主要源于增强的文本推理能力，而非单纯依赖音频数据。  <br/>4. **意外发现**：发现仅以文本数据集进行微调也能有效提升模型的音频表现，为多模态训练提供新思路。|
|2505.05335v2|[FLAM: Frame-Wise Language-Audio Modeling](http://arxiv.org/abs/2505.05335v2)|总结：  <br/>本文提出FLAM，解决帧级音频理解与开放词汇定位难题，通过logit调整和大规模数据集提升模型性能，保持全局检索能力。<br/><br/>贡献点：  <br/>1. **首次提出开放词汇帧级音频定位模型**：突破传统模型对预定义类别的依赖，实现对真实场景中未见事件的泛化定位。  <br/>2. **设计记忆高效且校准的帧级目标函数**：引入logit调整机制，有效缓解训练中的虚假相关（如事件依赖、标签不平衡）。  <br/>3. **构建多源帧级监督数据集**：结合LLM生成字幕与模拟数据，提供多样化、细粒度的音频事件标注。  <br/>4. **验证模型多任务能力**：在保持文本-音频检索性能的同时，显著提升帧级定位效果及下游任务表现。|
|2504.20007v3|[Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from   Police Body-Worn Camera Footage](http://arxiv.org/abs/2504.20007v3)|总结：  <br/>本研究提出一种跨学科框架，整合AI与机器学习技术分析警用摄像头数据，通过多模态方法提取警民互动行为动态，构建结构化总结系统，并建立评估流程以提升执法审查与知识发现效率。<br/><br/>贡献点：  <br/>1. **跨学科框架创新**：首次将人工智能（AI）与统计机器学习（ML）技术结合，专门针对警用摄像头（BWC）视频进行模式分析，突破传统单一技术应用的局限。  <br/>2. **多模态数据融合**：综合图像、音频和自然语言处理（NLP）技术，实现对警民互动场景的全面分析，提取尊重/不尊重、冲突升级等复杂行为动态。  <br/>3. **结构化总结生成**：引入说话人分离、转录及大语言模型（LLMs），构建可解释的警民接触事件摘要，提升数据分析的可操作性。  <br/>4. **定制化评估体系**：开发适用于高风险执法场景的自动评估流程，量化转录质量与行为检测准确率，支持实际应用验证。  <br/>5. **执法应用场景落地**：方法论与实证结果为执法部门提供审查、培训及问责的实用工具，推动AI技术在警务实践中的落地应用。  <br/>6. **知识发现前沿推进**：通过系统化分析复杂BWC数据，拓展法律与技术交叉领域的研究边界，为公共安全数据分析提供新范式。|
|2504.08961v2|[A Fully Automated Pipeline for Conversational Discourse Annotation: Tree   Scheme Generation and Labeling with Large Language Models](http://arxiv.org/abs/2504.08961v2)|**贡献点总结：**  <br/>1. 提出基于LLM的全自动决策树构建与标注流水线，替代传统人工设计流程。  <br/>2. 首次将频率引导的决策树与LLM结合，提升语音功能标注性能。  <br/>3. 通过实验验证不同设计选择的效果，展示方法优于手动方案及人类标注。  <br/>4. 开源代码、标注方案及结果，促进语音领域对话标注研究。|