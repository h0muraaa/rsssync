|Source|Title|Summary|
|---|---|---|
|2511.16901v1|[R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios](https://arxiv.org/abs/2511.16901v1)||
|2511.16660v1|[Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660v1)|总结（100字以内）：<br/>该研究构建了认知科学视角下的LLM推理分类体系，揭示人机推理差异及模型局限性，提出跨模态评估框架并开发引导性推理方法，为提升模型认知能力与理解人类思维机制提供了新路径。<br/><br/>贡献点：<br/>1. 提出首个基于认知科学的28要素推理分类体系，涵盖计算限制、元认知控制、知识表征等关键维度<br/>2. 构建大规模跨模态（文本/视觉/音频）推理行为分析框架，公开170K模型痕迹与54个人类think-aloud数据集<br/>3. 首次系统比较人机推理结构差异：人类采用层级嵌套与元认知监控，模型依赖浅层前向链式推理<br/>4. 发现研究社区对元认知能力的关注不足（仅8%），而该能力与推理成功率显著相关<br/>5. 开发测试时推理引导方法，通过结构化构建提升复杂问题处理性能达60%<br/>6. 建立认知机制与LLM研究的跨学科桥梁，推动基于原则性认知机制的模型发展|
|2511.16544v2|[WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue](https://arxiv.org/abs/2511.16544v2)||
|2511.16544v1|[WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue](https://arxiv.org/abs/2511.16544v1)|总结（100字以内）：<br/>本研究挑战传统ASR评估方法，提出基于临床专家判断的黄金标准，开发LLM-as-a-Judge框架，实现与人类专家相当的临床影响评估，推动ASR从文本准确性向临床安全性评估的转型。<br/><br/>贡献点：<br/>1. **挑战传统评估体系**：质疑WER等传统指标对临床对话ASR错误评估的适用性，揭示其与临床影响标签相关性差。<br/>2. **构建黄金标准基准**：通过临床专家对真实与ASR生成对话的标注，建立首个反映临床影响的评估标准。<br/>3. **提出LLM-as-a-Judge框架**：首次将大语言模型优化为临床评估者，通过GEPA算法实现与专家一致的评估性能（90%准确率，κ=0.816）。<br/>4. **实现自动化安全评估**：构建可扩展的自动化评估框架，推动ASR从文本准确度向临床对话安全性评估的范式转变。|
|2511.16345v1|[NLP Datasets for Idiom and Figurative Language Tasks](https://arxiv.org/abs/2511.16345v1)|总结（100字以内）：  <br/>本文构建了两个新型数据集，涵盖潜在和确定的隐喻表达，通过整合现有数据并结合大规模语料库，支持预训练模型在隐喻识别任务中的评估与训练，为提升LLMs对修辞语言的理解提供了重要资源。<br/><br/>贡献点分点：  <br/>1. **提出新型数据集**：构建了包含潜在（大规模语料）和确定（人工标注）隐喻表达的两个数据集，用于系统性评估预训练语言模型的隐喻识别能力。  <br/>2. **整合现有资源**：通过合并近期隐喻相关数据集，生成综合的隐喻列表，并利用其从大型语料库中提取上下文语句。  <br/>3. **多样化分类**：提供多种隐喻类别，为构建新模型和探索新方法提供基准与训练材料。  <br/>4. **后处理兼容性**：对数据集进行模型无关的预处理，确保其可应用于不同模型的训练和评估任务。  <br/>5. **任务适配性**：数据集支持槽位标注和序列标记等任务，拓展了隐喻研究在NLP中的应用范围。  <br/>6. **推动研究方向**：通过提供高质量数据，弥补LLMs处理隐喻的不足，为改进模型性能提供数据支持。|
|2511.15552v2|[Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552v2)|**贡献点：**  <br/>1. **构建首个俄语多模态基准**：提出Mera Multi框架，填补俄语多模态评估体系的空白，针对俄语语音架构进行系统性测试。  <br/>2. **多样化任务设计**：涵盖文本、图像、音频、视频四类模态，开发18个新任务，适用于通用模型和模态专用模型（如图像-文本、视频-文本、音频-文本）。  <br/>3. **文化适配性数据集**：完全从头构建数据集，注重俄语文化与语言特性，统一提示模板与评估指标，确保评价的严谨性与针对性。  <br/>4. **防泄露方法论**：提出水印技术和私有数据集许可机制，防止基准测试数据泄露，保障模型训练的合规性与安全性。  <br/>5. **通用性迁移**：方法论可复制至其他语言（尤其是斯拉夫语系），为构建多语言多模态基准提供标准化模板。|
|2511.15552v1|[Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552v1)|**贡献点：**  <br/>1. **提出首个俄语多模态基准**：构建面向俄语的开放多模态评估框架Mera Multi，填补俄语领域多模态基准的空白。  <br/>2. **18项新任务设计**：涵盖文本、图像、音频、视频等模态，针对通用模型和模态特异性架构（如图像-文本、视频-文本、音频-文本）开发任务。  <br/>3. **统一评估体系**：创建符合俄语文化与语言特性的全新开源数据集，采用统一提示和度量标准。  <br/>4. **防泄漏方法**：引入水印技术和许可证管理，确保私有数据集的完整性与保密性。  <br/>5. **通用方法论**：提供可复制的框架，支持未来在其他语言（尤其是斯拉夫语系）中构建多模态基准。  <br/><br/>**总结（100字内）：**  <br/>本研究提出首个俄语多模态评估框架Mera Multi，构建18项跨模态任务及符合语言特性的数据集，设计防泄漏机制，并为其他语言的基准开发提供可复制方法，推动多模态模型的跨语言研究。|
|2511.15296v1|[Detection of spiking motifs of arbitrary length in neural activity using bounded synaptic delays](https://arxiv.org/abs/2511.15296v1)|1. **提出基于子motif的神经网络结构**：设计了一种利用多个输出神经元和有界突触延迟的序列检测机制，通过子motif的逐步识别实现任意长度motif的检测，突破了传统单输出神经元中突触延迟受限的瓶颈。  <br/>2. **验证方法的有效性**：在Spiking Heidelberg Digits (SHD)数据库（音频转脉冲数据）和随机同时motif场景中，成功验证了网络对任意长度motif的识别能力。  <br/>3. **噪声鲁棒性表现**：在10个同时motif的测试中达到60%的正确率，在5个motif时提升至80%，证明了方法对噪声的抗干扰能力。  <br/>4. **优化重叠motif的识别**：在随机重叠模式中显示，输入神经元数量多且motif稀疏时，可更高效识别单个motif，揭示了方法在复杂场景下的适应性。  <br/>5. **为通用神经信息模型提供基础**：为存储和检索任意时序长度的神经信息提供了理论框架，拓展了脉冲神经网络在语音处理等领域的应用潜力。|
|2511.14688v1|[Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688v1)|**贡献点：**  <br/>1. 提出利用大语言模型（LLMs）生成历史文本（16-20世纪法语、1900-1950中文）的合成标注数据，解决低资源语料标注不足问题。  <br/>2. 通过微调spaCy模型，在历史文本的POS标注、词形还原和实体识别任务中实现显著性能提升。  <br/>3. 证明领域特定模型与合成数据结合的有效性，为计算人文学科提供可扩展的NLP工具改进方案。  <br/><br/>**总结（100字内）：**  <br/>本文通过LLMs生成历史文本标注数据，微调spaCy模型，提升低资源语料的NLP任务性能，验证合成数据与领域模型结合对计算人文学科研究的价值。|
|2511.14582v1|[OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models](https://arxiv.org/abs/2511.14582v1)|总结：  <br/>OmniZip提出训练免费的音频引导多模态token压缩框架，通过动态音频锚点识别和交错时空压缩，实现推理加速和内存减少，同时保持性能。<br/><br/>贡献点：  <br/>1. **首个训练免费框架**：OmniZip无需训练即可优化音频-视频token表示，降低部署成本。  <br/>2. **音频引导多模态压缩**：基于音频token的显著性动态指导视频token剪枝，提升跨模态信息保留。  <br/>3. **动态音频保留评分**：通过计算时间组信息密度，实现视频token的高效选择性压缩。  <br/>4. **交错时空压缩方案**：针对每个时间窗口设计spatio-temporal压缩策略，平衡时空信息保留。  <br/>5. **显著性能提升**：在保持模型性能的前提下，实现3.42倍推理加速和1.4倍内存减少。|
|2511.14445v1|[Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445v1)|贡献点列表：<br/>1. 提出Tell Me系统，整合RAG助手、合成对话生成器和Well-being AI crew三个核心组件，实现个性化、上下文感知的可访问性心理健康支持<br/>2. 首创基于客户档案的合成对话生成技术，解决专业治疗数据隐私限制问题<br/>3. 开发动态自适应的自我护理工作流，通过AI crew生成定制化周计划和冥想音频<br/>4. 构建包含自动LLM评估和人类用户研究的双重验证体系<br/>5. 强调AI作为专业治疗的补充工具，而非替代品的系统设计原则<br/>6. 推动NLP与心理健康领域的跨学科合作，探索负责任的人机交互创新路径<br/><br/>总结（99字）：论文提出Tell Me系统，整合RAG助手和合成对话生成技术，开发动态自适应自我护理工作流，创新性解决心理咨询数据隐私问题并验证系统效果，推动NLP与心理健康领域的跨学科协作。|
|2511.14255v1|[AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255v1)|**贡献点分点总结：**  <br/>1. **首个非洲多语言评估框架**：提出AfriSpeech-MultiBench，填补非洲英语口音缺乏标准化评测体系的空白，涵盖100+非洲英语口音及10+国家。  <br/>2. **覆盖多领域场景**：构建涵盖金融、法律、医疗等7大应用领域的基准测试，全面评估语音技术在实际场景中的适用性。  <br/>3. **多模型类型对比**：系统评测开源、闭源、单模态ASR与多模态LLM，揭示不同模型在语音识别任务中的性能差异。  <br/>4. **系统性性能分析**：发现开源模型在自发语音中表现优异但抗噪性差，多模态LLM更鲁棒但领域实体识别能力不足，专有模型因国家/领域差异显著。  <br/>5. **微调模型优势**：验证针对非洲英语微调的模型在准确性与低延迟上的平衡，为实际部署提供实践指导。  <br/>6. **推动包容性应用**：通过释放综合基准，助力开发者适配非洲本地化需求，促进欠发达地区的语音技术公平应用。  <br/><br/>**总结（100字内）**：  <br/>提出AfriSpeech-MultiBench，填补非洲语音评测空白，涵盖多领域与多模型对比，揭示性能差异，推动公平的语音技术应用。|
|2511.14249v1|[Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249v1)|总结（100字以内）:  <br/>本文提出Authentic-Dubber模型，通过构建多模态参考库、情感相似性检索增强和逐步图生成机制，模拟真实导演与演员协作流程，显著提升情感表达，经V2C Animation数据集验证有效。  <br/><br/>贡献点：  <br/>1. **引入多模态参考素材库**：整合LLM与视觉信号，模拟导演提供的上下文学习材料，强化情感语义理解。  <br/>2. **情感相似性检索增强策略**：精准匹配静音视频与参考素材的情感特征，提升语义一致性与表演覆盖度。  <br/>3. **渐进图神经网络生成框架**：分阶段融合检索到的情感知识，模拟演员逐步内化指令的配音过程，增强表达自然性。|
|2511.14143v1|[SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM](https://arxiv.org/abs/2511.14143v1)|总结：  <br/>本文提出SMART框架，通过整合音频信息、引入镜头级时序结构及优化提示设计，显著提升视频时刻检索性能，尤其在复杂视频场景中的表现优于现有方法。<br/><br/>贡献点：  <br/>1. **提出Shot-aware Multimodal Audio-enhanced Retrieval框架**：首次将音频特征与镜头级时序结构结合，突破传统方法依赖单一视觉模态的局限。  <br/>2. **创新性Shot-aware Token Compression技术**：在每镜头内选择性保留高信息量token，减少冗余并保留细粒度时间细节，提升表示效率。  <br/>3. **优化多模态提示设计**：改进提示工程策略，更高效地融合音频与视觉线索，增强模型对复杂查询的理解能力。  <br/>4. **实验证明有效性**：在Charades-STA和QVHighlights数据集上验证，R1@0.5和R1@0.7指标分别提升1.61%和2.59%，显著优于现有方法。|
|2511.14119v1|[Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services](https://arxiv.org/abs/2511.14119v1)|总结：  <br/>TeleEMS通过多模态融合与边缘部署，提升应急医疗预到达分析能力，实现症状识别、生命体征监测与医疗决策推荐，推动智能EMS系统发展。<br/><br/>贡献点：  <br/>1. **系统架构创新**：提出TeleEMS双端架构（客户端+边缘服务器），支持多设备协同与高效实时视频流传输。  <br/>2. **多模态分析模块**：融合音频-文本-视频数据，集成EMSLlama（症状提取）、rPPG（心率估计）、PreNet（多任务预测）三大模块。  <br/>3. **领域专用模型**：EMSLlama在急诊症状识别上超越GPT-4o（准确率0.89 vs. 0.57），提升鲁棒性。  <br/>4. **边缘部署优化**：EMSStream实现低延迟多方视频流，降低中心化依赖，适应高并发应急场景。  <br/>5. **预到达智能决策**：通过融合文本与生命体征数据，生成可靠医疗干预建议，缩短响应时间。|
|2511.13505v1|[Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505v1)|总结（100字以内）:  <br/>提出基于LLMs的公共叙事自动化分析框架，通过专家协作开发的编码本验证其近人表现，扩展分析至大规模数据集并应用于政治演讲，展示计算方法在公民叙事研究中的潜力，同时指出局限与未来研究方向。<br/><br/>贡献点：  <br/>1. **提出首个LLMs驱动的公共叙事自动化标注框架**，解决主观解读与专家标注成本高的问题。  <br/>2. **联合专家开发定制化编码本**，系统评估LLMs标注效能，达到0.80平均F1分（接近人类专家）。  <br/>3. **扩展分析至22个故事数据集**，揭示PN框架元素在更广泛语料中的表现模式。  <br/>4. **将方法应用于政治演讲研究**，建立分析政治修辞的新视角，推动计算公民叙事学发展。  <br/>5. **识别LLMs的局限性**，为未来计算社会科学研究提供改进方向与实证基础。|
|2511.13238v1|[Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238v1)|总结（100字以内）:  <br/>本文首次系统综述CT-IPE算法，分类四个方法家族，提供方法比较框架，并为研究者提供实用指导，强调结果差异的重要性及基准测试需求。<br/><br/>贡献点：  <br/>1. **系统性归纳**：首次对过去二十年CT-IPE算法发展进行结构化综述，梳理不同方法之间的关联与演进路径。  <br/>2. **实践指导**：提炼算法选择的权衡因素（透明度、技术需求、验证策略），为应用研究提供可操作的决策依据。  <br/>3. **方法论创新**：提出概念框架，从文本方差的生成、捕获与聚合角度分类算法，并强调结果差异的分析价值，推动系统性基准测试。|
|2511.13225v1|[Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225v1)|贡献点：<br/>1. 构建4k+孤立词语音数据集，包含同步的频谱图与波形<br/>2. 设计基于音素编辑距离的多选测试任务，评估VLM对语音视觉表征的理解<br/>3. 发现零样本与微调模型均表现欠佳，证明需特定参数知识解析语音图谱|
|2511.13159v1|[Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159v1)|**总结（100字以内）：**  <br/>本研究首次提出并公开了一个包含20000行的Bangla语料库，用于区分重复性歧义（拼写错误 vs 语法重叠），验证了模型微调的有效性，为语义保留的文本规范化系统提供基础数据。<br/><br/>**贡献点分列：**  <br/>1. **首个公开Bangla语料库**：构建了首个20,000行手动标注的Bangla ASR语料库，明确区分重复性歧义的两种类型（Repetition Disfluency vs. Morphological Reduplication）。  <br/>2. **基准测试方法创新**：首次系统评估多语言LLMs与任务特定编码器模型在Bangla文本规范化任务中的表现。  <br/>3. **模型性能对比**：发现语言特定模型（如BanglaBERT）通过微调在准确率（84.78%）和F1分数（0.677）上显著优于LLMs。  <br/>4. **语义保留系统基础**：为Bangla的语义保持文本规范化系统提供关键数据支持，推动低资源语言的语音处理研究。|
|2511.12690v1|[Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690v1)|总结（100字以内）：  <br/>提出一种直接语音到语音翻译系统，结合自监督预训练、离散语音单元和合成平行数据，显著提升低资源语言对（如波斯语-英语）的翻译性能，验证了该方法在数据稀缺场景下的有效性。<br/><br/>贡献点分点列出：  <br/>1. **构建新型合成平行语料**：首创通过大语言模型翻译波斯语转录文本生成英语目标文本，再结合零样本TTS系统合成语音，将平行数据量提升约6倍。  <br/>2. **提出直接S2ST系统架构**：设计包含Conformer编码器、因果Transformer解码器及基于单元的神经声码器的端到端模型，实现语音到语音的端对端翻译。  <br/>3. **自监督预训练优化**：利用预训练的Conformer编码器提升模型对低资源数据的适应性，减少对平行语音数据的依赖。  <br/>4. **离散语音单元翻译策略**：采用因果Transformer解码器生成离散目标语音单元，优化翻译过程的时序性和准确性。  <br/>5. **低资源场景效果验证**：在波斯语-英语数据集上实验证明，结合合成数据的方法比直接基线模型在ASR BLEU指标上提升4.6分。|
|2511.12452v1|[DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452v1)|总结（100字以内）：  <br/>本文提出音频驱动的DenseAnnotate平台，通过语音同步标注实现高效密集注释，构建了包含多语言、跨文化及3D资产的高质量数据集，显著提升了多语言、文化对齐和3D空间能力，为视觉-语言模型研究提供新方法和数据支持。<br/><br/>贡献点：  <br/>1. **提出音频驱动注释平台**：首次构建基于语音的在线标注系统，解决传统文本标注效率低、表达受限、难以捕捉细节的问题，特别适配多文化与3D场景的复杂注释需求。  <br/>2. **创建大规模多模态数据集**：收集3,531张图像、898个3D场景及7,460个3D物体的密集注释，覆盖20种语言，包含8,746张图像描述、2,000个场景描述和19,000个物体描述。  <br/>3. **验证数据集有效性**：通过实验证明，基于该数据集训练的模型在多语言能力、文化对齐度和3D空间理解上分别提升5%、47%和54%，凸显其在任务导向训练中的价值。  <br/>4. **通用性与跨领域应用**：平台设计适用于多种视觉-语言任务及多样化数据类型，为未来研究提供可扩展的注释框架，推动高密度多模态数据的生成与应用。|
|2511.12255v1|[Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255v1)|贡献点总结（100字以内）:  <br/>Fusionista2.0通过优化关键帧提取、OCR与语音识别模块，显著提升视频检索效率（时间减少75%），同时引入轻量级视觉-语言模型和人性化UI设计，兼顾准确性与用户体验，成为适用于大规模视频搜索的高效友好系统。  <br/><br/>具体贡献点：  <br/>1. **高效核心技术模块**：采用ffmpeg、Vintern-1B-v3.5和faster-whisper，分别优化关键帧提取、多语言OCR与实时语音识别，提升处理速度。  <br/>2. **轻量级模型应用**：在问答模块中使用轻量级视觉-语言模型，平衡响应速度与性能成本。  <br/>3. **用户友好界面设计**：重构UI以增强交互性、可访问性与工作流效率，降低非专业用户使用门槛。  <br/>4. **性能与体验提升**：实验证明系统在检索速度、准确性和用户满意度上均优于前代，验证其竞争力。|
|2511.12077v1|[Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077v1)|总结：  <br/>提出VAEmotionLLM框架，通过视觉引导音频对齐与轻量级跨模态情感适配器，减少对大规模音频预训练的依赖，提升艺术领域跨模态情感理解能力，并构建ArtEmoBenchmark基准验证效果。<br/><br/>贡献点：  <br/>1. **首次构建艺术中心情感基准**：提出ArtEmoBenchmark，全面评估音频、视觉及跨模态情感理解能力。  <br/>2. **两阶段跨模态情感框架**：设计Vision-Guided Audio Alignment（Stage 1）与Cross-Modal Emotion Adapter（Stage 2），实现以视觉引导音频学习和情感增强。  <br/>3. **减少音频预训练需求**：通过冻结视觉路径与音视频对齐，仅需有限音频数据即可赋予VLM“听觉”能力。  <br/>4. **轻量级情感适配器**：引入Emotion Enhancer与Emotion Supervisor，通过情感敏感残差和监督机制提升跨模态情感理解效率。  <br/>5. **验证组件有效性**：消融实验表明各模块互补，模型在ArtEmoBenchmark上达到SOTA性能。|
|2511.12052v1|[Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential](https://arxiv.org/abs/2511.12052v1)|总结：  <br/>该论文通过科学计量分析，揭示了AI驱动隐写术领域的研究热点、地域分布及与SDGs的关联性，指出当前研究存在SDGs对齐不足的问题，强调其跨学科潜力与全球AI安全挑战。<br/><br/>贡献点：  <br/>1. **方法创新**：首次采用主题建模方法对AI驱动的隐写术数据隐藏技术进行科学计量分析。  <br/>2. **数据覆盖**：系统梳理2017-2023年间654篇相关文献，揭示亚洲国家（中国、印度）主导研究趋势。  <br/>3. **主题分类**：识别出七个核心主题集群，涵盖图像、语音、视频隐写术及深度学习应用等多领域。  <br/>4. **SDGs关联评估**：首次探讨AI隐写术与可持续发展目标（SDGs）的跨学科关联，发现仅18篇符合SDGs框架。  <br/>5. **趋势分析**：解析AI隐写术研究趋势，强调深度学习发展、东亚研究活跃度及基础方法成熟度的影响。  <br/>6. **社会影响揭示**：指出研究在社会对齐和全球AI安全挑战中的局限性，为未来多学科融合发展提供方向。|
|2511.11108v1|[Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108v1)|**贡献点总结（100字以内）**  <br/>提出针对美总统辩论中个人攻击的分析框架，构建2016-2024年跨周期人工标注数据集，对比微调transformer模型与通用LLM的检测效果，证实任务适配可提升模型对政治语境中有害语言的理解能力。  <br/><br/>**分点贡献**  <br/>1. **提出分析框架**：设计用于分析政治辩论中个人攻击的系统性方法。  <br/>2. **构建标注数据集**：手动标注涵盖2016、2020、2024三届选举的辩论转录文本，为研究提供实证基础。  <br/>3. **模型性能对比**：评估微调的transformer模型与通用LLM在检测个人攻击任务中的效能差异。  <br/>4. **任务适配价值**：验证任务特定模型适配对理解政治传播中隐含意图的重要性。|
|2511.10287v1|[OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models](https://arxiv.org/abs/2511.10287v1)|总结：  <br/>本文提出首个全面覆盖多模态内容安全的测试平台OutSafe-Bench，创新性地设计MCRS指标与FairScore框架，系统评估九个MLLMs的安全性漏洞，推动安全基准的完善。<br/><br/>贡献点：  <br/>1. **首个多模态安全基准**：构建包含文本、图像、音频、视频的综合性测试集（18,000+中英文提示、4,500图像、450音频/视频），覆盖九类关键安全风险。  <br/>2. **多维交叉风险指标（MCRS）**：提出新量化方法，评估多模态内容间风险的重叠与关联性。  <br/>3. **FairScore框架**：设计可解释的多审稿人加权聚合系统，动态选择高表现模型作为陪审团，降低单模型偏见提升评估可靠性。  <br/>4. **实证分析**：系统检验九个先进MLLMs的安全性缺陷，揭示多模态模型在安全事实推理中的显著漏洞。|
|2511.10059v1|[When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?](https://arxiv.org/abs/2511.10059v1)|总结：  <br/>提出AV-ConfuseBench基准测试，揭示MLLMs在音频-视觉混淆场景中的短板，设计RL-CoMM模型通过引入音频参考模型和置信度优化策略显著提升任务准确性。<br/><br/>贡献点：  <br/>1. **提出新型基准AV-ConfuseBench**：模拟"音频-视觉混淆"场景，通过修改视频中对象的音频信号（如静音）评估MLLMs对不存在音频的识别能力。  <br/>2. **揭示MLLMs的视觉主导缺陷**：发现现有模型（如Qwen2.5-Omni、Gemini 2.5）易受视觉信息干扰，难以准确捕捉非存在的音频特征。  <br/>3. **设计RL-CoMM协作框架**：基于Qwen2.5-Omni构建多模型协作系统，引入外部音频语言模型生成音频-only推理，并通过Step-wise Reward函数优化跨模态推理。  <br/>4. **提出答案中心置信度优化**：通过降低多模型推理差异的不确定性，提升答案预测的准确性和一致性。  <br/>5. **验证显著性能提升**：在少样本条件下，RL-CoMM在音频-视觉问答与幻觉检测任务中实现10-30%的精度提升。|
|2511.10045v2|[Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism](https://arxiv.org/abs/2511.10045v2)|**贡献点总结**（100字以内）  <br/>本研究提出了LEX-ICON多模态语音象征数据集，通过音素级注意力分析揭示MLLMs在语音象征任务中的跨语言一致性及聚焦于具象音素的机制，首次实现对语音象征在AI模型中的大规模量化研究，连接语言学与人工智能领域。<br/><br/>**分点贡献**  <br/>1. **构建首个大规模多模态语音象征数据集LEX-ICON**  <br/>   - 包含8,052个自然语言词汇和2,930个伪词，覆盖英语、法语、日语、韩语四国语言，标注25个语义维度。<br/><br/>2. **提出基于音素注意力分数的量化分析方法**  <br/>   - 通过测量音素级别的注意力权重，系统研究MLLMs在文本与音频输入中的语音象征处理机制。<br/><br/>3. **揭示MLLMs的语音象征直觉与语言学理论的契合**  <br/>   - 发现模型在多个语义维度（如尖锐 vs. 圆润）的预测结果与语言学研究的非任意关联假设一致。<br/><br/>4. **识别音义相关的注意力模式**  <br/>   - 支持模型通过关注特定具象音素（如尖音/m/、圆音/p/）来捕捉语音象征特征的证据。<br/><br/>5. **首次实现多模态语言模型语音象征的可解释性研究**  <br/>   - 桥接人工智能与认知语言学，提供语音象征在MLLMs中的大规模量化分析框架。|
|2511.10045v1|[Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism](https://arxiv.org/abs/2511.10045v1)|**贡献点总结：**  <br/>1. 提出语音象征作为研究MLLMs听觉信息处理的探针，填补跨学科研究空白。  <br/>2. 构建首个大规模仿词数据集LEX-ICON（含8,052词及2,930伪词，覆盖4种语言与25语义维度）。  <br/>3. 开发音素级注意力分数分析方法，揭示模型分层信息处理机制。  <br/>4. 发现MLLMs在多语义维度上的语音直觉与语言学理论一致。  <br/>5. 识别出模型对象形音素的显著注意力模式，增强可解释性。  <br/>6. 实现AI与认知语言学的首次大规模量化关联，推动领域交叉探索。  <br/><br/>（100字以内）  <br/>研究提出语音象征作为探针，构建LEX-ICON数据集，通过音素级注意力分析揭示MLLMs对听觉信息的处理机制，发现其与语言学理论的契合性，并首次实现AI与认知语言学的大规模量化连接。|
|2511.10011v2|[Reinforcing Trustworthiness in Multimodal Emotional Support Systems](https://arxiv.org/abs/2511.10011v2)|**贡献点：**  <br/>1. 提出MultiMood框架，整合视频、音频、文本多模态数据，预测情感并生成符合专业治疗标准的回应。  <br/>2. 引入心理标准与强化学习（RL）优化大语言模型（LLM），提升情感支持的可信度和一致性。  <br/>3. 对先进LLM进行多模态情感支持能力分析，验证MultiMood在MESC和DFEW数据集上的SOTA性能。  <br/><br/>**总结（100字内）**：  <br/>MultiMood通过多模态融合与心理标准优化，提升情感支持的可信度与专业性，取得多数据集SOTA结果，推动语音领域情感交互技术发展。|
|2511.10011v1|[Reinforcing Trustworthiness in Multimodal Emotional Support Systems](https://arxiv.org/abs/2511.10011v1)|总结：  <br/>提出MultiMood框架，通过整合多模态数据与强化学习提升情感支持系统的可靠性与专业性，实现情感预测与响应生成的协同优化。<br/><br/>贡献点：  <br/>1. **多模态整合**：首次同时利用视频、音频、文本多模态嵌入，预测情感成分并生成符合专业治疗标准的响应。  <br/>2. **心理标准优化**：引入心理学评估指标，结合强化学习（RL）对大语言模型进行优化，增强输出一致性与可信度。  <br/>3. **模型评估分析**：系统分析多款先进LLM的多模态情感支持能力，为领域发展提供基准参考。  <br/>4. **实验验证**：在MESC和DFEW数据集上取得SOTA性能，RL驱动的信任度改进通过人机双评价验证有效性。|
|2511.09915v2|[HI-TransPA: Hearing Impairments Translation Personal Assistant](https://arxiv.org/abs/2511.09915v2)|**贡献点总结：**  <br/>1. **引入Omni-Model范式**：首次将Omni-Model应用于助听技术，开发指令驱动的音频-视觉个人助手HI-TransPA。  <br/>2. **多模态融合创新**：结合模糊语音与唇部动态，实现语音翻译与对话理解的统一框架。  <br/>3. **定制化预处理流程**：设计多模态数据预处理和筛选管道，检测面部关键点、稳定唇部区域并量化样本质量。  <br/>4. **课程学习策略**：利用质量评分引导渐进式训练，先强化高置信度样本再提升模型鲁棒性。  <br/>5. **高效编码架构**：提出统一的3D-Resampler，优化唇部动态编码以提升语义解析准确性。  <br/>6. **性能验证**：在HI-Dialogue数据集上取得SOTA结果，验证了字面准确性和语义保真度的突破。  <br/><br/>**100字内总结**：  <br/>HI-TransPA通过融合模糊语音与唇部动态，结合定制预处理、课程学习和3D编码技术，在助听对话场景中实现高效多模态理解，为Omni-Model在辅助通信中的应用提供了创新框架与关键工具。|
|2511.09915v1|[HI-TransPA: Hearing Impairments Translation Personal Assistant](https://arxiv.org/abs/2511.09915v1)|**贡献点总结**  <br/>1. **提出HI-TransPA**：开发指令驱动的音频-视觉多模态个人助听助手，整合模糊语音与高帧率唇部动态，实现翻译与对话功能。  <br/>2. **多模态数据融合**：通过融合语音和唇动信息，提升听障者日常交流的准确性与语义连贯性。  <br/>3. **预处理与质量评估**：构建针对听障数据的预处理流水线，包括面部关键点检测、唇部区域分离与稳定化，并引入量化数据质量评分系统。  <br/>4. **课程学习策略**：基于质量评分设计渐进式训练方案，先训练干净样本再逐步引入困难案例，增强模型鲁棒性。  <br/>5. **编码器优化**：结合SigLIP编码器与统一3D重采样器，高效编码高帧率唇动数据，提升模型效率。  <br/>6. **SOTA性能验证**：在自建的HI-Dialogue数据集上验证模型效果，取得字面准确性和语义保真度的最先进成果，并为未来研究提供框架与工具。  <br/><br/>**总结（100字以内）**：  <br/>本研究提出HI-TransPA，融合模糊语音与高帧率唇动，构建定制化预处理流程并引入课程学习策略，结合SigLIP与3D重采样器优化编码，实现听障交流的SOTA性能，为多模态辅助技术提供新框架与工具。|
|2511.09690v1|[Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages](https://arxiv.org/abs/2511.09690v1)|总结（100字以内）:  <br/>论文提出Omnilingual ASR，首个大规模可扩展语音识别系统，支持1600种语言（含500种新语言），通过7B参数自监督预训练实现零样本泛化，开源模型与工具降低研究门槛，强调社区协作与伦理考量。<br/><br/>贡献点分点：  <br/>1. **首次实现大规模语言可扩展性**：构建首个支持超1600种语言（含500种此前未被服务）的ASR系统，突破传统架构对语言覆盖的限制。  <br/>2. **高效低资源语言支持**：仅需少量数据样本即可引入未服务语言，显著降低语言扩展成本。  <br/>3. **参数规模与鲁棒性提升**：采用7B参数自监督预训练，学习更鲁棒的语音表示，增强模型适应未知语言的能力。  <br/>4. **创新编码器-解码器架构**：引入LLM启发的解码器，实现零样本泛化，提升跨语言迁移效果。  <br/>5. **多源数据融合训练**：整合公共资源与社区协作录制数据，通过本地合作伙伴补偿机制扩大数据多样性。  <br/>6. **模型家族部署灵活性**：发布300M、7B等多种模型变体，适配低功耗设备与高精度场景。  <br/>7. **伦理与社会影响考量**：强调社区协作与开源共享，降低技术门槛并推动语言平等，促进多方参与与可持续发展。|
|2511.09448v1|[MCAD: Multimodal Context-Aware Audio Description Generation For Soccer](https://arxiv.org/abs/2511.09448v1)|总结：本研究提出首个无需人工标注的跨领域视频音频描述生成系统MCAD，并开发新型评估指标ARGE-AD，验证其在电影与足球视频中的应用效果，同时提供了专业标注的足球数据集。<br/><br/>贡献点：<br/>1. 提出首个无需依赖人工标注音频描述（AD）的跨领域端到端生成系统MCAD，拓展至体育领域（如足球比赛）。<br/>2. 通过微调电影AD数据集训练的VideoLLM，使模型掌握AD的叙事结构与通用规则，适应新领域。<br/>3. 构建多模态上下文融合机制，结合球员身份、比赛事件、动作及实时解说生成完整AD文本。<br/>4. 开发新型评估指标ARGE-AD，从人名使用、动作事件提及、长度规范、代词避免、内容重叠等五维度量化AD质量。<br/>5. 首次验证ARGE-AD在跨领域（电影/体育）AD生成中的有效性，展现其泛化评估能力。<br/>6. 公布由两位AD专家标注的100段足球比赛视频的描述数据集，推动体育领域AD研究。|
|2511.09282v1|[End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering](https://arxiv.org/abs/2511.09282v1)|**主要贡献**  <br/>1. 提出 **CLSR**（Contrastive Language‑Speech Retriever），一种端到端对比学习检索模型，能够在长音频中高效提取与提问相关的片段。  <br/>2. 引入 **中间文本化步骤**：先将声学特征映射为类文本表征，再与语言表征对齐，显著缩小语音–文本模态差距，提升检索质量。  <br/>3. 在四个跨模态检索基准上实现 **显著优势**，超过现有端到端语音检索模型以及“语音识别+文本检索”流水线方法。  <br/>4. 为 **长篇语音问答（Long‑form SQA）** 提供可靠的前置检索模块，推动实际应用落地。<br/><br/>**100字以内总结**  <br/>本文提出端到端对比检索模型CLSR，通过将声学特征转化为类文本表征再进行对齐，显著提升长音频的相关片段检索效果，实验表明其在多数据集上优于现有语音检索和识别+检索流水线，为长篇语音问答提供了强大前置支持。|
|2511.08247v1|[ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247v1)|总结（100字以内）:  <br/>提出ParliaBench议会演讲生成基准，构建UK议会语料库，引入结合计算指标与LLM评估的多维框架，设计政治光谱与政党对齐新指标，验证微调模型在政治维度评估上的显著提升。<br/><br/>贡献点：  <br/>1. **提出议会演讲生成的特殊挑战**：强调政治真实性与意识形态一致性，区别于常规文本生成任务。  <br/>2. **构建专用数据集与基准**：基于UK议会数据创建ParliaBench，支持系统化模型训练与评估。  <br/>3. **创新评估框架**：结合计算指标（如语义连贯性）与LLM人工评估，从语言质量、语义连贯性、政治真实性三维度考核生成效果。  <br/>4. **设计新型政治指标**：提出基于嵌入的"政治光谱对齐"与"政党对齐"，量化意识形态定位。  <br/>5. **验证微调有效性**：通过5模型微调实验，证明其在多数指标及政治维度上的显著提升效果。|
|2511.07989v1|[State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989v1)|**贡献点总结：**  <br/>1. **评估模型跨语言表现**：首次系统比较BERT-like模型与LLMs在南斯拉夫语等低资源语言上的文本分类效果。  <br/>2. **多任务多领域验证**：覆盖情感分类、主题分类和体裁识别三项任务，涉及议会演讲、新闻文章及网络文本三个领域。  <br/>3. **零样本性能突破**：发现LLMs在零样本设置中表现优异，甚至超过传统微调模型，凸显其泛化能力。  <br/>4. **语言泛化能力分析**：证明LLMs在南斯拉夫语与英语中的表现一致性，为低资源语言应用提供参考。  <br/>5. **局限性揭示**：指出LLMs存在输出不可预测、推理速度慢和计算成本高的问题，强调微调模型在实际场景中的优势。  <br/><br/>**总结（100字以内）：**  <br/>本研究对比BERT-like模型与LLMs在南斯拉夫语等低资源语言的文本分类表现，发现LLMs零样本性能优越，但存在输出不可控、效率低等缺陷，表明微调模型在大规模文本标注中仍更具实用性。|
|2511.07821v1|[SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech](https://arxiv.org/abs/2511.07821v1)|总结（100字以内）:  <br/>本论文提出多语言合成语音指令数据集SYNTTS-COMMANDS，通过先进TTS模型与说话人嵌入解决KWS数据稀缺问题，验证合成数据在低功耗边缘设备上的有效性，为TinyML领域提供隐私、高效且可扩展的语音交互解决方案。<br/><br/>贡献点：<br/>1. **提出新型合成语音指令数据集**：构建首个基于TTS合成技术的多语言（英语&中文）语音指令数据集SYNTTS-COMMANDS，突破传统人工录制数据的瓶颈。<br/>2. **创新数据生成方法**：结合CosyVoice 2模型与公开语料库的说话人嵌入，实现高质量、可扩展的指令语音合成。<br/>3. **验证合成数据有效性**：通过多种高效声学模型测试，证明合成数据在指令识别任务中可达99.5%（英语）和98%（中文）的准确率，替代人工数据。<br/>4. **推动TinyML应用**：为资源受限的边缘设备提供隐私保护、低延迟、节能的语音交互训练基础，解决实际部署中的数据瓶颈问题。|
|2511.07477v1|[The Polite Liar: Epistemic Pathology in Language Models](https://arxiv.org/abs/2511.07477v1)|**贡献点：**  <br/>1. **提出“礼貌的骗子”概念**：揭示大语言模型通过RLHF训练产生的自信虚构行为，将其归因于结构性的认知冷漠而非主动欺骗。  <br/>2. **理论框架创新**：借鉴弗兰克福“bullshit”理论，区分结构冷漠与欺骗，指出RLHF奖励架构的偏差（追求真诚感而非证据准确性）。  <br/>3. **诊断对齐方法缺陷**：批判现有对齐策略（如帮助性、礼貌性）忽视知识基础，导致模型优先用户满意度而非真相。  <br/>4. **多视角分析行为**：结合认识论美德理论、言语行为哲学与认知对齐，揭示RLHF下模型模仿自信但缺乏认知依据的机制。  <br/>5. **提出“认识论对齐”原则**：主张以“有依据的自信”替代“感知流利度”作为对齐目标，解决语言合作与认知完整性的深层矛盾。  <br/><br/>**总结（100字以内）**：  <br/>论文指出RLHF导致模型成为“礼貌的骗子”，通过结构冷漠而非欺骗制造虚假自信，提出以认识论对齐替代流利度优先，解决AI对齐中认知与语言合作的矛盾。|
|2511.07392v2|[Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction](https://arxiv.org/abs/2511.07392v2)|总结：  <br/>本研究提出基于LLM的语音导向手术代理协调平台SAOP，结合分层多智能体框架与多级评估指标，实现手术场景中多模态数据处理的高效性与鲁棒性，为微创手术提供语音交互支持。<br/><br/>贡献点：  <br/>1. **提出SAOP平台架构**：设计语音导向的手术代理协调系统，通过分层多智能体框架整合任务规划、细化、验证与推理功能。  <br/>2. **部署LLM驱动的三任务代理**：构建三个任务专用代理（临床信息检索、CT扫描操作、3D模型导航），实现复杂语音指令到手术任务的映射。  <br/>3. **创新多级评估指标MOEM**：引入命令级与类别级双维度评估体系，全面衡量系统性能与对语音错误的鲁棒性。  <br/>4. **验证高鲁棒性与有效性**：在240条语音指令测试中达成高准确率与成功率，证明系统可应对多样、模糊的自由形式命令。  <br/>5. **推动微创手术场景应用**：展示语音交互技术在da Vinci手术中的潜力，为实时多模态数据处理提供智能化解决方案。|
|2511.07011v1|[Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011v1)|**贡献点：**  <br/>1. 提出利用纵向口语数据结合线性混合效应模型（LMM）和机器学习（ML）方法，探索抑郁症症状严重程度的可解释词汇特征。  <br/>2. 首次在多语言（英语、荷兰、西班牙）背景下分析语音特征与MDD症状关联，揭示不同语言中关键词汇特征的差异（如英语中词汇多样性，荷兰中句法结构，西班牙无显著关联）。  <br/>3. 评估了词汇特征与高维向量嵌入的预测性能，发现其在所有语言中均接近随机水平，凸显当前方法在临床应用中的局限性。  <br/>4. 指出非英语语音数据研究的不足（小样本、NL工具限制），强调多语言、跨文化研究的重要性。  <br/>5. 呼吁未来研究结合更大型多语言样本和改进ML协议，以捕捉个体内外语言变异，推动语音在临床诊断中的实际应用。  <br/><br/>**总结（100字以内）：**  <br/>本研究分析多语言纵向语音数据与MDD症状严重度的关系，发现不同语言中关键词汇特征差异，但预测性能有限，揭示方法与工具的局限性，为未来跨语言临床语音研究提供方向。|
|2511.06519v1|[On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception](https://arxiv.org/abs/2511.06519v1)|总结：  <br/>本研究揭示LLMs中存在类似人脑处理词性标签的神经元机制，通过分析Llama 3激活模式，发现关键神经元可预测词性并构建分类器，表明模型内部存在专门捕获语法概念的子空间。<br/><br/>贡献点：  <br/>1. **发现LLMs的神经元机制与人脑相似**：首次证实LLMs中不同词性标签由特定神经元处理，模拟人脑对语法范畴的神经分化。  <br/>2. **关键神经元识别**：基于Llama 3提出方法，定位与词性预测相关的最显著神经元，为模型内部表征研究提供新视角。  <br/>3. **构建可预测的分类器**：利用关键神经元激活模式训练分类器，实现对新数据的高准确词性预测，验证其潜在应用价值。  <br/>4. **揭示词性子空间结构**：证明LLMs存在专门捕获词性概念的子空间，与神经科学中的脑损伤研究模式一致，推动语言模型与认知科学的交叉理解。|
|2511.06483v1|[SAR-LM: Symbolic Audio Reasoning with Large Language Models](https://arxiv.org/abs/2511.06483v1)|贡献点总结：  <br/>1. 提出SAR-LM，首次将音频转化为结构化、可读的符号特征（语音、声音事件、音乐），突破密集嵌入的局限性。  <br/>2. 结合caption-based方法优势，通过符号化输入提升模型对结构化音频任务的推理能力。  <br/>3. 支持透明错误分析，可追溯模型失败原因至具体符号特征，增强可解释性。  <br/>4. 在MMAU、MMAR、OmniBench三个基准上表现优异，验证了符号化音频推理的有效性。  <br/><br/>（总结：SAR-LM通过结构化符号特征解决音频模型可解释性不足问题，显著提升推理能力和错误分析透明度，表现优于现有方法。）|
|2511.06441v1|[Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models](https://arxiv.org/abs/2511.06441v1)|**贡献点总结**：  <br/>1. 提出统一模块化框架，智能路由不同查询至最优模型，平衡成本与质量。  <br/>2. 针对视觉任务设计两阶段开源高效管道，结合经典模型与现代方法。  <br/>3. 在MMLU/VQA等基准中超越或匹配高端模型性能，降低67%对昂贵模型依赖。  <br/>4. 支持多代理协作扩展，实现可扩展、资源高效的AI系统。  <br/><br/>**总结（100字以内）**：  <br/>本文提出模块化框架与智能路由网络，结合高效开源模型与经典方法，显著降低对昂贵模型的依赖，在多项基准测试中匹配高端模型性能，实现高性价比、可扩展的多模态AI系统。|
|2511.04914v3|[MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages](https://arxiv.org/abs/2511.04914v3)|**论文贡献（分点列出）**  <br/>1. **提出 MERaLiON‑SER**：面向英语及东南亚语系的强韧语音情感识别模型。  <br/>2. **混合训练目标**：将加权分类交叉熵（离散情感）与一致相关系数（CCC）损失（维度情感）联合优化，实现对情感类别和细粒度维度（唤醒、效价、支配）的双重建模。  <br/>3. **跨语言实验验证**：在新加坡多语种（English、Chinese、Malay、Tamil）以及公开情感基准上进行大规模评估，持续超越现有开源语音编码器和大型 Audio‑LLM。  <br/>4. **强调专用语音‑Only 模型的重要性**：证明针对语音的专属模型在副语言信息捕获与跨语言泛化方面优于通用多模态模型。  <br/>5. **提供情感感知框架**：为未来具备情感感知的交互式音频系统（agentic audio agents）奠定基础，促进更具共情性与情境自适应的多模态推理。<br/><br/>**100字以内总结**  <br/>MERaLiON‑SER 通过加权交叉熵＋CCC 双目标实现离散与维度情感双建模，且在英语、华语、马来语、泰米尔语等多语言测试中超越现有语音编码器和 Audio‑LLM，证实专用语音模型在情感识别和跨语言迁移上的优势，为情感感知的智能音频系统提供了可靠基石。|
|2511.01670v1|[SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670v1)|**论文主要贡献点**  <br/><br/>1. **首个面向东南亚语言的 LALM**：提出 SeaLLMs‑Audio，支持印尼语 (id)、泰语 (th)、越南语 (vi)、英语 (en) 与中文 (zh) 五种语言。  <br/>2. **多模态输入**：模型可接受纯音频、纯文本以及音频+文本的组合输入，实现灵活的音文交互。  <br/>3. **多任务能力**：覆盖音频理解与交互任务，包括 Audio Captioning、ASR、语音‑文本翻译、情感识别、语音问答、语音摘要等；还能进行基于语音的事实、数学与常识问答。  <br/>4. **专属基准 SeaBench‑Audio**：构建面向东南亚语言的多任务音频评测套件，实现 LALM 自动化评估。  <br/>5. **竞争性能**：在实验中，SeaLLMs‑Audio 在东南亚语言上的表现与现有 LALM 相当，证明其在该地区的可用性。  <br/>6. **促进地区生态**：为东南亚学术界和产业提供开源/可商用的音频大模型与评测资源，推动区域研究与应用落地。  <br/><br/>**简要总结（≤100字）**  <br/>提出首个支持印尼语、泰语、越南语等五语种的海量音频语言模型 SeaLLMs‑Audio，具备音频/文本/音文融合多模态输入，覆盖音频标注、ASR、翻译、情感识别、问答等多任务，并推出对应评测套件 SeaBench‑Audio，实验表明在东南亚语言上达至与其他 LALM 竞争的水平，推动地区研究与产业应用。|
|2511.01299v1|[Towards General Auditory Intelligence: Large Multimodal Models for Machine Listening and Speaking](https://arxiv.org/abs/2511.01299v1)|**主要贡献点**  <br/>1. 梳理并系统化了音频与大语言模型（LLM）结合的最新研究，划分为四大主题：音频理解、音频生成、基于语音的交互、音频‑视觉理解。  <br/>2. 阐明 LLM 在音频感知与推理中的作用，展示其如何提升对声音的语义、情感及情境层面的深度理解。  <br/>3. 探讨生成式音频技术在实现富表达、自然逼真声音输出方面的进展与方法。  <br/>4. 分析基于语音的人机交互框架，说明 LLM 如何实现更自然的对话式交互与指令执行。  <br/>5. 研究音频‑视觉跨模态融合，展示其在情境感知、跨模态推理以及多模态智能体中的潜力。  <br/>6. 归纳当前面临的关键挑战（如数据稀缺、对齐效率、实时性、安全隐私等）并提出未来研究方向，指向构建“音频原生”AGI 的路线图。<br/><br/>**100字以内总结**  <br/>本文综述了音频在大语言模型中的融合进展，系统阐述了音频理解、生成、语音交互及音视跨模态四大方向，分析了 LLM 如何提升声学语义推理与自然生成，并提出实现音频原生 AGI 的挑战与发展路径。|
|2510.20850v1|[Can large audio language models understand child stuttering speech? speech summarization, and source separation](https://arxiv.org/abs/2510.20850v1)|评估最新音频语言模型在儿童口语（含卡顿）中的源分离与仅儿童摘要任务，结合LLM评判、人类评分与BERTScore，揭示模型在混合音频下的可信度及失效情形，并提供提示与复现脚本，为临床教育应用提供指导。|
|2510.19670v3|[CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670v3)|**总结**：  <br/>CoSense-LLM是边缘优先框架，将多模态传感器数据转化为语义token，结合LLM满足延迟、隐私和带宽约束，支持本地生成、检索和安全执行，实现高效部署与个性化。<br/><br/>---<br/><br/>**贡献点**：  <br/>1. **边缘优先框架设计**：将连续传感器流（Wi-Fi CSI、IMU、音频等）压缩为语义token，并在边缘设备与LLM协同，满足延迟、能耗、带宽和隐私约束。  <br/>2. **SenseFusion模块**：轻量级编码器对齐传感器嵌入与语言语义，压缩为离散代码序列，实现高效数据表征。  <br/>3. **Edge-RAG机制**：本地化检索层基于场地政策与知识库，提升生成准确性与事实一致性，减少矛盾。  <br/>4. **PromptRouter策略**：动态选择生成模式（边缘独占、边缘+检索、云端升级），平衡成本与不确定性，优化资源利用。  <br/>5. **Secure Execution保障**：通过数据最小化与元数据红acted，确保原始数据不离开设备，强化隐私保护。  <br/>6. **系统优化支持**：集成paged KV缓存、FlashAttention、量化LoRA等技术，实现低延迟与高能效；支持设备个性化和联邦学习更新。  <br/>7. **实测性能**：在家庭、办公、诊所场景中保持亚秒级端到端延迟，降低跨层级通信成本，同时保障隐私与语义一致性。|
|2510.19670v2|[CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670v2)|总结（100字以内）:  <br/>CoSense-LLM提出边缘优先框架，将多模态传感流转化为可验证语义标记，结合LLM实现低延迟、隐私保护与高可靠性，在居家、办公等场景中实现高效部署与动态策略选择。<br/><br/>贡献点：  <br/>1. **多模态传感压缩**：SenseFusion将Wi-Fi CSI、IMU等传感器数据压缩为离散语义标记，实现跨模态对齐与语言压缩。  <br/>2. **本地混合检索（Edge-RAG）**：结合本地政策与场景知识，在边缘设备实现事实一致性的生成响应。  <br/>3. **动态生成策略（PromptRouter）**：根据成本与不确定性选择边缘生成、本地检索或云端升级，优化资源分配。  <br/>4. **数据最小化安全机制（Secure Execution）**：通过审计路径确保数据不离开设备，仅传输离散代码与脱敏元数据。  <br/>5. **边缘优化与泛化支持**：集成KV缓存、量化LoRA等技术，支持设备个性化与非IID数据下的联邦更新。  <br/>6. **性能验证**：在多场景实现亚秒级延迟、降低跨层级通信成本，并通过消融实验验证各模块的可靠性与隐私保护效果。|
|2510.12089v2|[Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback](https://arxiv.org/abs/2510.12089v2)|**总结（100字以内）：**  <br/>本文提出基于DiT的高效视频生成框架，结合LoRA与位置偏移推理实现长视频生成，引入奖励反馈优化唇同步与身体动作，开发训练无关的Mask-CFG方法支持多角色动画，实现高质量、高效且低成本的多角色音频驱动视频生成。<br/><br/>**贡献点：**  <br/>1. **DiT框架支持任意长度视频生成**：提出基于扩散Transformer的框架，解决长视频生成的时序连贯性问题。  <br/>2. **高效训练策略**：采用LoRA微调结合位置偏移推理，保持基础模型能力的同时提升生成效率。  <br/>3. **多模态优化方法**：结合部分参数更新与奖励反馈，增强唇同步与自然身体动作的协调性。  <br/>4. **训练无关的多角色动画方法**：提出Mask-CFG，无需专门数据集或模型修改，支持三角色及以上音频驱动动画。|
|2509.21960v2|[Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models](https://arxiv.org/abs/2509.21960v2)|总结（100字以内）:  <br/>本研究提出难度自适应推理方法，通过动态奖励函数调节LALMs推理深度，在提升任务性能的同时减少冗余，为语音模型的高效推理提供了新思路。<br/><br/>贡献点分点列出:<br/>1. **提出难度自适应推理机制**：首次针对LALMs设计动态调节推理深度的框架，突破传统“一刀切”策略，实现根据问题复杂度智能分配推理资源。<br/>2. **创新奖励函数设计**：构建将推理长度与问题难度动态关联的奖励函数，引导模型对简单任务生成简洁推理，复杂任务输出深度分析。<br/>3. **验证方法有效性与效率**：通过大量实验证明该方法在任务性能提升和推理长度缩减方面均优于现有方案，展现显著的实用性优势。<br/>4. **揭示推理结构范式规律**：对推理结构进行深入分析，为后续优化模型推理策略和提升语音处理效率提供理论依据和研究方向。|