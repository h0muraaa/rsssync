|Source|Title|Summary|
|---|---|---|
|2508.20916v1|[SageLM: A Multi-aspect and Explainable Large Language Model for Speech   Judgement](http://arxiv.org/abs/2508.20916v1)|**贡献点：**<br/>1. 提出SageLM模型：首个端到端、多维度、可解释的S2S LLM评估框架，同时覆盖语义与声学维度评估。<br/>2. 创新监督机制：采用基于理由的监督方法提升模型可解释性，优于传统规则强化学习方法。<br/>3. 构建合成数据集：设计SpeechFeedback合成偏好数据集并引入两阶段训练范式，缓解语音偏好数据稀缺问题。<br/><br/>**总结（100字以内）：**  <br/>本研究提出SageLM，通过联合语义-声学评估、理由引导监督及合成数据集优化，显著提升语音LLM评估性能，达到82.79%的人类一致性，超越现有基线模型26.20%。|
|2508.20750v1|[Specializing General-purpose LLM Embeddings for Implicit Hate Speech   Detection across Datasets](http://arxiv.org/abs/2508.20750v1)|总结（100字以内）:  <br/>本文提出基于通用嵌入模型的纯微调方法，无需外部知识，实现隐含仇恨言论检测的SOTA性能，并在多个数据集验证其有效性，显著提升跨数据集表现。<br/><br/>贡献点：<br/>1. **提出纯微调通用嵌入模型的方法**：首次仅通过微调基于大语言模型（LLM）的通用嵌入模型（如Stella、Jasper、NV-Embed和E5）解决IHS检测问题，无需引入外部上下文、情感或领域知识。  <br/>2. **实现SOTA性能**：在多个IHS数据集上取得最佳结果，in-dataset提升1.10个百分点，cross-dataset提升20.35个百分点（F1-macro）。  <br/>3. **验证方法泛化能力**：通过跨数据集评估证明该方法具有强泛化性，突破传统依赖外部信息的检测框架，简化技术流程。|
|2508.19320v2|[MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time   Autoregressive Video Generation](http://arxiv.org/abs/2508.19320v2)|**贡献点总结（100字以内）:**  <br/>提出自回归视频生成框架，实现多模态交互控制与低延迟流式生成；构建20,000小时跨源对话数据集；引入64倍压缩深度自动编码器优化推理效率；通过实验验证在双人对话、多语言合成及交互式世界模型中的高效性与细粒度控制优势。<br/><br/>**分点贡献:**  <br/>1. **自回归多模态控制框架**：首次将自回归模型应用于数字人视频生成，支持音频、姿态与文本多模态输入，生成空间与语义一致的表示以引导去噪过程，提升交互性与控制精度。  <br/>2. **大规模对话数据集构建**：创建包含约20,000小时对话的多源数据集，覆盖丰富场景，为训练交互式视频生成模型提供高质量语料。  <br/>3. **深度压缩自动编码器设计**：提出具有64倍压缩率的自动编码器，显著降低自回归模型的长时推理计算负担，增强实际应用可行性。  <br/>4. **多场景实验验证**：通过双人对话、多语言合成及交互式世界模型测试，证明框架在低延迟、高效率和细粒度多模态控制方面的优越性。|
|2508.19320v1|[MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time   Autoregressive Video Generation](http://arxiv.org/abs/2508.19320v1)|总结：  <br/>本文提出一种自回归视频生成框架，通过多模态控制和流式处理实现低延迟交互，并结合大规模对话数据集和深度压缩自动编码器提升生成效率与控制精度。<br/><br/>贡献点：  <br/>1. **提出交互式多模态控制框架**：支持音频、姿态和文本的实时输入，并生成空间与语义连贯的表示以指导扩散模型。  <br/>2. **构建大规模对话数据集**：整合约20,000小时多源数据，覆盖复杂对话场景提升训练效果。  <br/>3. **开发深度压缩自动编码器**：实现64倍压缩比，显著降低长时推理计算负担。  <br/>4. **实验证明低延迟与高效性**：在双语对话、多语言合成等任务中验证框架的实时性能与多模态控制能力。|
|2508.18998v1|[MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in   LLM-based Multilingual ASR](http://arxiv.org/abs/2508.18998v1)|**总结（100字以内）:**  <br/>本文提出MOSA模型，通过轻量级适配器结合Mixture-of-Experts机制，有效实现跨语言知识共享，缓解多语言ASR数据稀缺问题，并在参数减少情况下仍保持高性能，验证了简单适配器优于单一复杂投影器的设计。<br/><br/>**贡献点分点:**  <br/>1. **提出MOSA框架**：设计基于Mixture-of-Experts的多语言ASR模型，通过轻量级适配器分离共享与语言特异性知识，提升跨语言迁移效率。  <br/>2. **解决数据不平衡问题**：利用高资源语言数据支持低资源语言，缓解数据稀缺对性能的制约，优化低资源语言的识别效果。  <br/>3. **参数效率优化**：MOSA-Base在仅使用60%参数的情况下超越基线模型，证明高效参数利用的可行性。  <br/>4. **实验验证有效性**：在多语言场景下（包括平均WER和数据不平衡情况）均优于基线，且通过消融研究证明适配器设计的优越性。  <br/>5. **方法创新性**：对比传统单一复杂投影器，提出简单适配器组合策略，为LLM-based ASR提供更高效的架构选择。|
|2508.18918v1|[DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM   with Audio Modality](http://arxiv.org/abs/2508.18918v1)|总结：  <br/>本文提出DESAMO系统，利用Audio LLM直接处理原始音频，解决老年人语音不清晰问题，实现意图理解与关键事件检测，提升智能家居的可用性与隐私性。<br/><br/>贡献点：  <br/>1. **方法创新**：采用Audio LLM直接处理原始音频，突破传统ASR或ASR-LLM级联架构的限制。  <br/>2. **老人友好设计**：针对性优化对老年用户模糊发音的识别能力，提升交互自然性。  <br/>3. **隐私与本地性**：系统部署于设备端，避免云端传输，增强数据隐私保护。  <br/>4. **多模态扩展**：支持非语音音频分析，实现跌倒、求助等关键事件的检测与响应。  <br/>5. **实际应用价值**：为智能家居场景提供安全、私密的语音交互解决方案。|
|2508.18783v1|[Controllable Conversational Theme Detection Track at DSTC 12](http://arxiv.org/abs/2508.18783v1)|**贡献点总结（100字以内）:**  <br/>提出对话主题检测作为新任务，区别于传统意图识别；设计可控制粒度的联合聚类与标签框架；构建公共竞赛赛道与配套数据集；提供自动与人工评估指标；分析参赛成果并开源数据与代码。  <br/><br/>**详细贡献点:**  <br/>1. **提出新任务**：定义Theme Detection为对话分析的核心任务，目标是自动识别对话主题并分类，提升客服/销售等领域的对话分析效率。  <br/>2. **任务差异**：与传统固定意图检测不同，主题作为用户可见的摘要，支持灵活粒度和个性化定制。  <br/>3. **竞赛机制**：在DSTC12中设立Controllable Conversational Theme Detection公开竞赛赛道，推动联合聚类与主题标签的研究。  <br/>4. **数据与评估**：构建包含用户偏好数据的基准数据集，并设计多维度评估指标（自动与人工）。  <br/>5. **成果分析**：汇总参赛团队方法、提供性能分析与启发，同时开放数据及代码促进研究复现。|
|2508.18240v1|[MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues   via Arena-style and Rubrics Protocols](http://arxiv.org/abs/2508.18240v1)|**贡献点：**  <br/>1. **提出多维度S2S基准MTalk-Bench**：涵盖语义信息、伴随信息（paralinguistic）和环境声音（ambient sound）三大核心维度，包含9个现实场景及针对性任务（如推理），系统化评估多轮对话中的复杂能力。  <br/>2. **构建双重评估框架**：结合Arena式（相对比较）和Rubrics式（绝对评分）方法，同时支持模型与人类输出的对比评估，增强评估的全面性与客观性。  <br/>3. **揭示S2S LLMs性能局限**：发现模型在语义处理上表现优异，但在伴随信息和环境声音感知上存在不足；回复长度与效率的平衡问题；以及模态感知与任务设计优于单纯模型扩展的重要性。  <br/>4. **评估框架可靠性分析**：指出LLM作为评估者在明显性能差距时与人类一致，但在位置和长度上存在偏差，需依赖文本注释辅助提升非语言信息评估的可靠性。|
|2508.18234v1|[Can AI Have a Personality? Prompt Engineering for AI Personality   Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](http://arxiv.org/abs/2508.18234v1)|**贡献点：**  <br/>1. **探索LLM人格模拟可行性**：验证了通过指令提示工程引导大语言模型模拟一致人格的可行性，为AI人格定制提供理论支持。  <br/>2. **应用特定领域训练场景**：将人格模拟技术应用于言语-语言病理学（SLP）学生培训，聚焦性别确认语音治疗，推动AI在专业医疗场景的实用性。  <br/>3. **构建个性化AI角色**：开发聊天机器人Monae Jackson，以跨性别女性身份进行对话，展示了AI角色设计在敏感话题中的精准性。  <br/>4. **量化人格特征验证**：基于大五人格测试框架，客观评估AI人格的一致性和稳定性，为评估体系提供参考依据。  <br/><br/>**总结（100字以内）：** 本研究通过提示工程实现AI人格模拟，应用于SLP性别确认语音治疗培训，构建个性化角色Monae Jackson，并以大五人格测试验证其有效性，证明LLM可通过技术手段模拟稳定人格特征，为AI在医疗领域的应用提供新思路。|
|2508.17674v1|[Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against   Large Language Models](http://arxiv.org/abs/2508.17674v1)|**总结**（100字以内）:  <br/>本文提出新型LLM攻击AEA，通过对抗提示和后门前缀检查点实现隐蔽内容注入，首次识别五类受害群体并设计基于提示的自检防御机制，揭示LLM安全中的隐蔽内容注入漏洞，呼吁AI安全社区加强检测、审计和政策协作。<br/><br/>**贡献点**:  <br/>1. **提出新型攻击范式**：定义AEA（广告嵌入攻击）作为LLM安全的新威胁，突破传统攻击（如准确率下降）的局限，聚焦于信息完整性破坏。  <br/>2. **揭示攻击技术路径**：提出两种低成本攻击手段——（1）劫持第三方平台注入对抗提示，（2）发布植入后门的开源检查点，实现隐蔽内容嵌入。  <br/>3. **构建受害者分类体系**：明确五类利益相关者受害群体（如用户、平台、开发者等），为攻击溯源和防护提供系统视角。  <br/>4. **设计轻量防御机制**：提出基于提示的自检方法，无需模型再训练即可有效缓解攻击，推动实用安全方案的落地。  <br/>5. **强调行业安全缺口**：通过实证研究揭示LLM安全的紧急漏洞，呼吁学术界、产业界和政策制定者协同应对。|
|2508.17597v1|[SonoCraftAR: Towards Supporting Personalized Authoring of Sound-Reactive   AR Interfaces by Deaf and Hard of Hearing Users](http://arxiv.org/abs/2508.17597v1)|**贡献点：**  <br/>1. 提出SonoCraftAR系统，首次实现听障/重听（DHH）用户通过自然语言输入创建个性化的声音反应AR界面。  <br/>2. 集成实时音频信号处理与多智能体大语言模型（LLM）管道，突破传统封闭式视觉化方法。  <br/>3. 利用矢量图形库生成动态2D动画界面，将音频频率映射为视觉属性（如大小、颜色），实现声音-视觉实时交互。  <br/>4. 验证开放性声音反应AR界面创作的可行性，为未来个性化与AI辅助的听觉辅助技术提供方向。  <br/><br/>**总结（100字内）：**  <br/>本研究开发了SonoCraftAR原型系统，使DHH用户可通过自然语言创建自定义声音可视化AR界面，结合音频处理与AI技术，提升声音可访问性，为未来个性化听觉辅助工具奠定基础。|
|2508.17164v1|[The Impact of Annotator Personas on LLM Behavior Across the   Perspectivism Spectrum](http://arxiv.org/abs/2508.17164v1)|总结：  <br/>本研究探讨了大语言模型在考虑预定义注解者角色时对仇恨言论的注解能力，发现LLMs选择性使用角色特征且在弱数据视角下表现优于人类，但在强数据视角下仍存在差距，为视角建模提供了新的方法论视角。<br/><br/>贡献点：  <br/>1. **引入LLM在视角建模中的应用**：首次探索大语言模型在注解仇恨言论和滥用性时，结合预定义注解者角色（personas）的能力，拓展了LLM在语音领域的应用场景。  <br/>2. **对比分析不同注解方法**：系统评估LLM生成的注解与传统注解建模技术的性能差异，揭示LLMs在强/弱数据视角下的表现特点。  <br/>3. **揭示LLMs的角色特征选择机制**：发现LLMs在注解过程中选择性地利用角色中的敏感属性，反映出其对社会偏见的建模能力及潜在偏差来源。  <br/>4. **提出原型注解者分类**：基于角色特征与人类注解者的对齐程度，识别出具有不同立场倾向的原型注解者，为视角分析提供分类依据。  <br/>5. **验证数据透视范式的有效性**：通过实验证明，无需依赖显式注解信息的模型在弱数据视角下表现更优，而个性化强数据集则需更精细的模型设计。  <br/>6. **定量对比LLM与人类注解**：明确LLM在强数据视角下接近但未超越人类注解性能，为实际应用中平衡自动化与人工标注提供了参考。|
|2508.16122v1|[Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection](http://arxiv.org/abs/2508.16122v1)|总结（100字以内）:  <br/>该研究揭示了多模态意图数据集的文本偏倚问题，提出去偏倚框架并分析模态相关性，强调构建无偏数据集对评估多模态模型的重要性。<br/><br/>贡献点:  <br/>1. **文本偏倚分析**：首次发现主流多模态意图数据集（如MIntRec）存在显著文本偏倚，超过90%的样本需依赖文本输入完成分类，且通过人工评估验证了该偏倚。  <br/>2. **模型性能对比**：实验证明Text-only LLM（Mistral-7B）在文本偏倚数据集上显著优于多模态模型，分别在MIntRec-1和MIntRec2.0上提升9%和4%。  <br/>3. **去偏倚框架设计**：提出系统性去偏倚方法，移除70%以上文本偏倚样本后，所有模型性能均大幅下降，尤其多模态融合模型准确率下降超50-60%。  <br/>4. **模态相关性研究**：通过实证分析揭示不同模态（文本、音频、视觉）在任务中的上下文相关性差异，为构建公平的数据集提供理论依据。|
|2508.15875v1|[NEAT: Concept driven Neuron Attribution in LLMs](http://arxiv.org/abs/2508.15875v1)|总结：  <br/>本文提出了基于概念向量定位关键神经元的高效方法，显著优化计算复杂度并提升概念理解性能，同时通过消融研究和实际应用场景分析，为理解LLM中的偏见与情绪言论提供了新路径。<br/><br/>贡献点：  <br/>1. **提出概念神经元定位方法**：首次利用概念向量识别对特定概念负责的关键神经元，并定义为"概念神经元"  <br/>2. **计算复杂度优化**：将传统O(n*m)的计算需求降为O(n)，显著减少正向传播次数  <br/>3. **性能对比验证**：在多个基准测试中证明方法优于现有技术，尤其在与SOTA方法对比中表现更优  <br/>4. **聚类辅助搜索优化**：通过消融实验引入聚类方法进一步提升概念神经元的搜索效率  <br/>5. **实际应用分析**：将方法应用于分析LLM中的仇恨言论与偏见，完成印度语境下的偏见评估  <br/>6. **拓展理解维度**：为理解神经元层面的语义责任提供了更广泛、类人化的视角  <br/>7. **未来研究路径**：构建了概念神经元识别与干预的研究框架，推动相关领域发展|
|2508.15827v1|[Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech   Models](http://arxiv.org/abs/2508.15827v1)|总结:  <br/>论文提出新型语音推理框架Mini-Omni-Reasoner，通过“思考中发言”机制实现实时响应，设计专用数据集Spoken-Math-Problems-3M，实验显示在Spoken-MQA基准上提升显著，具备零延迟和高效性能。<br/><br/>贡献点:  <br/>1. **提出"Thinking-in-Speaking"框架**：首次在语音领域实现推理与生成的实时交错，避免传统顺序处理导致的延迟问题。  <br/>2. **设计Spoken-Math-Problems-3M数据集**：构建大尺度、结构化的语音-推理混合数据集，支持即时推理与响应的学习。  <br/>3. **创新分层架构（Thinker-Talker）**：通过层次化设计提升生成流畅性和逻辑性，兼顾自然语言表达与任务准确性。  <br/>4. **实验证明效能提升**：在Spoken-MQA基准测试中，实现算术推理+19.1%、语境理解+6.4%的显著提升，且输出无延迟。|
|2508.15810v1|[Detecting Hope, Hate, and Emotion in Arabic Textual Speech and   Multi-modal Memes Using Large Language Models](http://arxiv.org/abs/2508.15810v1)|**贡献点总结：**  <br/>1. 提出针对阿拉伯语文本及迷因中希望、仇恨言论、冒犯性语言和情感识别的统一任务框架。  <br/>2. 系统性评估基础LLM、微调LLM及预训练嵌入模型在阿拉伯语任务中的性能差异。  <br/>3. 在阿拉伯NLP MAHED 2025挑战中，实现三项任务的SOTA结果（72.1%/57.8%/79.6% macro F1）。  <br/>4. 为阿拉伯语内容审核系统提供更精准、高效的解决方案，推动实际应用。  <br/><br/>**摘要压缩（100字内）：**  <br/>该研究提出基于LLM的阿拉伯语文本与迷因内容分析框架，在MAHED 2025挑战中取得多任务SOTA性能，为内容审核提供精准高效的方法。|
|2508.15801v1|[LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in   Structured Synthetic Spoken Transcriptions](http://arxiv.org/abs/2508.15801v1)|**贡献点：**  <br/>1. **提出LingVarBench合成数据生成框架**：通过自动化验证解决电话对话标注成本高、隐私限制等挑战，首次构建结构化提取领域的合成数据基准。  <br/>2. **创新三阶段生成流程**：  <br/>   - 用LLM生成多场景结构化字段值；  <br/>   - 将字段值转化为真实对话特征（如歧义、中断、重叠）的自然语句；  <br/>   - 通过独立LLM验证合成语句的准确性，实现结构信息的还原。  <br/>3. **开发自动化提示优化方法**：基于DSPy的SIMBA优化器，无需人工干预即可生成高效提取提示，显著提升数值、名称、日期字段的抽取准确率（95% vs. 88-89%零样本）。  <br/>4. **验证合成到真实数据的泛化能力**：证明模型在包含背景噪声和领域术语的真实对话中表现优异，为商业场景的大规模电话分析提供可行方案。  <br/><br/>**总结（100字内）：**  <br/>论文提出LingVarBench合成数据生成框架，通过自动化验证和提示优化，显著提升电话对话结构化提取准确率，突破隐私与成本限制，推动合成数据在商业分析中的应用。|
|2508.15524v1|[The Enemy from Within: A Study of Political Delegitimization Discourse   in Israeli Political Speech](http://arxiv.org/abs/2508.15524v1)|总结：  <br/>本研究首次大规模分析政治去合法化话语，构建希伯来语语料库并提出两阶段分类模型，揭示其在民主话语中的长期演变趋势及社会特征。<br/><br/>贡献点：  <br/>1. **首次系统性研究**：提出针对政治去合法化话语（PDD）的首个大规模计算分析框架，定义其为对政治实体规范有效性的象征性攻击。  <br/>2. **多源语料库构建**：创建并手动标注包含10,410句希伯来语文本的新型语料库（议会演讲、社交媒体、新闻媒体），明确PDD实例（17.4%）及附加的强度、粗鲁性、目标类型和情感框架标注。  <br/>3. **两阶段分类方法**：设计结合微调编码器模型与解码器大语言模型（LLM）的分类流程，提升PDD识别与特征分类性能。  <br/>4. **模型性能验证**：提出DictaLM 2.0模型，在二分类PDD检测和多特征分类任务中分别达到F1值0.74和0.67，验证其有效性。  <br/>5. **跨平台与动态分析**：揭示PDD在30年间的显著增长趋势，对比社交媒体与议会辩论的差异，发现性别与意识形态相关的使用模式及选举/事件中的峰值现象。  <br/>6. **理论实践价值**：证明自动化PDD分析在研究民主话语中的可行性，为理解政治极化与话语演变提供新工具。|
|2508.15483v1|[HebID: Detecting Social Identities in Hebrew-language Political Text](http://arxiv.org/abs/2508.15483v1)|总结：  <br/>提出首个多标签希伯来语政治文本语料库HebID，验证调优后大模型在身份识别的优势，并通过跨场景分析揭示社会身份表达的动态差异与公众舆论关联。<br/><br/>贡献点：  <br/>1. 构建首个多标签希伯来语语料库（HebID），覆盖以色列政治人物的社交媒体和议会演讲文本，包含12种细致社会身份标签。  <br/>2. 验证生成式大语言模型（2B-9B参数）在希伯来语身份识别任务中的性能优势，提出宏F1值达0.74的评估结果。  <br/>3. 研究政治文本中社会身份表达的多维差异，包括流行度、时间趋势、聚类模式及性别关联性。  <br/>4. 将社会身份标注与国家级民意调查数据结合，对比精英话语与公众身份优先级的差异。  <br/>5. 提供非英语政治身份研究的标准化框架，为其他语言背景的类似研究奠定基础。|
|2508.14976v1|[Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal   CAPTCHA System](http://arxiv.org/abs/2508.14976v1)|总结（100字以内）:  <br/>Aura-CAPTCHA通过融合GANs、RL和LLMs，构建了可抵御OCR和对抗图像处理的多模态系统，实现动态难度调整和用户友好性，显著提升安全性与有效性，填补了现有CAPTCHA研究的漏洞。<br/><br/>贡献点:  <br/>1. **多模态安全设计**：结合视觉（3x3网格图像选择）和音频（随机数词组合）挑战，双重验证提升抗AI攻击能力。  <br/>2. **GANs动态生成挑战**：利用生成对抗网络创建复杂、动态的图像谜题，增强对传统OCR技术的防御。  <br/>3. **RL自适应难度控制**：基于尝试次数、响应时间和行为异常实时调整难度，优化用户体验与安全性平衡。  <br/>4. **LLMs生成语义内容**：通过大语言模型实现文本与音频提示的智能化生成，提高挑战的语义复杂度。  <br/>5. **实证效果显著**：在真实流量测试中达到92%人类通过率和10%机器人绕过率，超越现有CAPTCHA系统性能。|
|2508.14916v1|[Transsion Multilingual Speech Recognition System for MLC-SLM 2025   Challenge](http://arxiv.org/abs/2508.14916v1)|总结：  <br/>本文提出一种融合预训练模型与任务特定微调的多语言ASR系统，包含冻结Whisper-large-v3编码器、可训练适配器模块和冻结Qwen2.5-7B-Instruct模型加LoRA的解码架构，在11种语言评测中达到9.83%的WER/CER，排名全球第三。<br/><br/>贡献点：  <br/>1. **多语言ASR架构创新**：设计包含冻结预训练语音编码器、适配器模块和冻结大型语言模型的三阶段系统。  <br/>2. **高效对齐机制**：引入Linear-ReLU-Linear适配器模块优化语音与文本表示对齐。  <br/>3. **低成本优化策略**：采用冻结大语言模型（LLM）与LoRA微调结合，降低计算开销并提升上下文解码能力。  <br/>4. **跨语言性能验证**：在11种语言的评测集上实现9.83%的WER/CER，优于多数参赛方案。|
|2508.14764v1|[Investigation of the Inter-Rater Reliability between Large Language   Models and Human Raters in Qualitative Analysis](http://arxiv.org/abs/2508.14764v1)|总结：  <br/>该研究验证了GPT-4o和GPT-4.5在物理教育领域的定性分析可靠性，探索了优化参数与提示的方法，揭示了其在领域通用构念评估中的局限性，为大規模语音研究提供了可行性方案。<br/><br/>贡献点：  <br/>1. **首验证LLM可靠性**：通过Cohen's Kappa分析，首次系统评估ChatGPT-4o和ChatGPT-4.5在物理教育领域定性研究中的评分一致性，证明其可替代人类验证。  <br/>2. **优化模型表现**：提出通过调整提示词和超参数提升LLM对音频转录文本编码的准确性，实现更高效的分析流程。  <br/>3. **领域应用案例**：以实际物理教育研究场景（项目讨论策略）为例，展示LLM在具体主题（如问题解决方法）分析中的有效性。  <br/>4. **识别模型局限**：明确指出LLM在评估跨领域通用构念时仍存在局限，为后续改进提供方向。  <br/>5. **推动方法创新**：为语音领域定性研究提供可扩展的自动化工具方案，打破小数据集瓶颈。|
|2508.14706v1|[ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](http://arxiv.org/abs/2508.14706v1)|总结：  <br/>本研究提出首个面向TCM的多模态语言模型ShizhenGPT，构建全球最大TCM数据集并解决数据稀缺问题，创新性地实现多模态诊疗感知整合，通过权威评估验证其性能优势。<br/><br/>贡献点：  <br/>1. **首个TCM专用多模态LLM**：提出ShizhenGPT，首次将多模态能力应用于TCM领域。  <br/>2. **大规模多模态数据集**：构建包含100GB+文本和200GB+多模态数据（1.2M图像、200小时音频、生理信号）的全球最大TCM数据集。  <br/>3. **多模态感知融合**：实现跨模态统一感知（声、脉、嗅、视觉），突破传统LLM单模态局限。  <br/>4. **权威评估体系**：基于国家TCM资格考试建立视觉基准，评估模型在药识与视觉诊断能力。  <br/>5. **开源与可复现性**：开放数据、模型及代码，推动TCM领域研究发展。|
|2508.14130v1|[EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion   Recognition](http://arxiv.org/abs/2508.14130v1)|**总结（100字以内）：**  <br/>本文提出一种融合音频与文本的多模态情感识别方法，通过可学习接口模块和文本提示引导LLM，结合LoRA实现参数高效微调，在标准基准上取得优于多数模型的性能且参数量显著减少。  <br/><br/>---<br/><br/>**贡献点分点：**  <br/>1. **多模态融合框架创新**：首次将音频特征与文本表征结合，利用LLM进行语音情感预测，突破传统单模态或语言-语音分离的范式。  <br/>2. **跨模态映射机制设计**：提出可学习的接口模块，将音频特征高效映射至LLM的表示空间，解决异构模态对齐问题。  <br/>3. **任务引导输入方法**：引入文本提示（如情感标签描述）作为任务引导，增强模型对情感任务的语境理解能力。  <br/>4. **参数效率优化**：基于LoRA技术实现低参数量微调，有效降低模型规模的同时保持性能，提升部署可行性。  <br/>5. **性能验证与对比**：在主流情感识别基准上超越大部分Speech-Text LLMs，参数量减少至竞争方法的不到一半，证明方法有效性与效率。|
|2508.13769v1|[Can Large Language Models (LLMs) Describe Pictures Like Children? A   Comparative Corpus Study](http://arxiv.org/abs/2508.13769v1)|总结：  <br/>本研究通过比较LLM生成文本与儿童语言的特征，揭示其在模拟儿童语言表达上的局限性，并为LLM在教育与心理语言学领域的应用提供新视角。<br/><br/>贡献点：  <br/>1. **首次系统比较LLMs与儿童语言**：构建了基于相同图片故事和两种提示策略（零样本与少样本）的LLM语料库，对比分析其与德国儿童描述文本的异同。  <br/>2. **揭示LLMs语言生成的特征差异**：发现LLMs生成文本更长但词汇贫乏，依赖高频词且名词使用不足，语义相似性较低。  <br/>3. **评估多模态提示策略效果**：证明少样本提示可略微提升LLMs与儿童文本的相似性，但未能复制词汇和语义模式。  <br/>4. **为教育工具合适性提供依据**：提出LLMs生成语言在儿童导向教育工具中的适当性问题，为教育应用与研究提供警示。|
|2508.13603v1|[Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of   a Speech-LLM](http://arxiv.org/abs/2508.13603v1)|总结：  <br/>该研究通过说话人分配分析语音模型的性别偏见，构建性别相关数据集验证，指出Bark无系统偏见但存在意识倾向，揭示语音生成模型与文本模型在性别处理上的差异。<br/><br/>贡献点：  <br/>1. **提出新的分析框架**：首次将“说话人分配”作为语音模型性别偏见研究的显式分析工具，突破文本模型隐式编码的局限性。  <br/>2. **构建针对性数据集**：设计“职业（Professions）”与“性别化词汇（Gender-Colored Words）”两个数据集，系统评估语音模型对性别关联的识别与生成能力。  <br/>3. **揭示模型特性差异**：发现Speech-LLM在性别表达上与文本模型的显著差异，即其需显式生成性别化声音，导致说话人选择成为直接的偏见线索。  <br/>4. **实证分析结果**：通过实验验证Bark模型未展现系统性性别偏见，但存在性别意识和非系统性倾向，为语音模型的公平性研究提供新视角。|
|2508.12854v1|[E3RG: Building Explicit Emotion-driven Empathetic Response Generation   System with Multimodal Large Language Model](http://arxiv.org/abs/2508.12854v1)|总结：  <br/>提出E3RG系统，通过任务分解和多模态生成技术实现情感对话生成，无需额外训练即可生成自然且身份一致的响应，实验表现优异。<br/><br/>贡献点：  <br/>1. 提出E3RG系统，首次将多模态情感响应生成（MERG）分解为“情绪理解-记忆检索-响应生成”三个模块性任务。  <br/>2. 集成先进的语音与视频生成模型，实现无需额外训练的自然、情感丰富且身份一致的多模态响应生成。  <br/>3. 在零样本和少样本场景下验证系统有效性，于ACM MM 2025 Avatar-based Multimodal Empathy Challenge中取得Top-1成绩。|
|2508.12666v1|[Cryfish: On deep audio analysis with Large Language Models](http://arxiv.org/abs/2508.12666v1)|**贡献点分点：**  <br/>1. **提出Cryfish模型**：首个结合WavLM音频编码器与Qwen2语言模型的多模态架构，通过Transformer-based连接器实现跨模态整合。  <br/>2. **专门化训练策略**：设计针对听觉任务的训练方法，提升模型在复杂语音和声音识别任务上的泛化能力。  <br/>3. **基准测试与评估**：基于Dynamic SUPERB Phase-2新基准，系统性验证Cryfish在语音相关多任务上的性能。  <br/>4. **对比分析**：与公开的语音模型进行深入比较，突出Cryfish在听觉能力集成方面的优势。  <br/><br/>**总结（100字内）：**  <br/>本文提出Cryfish，将WavLM与Qwen2结合，通过Transformer连接器实现语音能力整合，并采用专门训练策略提升多任务性能，基于动态SUPERB基准验证模型效果，与现有模型对比分析其在听觉理解中的优势。|
|2508.12622v1|[Consiglieres in the Shadow: Understanding the Use of Uncensored Large   Language Models in Cybercrimes](http://arxiv.org/abs/2508.12622v1)|总结：  <br/>本文首次系统研究未审查大语言模型（ULLMs），构建知识图谱并运用图神经网络识别出超11,000个潜在恶意模型，揭示其广泛危害及地下论坛的滥用现象，强调需加强AI安全防护。<br/><br/>贡献点：  <br/>1. **首次系统性研究**：提出首个针对未审查大语言模型的系统性分析框架，填补研究空白。  <br/>2. **关系建模**：建立开源LLMs与相关数据（如微调、合并、压缩模型及有害数据集）的关联网络。  <br/>3. **知识图谱技术**：将模型关系转化为知识图谱，结合图神经网络实现高效识别。  <br/>4. **大规模发现**：基于少量标记样本和未审查数据，识别出超11,000个ULLMs。  <br/>5. **危害分析**：揭示ULLMs可生成仇恨言论、暴力内容、色情材料及恶意代码，并被集成至数百个恶意应用。  <br/>6. **地下活动证据**：发现黑客通过论坛分享技术构建廉价替代商业恶意LLMs的手段。  <br/>7. **安全呼吁**：强调LLM技术滥用的严重性，倡导制定针对性的防御策略。|
|2508.12438v1|[Express4D: Expressive, Friendly, and Extensible 4D Facial Motion   Generation Benchmark](http://arxiv.org/abs/2508.12438v1)|总结：本文提出Express4D数据集，结合自然语言指令与ARKit格式，解决现有数据集在表情精细度和采集成本上的不足，为文本到面部动态表情生成提供基准。  <br/><br/>贡献点：  <br/>1. **提出新型数据集Express4D**：包含细粒度面部运动序列与语义标注，解决现有数据集仅基于语音驱动或粗粒度情感标签的局限性。  <br/>2. **低成本采集方案**：利用消费级设备与LLM生成的自然语言指令高效采集数据，提升数据获取的便捷性与可扩展性。  <br/>3. **支持多对多映射研究**：通过数据集验证文本与面部表情之间的复杂关联，推动细粒度控制和跨模态学习。  <br/>4. **开源共享资源**：提供数据集、代码及视频示例，促进研究社区在该领域的进一步探索与应用。|
|2508.11873v1|[SimInterview: Transforming Business Education through Large Language   Model-Based Simulated Multilingual Interview Training System](http://arxiv.org/abs/2508.11873v1)|**总结**：  <br/>本文提出SimInterview系统，结合多语言模型与合成AI技术，实现跨语言个性化面试训练，提升职场竞争力，并通过实验验证其有效性，同时探索文化差异对面试策略的影响及AI伦理设计框架。<br/><br/>**贡献点**：  <br/>1. **多语言个性化面试训练系统**：开发SimInterview，基于LLM构建支持英语和日语的虚拟招聘官，实现实时、个性化的模拟面试场景。  <br/>2. **动态场景匹配机制**：利用RAG技术动态适配简历与职位要求，提升面试针对性和真实性。  <br/>3. **技术集成创新**：整合Whisper（语音识别）、GPT-SoVITS（语音合成）、Ditto（虚拟形象生成）和ChromaDB（知识库）等工具，增强系统交互性和效率。  <br/>4. **实验验证有效性**：通过大学候选人实验，证明系统能精准匹配岗位需求、保留简历信息，并获得高满意度评价，尤其突出Gemma 3模型的对话质量。  <br/>5. **文化差异研究**：分析标准化日语简历与多样化英语简历对文档检索的影响，揭示文化规范对提问策略的塑造作用。  <br/>6. **可争议AI设计框架**：提出可解释、可检测偏见且保留人类监督的AI设计，满足新兴法规对透明性和公平性的要求。|
|2508.11808v1|[Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](http://arxiv.org/abs/2508.11808v1)|**总结（100字以内）:**  <br/>本文提出了一种双路径方法，通过优化提示结构和引入多模态数据增强技术提升仇恨言论的检测效果，验证了提示设计与数据组成对模型性能的关键影响，并展示了生成中性数据以增强分类器可靠性的新方向。<br/><br/>**贡献点分点列出:**  <br/>1. **双路径方法论**：首次结合提示优化与数据增强策略，系统性提升多模态仇恨检测能力。  <br/>2. **提示优化框架**：提出动态调整提示结构、监督粒度与训练模式的框架，提升模型鲁棒性与效果（如InternVL2在二分类与缩放设置中表现最佳）。  <br/>3. **数据增强技术**：设计多模态数据增强流水线，通过隔离并重写仇恨内容生成2,479个中性meme，减少虚假关联并增强模型泛化能力。  <br/>4. **多代理协同机制**：利用多代理LLM-VLM系统实现更高效的虚假关联消除与数据生成。  <br/>5. **理论启示**：证明提示结构与数据配置对模型性能的决定性作用，颠覆传统依赖模型规模的假设。  <br/>6. **实践价值**：为训练可靠、公平的VLM提供合成数据生成的新范式，支持上下文敏感的仇恨检测。|
|2508.11434v1|[Online Anti-sexist Speech: Identifying Resistance to Gender Bias in   Political Discourse](http://arxiv.org/abs/2508.11434v1)|总结：  <br/>本研究揭示了LLMs在识别反性别歧视言论时的误判问题，提出需超越二元分类框架，结合人类审核与训练数据改进，以保障数字政治空间中的抵抗性言论。<br/><br/>贡献点：  <br/>1. 首次系统分析LLMs对反性别歧视、性别歧视及中性政治推文的分类偏差，尤其关注女性议员涉事的高敏感度事件。  <br/>2. 发现模型在政治化事件中易混淆"有害"与"抵抗"的修辞表达，导致反性别歧视言论被错误过滤。  <br/>3. 提出多维度改进方案：引入人机协同审核、在训练数据中明确包含反言论样本、重构危害判定标准。  <br/>4. 跨学科整合女性主义理论与模型评估方法，为数字平台内容治理提供社会技术视角的理论支持。  <br/>5. 强调算法偏见对弱势群体发声的结构性压制，推动算法伦理在政治言论监督中的实践应用。|
|2508.10949v1|[Perturbed Public Voices (P$^{2}$V): A Dataset for Robust Audio Deepfake   Detection](http://arxiv.org/abs/2508.10949v1)|总结：  <br/>提出P$^{2}$V数据集，系统评估现有音频深度伪造检测模型的脆弱性，并展示基于该数据集训练的模型在鲁棒性和泛化能力上的显著提升，推动深度伪造检测技术发展。<br/><br/>贡献点：  <br/>1. **构建首个针对性数据集**：开发了P$^{2}$V数据集，首次全面覆盖恶意音频深度伪造的三大核心特征（身份一致的转录文本、环境噪声与对抗干扰、先进语音克隆技术），为后续研究提供真实场景基准。  <br/>2. **揭示现有检测器的严重局限**：通过实验证明当前22种检测器在P$^{2}$V数据集上的性能显著下降（平均下降43%），并量化对抗扰动和高级克隆技术对检测效果的削弱程度（16%和20-30%）。  <br/>3. **提出增强鲁棒性的新方法**：基于P$^{2}$V训练的模型在抗攻击能力上优于传统方法，同时保持对现有数据集的泛化性，为音频深度伪造检测建立了新的性能基准。  <br/>4. **公开共享数据集**：承诺在论文被会议/期刊接受后公开P$^{2}$V，促进学术界共同研究和验证深度伪造检测技术的改进。|
|2508.10494v1|[A Unified Multi-Agent Framework for Universal Multimodal Understanding   and Generation](http://arxiv.org/abs/2508.10494v1)|总结：  <br/>本文提出MAGUS框架，通过解耦多模态理解和生成流程，实现灵活的any-to-any能力，超越现有模型并在多任务中表现优异。<br/><br/>贡献点：  <br/>1. **模块化架构设计**：首次提出解耦的Cognition（认知）和Deliberation（审慎）两阶段框架，兼顾多模态理解和生成的灵活性与可扩展性。  <br/>2. **多角色协作机制**：引入三种角色条件的LLM代理（Perceiver、Planner、Reflector），通过协同对话实现结构化信息理解和规划。  <br/>3. **协同生成策略**：设计Growth-Aware Search机制，结合LLM推理与扩散模型生成，形成相互增强的生成流程。  <br/>4. **无需联合训练**：支持plug-and-play扩展性、跨模态转换及语义对齐，无需多模态联合训练即可实现端到端功能。  <br/>5. **性能突破**：在MME等基准测试中超越闭源模型GPT-4o，验证了框架在多模态任务中的优越性。|
|2508.10027v2|[LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by   LLM-Generated Synthetic Data](http://arxiv.org/abs/2508.10027v2)|**贡献点总结（100字以内）**  <br/>提出融合Transformer嵌入与手crafted语言特征的模型，提升ADRD检测性能；利用LLM生成合成语音进行数据增广，显著改善分类效果；对比单模态与多模态LLM分类器，揭示多模态模型需进一步优化的价值。  <br/><br/>**分点贡献**  <br/>1. **融合模型设计**：首次将Transformer嵌入与110个词法衍生语言学特征结合，提升ADRD检测的F1值至83.3（AUC=89.5），优于纯语言或纯Transformer基线。  <br/>2. **合成语音数据增广**：通过LLM生成的合成语音（如MedAlpaca-7B）增强训练数据，使F1值进一步提高至85.7，验证了数据增强的有效性。  <br/>3. **多模态分类效能评估**：系统测试单模态（F1=78.5）与多模态（GPT-4o=70.2, Qwen=66.0）LLM分类器，揭示多模态模型性能不足的瓶颈。  <br/>4. **临床适用性验证**：证明经过临床微调的LLM可有效支持ADRD的语音-文本分类与数据增广，为实际应用提供方法论参考。  <br/>5. **性能关联分析**：发现增广效果与合成语音与真实语音的分布相似性相关，为语音生成与疾病检测的结合提供理论依据。|
|2508.10016v2|[Training-Free Multimodal Large Language Model Orchestration](http://arxiv.org/abs/2508.10016v2)|**贡献点：**  <br/>1. **无需额外训练**：提出多模态大语言模型编排框架（MLLM Orchestration），无需额外训练即可实现多模态交互系统，简化集成流程。  <br/>2. **三重核心技术创新**：  <br/>   - (1) **中央控制器LLM**：通过设计智能代理动态分配任务至专用模型，提升模态协调效率。  <br/>   - (2) **并行TTS架构**：支持全双工交互与无缝中断处理，优化对话流畅性与实时性。  <br/>   - (3) **跨模态记忆系统**：智能合成与检索多模态信息，保持上下文连贯性并减少冗余调用。  <br/>3. **性能验证**：在标准基准上实现性能提升（+7.8%）、延迟降低（-10.3%）及解释性增强，证明框架有效性。  <br/><br/>**总结（100字以内）**：  <br/>本文提出无需额外训练的多模态大语言模型编排方法，通过中央控制器、并行TTS架构和跨模态记忆系统实现高效交互，显著提升性能与解释性，降低延迟，为多模态AI系统提供新范式。|
|2508.09767v1|[UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in   Multilingual Text-to-Speech](http://arxiv.org/abs/2508.09767v1)|**贡献点：**<br/>1. 提出UtterTune，一种轻量级适配方法，用于多语言TTS系统，无需显式G2P模块即可提升目标语言（日语）的发音可控性。<br/>2. 利用低秩微调技术，实现对日语音素级发音和音高重音的精准控制，同时保持零样本条件下其他语言的自然度与说话人相似性。<br/>3. 解决了直接处理简编码文本（如字节对编码）时的G2P建模与语调生成难题，提升多语言TTS系统的泛化能力。<br/>4. 通过客观和主观评估验证方法有效性，证明其在保持多语言性能的同时显著增强目标语言的可控性。<br/><br/>**总结：**<br/>该研究提出UtterTune方法，在无需显式G2P模块的前提下，通过低秩微调增强多语言TTS系统对日语音素级发音的控制，同时保持其他语言的自然度与零样本性能，经评估验证其有效性。|
|2508.09535v1|[AI Blob! LLM-Driven Recontextualization of Italian Television Archives](http://arxiv.org/abs/2508.09535v1)|**总结（100字以内）**  <br/>提出AI Blob!系统，整合ASR、语义嵌入与RAG技术，构建动态内容检索框架，实现档案电视素材的自动叙述生成与文化分析，推动跨学科媒体研究及AI技术应用。<br/><br/>**贡献点分点**  <br/>1. **开发AI Blob!系统**：首个将语义目录与LLMs结合用于档案电视素材检索与再语境化的实验系统。  <br/>2. **技术整合**：融合自动语音识别（ASR）、语义嵌入与检索增强生成（RAG）技术，实现音频文本化与语义化处理。  <br/>3. **动态数据处理**：采用动态内容检索机制，替代传统静态元数据，提升档案内容的语义关联性与可操作性。  <br/>4. **叙事生成创新**：通过算法筛选与结构化重组，生成具备讽刺拼贴与主题连贯性叙事片段，模拟编辑实践。  <br/>5. **提供开放资源**：公开数据集与方法论框架，支持后续跨学科研究与技术迭代。  <br/>6. **理论与实践结合**：回应媒体史与AI档案研究的学术争辩，拓展档案内容的分析维度与应用场景。|
|2508.08777v1|[Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge](http://arxiv.org/abs/2508.08777v1)|总结：  <br/>提出一种基于LLM的profile-aware框架，通过自然语言用户画像与播客匹配评估，解决传统推荐评估方法在曝光偏差和成本上的问题，实现高效、可解释的离线评估。<br/><br/>贡献点：  <br/>1. **提出新型评估框架**：首次将大语言模型（LLM）作为离线评估者，替代传统离线指标和高成本的在线A/B测试，提升播客推荐系统的评估效率与可扩展性。  <br/>2. **构建profile-aware用户画像**：从90天监听历史中提取自然语言用户画像，综合表征用户主题兴趣与行为模式，形成可解释的推荐质量分析依据。  <br/>3. **优化LLM输入机制**：通过用户画像提供高语义上下文，而非直接使用原始数据，降低输入复杂度并增强LLM对推荐与用户匹配度的推理能力。  <br/>4. **实现细粒度评估**：通过逐点（pointwise）和成对（pairwise）判断，量化用户画像与播客内容的匹配质量，提供更精确的推荐效果评估。  <br/>5. **实验验证有效性**：在47人对照研究中证明框架与人类判断高度一致，且优于仅使用原始监听数据的评估方法，验证其实际应用价值。|
|2508.08715v2|[MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation   Tutor with LLMs](http://arxiv.org/abs/2508.08715v2)|总结：  <br/>本研究提出MultiAiTutor，一种面向儿童教育的多语言生成式AI导师系统，通过LLM架构实现针对低资源语言（新加坡口音普通话、马来语、泰米尔语）的教育定制语音生成，并结合文化相关的图像描述任务验证其有效性。<br/><br/>贡献点：  <br/>1. **提出多语言儿童友好语音生成框架**：首次构建专门针对儿童语言学习的多语种生成式AI导师系统（MultiAiTutor），支持新加坡口音普通话、马来语和泰米尔三种低资源语言。  <br/>2. **整合年龄适配性与文化相关性**：通过设计适龄且具有文化背景的图像描述任务，提升儿童对多语言内容的理解与学习效果。  <br/>3. **优化LLM架构用于教育场景**：将大规模语言模型（LLM）技术应用于语音生成任务，定制化适配教育需求，改善低资源语言的生成质量。  <br/>4. **实验验证有效性**：通过客观与主观评估证明MultiAiTutor在低资源语言场景下的优越性能，为儿童语言教育提供新范式。|
|2508.08715v1|[MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation   Tutor with LLMs](http://arxiv.org/abs/2508.08715v1)|**贡献点**:  <br/>1. **提出MultiAiTutor模型**：开发首个面向儿童教育的多语言生成式AI导师系统，支持低资源语言（新加坡口音普通话、马来语、泰米尔语）的个性化语音生成。  <br/>2. **儿童友好设计**：集成年龄适应性和文化相关性，通过图像描述任务优化语音内容，提升儿童语言学习的沉浸感和实用性。  <br/>3. **多语言语音生成整合**：创新性地结合LLM架构，实现针对不同年龄段的多语言语音生成，解决低资源语言教育场景的挑战。  <br/>4. **文化相关任务应用**：在低资源语言中设计符合特定文化背景的图像描述任务，增强教育内容的本土化和情境真实感。  <br/>5. **实验验证有效性**：通过客观指标（如语音质量评估）和主观评价（如用户反馈）验证模型性能，证明其优于现有基线方法。  <br/><br/>**总结（100字以内）**:  <br/>本研究提出MultiAiTutor，一种多语言儿童友好的语音生成AI导师，解决低资源语言教育难题，通过文化相关任务和实验验证展示其优越性。|
|2508.08585v1|[Joint decoding method for controllable contextual speech recognition   based on Speech LLM](http://arxiv.org/abs/2508.08585v1)|总结：  <br/>该论文提出联合解码方法实现上下文信息的显式控制，提升语音识别性能，并扩展至敏感词抑制任务，证明未预训练的Speech LLM可通过此方法获得长上下文处理能力。<br/><br/>贡献点：  <br/>1. 提出联合解码方法，实现对上下文信息注入程度的显式控制，突破传统Prompt注入依赖模型内部注意力机制的局限性；  <br/>2. 构建支持敏感词抑制的语音识别框架，拓展了Speech LLM在隐私保护场景的应用潜力；  <br/>3. 验证该方法的有效性，证明即使非长上下文预训练的Speech LLM也能通过外部策略获得长上下文建模能力。|
|2508.08550v1|[Fine-grained Video Dubbing Duration Alignment with Segment Supervised   Preference Optimization](http://arxiv.org/abs/2508.08550v1)|**贡献点总结**  <br/>1. **问题建模创新**：首次将视频配音中的时长对齐问题转化为偏好优化框架，突破传统方法的局限性。  <br/>2. **方法提出**：设计Segment Supervised Preference Optimization (SSPO)方法，整合分段抽样策略与细粒度损失函数，针对性解决跨语言时长不匹配问题。  <br/>3. **技术优化**：通过分段抽样策略提升对局部语音片段的建模精度，结合细粒度损失函数精细化调整时长差异，增强音频与视频同步效果。  <br/>4. **实验验证**：在真实数据集上验证了SSPO方法的优越性能，显著提升视频配音中的时长对齐效果并改善观众体验。|
|2508.08131v1|[Optimal Transport Regularization for Speech Text Alignment in Spoken   Language Models](http://arxiv.org/abs/2508.08131v1)|总结：  <br/>提出OTReg方法，通过最优运输理论解决语音-文本模态间隙问题，提升多语言ASR模型泛化能力，具有轻量级和无需额外标注的特性。<br/><br/>贡献点：  <br/>1. **提出新方法OTReg**：将语音-文本对齐建模为最优运输问题，推导出用于改进SLM训练的正则化损失。  <br/>2. **解决模态间隙问题**：通过结构化对应关系减少语音嵌入的高变异性，促进跨数据集泛化。  <br/>3. **轻量级设计**：无需额外标签或可学习参数，直接集成到现有SLM训练流程中。  <br/>4. **实验证明有效性**：在多语言ASR任务中验证OTReg提升语音-文本对齐效果，显著增强模型泛化能力。|
|2508.08027v1|[Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking   Self-Supervised and Generative Approaches](http://arxiv.org/abs/2508.08027v1)|总结：  <br/>本研究系统评估了自监督ASR模型在语音障碍场景下的性能，提出基于LLM的解码方法提升可懂度，并分析了模型泛化能力和不同严重程度的识别错误，为改进dysarthric speech识别提供了新思路。<br/><br/>贡献点：  <br/>1. **系统性基准测试**：对Wav2Vec、HuBERT、Whisper等主流ASR架构在语音障碍场景下的表现进行系统性评估，明确其有效性瓶颈。  <br/>2. **LLM增强的解码方法**：首次引入BART、GPT-2、Vicuna等LLM用于ASR解码，通过语言约束优化音素恢复和语法纠错，提升可懂度。  <br/>3. **泛化能力分析**：对比不同数据集上的模型表现，揭示其在语音障碍场景中的泛化特性与局限性。  <br/>4. **识别错误分层研究**：针对语音障碍的严重程度分级，深入分析识别错误的分布规律，为错误定位与改进提供依据。|
|2508.08020v1|[EchoAid: Enhancing Livestream Shopping Accessibility for the DHH   Community](http://arxiv.org/abs/2508.08020v1)|总结：  <br/>本文提出EchoAid，整合语音转文字、RSVP与大语言模型技术，通过用户研究验证其有效性，显著改善聋哑用户直播购物体验，降低认知负担。<br/><br/>贡献点：  <br/>1. **提出针对DHH群体的语音购物辅助方案**：开发EchoAid移动应用，解决直播购物信息不畅与过载问题。  <br/>2. **整合多技术模块**：结合语音识别、RSVP视觉呈现及大语言模型，优化复杂信息流的处理。  <br/>3. **用户中心化设计**：基于8名聋哑用户的需求研究与3名参与者的反馈，迭代完善原型。  <br/>4. **系统验证与效果评估**：通过38人用户研究验证EchoAid的实用性，证实其对信息提取和认知负荷的改善作用。|
|2508.07781v1|[SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech   Translation](http://arxiv.org/abs/2508.07781v1)|**贡献点总结（100字以内）**  <br/>提出基于语法的分块策略与SASST端到端框架，融合冻结的Whisper模型与LLM，通过动态输出机制优化翻译时效性和内容质量，有效解决词序差异问题，验证了语法结构在多语言实时翻译中的关键作用。<br/><br/>**分点贡献**  <br/>1. **语法引导的分块策略**  <br/>   - 提出基于依存句法关系与标点特征的语义分块方法，精准分割音频输入为连贯语义单元，减少语义碎片化。  <br/><br/>2. **SASST框架设计**  <br/>   - 构建端到端框架，通过冻结Whisper编码器和解码器-only的大语言模型（LLM），实现高效的语法感知实时翻译。  <br/><br/>3. **动态输出优化机制**  <br/>   - 引入翻译token与<WAIT>符号的联合生成策略，同步优化翻译时机与内容质量，提升实时性。  <br/><br/>4. **目标端重排序技术**  <br/>   - 采用目标语言重排序模块，减少源语言与目标语言间词序差异带来的翻译误差。  <br/><br/>5. **跨语言翻译性能验证**  <br/>   - 在CoVoST2多语言语料库（En-De-Zh-Ja）上实验证明，该方法显著提升多语言实时翻译质量，验证语法结构对LLM驱动系统的有效性。|
|2508.07470v2|[AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning](http://arxiv.org/abs/2508.07470v2)|总结：  <br/>本文提出AURA基准，通过多模态推理评估揭示模型在因果性、同步性等六类任务中的逻辑缺陷，引入全新指标AuraScore以量化推理过程。<br/><br/>贡献点：  <br/>1. **提出AURA基准**：首个专注于跨模态（音频-视频）推理过程评估的基准，突破传统仅关注最终答案准确率的局限。  <br/>2. **六类认知任务设计**：涵盖因果性、音色音高、节奏同步、不可回答性、隐含干扰、技能画像等挑战性领域，需双模态联合推理。  <br/>3. **引入AuraScore指标**：首次分解推理为事实一致性（感知证据）和核心推理（逻辑有效性），量化评估推理质量而非仅结果。  <br/>4. **揭示模型推理缺陷**：实验发现SOTA模型虽准确率高（达92%），但推理过程得分低（<45%），证明其存在逻辑漏洞与幻觉问题。  <br/>5. **推动多模态评价研究**：指出现有基准的不足，为更全面的跨模态模型评估提供方法论和参考方向。|
|2508.07470v1|[AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning](http://arxiv.org/abs/2508.07470v1)|**贡献点总结**  <br/>该论文提出AURA基准，用于评估多模态模型的推理能力，而非仅答案准确率。引入AuraScore指标分解推理为事实一致性与逻辑有效性。发现当前SOTA模型虽高准确率，但推理得分低，揭示模型存在逻辑缺陷，推动更全面的多模态评估。  <br/><br/>**分点贡献**  <br/>1. 提出AURA基准：首个评估跨模态推理能力的多模态（音频-视频）基准，超越传统仅关注答案准确率的评测方法。  <br/>2. 设计多认知域问题：涵盖因果关系、音色音高、节奏同步、不可回答性、隐含干扰和技能分析六大领域，强制模型依赖多模态信息综合推理。  <br/>3. 创新AuraScore指标：首次将推理可信度拆解为“事实一致性”与“核心推理”两维度，提供更细致的模型性能评估工具。  <br/>4. 揭示推理缺陷：通过实验证明当前SOTA模型在推理得分上存在显著短板（<45%），提出改进多模态模型推理能力的研究方向。|
|2508.07302v2|[XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation](http://arxiv.org/abs/2508.07302v2)|**贡献点总结：**  <br/>1. 提出XEmoRAG框架，无需平行情感数据实现中泰零样本情感迁移。  <br/>2. 通过语言无关情感嵌入提取与情感匹配语句检索，实现可控情感生成。  <br/>3. 引入流匹配对齐模块，减少音高/时长差异以确保自然韵律。  <br/>4. 融合中文音色到泰语合成，提升节奏准确性及情感表达，保留说话人特征。  <br/>5. 实验证明在低资源场景下可有效跨语言传递情感，具灵活性与实用性。  <br/><br/>**精简版（100字内）:**  <br/>提出XEmoRAG框架，无需平行数据实现中泰零样本情感迁移，通过情感嵌入提取、流匹配对齐和音色融合技术，有效解决跨语言情感传递难题，实验验证其在低资源场景下的高效性和自然性。|
|2508.07302v1|[XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation](http://arxiv.org/abs/2508.07302v1)|总结（100字以内）:  <br/>XEmoRAG提出基于LLM的跨语言情感迁移框架，无需平行数据，通过语言无关情感嵌入和检索对齐模块生成自然泰语语音，有效提升情感表达与韵律一致性。<br/><br/>贡献点:  <br/>1. **开创性框架**：提出XEmoRAG，首次实现无需平行情感语料的中泰跨语言零样本情感迁移。  <br/>2. **语言无关情感表征**：通过LLM提取中文语音的情感嵌入，分离语言与情感特征，实现跨语言迁移。  <br/>3. **检索对齐机制**：从情感数据库中检索匹配情绪的泰语句子，无需显式情绪标签。  <br/>4. **韵律优化模块**：引入流程匹配对齐技术，减少音高/时长差异，提升自然度。  <br/>5. **音色融合技术**：将中文音色融入泰语合成，增强节奏准确性与情感表达，保留说话人特征。|
|2508.07295v1|[CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text   Factuality Evaluation](http://arxiv.org/abs/2508.07295v1)|总结（100字以内）:  <br/>提出跨语言-跨模态事实性评估基准CCFQA，揭示当前MLLMs在多语言语音任务中的缺陷，并开发少样本迁移策略提升性能，推动语音理解模型研究。  <br/><br/>贡献点:  <br/>1. **提出首个跨语言与跨模态事实性基准CCFQA**：包含8种语言的平行语音-文本事实性问题，系统性评估MLLMs在多语言语音输入下的事实性与可靠性。  <br/>2. **揭示当前多模态大模型的局限性**：实验表明现有模型在CCFQA中表现欠佳，凸显多语言语音处理能力不足的问题。  <br/>3. **设计高效少样本迁移策略**：将英语QA能力迁移至多语言SQA任务，仅需5样本即可达到与GPT-4o-mini-Audio相当的性能。  <br/>4. **开源代码与数据集**：公开CCFQA作为研究资源，为改进MLLMs的语音理解能力提供基础支持。|
|2508.07273v1|[Incorporating Contextual Paralinguistic Understanding in Large   Speech-Language Models](http://arxiv.org/abs/2508.07273v1)|总结：  <br/>本文提出两种方法提升语音大模型的情感理解能力，通过整合上下文与非语言信息，显著提高模型性能，并验证了LLM评估指标的可靠性。<br/><br/>贡献点：  <br/>1. 提出显式方法：直接引入情感元数据（如情绪标注）到模型训练，增强对非语言线索的感知。  <br/>2. 设计隐式方法：结合情感标注、语音转录及分类与维度标注，自动生成高质量QA对以优化训练数据。  <br/>3. 实验验证：隐式方法在基准测试中提升38.41%（LLM评分），与显式方法结合后达到46.02%，证明有效性。  <br/>4. 可靠性分析：通过LLM评分与分类指标的高相关性，论证评估方法的可信度。|
|2508.07173v1|[Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual   Large Language Models](http://arxiv.org/abs/2508.07173v1)|总结：  <br/>本文提出首个针对多模态大语言模型（OLLMs）的安全性综合评估基准Omni-SafetyBench，设计专用指标揭示模型在复杂输入下的安全缺陷，为提升OLLMs安全性提供基础。<br/><br/>贡献点：  <br/>1. 构建首个支持24种模态组合的OLLMs安全性平行基准Omni-SafetyBench，涵盖972个样本及专门的音频-视觉危害案例；  <br/>2. 提出Safety-score（基于条件攻击成功率C-ASR与拒绝率C-RR）和Cross-Modal Safety Consistency Score（CMSC-score）两类指标，分别解决模态理解失败与跨模态一致性评估问题；  <br/>3. 通过评估6个开源和4个闭源OLLMs，发现关键安全隐患：无模型在安全性和一致性上同时表现优异，复杂输入下安全防御显著弱化，部分模型在特定模态上得分极低；  <br/>4. 为OLLMs安全性研究提供系统性分析框架和改进方向，推动该领域安全评估体系的建立。|
|2508.06457v1|[ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](http://arxiv.org/abs/2508.06457v1)|**贡献点：**<br/>1. 提出ScamAgent：首个基于LLM的自主多轮对话诈骗代理，能生成模拟真实场景的流言语音剧本。<br/>2. 引入动态欺骗机制：通过对话记忆和多轮交互适应用户行为，结合欺骗性说服策略提升诈骗成功率。<br/>3. 暴露安全漏洞：证明现有LLM安全措施（如拒绝机制、内容过滤）对多轮代理框架下攻击无效。<br/>4. 实现自动化流程：利用现代TTS技术将文本剧本转化为逼真语音通话，构建完整诈骗链。<br/>5. 强调新安全需求：呼吁建立多轮对话安全审计体系，开发针对生成式AI对话欺骗的检测与干预方法。<br/><br/>**总结：**  <br/>该研究揭示了基于LLM的多轮对话诈骗代理ScamAgent对现有安全机制的突破，提出了自动化语音欺诈生成框架，并强调需构建新的对话层面安全防护方案。|
|2508.06284v1|[Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment](http://arxiv.org/abs/2508.06284v1)|总结（100字以内）:  <br/>提出基于大语言模型的两阶段训练策略，构建大规模伪标注数据集LibriAugmented，验证其在多语言和语音降质场景下的泛化能力，为语音质量评估提供低成本、可扩展的解决方案。<br/><br/>贡献点分点列出：  <br/>1. **构建大规模伪标注数据集**：提出LibriAugmented数据集（101,129条语音片段），利用微调的听觉LLM（Vicuna-7b-v1.5）生成模拟降质标注，解决SQA数据稀缺问题。  <br/>2. **提出两阶段训练框架**：设计“LLM预训练 + 人类标注微调”的混合策略，显著提升模型在DNSMOS Pro和DeePMOS上的泛化性能（如PCC提升0.08-0.12）。  <br/>3. **验证LLM作为伪评分者可行性**：通过跨语言和多降质场景的实验，证明LLM可有效替代人工标注，降低标注成本并提升系统可扩展性。|
|2508.06194v1|[Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak   Evaluation](http://arxiv.org/abs/2508.06194v1)|总结（100字以内）:  <br/>本文提出SceneJailEval框架，通过场景自适应多维评估解决现有方法的局限，构建涵盖14场景的高质量数据集，并在多个数据集上取得SOTA性能，显著提升jailbreak评估的精准度与适用性。<br/><br/>贡献点:  <br/>1. **场景自适应多维框架**：突破现有"一刀切"评估方式，支持灵活适配不同场景，增强方法扩展性。  <br/>2. **全面场景数据集**：构建包含14个场景、多样jailbreak变体及区域案例的基准数据集，填补高质量场景适配评估的空白。  <br/>3. **SOTA评估性能**：在全场景数据集上F1达0.917（+6%），在JBB数据集上F1达0.995（+3%），超越现有方法在异构场景中的准确率上限。|
|2508.06167v1|[Pragmatics beyond humans: meaning, communication, and LLMs](http://arxiv.org/abs/2508.06167v1)|总结：  <br/>该论文提出语用学作为语言与社会互动的动态接口，挑战传统理论框架，引入"上下文挫败"概念，强调人类在生成AI沟通中的主体性，并主张通过调整语用理论来适应机器学习模型的语用分析需求。<br/><br/>贡献点：  <br/>1. **理论重构**：提出语用学应被视为语言作为社会工具运作的动态接口，否定其作为"意义的第三维度"的传统定位。  <br/>2. **框架创新**：提出"人类-机器通信（HMC）"框架，替代传统符号三分法，强调连接主义LLM架构对语义等级结构的削弱。  <br/>3. **方法论转向**：主张概率语用学（如Rational Speech Act框架）取代Gricean语用学，以优化目标替代真理评估，适应LLM的预测机制。  <br/>4. **批判替换主义**：揭示LLM评估中存在的人类中心偏见，区分通用、语言和沟通三类替换主义现象，强调人类在语用主体中的不可替代性。  <br/>5. **概念提出**：引入"上下文挫败"（context frustration），描述LLM在海量上下文输入下出现的语用理解崩溃现象，突显用户在模型训练中的能动性角色。|
|2508.05880v1|[Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large   Language Models](http://arxiv.org/abs/2508.05880v1)|**贡献点：**  <br/>1. **提出CoRE基准**：构建首个大规模情感认知推理基准（CoRE），用于隐式评估LLMs在情感推理任务中所依赖的内部认知结构。  <br/>2. **基于认知评价理论**：将认知评价理论引入情感计算领域，系统分析LLMs在处理情感刺激时是否能生成连贯、合理的认知推理。  <br/>3. **设计关键实验问题**：通过三组实验，探索模型对特定认知维度的隐含依赖、情感特征的维度划分，以及情感类别表征的可解释性。  <br/>4. **揭示模型差异**：发现不同LLMs在情感认知推理中表现出显著的模式差异，为理解情感计算的模型特性提供实证依据。  <br/>5. **公开资源**：提供CoRE基准及代码，促进该领域的研究与复现。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个情感认知推理基准CoRE，基于认知评价理论分析LLMs的情感处理机制，揭示模型间差异，并公开资源推动研究进展。|
|2508.05473v1|[Embedding Alignment in Code Generation for Audio](http://arxiv.org/abs/2508.05473v1)|贡献点（分点）:  <br/>1. **提出音乐创作中代码生成的LLM应用潜力**：通过LLM辅助，用户可专注于结构动机而非语法细节，提升创意编程效率。  <br/>2. **揭示代码与音频嵌入空间的复杂映射关系**：发现代码和音频嵌入不存在简单线性关联，需通过建模学习嵌入对齐。  <br/>3. **构建代码-音频嵌入对齐预测模型**：开发新模型实现代码到音频嵌入的映射，增强生成结果的多样性与可解释性。  <br/><br/>总结（100字以内）:  <br/>本研究通过分析代码与音频嵌入空间的拓扑关系，构建预测模型实现代码到音频的对齐，提升LLM在创意编程中的多样性与效果。|
|2508.05149v1|[Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages](http://arxiv.org/abs/2508.05149v1)|**贡献点总结：**  <br/>1. 提出SLAM-ASR框架，通过可训练轻量级投影器连接语音编码器与LLM，解决低资源语言ASR问题。  <br/>2. 系统评估训练数据量需求，验证在低数据条件下达到Whisper-only性能的挑战与可行性。  <br/>3. 证明单语或多语投影器（基于高资源语言预训练）可显著缓解数据稀缺问题，提升小数据集效果。  <br/>4. 在多语言LLM（EuroLLM、Salamandra）与Whisper-large-v3-turbo结合的基准测试中，提供低资源语言与多语言优化的研究方向参考。  <br/><br/>**（总结：研究提出SLAM-ASR框架，通过轻量级投影器连接语音编码器与LLM，评估低数据环境下模型性能，并验证多语言预训练对低资源ASR的优化潜力，为相关研究提供实验依据与方向参考。）**|
|2508.04904v1|[Root Cause Analysis Training for Healthcare Professionals With   AI-Powered Virtual Simulation: A Proof-of-Concept](http://arxiv.org/abs/2508.04904v1)|**总结（100字以内）：**  <br/>提出AI驱动的3D模拟游戏用于提升医疗人员RCA能力，通过自然交互与虚拟角色、AI反馈、情感语音等技术，提供低成本、可扩展的培训方案，并计划实证评估其效果。<br/><br/>---<br/><br/>**贡献点分点列出：**  <br/>1. **开发新型RCA培训工具**：首创AI-powered 3D模拟游戏，替代传统高资源消耗的培训模式，提高训练效率与实施一致性。  <br/>2. **沉浸式交互场景**：构建模拟ICU死亡事件的虚拟环境，支持学习者通过对话与五位虚拟医护人员角色互动，提升情境理解。  <br/>3. **AI核心技术集成**：融合大语言模型（LLM）、情感文本语音合成与动画技术，实现自然、生动的交互体验。  <br/>4. **多维度反馈机制**：引入LLM提供形成性与总结性反馈，助力学习者持续改进RCA分析能力。  <br/>5. **实证研究规划**：提出系统的实证评估计划，为验证其临床培训价值奠定实验基础。|
|2508.04749v1|[Bridging Brains and Models: MoE-Based Functional Lesions for Simulating   and Rehabilitating Aphasia](http://arxiv.org/abs/2508.04749v1)|**贡献点：**  <br/>1. 提出基于模块化MoE语言模型的失语症模拟方法论，通过选择性禁用语言模块实现病理特征建模。  <br/>2. 验证模拟的失语症亚型（如布罗卡/维尔尼克失语）输出与真实患者语音高度相似。  <br/>3. 首次展示通过冻结受损模块并重新训练健康模块，可实现语言功能的计算修复。  <br/>4. 揭示模块化LLM在模拟语言障碍机制及探索治疗路径中的临床适用性与计算潜力。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过模块化MoE模型模拟失语症，验证病理输出与真实患者数据的一致性，并实现语言功能的计算修复，为语言障碍机制研究及治疗探索提供新框架。|
|2508.04531v1|[Unveiling the Landscape of Clinical Depression Assessment: From   Behavioral Signatures to Psychiatric Reasoning](http://arxiv.org/abs/2508.04531v1)|总结：  <br/>本研究提出C-MIND多模态数据集，分析行为特征与诊断相关性，评估多模态整合有效性，探索LLM在临床诊断中的局限并提出改进方法，构建数据与算法双重基础设施以推动抑郁症研究的可靠性。<br/><br/>贡献点：  <br/>1. **构建首个临床验证的多模态数据集**：C-MIND是首个基于真实医院访问收集的神经精神病学多模态诊断数据集，包含音频、视频、文本和fNIRS信号，为研究提供高质量临床数据支持。  <br/>2. **系统分析多模态任务与行为特征的诊断贡献**：通过经典模型量化不同任务（如结构化心理测试）和模态（音频、视频等）对诊断性能的影响，揭示其组合有效性。  <br/>3. **揭示LLM在临床诊断中的局限性及改进路径**：验证大语言模型在 psychiatric 推理中的不足，并提出通过整合临床专业知识提升其诊断性能（Macro-F1 提升10%）。  <br/>4. **建立临床抑郁症评估的基础设施框架**：从数据和算法角度提出系统性解决方案，推动心理健康评估研究的实用化和可靠性。|
|2508.04353v1|[LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for   Learned Thematic Significance Tracking in Multimedia Content](http://arxiv.org/abs/2508.04353v1)|**贡献点总结（100字以内）**：  <br/>提出LUST框架，结合视觉与语音多模态分析，通过LLMs实现分层的两阶段主题相关性评分，输出时序感知的视频注释与分析日志，提升用户定义重要性内容的量化评估能力。<br/><br/>**分点贡献**：  <br/>1. **多模态整合**：首次将视频视觉信息与自动语音识别（ASR）提取的文本描述结合，构建统一分析流程。  <br/>2. **分层评分机制**：设计“直接相关性”与“上下文相关性”双阶段模型，通过LLMs动态评估内容与主题的关联。  <br/>3. **时序感知分析**：引入时间维度，利用历史评分优化当前段落评估，实现对叙事演变的深度理解。  <br/>4. **可视化输出**：提供带有主题相关性标注的视频及详细分析日志，增强结果可解释性与实用性。|
|2508.04350v1|[Chain of Questions: Guiding Multimodal Curiosity in Language Models](http://arxiv.org/abs/2508.04350v1)|**贡献点总结（100字以内）：**  <br/>本文提出Chain of Questions (CoQ)框架，通过好奇心驱动的动态问题生成机制，提升多模态语言模型对复杂环境的感知模态选择与整合能力，验证了其在多模态任务中的有效性、可解释性及推理对齐性。<br/><br/>**分点贡献：**  <br/>1. **提出新框架CoQ**：首次引入基于好奇心驱动的推理方法，引导多模态模型动态生成环境相关问题，从而选择性激活视觉、听觉等感官模态。  <br/>2. **构建多模态基准数据集**：将WebGPT、ScienceQA、AVSD、ScanQA等多模态任务数据整合为新型基准，支持框架评估与对比。  <br/>3. **提升模型性能**：实验表明CoQ显著增强基础模型在复杂多模态任务中识别关键信息的能力，优化准确率、可解释性及推理过程与任务的对齐性。|
|2508.04240v1|[ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and   Neural Decoding during Reading and Listening](http://arxiv.org/abs/2508.04240v1)|贡献点：  <br/>1. **填补非英语语言数据空白**：首个针对中文的多模态神经解码基准数据集，解决非英语语料缺失问题。  <br/>2. **扩展模态覆盖**：基于原静默阅读数据集，新增朗读（RA）和被动听觉（PL）模态，覆盖更全面的语言任务。  <br/>3. **多模态数据整合**：包含高密度EEG信号、精确音频及预训练语言模型的语义嵌入，实现跨模态对齐。  <br/>4. **创新实验设计**：通过同步记录朗读EEG与音频、播放录音收集听觉EEG，建立语音时序和语义的跨模态对齐。  <br/>5. **支持联合学习**：与中文EEG结合，推动跨说话、听觉、阅读的联合语义对齐训练，促进脑-LLM协同。  <br/>6. **提供未来研究基础**：为下一代神经语义解码技术提供中文语言系统的基准数据。  <br/><br/>总结：本研究提出中文EEG-2数据集，涵盖朗读、听觉和阅读多模态，整合EEG、音频及语义嵌入，支持跨模态语义对齐学习，为中文神经解码提供基准。|
|2508.04206v1|[ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal   Movie Recommendation](http://arxiv.org/abs/2508.04206v1)|**贡献点：**  <br/>1. 提出ViLLA-MMBench，首个针对LLM增强的可复现、可扩展多模态电影推荐基准。  <br/>2. 集成MovieLens与MMTF-14K数据，实现音频、视觉、文本三模态密集嵌入对齐。  <br/>3. 利用LLM自动补全缺失/稀疏元数据，生成高质量电影剧情摘要。  <br/>4. 支持多种可配置文本编码器（Ada, LLaMA-2, Sentence-T5）生成多组嵌入数据。  <br/>5. 提供灵活的多阶段融合策略（早、中、晚融合）与多种推荐模型消融实验。  <br/>6. 实验通过单YAML文件声明式配置，简化流程并提高可操作性。  <br/>7. 引入综合评估体系，涵盖准确率（Recall, nDCG）与冷启动、覆盖率、新颖性、多样性、公平性等非传统指标。  <br/>8. 验证LLM增强对冷启动和覆盖率的显著提升，尤其在音频-视觉融合场景下表现突出。  <br/>9. 通过系统性基准揭示通用性与模型/指标特异性组合的差异性。  <br/><br/>**总结：**  <br/>提出ViLLA-MMBench，整合多模态数据并支持自动补全和灵活融合，建立全面评估体系，验证LLM增强对推荐效果的提升，开源促进多模态推荐研究。|
|2508.03360v1|[CogBench: A Large Language Model Benchmark for Multilingual Speech-Based   Cognitive Impairment Assessment](http://arxiv.org/abs/2508.03360v1)|贡献点总结：<br/>CogBench是首个跨语言跨临床场景评估LLMs语音领域泛化能力的基准，提出统一多模态评估框架，揭示思维链提示对模型性能的影响，以及LoRA微调提升泛化性的有效方法。<br/><br/>分点贡献：<br/>1. 提出CogBench基准：首个系统评估大语言模型在跨语言（英语/中文）和跨临床场景下语音认知障碍评估能力的基准框架。<br/>2. 建立统一评估流程：构建覆盖ADReSSo、NCMMSC2021-AD及新数据集CIR-E的多模态评估体系，支持多语言数据对比。<br/>3. 揭示模型泛化特性：证明传统深度学习模型在跨域迁移时性能显著下降，而思维链提示增强的LLMs展现出更好的适应性。<br/>4. 提出LoRA优化方案：通过低秩适配技术实现轻量级微调，有效提升目标领域泛化能力，为临床应用提供新思路。|
|2508.02958v1|[VRSight: An AI-Driven Scene Description System to Improve Virtual   Reality Accessibility for Blind People](http://arxiv.org/abs/2508.02958v1)|总结：  <br/>本研究提出VRSight系统和DISCOVR数据集，通过AI技术实现VR场景的无障碍访问，使盲人用户可通过空间音频反馈参与VR交互，验证了其在社交任务中的有效性。<br/><br/>贡献点：  <br/>1. **提出VRSight系统**：首个端到端VR场景后处理系统，结合AI模型（目标检测、深度估计、LLM氛围解析）生成基于音调的空间音频反馈，无需开发者干预即可增强盲人用户的VR交互能力。  <br/>2. **构建DISCOVR数据集**：首个针对VR环境的虚拟物体数据集（含30类虚拟物体，来自17款社交VR应用），替代传统现实数据集，解决VR场景建模的适配性问题。  <br/>3. **实证效果验证**：通过九名盲人用户测试VRSight在Rec Room中的应用，证明其在社交任务（如avatar感知、座位识别）中的实际有效性。|
|2508.02232v1|[Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities   via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults](http://arxiv.org/abs/2508.02232v1)|总结（100字以内）:  <br/>本研究提出Eye2Recall系统，通过整合眼动追踪与自然语言交互实现混合启动回忆体验，填补LLM驱动回忆中眼动-语音交互的研究空白，并验证其在提升老年人正向老龄化中的有效性。<br/><br/>贡献点:  <br/>1. **提出混合交互模型**：首次将眼动追踪与语音交互结合，构建基于LLM的混合启动（mixed-initiative）回忆对话框架。  <br/>2. **识别老年人挑战**：通过专家访谈系统化梳理老年人在照片回忆技术使用中的核心痛点与需求。  <br/>3. **开发Eye2Recall系统**：设计并实现支持视觉兴趣检测与自然语言交互的互动式回忆系统。  <br/>4. **实验验证有效性**：通过十人用户研究验证系统对提升老年人回忆体验、幸福感及技术适配性的实际效果。|
|2508.02228v1|[Guiding an Automatic Speech Recognition Decoder Using Large Language   Models](http://arxiv.org/abs/2508.02228v1)|**贡献点分点总结：**  <br/>1. **提出可分离的ASR框架**：通过分解MAP估计器，实现声学模型（AM）和语言模型（LM）的解耦整合，允许两者独立训练与优化。  <br/>2. **实验验证广泛性**：对比N-gram、GCNN、TransformerLM三类语言模型，在ALLSSTAR、WSJ0、TED-LIUM 3等多语种、多风格数据集上验证方法有效性。  <br/>3. **提升复杂场景性能**：方法在处理复杂句子、缩略语及领域术语等挑战性任务中表现尤为突出，显著增强ASR的泛化能力。  <br/>4. **模型兼容性**：兼容主流声学模型（wav2vec 2.0、HuBERT）与大语言模型（GPT-2、LLaMA 2、Falcon），实现技术路线的灵活适配。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种解耦ASR框架，通过迭代方法整合声学与语言模型，实现独立优化，显著提升复杂场景下的语音识别性能。|
|2508.01274v1|[Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question   Answering in Taiwan](http://arxiv.org/abs/2508.01274v1)|总结：  <br/>本文提出首个面向传统中文的三模态评估基准Multi-TW，系统对比闭源与开源模型性能，揭示端到端多模态处理在延迟上的优势，强调传统中文微调与高效架构的重要性。<br/><br/>---<br/><br/>贡献点：  <br/>1. **首个中文三模态基准**：构建Multi-TW，填补传统中文领域对视觉、听觉、文本三模态联合评估的空白。  <br/>2. **权威数据来源**：采用由SC-TOP官方开发的考试题，确保基准的规范性和实用性。  <br/>3. **跨模态性能对比**：评估多种any-to-any模型与视觉语言模型，明确闭源模型在跨模态任务中的优势。  <br/>4. **延迟优化分析**：量化端到端多模态模型与分离音频转录方案的延迟差异，为实际部署提供参考。  <br/>5. **研究方向启示**：突出传统中文微调和高效多模态架构的必要性，推动领域相关技术改进。|
|2508.00760v1|[MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese   Hate Speech Detection under Cloaking Perturbations](http://arxiv.org/abs/2508.00760v1)|总结：本文提出MMBERT框架，通过多模态整合和三阶段训练提升中文仇恨言论检测的鲁棒性，显著优于现有文本和语言模型方法。<br/><br/>贡献点：<br/>1. 首次针对中文社交网络特点，提出融合文本、语音、视觉多模态的仇恨言论检测框架MMBERT；<br/>2. 创新性地设计渐进式三阶段训练范式，解决多模态MoE架构整合中的稳定性问题；<br/>3. 引入模态特定专家模块与路由器分配策略，构建兼具模态特异性与跨模态协作的混合架构；<br/>4. 在中文多模态数据集上验证了模型有效性，显著超越传统BERT、LLMs及上下文学习方法。|
|2508.00632v1|[Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](http://arxiv.org/abs/2508.00632v1)|**贡献点总结：**  <br/>1. 提出 AVR-Eval，首个用于评估交互式音视频内容质量的相对指标，通过音频-视觉记录实现多模态对比。  <br/>2. 构建 AVR-Agent 系统，基于多模态资产库生成 JavaScript 代码，结合自动评估与多智能体协作优化内容。  <br/>3. 通过实验验证 AVR-Agent 生成内容在对抗单次生成内容时表现优异，揭示当前模型在利用自定义资产和反馈上的局限。  <br/>4. 指出人机创作本质差异：人类高效利用高质量资产和音视频反馈，而现有模型尚未充分实现这一能力。  <br/><br/>**摘要总结（100字以内）：**  <br/>本研究提出 AVR-Eval 评估指标与 AVR-Agent 系统，解决 AI 生成交互式音视频内容的质量评估与复杂生成难题，揭示人机创作差异，为多模态内容生成提供新方法。|
|2507.23088v1|[Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for   Interoperative Surgical Assistance](http://arxiv.org/abs/2507.23088v1)|总结：  <br/>本论文提出了一种结合语音控制与视觉分割技术的新型感知代理，通过创新分割机制和记忆库，显著提升手术机器人实时辅助的灵活性与自然交互能力，验证了其在动态手术环境中的有效性。<br/><br/>贡献点：  <br/>1. **提出新型感知代理（Perception Agent）**  <br/>   - 首次将语音集成的prompt-engineered LLM（大语言模型）与SAM（Segment Anything Model）及任意点跟踪模型结合，实现动态手术场景中的实时人机交互。  <br/><br/>2. **开发创新分割机制**  <br/>   - 引入两种新型机制，支持对已知和未知手术元素（如仪器、假体、纱布）的灵活分割，突破传统固定类别限制。  <br/><br/>3. **构建记忆仓库系统**  <br/>   - 设计可记忆新元素的机制，使系统具备跨手术场景的知识迁移能力，推动手术机器人向人机共生方向发展。  <br/><br/>4. **实验验证有效性**  <br/>   - 通过公开数据集定量分析与定制数据集定性评估，证明其性能可媲美传统高强度人工提示方案，且具备更自然的交互特性。|
|2507.22367v1|[Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided   LLM Representations and Multimodal Apparent Behaviors](http://arxiv.org/abs/2507.22367v1)|贡献点列表（分点说明）:<br/><br/>1. **提出新颖框架**  <br/>   设计"Traits Run Deep"人格评估框架，首次融合心理学引导的提示与多模态信号处理，突破传统方法对表面特征的依赖。<br/><br/>2. **创新文本中心融合机制**  <br/>   引入文本-中心人格融合网络，实现跨模态异步信号的对齐与整合，提升多模态信息协同表征能力。<br/><br/>3. **构建多模块处理系统**  <br/>   开发包含Chunk-Wise Projector（降维）、Cross-Modal Connector（跨模态连接）、Text Feature Enhancer（文本增强）和ensemble regression head（泛化回归头）的层级融合模块。<br/><br/>4. **人格提示的首次应用**  <br/>   首次将人格特性驱动的提示应用于大语言模型（LLM），显著提升人格感知语义的表征质量。<br/><br/>5. **跨模态行为特征融合**  <br/>   通过提取并融合音频-视觉显性行为特征，提升人格评估的准确性，实验验证MSE降低45%。<br/><br/>6. **行业竞赛验证有效性**  <br/>   在AVI Challenge 2025中取得Personality Assessment赛道第一名，证明方法优越性。<br/><br/>总结（100字以内）:  <br/>提出"Traits Run Deep"框架，创新融合心理学提示与多模态信号处理，构建文本中心融合网络，首次应用人格引导的LLM技术，在AVI Challenge 2025中取得首名，显著提升人格评估准确性。|
|2507.21411v3|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v3)|总结（100字以内）:  <br/>该研究提出通过物理对象交互实现数据讲故事的新方法，开发出InSituTale原型系统，并通过实证研究验证其直观性与实用性，为增强叙事体验提供了创新的物理-数字融合交互范式。  <br/><br/>**贡献点分点列出:**  <br/>1. **提出新交互范式**：首次引入“augmented physical data storytelling”，通过物理对象操作融合视觉化与现实环境，突破传统基于语音/手势的控制方式。  <br/>2. **构建交互映射体系**：通过调研和专家工作坊，系统化梳理常用可视化命令及其与物理操作的映射关系，为设计提供理论依据。  <br/>3. **开发原型系统**：构建InSituTale系统，结合深度摄像头物体追踪与Vision-LLM事件检测，实现动态可视化命令执行与沉浸式叙事。  <br/>4. **实证有效性验证**：通过用户研究验证系统在直观性、实用性和参与度方面的优势，证明物理-数字混合交互的可行性。|
|2507.21411v2|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v2)|总结：  <br/>提出一种结合物理对象交互与可视化技术的新型数据讲故事方法，并通过用户研究验证其交互直观性和实用价值。<br/><br/>贡献点：  <br/>1. **提出新范式**：引入“增强物理数据讲故事”（Augmented Physical Data Storytelling），首次将物理对象操作与数据可视化深度融合，拓展传统基于手势或语音的交互方式。  <br/>2. **需求驱动设计**：通过文献调查和专家工作坊，系统化梳理数据驱动展示中的可视化指令及物理操作映射规则，为后续系统设计提供理论依据。  <br/>3. **原型开发**：设计InSituTale原型系统，集成深度摄像头对象追踪与Vision-LLM事件检测技术，实现物理交互实时触发可视化操作。  <br/>4. **实证验证**：通过用户实验验证系统有效性，证明其能够支持连贯的数据叙事体验，并提升交互的直观性、实用性和沉浸感。|
|2507.21411v1|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v1)|总结（100字以内）：  <br/>该研究提出通过物理对象交互实现增强数据叙事的新方法，结合深度摄像头追踪与AI技术开发原型系统，并通过用户研究验证其直观性和有效性，推动了数据可视化与物理环境的融合应用。<br/><br/>贡献点：  <br/>1. **提出新范式**：创新性地引入“增强物理数据叙事”，将物理对象交互作为控制可视化的核心手段，突破传统依赖肢体动作或语音的交互方式。  <br/>2. **系统化研究方法**：通过调研与专家工作坊，系统归纳数据驱动展示的常用命令，并建立物理操作与命令的映射关系。  <br/>3. **原型开发**：设计InSituTale系统，整合物体追踪（深度摄像头）与Vision-LLM（多模态AI），实现现实事件的实时检测与可视化响应。  <br/>4. **验证有效性**：通过用户实验证明系统在交互直观性、实用性和沉浸式叙事体验方面的显著优势，为跨模态数据叙事提供实证支持。|
|2507.20924v1|[FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech   Concept Bottleneck Models](http://arxiv.org/abs/2507.20924v1)|总结：  <br/>本文针对社交媒体中的性别歧视识别提出三种模型（SCBM、SCBMT和XLM-RoBERTa），通过结合可解释性瓶颈概念与Transformer技术，提升分类性能，并探索元数据对任务的影响，实现多语言场景下的有效分析。<br/><br/>贡献点：  <br/>1. **提出SCBM模型**：利用描述性形容词作为人类可解释的瓶颈概念，通过大语言模型编码文本特征，构建轻量分类器解决性别歧视识别任务。  <br/>2. **SCBMT模型创新**：融合形容词表示与Transformer上下文嵌入，平衡模型可解释性与分类性能，提升细粒度解释能力。  <br/>3. **多语言模型优化**：基于XLM-RoBERTa的微调方法，结合数据增强技术，在英语和西班牙语任务中取得优异排名。  <br/>4. **元数据应用研究**：探索标注者人口统计信息等元数据对性别歧视分析的辅助作用，提升模型对源意图的识别。  <br/>5. **跨子任务方案**：为性别识别、源意图分析和分类三个子任务设计统一框架，实现多任务协同处理与性能评估。|
|2507.20730v1|[Vocalize: Lead Acquisition and User Engagement through Gamified Voice   Competitions](http://arxiv.org/abs/2507.20730v1)|**贡献点：**  <br/>1. 提出Vocalize系统：首个端到端的**游戏化语音竞赛平台**，专门用于提升用户参与度和潜在客户获取。  <br/>2. 技术融合：结合**音频处理技术**与**大型语言模型（LLMs）**，实现语音交互体验的创新设计。  <br/>3. 技术架构描述：从**技术实现角度**详细阐述了系统的设计与运作机制。  <br/>4. 实验验证：通过在**4个现场活动**中的部署，验证了系统在提升用户参与度的实际效果。  <br/>5. 应用前景拓展：指出Vocalize在**营销及品牌推广**等领域的广泛应用潜力，强调其对客户忠诚度与品牌认知的促进作用。  <br/><br/>**总结（100字以内）：**  <br/>论文提出Vocalize语音竞赛系统，整合音频处理与LLMs技术，验证其提升用户参与和营销效果，为游戏化音频应用在品牌推广中的落地提供了新思路。|
|2507.20241v1|[Reframe Your Life Story: Interactive Narrative Therapist and Innovative   Moment Assessment with Large Language Models](http://arxiv.org/abs/2507.20241v1)|总结：  <br/>本研究提出了一种结合INT与IMA的综合框架，通过模拟叙事治疗师和创新时刻评估提升治疗效果，验证其在治疗质量和深度上的优势，并推动心理健康支持的社交应用。<br/><br/>贡献点：  <br/>1. **提出综合框架**：设计包含INT（互动叙事治疗师）和IMA（创新时刻评估）的双组件系统，解决现有LLM在模拟专业心理治疗和量化治疗进展方面的不足。  <br/>2. **INT的核心功能**：实现对叙事治疗师行为的模拟，包括规划治疗阶段、引导反思层次和生成情境化、专业化的回应。  <br/>3. **IMA的评估方法**：引入“创新时刻”（IMs）作为关键指标，通过跟踪客户言语中的叙事转变量化治疗有效性。  <br/>4. **实验证明有效性**：在260个模拟客户和230个真实参与者上验证INT的显著优势，展示其在治疗质量和深度上超越标准LLMs的能力。  <br/>5. **实际应用潜力**：证明INT在合成高质量支持对话中的可行性，为心理健康领域的实际应用提供技术支持。|
|2507.20220v1|[Motion-example-controlled Co-speech Gesture Generation Leveraging Large   Language Models](http://arxiv.org/abs/2507.20220v1)|**贡献点总结**（100字以内）:  <br/>MECo提出基于动词示例和LLMs的共语手势生成框架，解决传统方法细节丢失问题，支持多模态输入与身体部位精细控制，在FGD、多样性、相似性等指标上达到SOTA，并开源代码与模型。<br/><br/>---  <br/>**分点贡献**:<br/>1. **提出新型控制方法**：通过动词示例而非预定义标签或伪标签控制手势生成，保留原始运动细节。  <br/>2. **融合多模态理解**：利用大语言模型（LLMs）微调能力，同时解析语音音频和动词示例，实现语义一致性与特征保留。  <br/>3. **显式上下文引导**：将动词示例作为显式查询上下文嵌入提示结构，取代隐式伪标签范式，提升生成可控性。  <br/>4. **多输入模态支持**：兼容动画片段、静态姿势、视频序列和文本描述等多种输入形式，增强框架通用性。  <br/>5. **SOTA性能表现**：在Fréchet Gesture Distance、运动多样性及示例-手势相似性三项指标上达到当前最优水平。  <br/>6. **开源实现**：提供代码、预训练模型和演示视频，便于研究与应用。|
|2507.20169v1|[Self-Improvement for Audio Large Language Model using Unlabeled Speech](http://arxiv.org/abs/2507.20169v1)|总结（100字以内）:  <br/>提出SI-SDA自改进方法，通过大模型解码信息评估伪标签并结合强化学习优化，实现无监督领域自适应，显著提升语音任务性能（WER、BLEU）且具备高数据效率，适用于实际部署。<br/><br/>贡献点分点:<br/>1. **提出SI-SDA方法**：首次设计无需标注数据的自提升框架，解决音频LLM在特定目标域性能下降的问题，实现高效领域自适应。  <br/>2. **伪标签质量评估机制**：创新性利用大模型解码过程中的信息，动态评估生成的伪标签质量，提升训练可靠性。  <br/>3. **强化学习优化策略**：结合强化学习优化领域适应过程，实现在多个ASR/SQA/S2TT数据集上超越现有基线的性能表现，并验证其高数据效率。|
|2507.20091v2|[ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in   Speech Language Models](http://arxiv.org/abs/2507.20091v2)|总结：  <br/>本文提出ProsodyLM，通过改进语音转文本的tokenization方式，保留更多prosody信息，使模型在预训练中能自主学习多样的语音韵律处理能力，提升情感理解、重音识别及长文本韵律一致性等任务效果。<br/><br/>贡献点：  <br/>1. **创新tokenization方案**：设计词级prosody tokens，将语音转写文本与韵律特征结合，保留更完整的语音韵律信息。  <br/>2. **揭示传统方法局限**：指出现有基于离散token的语音语言模型无法有效学习韵律，缺乏预训练阶段的韵律处理能力。  <br/>3. **实现多样化韵律能力**：证明ProsodyLM通过预训练可自主发展出对比焦点、情感识别、重音理解及长上下文韵律一致性等新兴能力。|
|2507.20091v1|[ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in   Speech Language Models](http://arxiv.org/abs/2507.20091v1)|总结：  <br/>该研究提出ProsodyLM，通过改进的语音分词方案保留完整韵律信息，使文本模型更易理解语音特征，并验证其在预训练下能自主学习多种韵律处理能力，如情感识别、焦点对比和长文本韵律一致性维护。<br/><br/>贡献点：  <br/>1. 提出ProsodyLM框架：设计基于语音转录与词级韵律标记的分词方案，直接整合语音内容与韵律特征；  <br/>2. 保留完整韵律信息：较传统语音分词方法更有效捕捉语音的连续韵律细节；  <br/>3. 实现自主学习能力：验证模型通过预训练即可发展出对比焦点、情感理解等多维度韵律处理能力；  <br/>4. 提升语音理解效果：优化后的分词方案显著增强文本模型对语音韵律特征的建模与推理能力。|
|2507.19616v1|[HITSZ's End-To-End Speech Translation Systems Combining   Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language   Model for IWSLT 2025 in Indic Track](http://arxiv.org/abs/2507.19616v1)|**贡献点总结:**<br/>1. 提出首个基于Whisper和Krutrim的端到端语音-文本翻译系统，专为低资源英语-印地语语言对设计  <br/>2. 验证端到端系统在双向翻译中达到行业领先BLEU得分（28.88/27.86）  <br/>3. 探索链式推理（CoT）方法在翻译任务中的应用潜力，发现其对解析输出的显著提升（+13.84 BLEU）  <br/>4. 指出CoT方法在格式一致性上的实施挑战，为后续研究提供方向  <br/><br/>**100字内总结:**  <br/>本文提出基于Whisper和Krutrim的端到端ST系统，达成了英语-印地语双向翻译的高水平BLEU得分，并系统评估了CoT方法在提升翻译质量中的潜力与挑战，为低资源语音翻译领域提供了新思路与实践参考。|
|2507.19303v1|[Identifying Fine-grained Forms of Populism in Political Discourse: A   Case Study on Donald Trump's Presidential Campaigns](http://arxiv.org/abs/2507.19303v1)|**贡献点：**  <br/>1. **构建民粹主义语料库**：首次创建并发布专门捕捉民粹主义细微形式的高质量数据集。  <br/>2. **模型性能对比**：系统评估开源与闭源LLMs在不同提示范式下的表现，揭示其在识别民粹主义上的局限性。  <br/>3. **微调模型优势**：证明微调后的RoBERTa分类器显著优于未微调的指令式LLMs，强调模型优化的重要性。  <br/>4. **实际应用分析**：利用最佳模型解析特朗普竞选演讲，揭示其民粹修辞策略的文本特征。  <br/>5. **泛化性研究**：通过跨区域（欧洲政治家）的演讲分析，验证模型的跨上下文迁移能力。  <br/><br/>**总结**（100字内）：  <br/>本文构建民粹主义数据集，对比LLMs性能，证实微调RoBERTa在分类任务中更优，解析特朗普演讲并验证模型跨区域泛化能力，为政治话语分析提供新视角。|
|2507.19040v1|[FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex   Spoken Dialogue Systems](http://arxiv.org/abs/2507.19040v1)|总结：该论文提出首个全面评估全双工语音对话系统的基准测试框架，通过LLMs、TTS和ASR构建测试流程，揭示现有系统的挑战，并开源相关资源促进研究。<br/><br/>贡献点：<br/>1. 提出首个针对全双工场景（FD）的综合基准测试框架，填补了现有语音对话系统评估的空白。  <br/>2. 设计多种新型指标，全面评估模型处理用户打断、延迟和复杂环境（如高噪声）的性能。  <br/>3. 对三个开源FDSDS系统进行大规模实证测试（293次对话、1200次打断、40小时生成语音），揭示其在频繁干扰下的局限性。  <br/>4. 开源演示、数据和代码，推动FDSDS领域研究与技术发展。|
|2507.18863v1|[Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and   Language Model Reconstruction](http://arxiv.org/abs/2507.18863v1)|**总结（100字以内）：**  <br/>提出一种基于音素的两阶段V-ASR框架，融合视觉特征与面部关键点，结合LLM模型实现词级重建，有效缓解音素歧义问题，在LRS2/LRS3数据集上取得17.4%和21.0%的WER性能提升。<br/><br/>**贡献点分点：**  <br/>1. 提出**音素-基于的两阶段框架**：通过分离音素预测（Stage 1）与词级重建（Stage 2），降低训练复杂度并提升最终识别准确性。  <br/>2. 融合**视觉与面部特征**：整合唇部运动和面部关键点信息，增强对说话者特异性面部特征的建模能力。  <br/>3. 引入**LLM模型（NLLB）**：利用编码器-解码器架构进行词级重建，避免直接预测带来的高错误率问题。  <br/>4. **高效数据利用**：在大型视觉数据集上进行微调，减少对超大规模预训练数据的依赖，提升模型泛化能力。  <br/>5. 打破**传统V-ASR范式**：首次将音素歧义问题与LLM结合，通过分阶段处理优化视觉信息到语言的映射路径。|
|2507.18051v1|[The TEA-ASLP System for Multilingual Conversational Speech Recognition   and Speech Diarization in MLC-SLM 2025 Challenge](http://arxiv.org/abs/2507.18051v1)|总结：  <br/>本文提出TEA-ASLP系统，针对多语言对话ASR和语音对齐任务进行创新，在Task I通过语言识别整合与多语言MOE LoRA结构提升识别性能，Task II采用英语专用模型优化，最终在两个任务中分别取得9.60%和17.49%的WER，获挑战赛第一、第二名。<br/><br/>贡献点：  <br/>1. **多语言ASR优化**（Task I）：  <br/>   - 整合语言识别模块，增强模型对多语言输入的适应性；  <br/>   - 引入多语言MOE LoRA结构，提升跨语言迁移能力；  <br/>   - 利用CTC预测的token作为自回归生成的提示，优化解码过程。  <br/><br/>2. **语音对齐模型改进**（Task II）：  <br/>   - 替换原英语-中文基线模型为英语专用模型，提升任务适配性；  <br/>   - 在时间约束下实现最小排列WER的显著降低，获挑战赛第二名。  <br/><br/>3. **大规模数据支持**：  <br/>   - 基于约180k小时多语言ASR数据训练，提升模型泛化能力。|
|2507.17718v1|[AI Telephone Surveying: Automating Quantitative Data Collection with an   AI Interviewer](http://arxiv.org/abs/2507.17718v1)|总结（100字以内）：  <br/>该研究提出基于大语言模型、ASR和语义合成的AI电话调查系统，解决了传统IVR技术的不足，通过实证验证了其在数据质量、效率和受访体验方面的有效性，并发现问卷长度与AI交互性对效果的显著相关性。  <br/><br/>贡献点：  <br/>1. **提出AI电话调查新范式**：首次将语音AI应用于定量研究，结合LLM、ASR和语音合成技术，实现自然、适应性的访谈交互，平衡人机交互与方法严谨性。  <br/>2. **设计合规性系统架构**：严格遵循定量研究规范（如问题/答案随机化、精确用语），确保数据采集过程符合研究最佳实践。  <br/>3. **构建多维度验证框架**：通过对比试点AI调查与人工调查，提出完成率、中断率、满意度三类核心评估指标，验证系统的有效性与可靠性。  <br/>4. **实证发现关键影响因子**：证明短问卷和更具响应能力的AI访谈者能显著提升三个核心指标，为优化语音调查设计提供建议。|
|2507.17578v1|[Synthetic Voice Data for Automatic Speech Recognition in African   Languages](http://arxiv.org/abs/2507.17578v1)|总结：  <br/>首次系统评估非洲语言合成语料库，提出LLM+TTS+ASR微调的三步生成框架，验证合成数据在降低标注成本的同时提升ASR性能，并推动性别差异评估与审阅协议改进。<br/><br/>贡献点：  <br/>1. **系统评估方法**：首次对非洲多语言大规模合成语音语料库进行系统性评估，填补该领域的研究空白。  <br/>2. **三步生成流程**：提出LLM驱动文本生成、TTS声音合成、ASR微调的联合框架，提升数据质量与适用性。  <br/>3. **成本与效率验证**：证明异构合成数据（250h真实+250h合成）可匹配500h真实数据基线，且成本低于真实数据1%。  <br/>4. **性别差异分析**：首次在非洲低资源语境下进行性别分开的ASR性能评估，揭示合成数据对性别模型的差异化影响。  <br/>5. **审阅协议优化**：通过研究编码员可靠性，发现合成数据需更严格的审阅标准与更精准的评估数据，推动方法改进。  <br/>6. **数据公开共享**：开放所有合成数据与模型，促进非洲语言语音技术的开放研究与社区协作。|
|2507.17288v1|[Triple X: A LLM-Based Multilingual Speech Recognition System for the   INTERSPEECH2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.17288v1)|贡献点总结：<br/>1. 提出Triple X多语言对话语音识别系统  <br/>2. 创新性编码器-适配器-LLM架构设计  <br/>3. 多阶段训练战略与多语言数据集应用  <br/>4. 在MLC-SLM挑战中取得第二名的优异成绩|
|2507.16835v2|[Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI   Interview Systems](http://arxiv.org/abs/2507.16835v2)|**贡献点分点：**  <br/>1. **大规模数据实证研究**：基于超过300,000次AI主持的面试数据，系统比较了STT-LLM-TTS组合的性能。  <br/>2. **创新评价框架**：提出LLM-as-a-Judge自动化评估框架，用于量化对话质量、技术准确性和技能评估能力。  <br/>3. **最优组件组合验证**：发现Google STT + GPT-4.1 + Cartesia TTS的组合在客观指标和用户满意度上优于竞争对手。  <br/>4. **用户体验与技术性能的关联性揭示**：揭露客观质量指标与用户满意度相关性较弱，指出用户体验依赖非纯粹技术因素。  <br/>5. **实用指导与方法论贡献**：为多模态对话系统组件选择提供实践建议，并验证了一种适用于人类-AI交互的评估方法。  <br/><br/>**总结（100字以内）**：  <br/>本文通过大规模实验揭示语音AI系统中STT-LLM-TTS组合的性能差异，发现客观指标与用户满意度弱相关，提出创新评估框架，为用户体验优化提供理论支持与应用指南。|
|2507.16291v1|[Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers](http://arxiv.org/abs/2507.16291v1)|**贡献点总结**（100字以内）:  <br/>该研究首次揭示LLMs可生成对抗性vishing脚本以绕过ML分类器，提出系统化攻击框架并验证其有效性，强调需改进检测系统和LLM的 safeguards。<br/><br/>**分项贡献点**:<br/>1. **提出新攻击向量**：首次发现大语言模型（LLMs）可被用于生成对抗性vishing转录本，通过保留欺骗意图但逃过检测来威胁网络安全。<br/>2. **构建系统化攻击流程**：设计结合提示工程技术（prompt engineering）和语义模糊化（semantic obfuscation）的攻击管道，实现对真实vishing脚本的高效改造。<br/>3. **验证攻击有效性**：在真实韩国vishing数据集（KorCCViD）上测试生成的转录本，证明其对ML分类器的欺骗效果（准确率下降30.96%）和语义相似性（BERTScore高分）。<br/>4. **量化攻击效率与成本**：展示生成过程的时间效率（均<9秒）和可扩展性（低成本操作），证明对抗性攻击的可行性及大规模应用潜力。<br/>5. **呼吁防御与监管改进**：凸显现有vishing检测框架的不足，提出需要开发更鲁棒的防御系统，并要求LLM提供商加强对抗性提示的防护措施。|
|2507.16130v1|[Disability Across Cultures: A Human-Centered Audit of Ableism in Western   and Indic LLMs](http://arxiv.org/abs/2507.16130v1)|总结（100字以内）:  <br/>该研究表明，西方LLMs在印度语料中高估计能力建设危害，而印地语模型低估；本土残疾人群的解读更注重意图、关系与韧性，凸显AI设计需结合本地语境，为全球包容性能力建设标准奠定基础。<br/><br/>贡献点：<br/>1. **跨文化情景分析**：首次系统对比西方与印度本土LLMs在非西方语境下的能力建设识别能力，揭示文化差异对模型效果的显著影响。  <br/>2. **数据集本地化**：将公开能力建设语音数据集翻译为印地语，填补了印度本地语言资源的空白，增强研究的实证基础。  <br/>3. **用户视角差异揭示**：发现印度残疾人群的能力建设解读更关注意图与社会联结，与西方模型基于文本框架的判定存在根本分歧。  <br/>4. **模型偏见机制分析**：证实LLMs对印地语表达的能力建设更宽容，暴露其对西方话语范式的盲从，提示算法需突破文化预设。  <br/>5. **全球标准设计启示**：提出需将本土残疾体验纳入AI伦理与评估框架，为构建包容性、语境敏感的全球能力建设标准提供实证依据。|
|2507.16124v1|[Benchmarking LLM Privacy Recognition for Social Robot Decision Making](http://arxiv.org/abs/2507.16124v1)|总结：  <br/>本研究通过情境完整性理论设计隐私相关场景，分析用户在智能家居环境中对机器人互动行为的偏好及隐私导向影响，发现现有LLMs在隐私管理上与人类共识较低，并提出优化提示策略以评估AI隐私感知的潜在。<br/><br/>贡献点：  <br/>1. 提出基于情境完整性理论（CI）的隐私相关场景框架，用于评估LLMs在家庭社交机器人中的隐私意识。  <br/>2. 调查用户隐私偏好与行为选择之间的关系（N=450），揭示隐私导向对交互决策的影响。  <br/>3. 对比分析10种前沿LLMs在隐私敏感场景中的表现，发现其与人类共识度显著偏低。  <br/>4. 设计并验证四种新型提示策略，探索提升LLMs隐私管理能力的潜在方法。  <br/>5. 探讨AI隐私意识在人机交互中的实际意义与未来发展方向。|
|2507.15729v1|[Gaze-supported Large Language Model Framework for Bi-directional   Human-Robot Interaction](http://arxiv.org/abs/2507.15729v1)|**贡献点总结：**  <br/>1. 提出结合注视和语音的多模态HRI系统，实现环境感知与动态用户支持。  <br/>2. 设计模块化、可移植架构，适配不同任务与机器人。  <br/>3. 引入实时语言交互状态表示及高效感知模块。  <br/>4. 通过公开活动提升系统鲁棒性与用户体验。  <br/>5. 实验对比LLM与传统脚本方法，揭示其适应性优势和冗余问题。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出一种基于LLM的多模态HRI接口，通过注视与语音信息提升辅助机器人对环境的感知与动态支持，设计模块化架构以适配多任务，验证其在适应性与用户参与度上的优势，并通过实验揭示LLM潜在的冗余问题。|
|2507.15221v1|[EchoVoices: Preserving Generational Voices and Memories for Seniors and   Children](http://arxiv.org/abs/2507.15221v1)|**贡献点总结（分点）:**  <br/>1. **提出EchoVoices系统**：构建专为老年人和儿童设计的端到端数字人框架，实现语音与记忆的永久保存。  <br/>2. **三项核心技术创新**：  <br/>   - k-NN增强的Whisper模型：提升非典型语音（如老年人或儿童发音）的识别准确率。  <br/>   - 年龄自适应VITS模型：生成高保真、符合特定年龄群体语音特征的合成音频。  <br/>   - LLM驱动智能体：自动创建个性化数字人“人设卡”，并结合RAG系统保障对话连贯性。  <br/>3. **实验验证有效性**：在SeniorTalk和ChildMandarin数据集上证明系统提升语音识别、合成质量和跨代一致性。  <br/>4. **创新应用场景**：为代际交流提供新途径，推动数字遗产保存与跨代文化传承。  <br/><br/>**总结（100字以内）：**  <br/>论文针对老年人和儿童的语音特性，提出EchoVoices系统及三项核心技术改进，通过实验验证其有效性，并探索数字人技术在保存代际语音、促进跨代连接中的应用价值。|
|2507.14815v1|[FastLongSpeech: Enhancing Large Speech-Language Models for Efficient   Long-Speech Processing](http://arxiv.org/abs/2507.14815v1)|**总结（100字以内）:  <br/>本文提出FastLongSpeech框架，通过迭代融合和动态压缩训练解决长语音处理中的数据不足和计算效率问题，并开发LongSpeech-Eval基准评估效果。实验显示方法在长、短语音任务中均表现优异，显著提升推理效率。**<br/><br/>**贡献点分点总结:  <br/>1. 提出FastLongSpeech框架：首次设计无需专用长语音训练数据的模型扩展方案，支持高效长语音处理。  <br/>2. 创新迭代融合策略：通过动态压缩长序列，将其转化为可管理长度以降低计算成本。  <br/>3. 动态压缩训练方法：利用不同压缩比的短语音数据训练模型，迁移其能力至长语音任务。  <br/>4. 构建LongSpeech-Eval基准：提出专门评估长语音理解能力的基准测试工具。  <br/>5. 验证综合性能：实验证明方法在长/短语音任务中均表现良好，且显著提升推理效率。**|
|2507.13468v1|[ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in   Human-Robot Conversations](http://arxiv.org/abs/2507.13468v1)|总结：  <br/>本论文提出了ERR@HRI 2.0挑战，构建了一个包含多模态特征的对话机器人失败数据集，并推动基于社会信号的错误检测研究，通过系统标注和性能评估促进人机交互的改进。<br/><br/>**分点贡献：**  <br/>1. **构建多模态失败数据集**：提供16小时真实双人对话数据，涵盖面部、语音及头部运动特征，用于研究对话机器人在交互中的错误模式。  <br/>2. **系统标注错误与用户意图**：对每个交互标注机器人错误（系统角度）和用户意图（纠正预期），解决行为与期望不匹配的问题。  <br/>3. **推动错误检测方法研究**：邀请团队开发基于多模态数据的机器学习模型，聚焦于检测对话机器人失败行为。  <br/>4. **多维度评估框架**：采用检测准确性和误报率为指标，标准化对模型性能的评估方法。  <br/>5. **促进社会信号分析应用**：通过挑战推动社会信号（如面部表情、头部动作）在人机交互错误检测中的实际应用与研究。|
|2507.13205v2|[Automatically assessing oral narratives of Afrikaans and isiXhosa   children](http://arxiv.org/abs/2507.13205v2)|**贡献点总结（100字以内）：**  <br/>该研究提出用于自动评估南非荷兰语和祖鲁语学前口语叙述的系统，通过对比线性模型与大型语言模型（LLM）验证其有效性，表明LLM在识别干预需求方面可与人类专家媲美，为教师提供自动化评估工具，提升个性化教学支持能力。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **双语言评估系统开发**：首次构建支持南非荷兰語（ Afrikaans ）和祖魯語（ isiXhosa ）的自动口语叙述评估系统。  <br/>2. **模型对比与优化**：提出线性模型与大型语言模型（LLM）的对比框架，验证LLM在预测叙事与理解评分中的优越性。  <br/>3. **人工干预识别有效性**：证明LLM系统在标识需干预儿童的表现上与人类专家相当，具备实际教育应用潜力。  <br/>4. **教学支持创新**：为教师提供自动化评估工具，释放其时间用于个性化教学，推动教育效率提升。|
|2507.13205v1|[Automatically assessing oral narratives of Afrikaans and isiXhosa   children](http://arxiv.org/abs/2507.13205v1)|总结（100字以内）:  <br/>本文提出跨语言（南非语和祖鲁语）的自动叙事评估系统，验证大语言模型在干预识别上的有效性，并展现出线性模型的实用性，为教师提供高效支持工具以促进幼儿读写能力发展。<br/><br/>贡献点:  <br/>1. **跨语言评估系统**：开发适用于南非语（Afrikaans）和祖鲁语（isiXhosa）的自动口语叙事评估系统，填补了多语言场景下的研究空白。  <br/>2. **模型对比实验**：系统性比较线性模型与大语言模型（LLM）在叙事和理解评分中的表现，证明LLM在多数情况下更优，同时验证线性模型的可行性。  <br/>3. **干预识别效能**：LLM系统在标记需要干预的儿童方面达到与人类专家相当的准确性，提升评估的效率和客观性。  <br/>4. **实际应用价值**：为教师提供自动化工具，减少人工负担，使他们能更专注个性化教学支持，助力早期教育发展。|
|2507.12619v1|[BootSeer: Analyzing and Mitigating Initialization Bottlenecks in   Large-Scale LLM Training](http://arxiv.org/abs/2507.12619v1)|总结（100字以内）:  <br/>该论文首次深入分析LLM训练启动开销，提出系统优化框架Bootseer，通过解决容器镜像加载、依赖安装和模型恢复三大瓶颈的三种技术，实测将启动延迟降低50%，为工业级大模型训练效率提升提供关键解决方案。<br/><br/>贡献点：  <br/>1. **首次系统分析**：基于真实生产数据，首次对LLM训练启动开销进行深入量化分析，揭示其成分与对GPU资源的直接消耗。  <br/>2. **启动瓶颈识别**：明确识别出容器镜像加载、运行时依赖安装、模型检查点恢复三类关键启动瓶颈。  <br/>3. **优化框架设计**：提出系统级优化框架Bootseer，针对上述瓶颈设计出三种创新技术（热块预取、依赖快照、条纹HDFS-FUSE）。  <br/>4. **实测效果验证**：在生产环境中部署并评估Bootseer，实验证明其能有效减少50%的启动开销，显著提升训练效率。|
|2507.12252v1|[Improving Contextual ASR via Multi-grained Fusion with Large Language   Models](http://arxiv.org/abs/2507.12252v1)|总结：  <br/>本文提出一种基于多粒度融合的ASR增强方法，结合LLMs的上下文知识与ASR的声学信息，有效提升关键词识别性能，同时保持非关键词文本准确性，并通过消融实验验证了其组件的互补性。<br/><br/>贡献点：  <br/>1. 提出**多粒度融合框架**，联合利用token-level与phrase-level融合策略，综合提升关键字识别精度与整体语义理解能力。  <br/>2. 设计**late-fusion策略**，创新性地将ASR的声学信息与LLMs的语义知识结合，实现细粒度与粗粒度信息的平衡协同。  <br/>3. 在**中英文数据集**上验证方法有效性，取得关键词相关指标的SOTA性能，同时保持非关键词文本的高准确率。  <br/>4. 通过**消融研究**明确token-level与phrase-level组件对性能的独立贡献，证明其在联合框架中的互补性。|
|2507.11966v1|[Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](http://arxiv.org/abs/2507.11966v1)|总结：  <br/>本文提出一种两阶段框架，解决低资源语言毒性翻译问题，通过少样本提示工程与模型优化提升翻译质量与文化敏感度，推动包容性NLP在内容审核等场景的应用。  <br/><br/>**贡献点：**  <br/>1. **提出毒性保持翻译框架**：开发可复现的双阶段方法，专门应对低资源语言对中文化嵌入的有害内容翻译挑战。  <br/>2. **人验证少样本提示工程**：通过迭代筛选和评估人工选中的Singlish目标示例，精准捕捉俚语、语调与毒性特征。  <br/>3. **模型-提示对优化策略**：利用语义相似度（直接与回译）对比多个大语言模型，提升翻译效率与效果。  <br/>4. **定量人类评估验证**：通过实证数据验证框架的有效性与效率，为低资源语言翻译提供可靠依据。  <br/>5. **支持文化敏感度治理**：为多文化大模型提供基于语境的审核基准，增强跨语言、跨文化的平台治理能力。  <br/>6. **推动包容性NLP实践**：以Singlish为试验平台，强调保护社会语言学细微差别对实际应用的重要性。|