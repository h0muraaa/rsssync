|Source|Title|Summary|
|---|---|---|
|2509.17965v1|[Benchmarking Humans and Machines on Complex Multilingual Speech   Understanding Tasks](http://arxiv.org/abs/2509.17965v1)|总结（100字以内）:  <br/>本研究提出系统范式，比较人机在多语言混合语音场景中的语音问答表现。发现人类母语注意力优于第二语言，机器在干净语音中表现优异，但难以在多说话人情况下聚焦。揭示了人机注意力机制的关键差异。<br/><br/>贡献点:  <br/>1. 提出首个系统性范式，用于同时研究人类与机器在多语言及混合通道语音环境下的问答任务。  <br/>2. 首次揭示人类在母语（L1）中相较第二语言（L2）具有显著更强的选择性注意力能力。  <br/>3. 通过对比实验发现，语音大模型在单说话人场景匹配或超越人类表现，但难以处理双说话人混合场景，凸显人机注意力机制的本质差异（人类依赖高效线索，机器采用并行提取）。|
|2509.17855v1|[Make Every Letter Count: Building Dialect Variation Dictionaries from   Monolingual Corpora](http://arxiv.org/abs/2509.17855v1)|**贡献点：**  <br/>1. **提出DiaLemma标注框架**：开发了一种新型标注方法，仅基于单语数据构建方言变异词典，解决方言标准化缺失的问题。  <br/>2. **构建大规模方言对照数据集**：利用DiaLemma框架生成包含100K人类标注的德语-巴伐利亚词对数据集，作为方言理解的基准数据。  <br/>3. **系统评估LLMs的方言处理能力**：对九种主流的大语言模型进行实验，分析其在方言词汇识别、翻译及词形变体判断任务中的表现差异。  <br/>4. **揭示方言识别的关键难点**：发现模型在区分直接翻译与词形变体方面存在显著困难，尤其在动词等词类上表现不佳。  <br/>5. **探索上下文对性能的影响**：证明上下文信息可提升翻译准确率，但可能干扰对方言变体的识别，揭示任务间的权衡关系。  <br/>6. **强调方言适配的重要需求**：指出现有LLMs在正字法方言处理中的局限性，呼吁加强方言适应性的研究与技术改进。  <br/><br/>**总结（100字内）：**  <br/>本研究提出DiaLemma框架并构建德语-巴伐利亚方言数据集，系统评估LLMs在方言理解中的表现，揭示其在区分翻译与变体上的局限性，强调需进一步优化模型以适应方言处理需求。|
|2509.17449v1|[SLAyiNG: Towards Queer Language Processing](http://arxiv.org/abs/2509.17449v1)|总结：  <br/>本研究构建了首个专注酷儿俚语的SLAyiNG数据集，涵盖多源真实语料，提出专家与社区协作的注释方法，并评估了模型在意义消歧任务的表现，为应对非法语料标注挑战提供新思路。<br/><br/>贡献点：  <br/>1. **首次构建专用数据集**：提出SLAyiNG，首个系统收集注释的酷儿俚语数据集，来源包括字幕、社交媒体和播客。  <br/>2. **多源真实语料整合**：覆盖多样化语境（影视对话、社交媒体、播客）以反映真实社会身份表达场景。  <br/>3. **注释流程标准化**：详细描述术语收集、定义匹配、语境示例抓取及持续迭代的注释方法。  <br/>4. **跨模态评估**：通过人机协作计算Krippendorff's alpha（平均0.746），验证数据质量并暴露模型对敏感语料的处理局限。  <br/>5. **推动伦理实践**：强调酷儿语言的敏感性，倡导专家与社区驱动的注释机制以提升数据可靠性。|
|2509.17022v1|[VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven   Module](http://arxiv.org/abs/2509.17022v1)|**总结（100字以内）:**  <br/>本文提出VAInpaint方法，结合视觉分割与语言模型多阶段处理，实现音视频协同修复，并通过定制数据集提升音频分离性能，实验结果达到现有基准水平。<br/><br/>---<br/><br/>**贡献点:**  <br/>1. **提出VAInpaint新框架**：首次整合视觉分割模型与大语言模型（LLM），通过多阶段协同处理实现视频和音频的同步修复。  <br/>2. **多尺度描述生成机制**：利用全局LLM分析场景，结合区域特定模型的局部描述，生成更精确的文本查询引导音频分离。  <br/>3. **定制化数据集优化**：构建包含分段乐器图像和VGGSound背景的专用数据集，针对性提升音频分离模型的泛化能力。  <br/>4. **效果验证**：在音视频修复任务中实现与当前主流方法相当的性能，验证了跨模态协同处理的有效性。|
|2509.16648v1|[FESTA: Functionally Equivalent Sampling for Trust Assessment of   Multimodal LLMs](http://arxiv.org/abs/2509.16648v1)|贡献点：<br/>1. 提出FESTA方法：首个针对多模态大语言模型的不确定性度量框架，通过等效采样与互补采样生成输入空间扩展<br/>2. 任务保留设计：创新性地保持任务相关性的同时，同时评估模型一致性（等效样本）和敏感性（互补样本）<br/>3. 无监督特性：仅需模型输入输出接口（黑盒），无需依赖ground truth标注数据<br/>4. 多模态验证：在视觉和音频推理任务中均实现显著性能提升（视觉LLMs提升33.3%，音频LLMs提升29.6%）<br/>5. 开源实现：提供可复现的代码框架，促进后续研究与应用<br/><br/>总结：本研究提出FESTA方法，通过等效与互补采样生成不确定性度量，实现无监督多模态模型信任评估，显著提升视觉和音频任务的预测选择性性能。|
|2509.16589v1|[Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A   Case Study with In-the-Wild Data](http://arxiv.org/abs/2509.16589v1)|总结（100字以内）:  <br/>本研究提出CP-Bench基准，聚焦语音大模型在语境中理解情感与语调等非语言线索的能力，构建了需语言与共情理解的问答数据集，评估多类模型并分析温度调优效果，揭示当前评估体系缺陷，为提升语音模型的社会情感理解能力提供指导。<br/><br/>贡献点：  <br/>1. **提出新型基准**：CP-Bench专为评估语音大模型的语境下语用推理能力（如情感、语调理解）设计，填补现有评测在社交情感智能方向的空白。  <br/>2. **构建多模态数据集**：包含两个需同时整合语言内容与非语言线索（如情感、韵律）的问答数据集，强调共情理解与多模态交互。  <br/>3. **多模型对比分析**：系统评估开放源与闭源的SOTA语音大模型，覆盖不同问题类型，揭示模型在语用理解上的差异。  <br/>4. **温度调优研究**：针对Top 2模型进行温度调优实验，分析其对语用推理任务的调控作用，探讨模型响应的可解释性。  <br/>5. **启发模型优化**：通过基准测试结果指出现有评估的局限性，为开发更情境感知、情感智能的语音模型提供理论与实践依据。|
|2509.16496v1|[Synergies between Federated Foundation Models and Smart Power Grids](http://arxiv.org/abs/2509.16496v1)|总结：  <br/>本文首次将多模态多任务联邦基础模型应用于智能电网领域，提出双向视角框架，探讨模型在电网关键任务中的增强潜力及电网约束对模型设计的影响，推动跨学科融合研究。<br/><br/>贡献点：  <br/>1. **首次引入M3T FedFMs到电力系统领域**：提出将多模态多任务基础模型与联邦学习结合，探索其在智能电网中的应用潜力。  <br/>2. **双向视角分析**：从"智能电网赋能FedFMs"和"FedFMs赋能智能电网"两方面，系统研究模型与电网系统的相互作用。  <br/>3. **隐私保护与可扩展性**：强调FedFMs在分布式数据源上实现隐私安全建模的特性，适用于电网边缘异构数据处理。  <br/>4. **多任务场景验证**：验证模型在负荷预测、故障检测等电网关键任务中的有效性，展示其跨模态处理能力。  <br/>5. **电网约束建模**：分析电力系统在能源、通信、监管等维度的约束条件，指导FedFMs的架构设计与部署优化。|
|2509.16264v1|[Gender and Political Bias in Large Language Models: A Demonstration   Platform](http://arxiv.org/abs/2509.16264v1)|总结：  <br/>ParlAI Vote是一款整合欧洲议会辩论、演讲及投票数据的交互平台，揭示LLMs在政治分析中的系统性偏差，支持多场景应用与可视化分析，降低研究复现门槛。<br/><br/>贡献点：  <br/>1. **构建交互式政治分析平台**：首次整合欧洲议会的辩论、演讲与投票结果，提供统一的数据访问和可视化界面。  <br/>2. **丰富多维度数据支持**：包含性别、年龄、国家、政治派别等详细人口统计学数据，增强分析的全面性。  <br/>3. **量化LLMs的预测偏差与偏见**：通过对比真实投票结果与LLMs预测，揭示其在不同群体间的系统性性能差异。  <br/>4. **统一数据-模型-分析流程**：集成数据、前沿LLMs和可视化工具，简化研究复现、行为审计及反事实实验的流程。  <br/>5. **促进多方应用与透明度**：支持立法决策的研究、教育及公众参与，客观展现LLMs在政治分析中的优势与局限。|
|2509.16028v1|[Think, Verbalize, then Speak: Bridging Complex Thoughts and   Comprehensible Speech](http://arxiv.org/abs/2509.16028v1)|总结（100字以内）：  <br/>提出Think-Verbalize-Speak框架，解耦LLM推理与语音生成，引入ReVerT延迟优化算法，提升语音自然度与简洁性，同时保持推理性能，为语音对话系统提供高效解决方案。<br/><br/>贡献点：  <br/>1. **提出新型框架**：设计Think-Verbalize-Speak架构，分离LLM推理过程与语音生成，保留LLM完整推理能力。  <br/>2. **引入verbalizing机制**：通过将思维转化为自然语音文本，解决文本与语音传递的适配问题。  <br/>3. **开发ReVerT算法**：基于增量异步摘要的延迟高效verbalizer，优化生成效率与质量。  <br/>4. **实验证明有效性**：在多基准测试中验证方法对语音自然度、简洁性的提升，且推理性能损失可忽略。|
|2509.15476v1|[Evaluating Multimodal Large Language Models on Spoken Sarcasm   Understanding](http://arxiv.org/abs/2509.15476v1)|**贡献点：**  <br/>1. 系统性评估LLMs与多模态LLMs在英语（MUStARD++）和中文（MCSD 1.0）数据集上的讽刺检测性能，涵盖零样本、少样本及LoRA微调场景。  <br/>2. 提出协作门控融合模块，用于整合多模态特征表示，提升模型在跨模态任务中的表现。  <br/>3. 揭示音频单模态模型在讽刺检测中的最强性能，同时验证文本-语音和音频-视觉组合优于传统单模态及三模态模型。  <br/>4. 验证多模态LLMs（如Qwen-Omni）在跨语言、零样本及微调设置下的竞争力。  <br/>5. 强调多模态LLMs在跨语言音频-视觉-文本讽刺理解中的潜力，推动综合模态分析方法的探索。  <br/><br/>**总结：**  <br/>本文系统评估多模态大模型在中文和英文数据集的讽刺检测能力，提出协同融合机制，发现语音单模态最优，且多模态组合优于传统方法，验证了模型在跨语言任务中的潜力。|
|2509.15362v1|[Speech Language Models for Under-Represented Languages: Insights from   Wolof](http://arxiv.org/abs/2509.15362v1)|**总结（100字以内）:**  <br/>该研究首次构建沃洛夫语Speech LLM，通过大规模自发语音数据预训练实现优于基线和非洲专用模型的ASR性能，并探索链式思维提升翻译效果，所有模型与代码将开源。<br/><br/>**贡献点:**  <br/>1. **数据构建与预训练策略**：提出针对沃洛夫语的大规模自发语音数据收集方案，证明持续预训练HuBERT在ASR任务中超越基线和非洲专用模型。  <br/>2. **首个沃洛夫语Speech LLM**：将语音编码器整合到沃洛夫语语言模型中，开发出首个支持该语言的Speech LLM，实现语音翻译等多任务能力。  <br/>3. **链式推理增强性能**：探索Speech LLM在转录或翻译前的多步骤链式思维训练，提升模型对复杂任务的处理能力。  <br/>4. **跨任务效果验证**：实验证明Speech LLM在语音识别和语音翻译任务中均表现优异，验证其综合性能优势。  <br/>5. **开源共享**：公开模型与代码，推动资源匮乏语言的语音模型研究与应用。|
|2509.15095v2|[Listening, Imagining & Refining: A Heuristic Optimized ASR Correction   Framework with LLMs](http://arxiv.org/abs/2509.15095v2)|总结：  <br/>本文提出LIR-ASR框架，结合LLM与人类听觉机制，通过“听-想-改”策略生成并修正语音变体，引入FSM优化和规则约束提升准确性，实验验证其在英汉ASR中显著降低CER/WER。<br/><br/>贡献点：  <br/>1. 提出LIR-ASR框架：基于LLM的启发式迭代修正方法，模拟人类听觉感知机制。  <br/>2. “听-想-改”策略：生成语音变体并结合上下文进行修正，提升转录质量。  <br/>3. FSM启发式优化：防止修正过程陷入局部最优，增强算法鲁棒性。  <br/>4. 规则约束机制：维护语义一致性，避免错误修正导致语义偏差。  <br/>5. 实验验证：在英汉ASR任务中实现平均1.5个百分点的CER/WER降低，证明有效性。|
|2509.15095v1|[Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction   Framework with LLMs](http://arxiv.org/abs/2509.15095v1)|总结：  <br/>提出LIR-ASR框架，结合LLM与人类听觉机制，通过"听-想-改"策略显著提升语音识别准确率。<br/><br/>贡献点：  <br/>1. **创新框架设计**：提出基于LLMs的迭代修正框架LIR-ASR，模仿人类听觉感知过程，实现更精准的语音转文字。  <br/>2. **"听-想-改"策略**：通过生成音素变体并上下文优化，逐层修正识别错误，提升语义一致性。  <br/>3. **有限状态机优化**：引入FSM防止修正过程陷入局部最优，提升算法收敛效率。  <br/>4. **规则约束机制**：通过规则保持语义 fidelity，避免语义偏差。  <br/>5. **跨语言有效性验证**：在英汉ASR任务中均实现平均CER/WER降低1.5个百分点的显著提升。|
|2509.15082v1|[From Who Said What to Who They Are: Modular Training-free Identity-Aware   LLM Refinement of Speaker Diarization](http://arxiv.org/abs/2509.15082v1)|贡献点总结：<br/>1. 提出首个无需额外训练的模块化说话人对齐系统，整合SD、ASR和LLM<br/>2. 创新性地使用结构化LLM提示，通过语义连续性优化低置信度说话人标签<br/>3. 实现说话人身份识别的端到端流程，突破SD的伪标签限制<br/>4. 在真实医疗场景数据集上验证，相较基线方法降低29.7%错误率<br/>5. 构建了可扩展的实用化技术管道，无需数据标注即可完成语音内容分析<br/><br/>（99字）  <br/>该研究提出无需训练的模块化框架，结合SD、ASR与LLM实现真实场景中说话人对齐、语音识别和身份识别，相较基线方法降低29.7%错误率，为实际应用提供完整解决方案。|
|2509.14930v1|[Cross-Modal Knowledge Distillation for Speech Large Language Models](http://arxiv.org/abs/2509.14930v1)|总结：  <br/>该论文首次系统评估语音大模型中的灾难性遗忘与模态不等价问题，提出跨模态知识蒸馏框架解决跨模态对齐与推理能力下降问题，并通过实验验证其有效性。<br/><br/>贡献点：  <br/>1. **首次系统评估**：提出首个针对语音大语言模型（Speech LLM）的灾难性遗忘（Catastrophic Forgetting）和模态不等价（Modality Inequivalence）问题的系统性分析。  <br/>2. **提出解决方案**：设计跨模态知识蒸馏框架（Cross-modal Knowledge Distillation），通过文本-文本和语音-文本双向通道迁移文本教师模型的知识至语音LLM。  <br/>3. **实验验证有效性**：在对话与音频理解任务中验证方法有效性，证明其可保留文本知识、提升跨模态对齐能力，并增强语音交互中的推理性能。|
|2509.14882v1|[Llama-Mimi: Speech Language Models with Interleaved Semantic and   Acoustic Tokens](http://arxiv.org/abs/2509.14882v1)|**贡献点：**<br/>1. 提出Llama-Mimi架构：首次联合建模语义与声学标记，采用统一tokenizer和单Transformer解码器处理交错序列。<br/>2. 实现SOTA性能：在声学一致性与说话人身份保持方面达到当前最优效果。<br/>3. 揭示量化器矛盾：分析发现量化器数量增加虽提升声学保真度，但会损害语言性能，凸显长期连贯性挑战。<br/>4. 创新评估方法：引入LLM-as-a-Judge框架，系统评估生成语音内容质量。<br/>5. 开源资源：公开模型、代码及语音样本，促进研究复现与应用。<br/><br/>**总结（100字内）：**  <br/>Llama-Mimi通过统一token化与Transformer解码器联合建模语义和声学信息，在声学一致性与说话人身份保持上达SOTA。研究揭示量化器数量与性能的权衡，并提出LLM-based评估方法，开源资源推动领域发展。|
|2509.14880v2|[From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition](http://arxiv.org/abs/2509.14880v2)|总结：  <br/>本研究系统评估LLM解码器对VSR的影响，发现其性能提升主要源于词汇处理而非语义理解，组合数据集可增强泛化能力，并确立了Llama-2-13B在无额外监督下的SOTA表现，强调需强化视觉编码器以推动模型发展。<br/><br/>贡献点：  <br/>1. **系统性对比实验设计**：通过冻结/选择性更新视觉编码器、扩展解码器规模、对比不同适应策略与架构，全面分析LLM解码器对VSR的贡献。  <br/>2. **跨数据集泛化验证**：在LRS2、LRS3及WildVSR上评估，揭示组合多数据集可显著提升模型的通用性与性能。  <br/>3. **语义分析区分贡献来源**：明确性能提升主要来自**词汇层面**的处理而非语义层面，推动对VSR机制的理解。  <br/>4. **SOTA模型表现**：基于组合数据集训练的Llama-2-13B模型在LRS3和WildVSR上分别达到24.7%和47.0%的WER，超越无额外监督的基线模型。  <br/>5. **关键结论与方向指引**：指出LLM解码器优化更多是提升**上下文推理能力**而非视觉特征，强调需进一步加强视觉编码器以实现突破。|
|2509.14880v1|[From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition](http://arxiv.org/abs/2509.14880v1)|总结（100字以内）:  <br/>本研究通过系统评估LLM解码器在VSR中的作用，发现其主要提升来源于词汇处理而非语义理解，强调需加强视觉编码器能力。所提模型在LRS3和WildVSR上取得SOTA性能，为自监督语音识别的优化提供新方向。<br/><br/>贡献点:  <br/>1. **系统评估LLM解码器的作用机制**：通过冻结/更新视觉编码器、扩展解码器规模、对比不同适应策略，明确LLM解码器的性能提升是否源于视觉理解或语言建模能力。  <br/>2. **揭示数据集组合的泛化优势**：实验显示跨数据集训练（LRS2 + LRS3）显著提升模型在LRS3和WildVSR的泛化能力，而单一数据集扩展效果有限。  <br/>3. **区分词汇与语义处理的影响**：语义分析表明，LLM解码器对VSR的提升主要来自词汇层面的建模，而非深层次语义理解。  <br/>4. **提供SOTA性能基准**：基于组合数据集训练的Llama-2-13B模型在LRS3（24.7% WER）和WildVSR（47.0% WER）中达到当前无额外监督模型的最优性能。  <br/>5. **指导未来模型改进方向**：研究指出LLM解码器优化更多在于上下文推理能力，而非视觉特征，进而强调视觉编码器的改进是推动VSR突破的关键。|
|2509.14627v1|[Towards Human-like Multimodal Conversational Agent by Generating   Engaging Speech](http://arxiv.org/abs/2509.14627v1)|**贡献点总结（100字以内）:**  <br/>提出多感官对话数据集与基于多模态LLM的语音生成模型，结合视觉和语音模态提升对话的自然性与吸引力，验证多模态融合的有效性，并开放源代码促进研究复现。<br/><br/>**分点贡献：**  <br/>1. **构建首个针对语音生成的多感官对话数据集**  <br/>   - 突破传统文本为中心的数据局限，提供语音与视觉多模态关联数据，支持生成自然语音响应。<br/><br/>2. **提出多模态LLM-based语音生成框架**  <br/>   - 首次结合文本响应与语音描述生成，融入语调、语速等非语言信息（paralinguistic），实现更丰富的语音表达。<br/><br/>3. **实验验证多模态融合对语音生成的效果**  <br/>   - 通过对比实验证明，整合视觉与音频模态显著提升生成语音的吸引力与互动性。<br/><br/>4. **开源实现推动领域研究**  <br/>   - 开放源代码（GitHub链接）促进技术复现与后续研究，加速语音对话系统的发展。|
|2509.14515v1|[From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken   Language Models](http://arxiv.org/abs/2509.14515v1)|**贡献点：**  <br/>1. **提出TFD语音通信的重要性**：强调TFD作为实现类人AI互动的关键技术，支持自然对话中同时听与说、回合切换、语音重叠和打断等特性。  <br/>2. **建立分类框架**：区分Engineered Synchronization（模块化架构）与Learned Synchronization（端到端架构），为FD-SLMs设计提供理论依据。  <br/>3. **统一评估体系**：整合碎片化评价方法，提出涵盖时间动态性、行为仲裁、语义连贯性和声学性能的综合性评估框架。  <br/>4. **识别核心挑战与路线图**：通过主流模型对比，指出同步数据稀缺、架构多样性及评估标准缺失等问题，并给出技术发展方向建议。  <br/><br/>**总结（100字以内）：**  <br/>该综述系统梳理了LLM时代FD-SLMs研究，构建同步方法分类与评估框架，揭示同步数据不足、架构差异和评估漏洞等挑战，为提升人机自然语音交互提供技术指引。|
|2509.14480v1|[Process-Supervised Reinforcement Learning for Interactive Multimodal   Tool-Use Agents](http://arxiv.org/abs/2509.14480v1)|总结（100字以内）:  <br/>提出TARL框架通过LLM作为回合级裁判解决长时任务信用分配问题，构建支持语音-文本交替的强化学习沙箱，结合数学推理课程提升探索，显著提升文本基准性能，并验证框架在多模态模型微调中的有效性，推动语音驱动交互代理的发展。<br/><br/>贡献点:<br/>1. **提出TARL策略**：通过LLM作为回合级裁判，解决多模态长时任务中信用分配问题。<br/>2. **构建语音-文本沙箱环境**：支持多模态上下文下的交替式强化学习训练。<br/>3. **混合任务训练课程**：整合数学推理问题，增强模型探索能力与任务完成效率。<br/>4. **文本基准性能提升**：在τ-bench上任务通过率优于强RL基线6%。<br/>5. **多模态模型适配性验证**：证明框架适用于微调多模态基础模型以实现工具使用能力，推动自然语音交互发展。|
|2509.14187v1|[Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual   Descriptions and LLMs](http://arxiv.org/abs/2509.14187v1)|总结：  <br/>本文提出TextPA，一种基于文本描述和LLM的零样本发音评估方法，提供评分解释并优化准确性，克服传统系统仅输出数值评分的局限，展现成本效率与领域外数据表现优势。<br/><br/>贡献点：  <br/>1. **零样本发音评估**：无需音频-评分对训练，直接通过文本描述进行发音分析。  <br/>2. **文本特征替代声学模型**：利用人类可读的文本表示替代传统声学特征，增强可解释性。  <br/>3. **评分解释机制**：LLM生成评分原因，帮助学习者理解错误，提升学习指导价值。  <br/>4. **音素序列匹配优化**：引入音素序列匹配方法精炼准确性评分。  <br/>5. **新方向探索**：首次将LLM的文本知识应用于发音评估，突破监督学习的局限。  <br/>6. **高效且泛化性强**：在成本和跨领域数据表现上优于传统模型，验证方法有效性。|
|2509.14128v1|[Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST](http://arxiv.org/abs/2509.14128v1)|总结：  <br/>本研究提出Canary-1B-v2多语言ASR/AST模型，结合FastConformer与Transformer架构，通过非语音音频数据优化减少幻觉，并采用两阶段训练方法，实现在英语ASR上超越Whisper-large-v3，同时推出更轻量的Parakeet-TDT-0.6B-v3，兼顾性能与效率。<br/><br/>贡献点：  <br/>1. **模型架构创新**：采用FastConformer编码器与Transformer解码器组合，支持25种欧洲语言，兼顾速度与准确性。  <br/>2. **数据增强策略**：引入非语音音频数据训练，有效降低ASR和AST的幻觉问题。  <br/>3. **两阶段训练优化**：设计动态数据平衡机制的预训练与微调流程，提升模型泛化能力。  <br/>4. **时间戳生成技术**：集成NeMo Forced Aligner（NFA）与辅助CTC模型，实现可靠的分段时间戳输出。  <br/>5. **性能对比优势**：在英语ASR任务中超越Whisper-large-v3，速度提升10倍；多语言性能与大模型（如Seamless-M4T-v2-large）竞争。  <br/>6. **轻量级变体发布**：推出参数仅为600M的Parakeet-TDT-0.6B-v3，保持25语言支持的同时降低计算成本。|
|2509.13899v1|[AI as a teaching tool and learning partner](http://arxiv.org/abs/2509.13899v1)|总结（100字以内）:  <br/>本研究探讨LLM在教学中的应用，开发聊天机器人和播客工具，评估其对学生学习的影响，发现聊天机器人更受欢迎，播客接受度有限，为教育实践提供数据支持与改进建议。<br/><br/>贡献点:  <br/>1. **创新性工具整合**：首次将RAG技术与LLM结合，开发针对课程内容的智能聊天机器人，探索其在教学场景中的实际应用价值。  <br/>2. **多模态教学工具设计**：提出AI生成的音频播客作为补充教学资源，验证不同形式AI工具对学习效果的差异化影响。  <br/>3. **实证研究方法**：通过学期末学生调查，系统评估AI工具对学习态度和教学效果的影响，提供可量化的教学反馈数据。  <br/>4. **跨层次教学验证**：在本科与研究生两个不同的教学阶段同步测试工具效果，揭示AI干预的适应性与适用性差异。  <br/>5. **生物学科应用案例**：聚焦生物课程领域，为学科特定的AI教学工具开发与实施提供实证依据和实践参考。|
|2509.13785v1|[Summary on The Multilingual Conversational Speech Language Model   Challenge: Datasets, Tasks, Baselines, and Methods](http://arxiv.org/abs/2509.13785v1)|总结：  <br/>本论文总结了Interspeech2025多语言对话语音语言模型挑战，发布了1,604小时真实场景多语言对话语料库，提供基线系统，并分析了78支团队的参与成果，为构建高效多语言对话SLLM提供了宝贵见解。<br/><br/>贡献点：  <br/>1. **任务框架总结**：系统阐述MLC-SLM挑战的目标与任务设置，推动多语言对话语音LLM研究。  <br/>2. **大规模数据集发布**：提供约1,604小时的多语言对话语音数据，支持模型训练与评估。  <br/>3. **基线系统构建**：设计并发布参与者的基线系统，为后续研究提供基准。  <br/>4. **参与成果分析**：汇总78支国际团队的489个有效排名结果及14篇技术报告，展示研究进展。  <br/>5. **实践洞见提炼**：通过参与者经验总结，为多语言对话SLLM的开发提供方法论与技术参考。|
|2509.12647v1|[PAC: Pronunciation-Aware Contextualized Large Language Model-based   Automatic Speech Recognition](http://arxiv.org/abs/2509.12647v1)|总结：  <br/>本研究提出PAC框架，通过两阶段学习提升LLM-ASR系统的发音建模与同音字区分能力，显著降低词错误率，尤其在长尾词识别中表现突出。<br/><br/>贡献点：  <br/>1. **解决发音建模与同音区分挑战**：针对LLM-ASR系统在原始词和长尾词识别中的关键问题，提出发音感知的上下文建模方法。  <br/>2. **双阶段学习策略**：  <br/>   - 第一阶段：采用交织的音素-字符上下文建模，引入字符干扰项以强化音素线索利用。  <br/>   - 第二阶段：通过扰动标签抽样的发音区分强化学习，提升上下文同音字的区分能力。  <br/>3. **实验验证有效性**：在Librispeech和AISHELL-1数据集上，PAC实现30.2%和53.8%的绝对WER降低，长尾词偏见WER分别减少31.8%和60.5%。|
|2509.12591v1|[MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with   CLIP Models](http://arxiv.org/abs/2509.12591v1)|总结：  <br/>本文提出一种零样本音频字幕生成方法，结合预训练音频CLIP模型与LLM，通过结构化提示和优化token选择显著提升生成质量，验证了关键词选择对性能的关键影响。<br/><br/>贡献点：  <br/>1. **零样本框架**：提出无需大规模训练的AAC系统，利用预训练模型解决数据集不足问题。  <br/>2. **多模态融合**：结合音频CLIP模型提取听觉特征，并通过结构化提示引导LLM生成字幕。  <br/>3. **动态Token优化**：改进传统贪心解码，通过音频CLIP模型动态调整token选择以增强内容对齐。  <br/>4. **关键词影响验证**：实验表明关键词选择对性能有显著影响，单关键词提示效果最佳，无关键词列表时性能下降50%。  <br/>5. **性能提升**：使用MAGIC搜索方法，在WavCaps模型上实现NLG mean score 35%的提升（4.7→7.3）。|
|2509.11937v1|[MMORE: Massive Multimodal Open RAG & Extraction](http://arxiv.org/abs/2509.11937v1)|**贡献点：**  <br/>1. 提出MMORE，支持15种以上异构文档格式（含文本、表格、图像、音频等），统一处理格式方便下游应用。  <br/>2. 架构具备模块化和分布式特性，支持CPU/GPU并行化，实现可扩展的高效处理。  <br/>3. 在扫描PDF处理任务中，相比Docling提升40%准确性，且比单节点基线快3.8倍。  <br/>4. 集成混合稠密-稀疏检索技术，兼容交互式API和批量RAG端点。  <br/>5. 通过PubMedQA验证，MMORE增强的医学LLM随检索深度增加显著提升问答准确性。  <br/>6. 提供任务无关的多模态RAG系统基础，支持多样化真实数据部署，代码开源。  <br/><br/>**总结：**  <br/>MMORE是首个支持多模态数据的开源处理管道，集成高效检索与分布式架构，在扫描文档处理和医学问答等任务中均取得显著性能优势。|
|2509.11914v1|[EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](http://arxiv.org/abs/2509.11914v1)|**贡献点**  <br/>1. 提出EgoMem，首个针对全双工多模态实时模型的终身记忆代理。  <br/>2. 实现从原始音频视觉流中识别多用户，并提供个性化响应。  <br/>3. 通过长期记忆维护用户事实、偏好及社会关系信息。  <br/>4. 设计三个异步模块：用户检索（基于人脸/语音）、多模态对话生成、记忆管理（自动检测对话边界并更新记忆）。  <br/>5. 完全依赖原始音频视觉流，适用于实时、具身场景无需额外标注数据。  <br/>6. 实验验证检索与记忆管理模块准确率超95%，整合后对话事实一致性达87%，建立未来研究基准。  <br/><br/>**总结（100字内）**  <br/>本文提出EgoMem，首个全双工多模态实时模型的终身记忆代理，通过异步模块实现用户识别、个性化响应及记忆管理，实验表明其准确率超95%，整合后对话事实一致性达87%，为实时具身场景下的记忆系统提供新范式和基准。|
|2509.11868v1|[Growing Perspectives: Modelling Embodied Perspective Taking and Inner   Narrative Development Using Large Language Models](http://arxiv.org/abs/2509.11868v1)|**贡献点总结（100字以内）**  <br/>本研究提出PerspAct系统，结合ReAct范式与LLMs模拟视角转换的发展动态，基于Selman理论设计导演任务评估框架，揭示语言交流对内部表征的调节作用，并发现发展阶段与协作效果的关联，为建模跨模态认知和评估LLMs内部过程提供了新视角。<br/><br/>**分点贡献：**  <br/>1. **系统开发**：首次将ReAct范式与LLMs结合，构建PerspAct系统，模拟视角转换的发展阶段，嵌入Selman理论框架。  <br/>2. **评估框架创新**：设计扩展的导演任务，量化评估模型生成的内部叙事与发展阶段的一致性及其对协作表现的影响。  <br/>3. **认知动态发现**：揭示LLMs在交互过程中动态调整视角阶段的现象，证明语言交换对内部表征的持续优化作用。  <br/>4. **发展阶段差异**：实证表明高级阶段提升协作效率，但早期阶段在复杂任务中表现更不稳定，为认知发展建模提供依据。  <br/>5. **理论与实践意义**：强调隐式整合具身视角转换与语言模型的重要性，推动对LLMs内部语言过程的系统性评估研究。|
|2509.11127v1|[Joint Effects of Argumentation Theory, Audio Modality and Data   Enrichment on LLM-Based Fallacy Classification](http://arxiv.org/abs/2509.11127v1)|总结（100字以内）:  <br/>该研究探讨了上下文与情感元数据对LLM在政治辩论谬误分类任务中的影响，发现情感元数据易导致模型偏倚并损害逻辑推理，基本提示策略反更优，为多模态数据下的语言模型优化提供了新思路。<br/><br/>贡献点分点:  <br/>1. **提出理论框架**：首次引入Pragma-Dialectics和Periodic Table of Arguments两个理论指导的Chain-of-Thought框架，用于增强LLM在谬误分类中的推理能力。  <br/>2. **多模态实验设计**：系统评估了文本、上下文信息及音频情感元数据三种输入设置对模型性能的影响，探索了多模态数据对语言模型任务的具体作用机制。  <br/>3. **发现情感元数据负面影响**：实验证明情感元数据可能干扰模型逻辑推理，导致误标为"诉诸情感"谬误，揭示了多模态输入潜在的注意力分散问题。  <br/>4. **对比实验结论**：验证了基础提示策略在特定场景下优于复杂增强策略，为语音领域中语言模型任务优化提供了实证依据。  <br/>5. **政治辩论场景应用**：聚焦政治辩论数据，填补了语音语义分析与特定领域谬误分类结合的研究空白，强调了上下文与情感信息在复杂文本分析中的挑战。|
|2509.10748v1|[SCOPE: Speech-guided COllaborative PErception Framework for Surgical   Scene Segmentation](http://arxiv.org/abs/2509.10748v1)|总结：  <br/>提出语音引导的协作感知（SCOPE）框架，结合大语言模型与开放集视觉基础模型，实现手术场景中实时分割、标注与跟踪，解决传统方法依赖标注数据的局限性，展示人机协作在动态手术环境中的潜力。<br/><br/>贡献点：  <br/>1. **提出SCOPE框架**：首次将语音指导机制与视觉分割模型结合，构建支持手术场景实时分割（intraoperative video streams）的协同感知系统。  <br/>2. **引入协作感知代理**：通过整合LLM的推理能力与VFM的感知能力，生成高质量分割候选并利用自然语言反馈动态优化手术器械分割。  <br/>3. **动态标注机制**：利用手术器械作为交互指针，实现对其他术中元素（如解剖结构）的实时补充标注与跟踪。  <br/>4. **开放集与零样本能力**：在无预定义类别的场景中，通过语音与视觉协同实现灵活分割和泛化，突破传统监督模型的限制。  <br/>5. **多数据验证**：基于公开Cataract1k数据集和内部颅底数据集的评估，结合实时模拟实验，验证框架的动态适应性与实用性。|
|2509.10729v1|[Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](http://arxiv.org/abs/2509.10729v1)|**贡献点总结**<br/>1. **构建多场景活动识别数据集**：从Ego4D数据集中筛选出涵盖家庭活动、体育等多样化场景的子集，用于跨上下文活动分类研究。<br/>2. **提出LLM晚期融合方法**：首次将大语言模型应用于音频与运动时间序列的晚期融合，实现多模态活动分类。<br/>3. **实现零/单样本分类性能突破**：在无需任务特定训练的前提下，LLM融合模型在12类零样本和单样本分类中F1分数显著高于随机基线。<br/>4. **解决嵌入空间共享难题**：通过LLM融合，减少对对齐训练数据的依赖，适用于多模态时序应用中嵌入空间学习的挑战。<br/>5. **提升模型部署效率**：LLM融合无需额外内存和计算资源，支持轻量级部署针对特定应用场景的多模态模型。<br/><br/>**摘要总结**  <br/>该研究提出利用大语言模型进行音频与运动数据的晚期融合，构建了跨场景活动识别数据集，并在零/单样本分类中取得显著性能提升，同时解决了共享嵌入空间学习与部署效率的双重挑战。|
|2509.09921v1|[A Taxonomy of Response Strategies to Toxic Online Content: Evaluating   the Evidence](http://arxiv.org/abs/2509.09921v1)|总结：  <br/>本文提出在线话语参与（ODE）分类框架，系统梳理25种应对网络有害言论策略，澄清概念并开展元分析，为构建建设性网络公共话语提供证据支持。<br/><br/>贡献点：  <br/>1. **构建ODE理论框架**：首次提出Online Discourse Engagement（在线话语参与）的分类体系，涵盖五类响应策略。  <br/>2. **系统化策略分类**：归纳25种具体策略（如幽默、共情、事实反驳等），按功能划分为五大类别，填补现有文献空白。  <br/>3. **实证证据整合**：通过系统文献综述与元分析，评估各策略对促进健康网络讨论的有效性，增强研究可信度。  <br/>4. **概念澄清与区分**：明确ODE目标与方法的区别，解决现有研究中目标模糊、术语混用的问题。  <br/>5. **推动实践应用**：为制定基于证据的应对网络有害内容措施提供理论指导与实证依据。|
|2509.09321v1|[Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain   Expansion, and Metric Optimization](http://arxiv.org/abs/2509.09321v1)|**贡献点分点总结：**  <br/>1. **自动化任务收集系统**：基于浏览器自动化与LLM，自动从Kaggle、AIcrowd等平台提取多类型（表格、文本、图像等）和多模态的ML挑战，实现任务多样性。  <br/>2. **基于排行榜的难度建模机制**：通过参与者数量和分数分布动态估计任务复杂度，提升难度评估的客观性和可扩展性。  <br/>3. **多维评估框架**：综合性能、格式合规性、约束遵守及任务泛化能力，全面验证LLM代理的能力。  <br/>4. **多规模基准子集设计**：构建Lite（18任务）、Medium和Full三个子集，覆盖不同难度和模态，适配多样化的评估场景。  <br/><br/>**总结（100字以内）：**  <br/>TAM Bench通过三个创新（自动化收集、难度建模、多维评估）构建了多模态、多难度的基准测试，同时推出Lite、Medium、Full三个子集以适应不同评估需求，全面衡量LLM代理在端到端ML任务中的能力。|
|2509.09198v2|[GmSLM : Generative Marmoset Spoken Language Modeling](http://arxiv.org/abs/2509.09198v2)|总结（100字以内）：  <br/>提出GmSLM模型，通过无监督数据评估Marmoset声通信，揭示其类似人类语言的特征，为神经科学与进化生物学提供跨学科研究框架。<br/><br/>**贡献点：**  <br/>1. **挑战传统观点**：通过Marmoset复杂声通信揭示非人灵长类发声行为并非完全先天，为语言起源研究提供新视角。  <br/>2. **构建研究桥梁**：连接Marmoset声通信与脑活动研究，弥补人类语言研究中对大脑难以直接探索的局限。  <br/>3. **提出专用模型**：设计生成式Marmoset语音建模框架GmSLM，针对性解决其发声数据特性与标准LLM方法不兼容的问题。  <br/>4. **创新评估方法**：开发基于无监督野外数据和弱标签对话数据的零样本评估指标，验证模型有效性并优于人类语音基线。  <br/>5. **高质量生成能力**：GmSLM生成的语音在声学特征上高度接近真实样本，并在下游任务中表现优异。  <br/>6. **跨学科应用价值**：模型可区分真实与人工对话，为探索声通信的神经基础及联结语音与脑活动的实践框架提供工具。|
|2509.08344v1|[Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model](http://arxiv.org/abs/2509.08344v1)|总结：  <br/>提出基于上下文学习的语音情感识别个性化方法，通过少量目标说话人的情感样本学习其特征，解决传统方法依赖完整情绪数据集的问题，并设计扩展的语音-语言模型进行元训练，实验验证其有效性。<br/><br/>贡献点：  <br/>1. **提出新型个性化方法**：首次将上下文学习（ICL）应用于语音情感识别（SER），通过条件化目标说话人的情感样本实现个性化适应。  <br/>2. **解决数据不足难题**：克服传统方法需收集目标说话人所有情绪标签语音的限制，仅需少量样本即可完成特征学习。  <br/>3. **构建扩展语音-语言模型**：基于大语言模型（LLM）设计专门的语音-语言模型，用于元训练以支持ICL驱动的SER个性化。  <br/>4. **实验验证有效性**：通过全新构建的SER数据集验证方法性能，证明其优于传统个性化技术。|
|2509.08031v2|[AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs](http://arxiv.org/abs/2509.08031v2)|总结（100字以内）：  <br/>AU-Harness 提出高效、全面的LALM评估框架，解决现有工具效率低、提示不一致和任务覆盖不足的问题，引入新评估类别并揭示模型在时间理解和复杂语音推理上的显著差距，推动系统化开发。<br/><br/>贡献点：  <br/>1. **高效评估框架**：通过优化批处理与并行执行，实现比现有工具快127%的处理速度，支持大规模模型评估。  <br/>2. **标准化与灵活性**：提供统一的提示协议和可配置接口，提升模型间比较的公平性与结果可复现性。  <br/>3. **新评估类别**：提出LLM-Adaptive Diarization（时序音频理解）和Spoken Language Reasoning（复杂语音推理任务）两大新型评估方向。  <br/>4. **全面实验验证**：对380+任务进行系统评估，揭示当前LALMs在时序理解与复杂推理任务中的性能短板。  <br/>5. **标准化缺失分析**：指出音频基准中指令模态标准化不足，导致复杂任务性能差异高达9.5分，引发对评估体系的反思。  <br/>6. **工具与洞察结合**：不仅提供实用评估工具，还通过结果分析为模型改进和领域发展提供关键方向指引。|
|2509.07274v1|[LLM Analysis of 150+ years of German Parliamentary Debates on Migration   Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](http://arxiv.org/abs/2509.07274v1)|总结（100字以内）:  <br/>该研究评估多LLM在标注德国议会政治话语中（反）团结类型的准确性，比较人类标注结果，分析模型规模与训练数据影响，并揭示战后至2015年后德国移民议题的团结趋势变化，强调LLM在政治文本分析中的应用价值。<br/><br/>贡献点：  <br/>1. **首次系统评估多LLM在标注政治话语（反）团结子类型中的表现**，涵盖德国议会历史与当代数据。  <br/>2. **建立大规模人类标注基准**（数千条标注，历时一年），提供可靠的对照数据。  <br/>3. **探究关键影响因素**：模型规模、提示策略差异、微调效果及历史/当代数据分布对标注结果的影响。  <br/>4. **识别系统性错误**（如偏见或分类偏差），提升模型在敏感政治话题中的可靠性。  <br/>5. **从社会科学研究视角解读标注结果**，揭示德国移民政策中的团结与反团结趋势演变。  <br/>6. **发现时间维度上的显著趋势**：战后时期存在高程度 migrant-directed solidarity，2015年后反-solidarity 趋势增强。  <br/>7. **推动跨学科应用**：验证LLM在政治文本分析中的潜力，为社会科学研究提供自动化工具支持。|
|2509.06382v1|[Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn   Multimodal LLM Conversation](http://arxiv.org/abs/2509.06382v1)|总结：  <br/>提出CAFA系统，通过多代理LLM架构实现动态环境下的个性化助听器实时调参，融合环境音频、听力图与用户反馈，结合轻量级YAMNet嵌入分类和伦理监管模块，显著提升对话效率。<br/><br/>贡献点：  <br/>1. **动态适配机制**：首次提出基于多代理架构的Context-Adaptive Fitting Advisor (CAFA)，突破传统静态助听器配置的局限性。  <br/>2. **多模态融合处理**：整合实时环境音频、听力图及用户反馈，构建多任务协同的交互系统。  <br/>3. **轻量级音频分类**：采用基于YAMNet嵌入的神经网络实现91.2%准确率的环境声分类（对话/噪声/静音）。  <br/>4. **模块化LLM流程**：设计包含上下文获取、子问题分类、策略生成及伦理监管的分层处理框架。  <br/>5. **实时调参验证**：通过实验验证实时音频分类对提升对话效率的关键作用，证明系统有效性。|
|2509.06221v1|[Beamforming-LLM: What, Where and When Did I Miss?](http://arxiv.org/abs/2509.06221v1)|**总结（100字以内）:**  <br/>提出Beamforming-LLM系统，融合麦克风阵列空间音频分离、RAG检索与轻量级LLM摘要，实现多说话者环境中语义级对话回忆，支持自然语言查询与时空对齐，为智能听觉记忆系统提供新方案。<br/><br/>**贡献点分点列出:**  <br/>1. **空间音频与语义检索结合**：首次将定向音频分离（beamforming）与检索增强生成（RAG）技术结合，解决多说话者环境下对话内容缺失问题。  <br/>2. **实时转录与向量化处理**：采用Whisper模型进行高精度转录，并通过sentence encoder将音频片段嵌入向量数据库，提升语义检索效率。  <br/>3. **轻量级摘要生成**：利用GPT-4o-mini等轻量模型实现非关注音频段的语义摘要，降低计算成本同时保持信息完整性。  <br/>4. **时空对齐与对比摘要**：通过时间对齐技术整合查询相关片段与未关注内容，生成对比性总结并支持时间戳音频播放，增强用户交互体验。  <br/>5. **多场景应用潜力**：为助听设备、会议纪要生成及空间计算等场景提供基础框架，推动智能听觉记忆技术的实际落地。|
|2509.06164v2|[Benchmarking Gender and Political Bias in Large Language Models](http://arxiv.org/abs/2509.06164v2)|总结：提出EuroParlVote数据集，评估LLMs在政治敏感场景下的性别分类与投票预测偏差，揭示模型对女性与极左/右立场的系统性偏见，并对比专有模型与开源模型的性能差异。<br/><br/>贡献点：<br/>1. 构建首个融合欧洲议会辩论视频与投票数据的基准EuroParlVote，包含性别、年龄、国籍、政党等多维元数据<br/>2. 首次系统评估LLMs在政治敏感场景下的两个关键任务：性别分类与投票预测<br/>3. 揭示LLMs存在系统性偏差：女性MEP误分类率高、对女性演讲者投票预测准确率下降<br/>4. 发现LLMs对中间派政治团体表现优异，而对极左/右团体预测能力显著不足<br/>5. 对比分析专有模型（如GPT-4o）与开源模型的性能差异，证实其在稳健性与公平性上的优势<br/>6. 公开发布数据集、代码及演示工具，推动NLP政治应用领域的公平性与问责研究|
|2509.04606v1|[Sample-efficient Integration of New Modalities into Large Language   Models](http://arxiv.org/abs/2509.04606v1)|总结：本论文提出一种样本高效多模态整合方法，利用超网络动态生成适配器，显著减少低资源模态训练数据需求，拓展基础模型的模态覆盖能力。<br/><br/>贡献点：<br/>1. 提出SEMI方法，实现通过少量样本（如32个）高效整合新模态至LLM，相比从头训练需64倍数据量。<br/>2. 设计可适配任意模态的共享投影器架构，通过超网络实现动态参数调整（Hypernetwork adaptation）。<br/>3. 引入等距变换策略，人工扩展编码器多样性以增强模型泛化能力（Isometric transformation for encoder diversity）。<br/>4. 验证方法对多种低资源模态（卫星图像、天文学图像、惯性测量、分子结构）的适用性，突破模态维度限制。|
|2509.04356v1|[SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic   Avatars](http://arxiv.org/abs/2509.04356v1)|总结（100字以内）：  <br/>SRWToolkit是一款开源工具，通过本地LLM实现社会机器人化身快速原型设计，支持多模态交互与实时配置，强调模块化和设备端运行，经小规模用户研究验证其在可用性、信任及用户体验方面的有效性，助力可扩展人机交互研究。<br/><br/>贡献点：  <br/>1. **开源工具开发**：提供首个基于本地LLM的Wizard of Oz工具包，支持社会机器人快速原型设计。  <br/>2. **本地化LLM推理**：替代云端服务，确保数据隐私与设备端运行，提升实时性和稳定性。  <br/>3. **多模态交互支持**：集成文本输入、按钮触发语音及唤醒词命令，增强人机交互的自然性。  <br/>4. **实时配置功能**：通过直观控制面板动态调整机器人外观、行为、语言与语音特性。  <br/>5. **模块化架构设计**：强调组件化可扩展性，便于定制化开发与功能组合。  <br/>6. **实证研究验证**：通过小规模用户测试（n=11），证实工具在可用性、信任感和用户体验上的有效性。  <br/>7. **研究支持能力**：为研究人员提供高效开发环境，推动多样化人机交互场景的可扩展研究。|
|2509.04104v1|[Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue](http://arxiv.org/abs/2509.04104v1)|总结：  <br/>本研究首次探索人机对话中词汇对齐的实现方法，提出构建稳定个性化词汇档案的实验设计，确定最优数据配置，为对话代理的词汇对齐策略奠定基础。<br/><br/>贡献点：  <br/>1. 提出在人机对话中实现词汇对齐的初步研究，作为人类与Agent对话的基础。  <br/>2. 探索构建稳定、个性化的词汇档案方法，结合个性化对话代理策略。  <br/>3. 设计实验调整转录数据量及词性类别词汇数量（如形容词、连词、副词等），评估时间维度上的档案性能。  <br/>4. 发现最小化数据需求的最优配置：10分钟转录语料中包含5个形容词、5个连词及10个其他词性词汇，平衡性能与数据效率。  <br/>5. 为对话代理的词汇对齐策略提供实践指导，强调数据效率与稳定性的重要性。|
|2509.03959v2|[WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation](http://arxiv.org/abs/2509.03959v2)|1. 提出WenetSpeech-Pipe集成流水线，包含音频收集、多维标注（说话人属性、语音质量）、ASR处理及注释增强模块，解决粤语注释资源匮乏问题。  <br/>2. 发布首个大规模粤语语料库WenetSpeech-Yue，涵盖10个领域、21,800小时语音数据，提供ASR转录、文本置信度、说话人身份/年龄/性别等多维度标注。  <br/>3. 构建WSYue-eval综合评估基准，包含ASR与TTS两部分子集，支持短长句、代码切换及多样声学环境的测试。  <br/>4. 实验证明基于该数据集的模型在粤语ASR和TTS任务中达到SOTA水平，超越商业系统及LLM模型，验证数据与流水线价值。  <br/><br/>总结（100字以内）:  <br/>本研究提出面向粤语的WenetSpeech-Pipe流水线，构建首个大规模多维标注语料库及评估基准，显著提升粤语ASR和TTS模型性能，验证了数据构建方法的有效性。|
|2509.03212v1|[AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](http://arxiv.org/abs/2509.03212v1)|**贡献点总结：**  <br/>1. **提出情感感知虚拟伴侣系统**：设计\ours，通过多模态情感感知实现情感对齐与生动的人机交互。  <br/>2. **创新网络架构**：开发Multimodal Sentiment Perception Network (MSPN)，融合跨模态Transformer与监督对比学习。  <br/>3. **情感导向的提示策略**：构建情感感知的提示工程方法，生成共情回应。  <br/>4. **多模态交互模块**：集成TTS系统与动画头像，增强情感表达能力。  <br/>5. **应用框架拓展**：提供情感感知代理的通用框架，应用于陪伴机器人、社会护理、心理健康及人本AI领域。  <br/><br/>**总结（100字以内）：**  <br/>本文提出\ours，通过多模态情感感知网络与情感导向提示策略，构建情感意识的虚拟伴侣，实现沉浸式交互。系统集成TTS与动画模块，拓展应用至社会护理与心理健康等领域，推动人本AI发展。|
|2509.03116v2|[Measuring Scalar Constructs in Social Science with LLMs](http://arxiv.org/abs/2509.03116v2)|总结：提出四种LLM测量标量构造除直接评分外，成对比较和加权均值方法更优，微调小模型可媲美大模型性能。<br/><br/>贡献点：<br/>1. 系统分析LLM测量连续语义标量构造的适用方法<br/>2. 证实成对比较法优于直接输出评分法（解决bunching问题）<br/>3. 提出token概率加权评分方法提升测量精度<br/>4. 验证微调小规模模型在少量数据下可达到或超越大模型效果|
|2509.03116v1|[Measuring Scalar Constructs in Social Science with LLMs](http://arxiv.org/abs/2509.03116v1)|总结（100字以内）:  <br/>该研究评估LLMs在测量语言标量特征（如复杂性、情感）中的表现，提出四类方法并验证其有效性，发现token概率加权评分优于直接评分，且微调小模型可达到或超越大型模型性能，为社会科学研究提供实用工具和方法论指导。<br/><br/>贡献点分点列出:  <br/>1. **提出系统化评估框架**：首次对LLM在社会科学研究中测量标量语言特征的四种方法（直接评分、成对比较、加权评分、微调）进行全面比较，使用多领域政治科学数据集验证方法有效性。  <br/>2. **揭示直接评分的局限性**：发现LLMs直接生成的文本点评分存在分布不连续性和任意数值聚集问题，推翻传统"简单/复杂"二分类假设，强调连续量化需求。  <br/>3. **开发加权评分策略**：提出基于token概率的加权点评分方法，通过赋予词汇重要性差异显著提升测量质量，为连续特征建模提供改进方案。  <br/>4. **证明小模型微调可行性**：验证仅需1000个训练对即可微调小模型达到或超越大模型性能，降低研究成本，推动资源受限场景下的语音/语言分析应用。|
|2508.20916v1|[SageLM: A Multi-aspect and Explainable Large Language Model for Speech   Judgement](http://arxiv.org/abs/2508.20916v1)|**贡献点：**<br/>1. 提出SageLM模型：首个端到端、多维度、可解释的S2S LLM评估框架，同时覆盖语义与声学维度评估。<br/>2. 创新监督机制：采用基于理由的监督方法提升模型可解释性，优于传统规则强化学习方法。<br/>3. 构建合成数据集：设计SpeechFeedback合成偏好数据集并引入两阶段训练范式，缓解语音偏好数据稀缺问题。<br/><br/>**总结（100字以内）：**  <br/>本研究提出SageLM，通过联合语义-声学评估、理由引导监督及合成数据集优化，显著提升语音LLM评估性能，达到82.79%的人类一致性，超越现有基线模型26.20%。|
|2508.20750v1|[Specializing General-purpose LLM Embeddings for Implicit Hate Speech   Detection across Datasets](http://arxiv.org/abs/2508.20750v1)|总结（100字以内）:  <br/>本文提出基于通用嵌入模型的纯微调方法，无需外部知识，实现隐含仇恨言论检测的SOTA性能，并在多个数据集验证其有效性，显著提升跨数据集表现。<br/><br/>贡献点：<br/>1. **提出纯微调通用嵌入模型的方法**：首次仅通过微调基于大语言模型（LLM）的通用嵌入模型（如Stella、Jasper、NV-Embed和E5）解决IHS检测问题，无需引入外部上下文、情感或领域知识。  <br/>2. **实现SOTA性能**：在多个IHS数据集上取得最佳结果，in-dataset提升1.10个百分点，cross-dataset提升20.35个百分点（F1-macro）。  <br/>3. **验证方法泛化能力**：通过跨数据集评估证明该方法具有强泛化性，突破传统依赖外部信息的检测框架，简化技术流程。|
|2508.19320v2|[MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time   Autoregressive Video Generation](http://arxiv.org/abs/2508.19320v2)|**贡献点总结（100字以内）:**  <br/>提出自回归视频生成框架，实现多模态交互控制与低延迟流式生成；构建20,000小时跨源对话数据集；引入64倍压缩深度自动编码器优化推理效率；通过实验验证在双人对话、多语言合成及交互式世界模型中的高效性与细粒度控制优势。<br/><br/>**分点贡献:**  <br/>1. **自回归多模态控制框架**：首次将自回归模型应用于数字人视频生成，支持音频、姿态与文本多模态输入，生成空间与语义一致的表示以引导去噪过程，提升交互性与控制精度。  <br/>2. **大规模对话数据集构建**：创建包含约20,000小时对话的多源数据集，覆盖丰富场景，为训练交互式视频生成模型提供高质量语料。  <br/>3. **深度压缩自动编码器设计**：提出具有64倍压缩率的自动编码器，显著降低自回归模型的长时推理计算负担，增强实际应用可行性。  <br/>4. **多场景实验验证**：通过双人对话、多语言合成及交互式世界模型测试，证明框架在低延迟、高效率和细粒度多模态控制方面的优越性。|
|2508.19320v1|[MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time   Autoregressive Video Generation](http://arxiv.org/abs/2508.19320v1)|总结：  <br/>本文提出一种自回归视频生成框架，通过多模态控制和流式处理实现低延迟交互，并结合大规模对话数据集和深度压缩自动编码器提升生成效率与控制精度。<br/><br/>贡献点：  <br/>1. **提出交互式多模态控制框架**：支持音频、姿态和文本的实时输入，并生成空间与语义连贯的表示以指导扩散模型。  <br/>2. **构建大规模对话数据集**：整合约20,000小时多源数据，覆盖复杂对话场景提升训练效果。  <br/>3. **开发深度压缩自动编码器**：实现64倍压缩比，显著降低长时推理计算负担。  <br/>4. **实验证明低延迟与高效性**：在双语对话、多语言合成等任务中验证框架的实时性能与多模态控制能力。|
|2508.18998v1|[MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in   LLM-based Multilingual ASR](http://arxiv.org/abs/2508.18998v1)|**总结（100字以内）:**  <br/>本文提出MOSA模型，通过轻量级适配器结合Mixture-of-Experts机制，有效实现跨语言知识共享，缓解多语言ASR数据稀缺问题，并在参数减少情况下仍保持高性能，验证了简单适配器优于单一复杂投影器的设计。<br/><br/>**贡献点分点:**  <br/>1. **提出MOSA框架**：设计基于Mixture-of-Experts的多语言ASR模型，通过轻量级适配器分离共享与语言特异性知识，提升跨语言迁移效率。  <br/>2. **解决数据不平衡问题**：利用高资源语言数据支持低资源语言，缓解数据稀缺对性能的制约，优化低资源语言的识别效果。  <br/>3. **参数效率优化**：MOSA-Base在仅使用60%参数的情况下超越基线模型，证明高效参数利用的可行性。  <br/>4. **实验验证有效性**：在多语言场景下（包括平均WER和数据不平衡情况）均优于基线，且通过消融研究证明适配器设计的优越性。  <br/>5. **方法创新性**：对比传统单一复杂投影器，提出简单适配器组合策略，为LLM-based ASR提供更高效的架构选择。|
|2508.18918v1|[DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM   with Audio Modality](http://arxiv.org/abs/2508.18918v1)|总结：  <br/>本文提出DESAMO系统，利用Audio LLM直接处理原始音频，解决老年人语音不清晰问题，实现意图理解与关键事件检测，提升智能家居的可用性与隐私性。<br/><br/>贡献点：  <br/>1. **方法创新**：采用Audio LLM直接处理原始音频，突破传统ASR或ASR-LLM级联架构的限制。  <br/>2. **老人友好设计**：针对性优化对老年用户模糊发音的识别能力，提升交互自然性。  <br/>3. **隐私与本地性**：系统部署于设备端，避免云端传输，增强数据隐私保护。  <br/>4. **多模态扩展**：支持非语音音频分析，实现跌倒、求助等关键事件的检测与响应。  <br/>5. **实际应用价值**：为智能家居场景提供安全、私密的语音交互解决方案。|
|2508.18783v1|[Controllable Conversational Theme Detection Track at DSTC 12](http://arxiv.org/abs/2508.18783v1)|**贡献点总结（100字以内）:**  <br/>提出对话主题检测作为新任务，区别于传统意图识别；设计可控制粒度的联合聚类与标签框架；构建公共竞赛赛道与配套数据集；提供自动与人工评估指标；分析参赛成果并开源数据与代码。  <br/><br/>**详细贡献点:**  <br/>1. **提出新任务**：定义Theme Detection为对话分析的核心任务，目标是自动识别对话主题并分类，提升客服/销售等领域的对话分析效率。  <br/>2. **任务差异**：与传统固定意图检测不同，主题作为用户可见的摘要，支持灵活粒度和个性化定制。  <br/>3. **竞赛机制**：在DSTC12中设立Controllable Conversational Theme Detection公开竞赛赛道，推动联合聚类与主题标签的研究。  <br/>4. **数据与评估**：构建包含用户偏好数据的基准数据集，并设计多维度评估指标（自动与人工）。  <br/>5. **成果分析**：汇总参赛团队方法、提供性能分析与启发，同时开放数据及代码促进研究复现。|
|2508.18240v2|[MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues   via Arena-style and Rubrics Protocols](http://arxiv.org/abs/2508.18240v2)|**总结**  <br/>本研究提出MTalk-Bench，首次系统评估多轮语音对话模型的语义、副语言和环境音处理能力，并构建双方法评估框架，揭示当前模型在感知与效率上的局限性。<br/><br/>**贡献点**  <br/>1. **提出首个多模态多轮语音基准**：构建涵盖语义、副语言及环境音三大维度的MTalk-Bench，包含9个真实场景与针对性任务（如推理）。  <br/>2. **双方法评估框架**：结合Arena式相对评估（两两对比）与Rubrics式绝对评分，支持对模型性能的全面分析。  <br/>3. **实验揭示模型局限**：发现S2S模型在副语言和环境音感知上表现不足，依赖长响应恢复连贯性，且模态感知设计优于简单扩展。  <br/>4. **评估框架可靠性分析**：指出Arena与Rubrics在性能差距大时互补可靠，而LLM作为评判者存在位置与长度偏差，需依赖文本注释进行非语言评估。|
|2508.18240v1|[MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues   via Arena-style and Rubrics Protocols](http://arxiv.org/abs/2508.18240v1)|**贡献点：**  <br/>1. **提出多维度S2S基准MTalk-Bench**：涵盖语义信息、伴随信息（paralinguistic）和环境声音（ambient sound）三大核心维度，包含9个现实场景及针对性任务（如推理），系统化评估多轮对话中的复杂能力。  <br/>2. **构建双重评估框架**：结合Arena式（相对比较）和Rubrics式（绝对评分）方法，同时支持模型与人类输出的对比评估，增强评估的全面性与客观性。  <br/>3. **揭示S2S LLMs性能局限**：发现模型在语义处理上表现优异，但在伴随信息和环境声音感知上存在不足；回复长度与效率的平衡问题；以及模态感知与任务设计优于单纯模型扩展的重要性。  <br/>4. **评估框架可靠性分析**：指出LLM作为评估者在明显性能差距时与人类一致，但在位置和长度上存在偏差，需依赖文本注释辅助提升非语言信息评估的可靠性。|
|2508.18234v1|[Can AI Have a Personality? Prompt Engineering for AI Personality   Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](http://arxiv.org/abs/2508.18234v1)|**贡献点：**  <br/>1. **探索LLM人格模拟可行性**：验证了通过指令提示工程引导大语言模型模拟一致人格的可行性，为AI人格定制提供理论支持。  <br/>2. **应用特定领域训练场景**：将人格模拟技术应用于言语-语言病理学（SLP）学生培训，聚焦性别确认语音治疗，推动AI在专业医疗场景的实用性。  <br/>3. **构建个性化AI角色**：开发聊天机器人Monae Jackson，以跨性别女性身份进行对话，展示了AI角色设计在敏感话题中的精准性。  <br/>4. **量化人格特征验证**：基于大五人格测试框架，客观评估AI人格的一致性和稳定性，为评估体系提供参考依据。  <br/><br/>**总结（100字以内）：** 本研究通过提示工程实现AI人格模拟，应用于SLP性别确认语音治疗培训，构建个性化角色Monae Jackson，并以大五人格测试验证其有效性，证明LLM可通过技术手段模拟稳定人格特征，为AI在医疗领域的应用提供新思路。|
|2508.17674v2|[Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against   Large Language Models](http://arxiv.org/abs/2508.17674v2)|总结：本文提出新型LLM安全威胁AEA，通过两种低成本手段注入隐蔽广告内容，揭示现有安全机制不足，并提供基于提示的防御方法。<br/><br/>贡献点：<br/>1. 提出新型安全威胁"Advertisement Embedding Attacks (AEA)"，揭示LLM在信息完整性方面存在未被关注的漏洞；<br/>2. 识别两种低成本攻击矢量：(1)通过劫持第三方分发平台注入对抗性提示，(2)发布带有后门的微调开源模型；<br/>3. 构建完整的AEA攻击流程框架，系统性分析其运作机制；<br/>4. 划分并定义五类利益相关者受害者群体，明确攻击影响范围；<br/>5. 开发无需模型重训练的提示型自我检测防御方案，提供实用防护手段。|
|2508.17674v1|[Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against   Large Language Models](http://arxiv.org/abs/2508.17674v1)|**总结**（100字以内）:  <br/>本文提出新型LLM攻击AEA，通过对抗提示和后门前缀检查点实现隐蔽内容注入，首次识别五类受害群体并设计基于提示的自检防御机制，揭示LLM安全中的隐蔽内容注入漏洞，呼吁AI安全社区加强检测、审计和政策协作。<br/><br/>**贡献点**:  <br/>1. **提出新型攻击范式**：定义AEA（广告嵌入攻击）作为LLM安全的新威胁，突破传统攻击（如准确率下降）的局限，聚焦于信息完整性破坏。  <br/>2. **揭示攻击技术路径**：提出两种低成本攻击手段——（1）劫持第三方平台注入对抗提示，（2）发布植入后门的开源检查点，实现隐蔽内容嵌入。  <br/>3. **构建受害者分类体系**：明确五类利益相关者受害群体（如用户、平台、开发者等），为攻击溯源和防护提供系统视角。  <br/>4. **设计轻量防御机制**：提出基于提示的自检方法，无需模型再训练即可有效缓解攻击，推动实用安全方案的落地。  <br/>5. **强调行业安全缺口**：通过实证研究揭示LLM安全的紧急漏洞，呼吁学术界、产业界和政策制定者协同应对。|
|2508.17597v1|[SonoCraftAR: Towards Supporting Personalized Authoring of Sound-Reactive   AR Interfaces by Deaf and Hard of Hearing Users](http://arxiv.org/abs/2508.17597v1)|**贡献点：**  <br/>1. 提出SonoCraftAR系统，首次实现听障/重听（DHH）用户通过自然语言输入创建个性化的声音反应AR界面。  <br/>2. 集成实时音频信号处理与多智能体大语言模型（LLM）管道，突破传统封闭式视觉化方法。  <br/>3. 利用矢量图形库生成动态2D动画界面，将音频频率映射为视觉属性（如大小、颜色），实现声音-视觉实时交互。  <br/>4. 验证开放性声音反应AR界面创作的可行性，为未来个性化与AI辅助的听觉辅助技术提供方向。  <br/><br/>**总结（100字内）：**  <br/>本研究开发了SonoCraftAR原型系统，使DHH用户可通过自然语言创建自定义声音可视化AR界面，结合音频处理与AI技术，提升声音可访问性，为未来个性化听觉辅助工具奠定基础。|
|2508.17164v1|[The Impact of Annotator Personas on LLM Behavior Across the   Perspectivism Spectrum](http://arxiv.org/abs/2508.17164v1)|总结：  <br/>本研究探讨了大语言模型在考虑预定义注解者角色时对仇恨言论的注解能力，发现LLMs选择性使用角色特征且在弱数据视角下表现优于人类，但在强数据视角下仍存在差距，为视角建模提供了新的方法论视角。<br/><br/>贡献点：  <br/>1. **引入LLM在视角建模中的应用**：首次探索大语言模型在注解仇恨言论和滥用性时，结合预定义注解者角色（personas）的能力，拓展了LLM在语音领域的应用场景。  <br/>2. **对比分析不同注解方法**：系统评估LLM生成的注解与传统注解建模技术的性能差异，揭示LLMs在强/弱数据视角下的表现特点。  <br/>3. **揭示LLMs的角色特征选择机制**：发现LLMs在注解过程中选择性地利用角色中的敏感属性，反映出其对社会偏见的建模能力及潜在偏差来源。  <br/>4. **提出原型注解者分类**：基于角色特征与人类注解者的对齐程度，识别出具有不同立场倾向的原型注解者，为视角分析提供分类依据。  <br/>5. **验证数据透视范式的有效性**：通过实验证明，无需依赖显式注解信息的模型在弱数据视角下表现更优，而个性化强数据集则需更精细的模型设计。  <br/>6. **定量对比LLM与人类注解**：明确LLM在强数据视角下接近但未超越人类注解性能，为实际应用中平衡自动化与人工标注提供了参考。|
|2508.16122v1|[Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection](http://arxiv.org/abs/2508.16122v1)|总结（100字以内）:  <br/>该研究揭示了多模态意图数据集的文本偏倚问题，提出去偏倚框架并分析模态相关性，强调构建无偏数据集对评估多模态模型的重要性。<br/><br/>贡献点:  <br/>1. **文本偏倚分析**：首次发现主流多模态意图数据集（如MIntRec）存在显著文本偏倚，超过90%的样本需依赖文本输入完成分类，且通过人工评估验证了该偏倚。  <br/>2. **模型性能对比**：实验证明Text-only LLM（Mistral-7B）在文本偏倚数据集上显著优于多模态模型，分别在MIntRec-1和MIntRec2.0上提升9%和4%。  <br/>3. **去偏倚框架设计**：提出系统性去偏倚方法，移除70%以上文本偏倚样本后，所有模型性能均大幅下降，尤其多模态融合模型准确率下降超50-60%。  <br/>4. **模态相关性研究**：通过实证分析揭示不同模态（文本、音频、视觉）在任务中的上下文相关性差异，为构建公平的数据集提供理论依据。|
|2508.15875v1|[NEAT: Concept driven Neuron Attribution in LLMs](http://arxiv.org/abs/2508.15875v1)|总结：  <br/>本文提出了基于概念向量定位关键神经元的高效方法，显著优化计算复杂度并提升概念理解性能，同时通过消融研究和实际应用场景分析，为理解LLM中的偏见与情绪言论提供了新路径。<br/><br/>贡献点：  <br/>1. **提出概念神经元定位方法**：首次利用概念向量识别对特定概念负责的关键神经元，并定义为"概念神经元"  <br/>2. **计算复杂度优化**：将传统O(n*m)的计算需求降为O(n)，显著减少正向传播次数  <br/>3. **性能对比验证**：在多个基准测试中证明方法优于现有技术，尤其在与SOTA方法对比中表现更优  <br/>4. **聚类辅助搜索优化**：通过消融实验引入聚类方法进一步提升概念神经元的搜索效率  <br/>5. **实际应用分析**：将方法应用于分析LLM中的仇恨言论与偏见，完成印度语境下的偏见评估  <br/>6. **拓展理解维度**：为理解神经元层面的语义责任提供了更广泛、类人化的视角  <br/>7. **未来研究路径**：构建了概念神经元识别与干预的研究框架，推动相关领域发展|
|2508.15827v2|[Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech   Models](http://arxiv.org/abs/2508.15827v2)|总结：  <br/>提出"Thinking-in-Speaking"新范式，通过token级交错推理与响应机制解决语音推理延迟问题，构建专用数据集Spoken-Math-Problems-3M，并在Spoken-MQA基准上实现显著性能提升与零解码延迟。<br/><br/>贡献点：  <br/>1. 提出"Thinking-in-Speaking"框架，实现语音生成与推理的实时协同（替代传统"Thinking-before-Speaking"的串行模式）  <br/>2. 设计token级交错机制，通过嵌入推理token与响应token提升生成效率与逻辑性  <br/>3. 创建大规模数据集Spoken-Math-Problems-3M，专门支持交错式语音-推理联合训练  <br/>4. 构建层次化Thinker-Talker架构，实现自然流畅且逻辑严谨的语音生成  <br/>5. 在Spoken-MQA基准测试中取得算术推理+19.1%、上下文理解+6.4%的显著提升，且无解码延迟|
|2508.15827v1|[Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech   Models](http://arxiv.org/abs/2508.15827v1)|总结:  <br/>论文提出新型语音推理框架Mini-Omni-Reasoner，通过“思考中发言”机制实现实时响应，设计专用数据集Spoken-Math-Problems-3M，实验显示在Spoken-MQA基准上提升显著，具备零延迟和高效性能。<br/><br/>贡献点:  <br/>1. **提出"Thinking-in-Speaking"框架**：首次在语音领域实现推理与生成的实时交错，避免传统顺序处理导致的延迟问题。  <br/>2. **设计Spoken-Math-Problems-3M数据集**：构建大尺度、结构化的语音-推理混合数据集，支持即时推理与响应的学习。  <br/>3. **创新分层架构（Thinker-Talker）**：通过层次化设计提升生成流畅性和逻辑性，兼顾自然语言表达与任务准确性。  <br/>4. **实验证明效能提升**：在Spoken-MQA基准测试中，实现算术推理+19.1%、语境理解+6.4%的显著提升，且输出无延迟。|
|2508.15810v1|[Detecting Hope, Hate, and Emotion in Arabic Textual Speech and   Multi-modal Memes Using Large Language Models](http://arxiv.org/abs/2508.15810v1)|**贡献点总结：**  <br/>1. 提出针对阿拉伯语文本及迷因中希望、仇恨言论、冒犯性语言和情感识别的统一任务框架。  <br/>2. 系统性评估基础LLM、微调LLM及预训练嵌入模型在阿拉伯语任务中的性能差异。  <br/>3. 在阿拉伯NLP MAHED 2025挑战中，实现三项任务的SOTA结果（72.1%/57.8%/79.6% macro F1）。  <br/>4. 为阿拉伯语内容审核系统提供更精准、高效的解决方案，推动实际应用。  <br/><br/>**摘要压缩（100字内）：**  <br/>该研究提出基于LLM的阿拉伯语文本与迷因内容分析框架，在MAHED 2025挑战中取得多任务SOTA性能，为内容审核提供精准高效的方法。|
|2508.15801v1|[LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in   Structured Synthetic Spoken Transcriptions](http://arxiv.org/abs/2508.15801v1)|**贡献点：**  <br/>1. **提出LingVarBench合成数据生成框架**：通过自动化验证解决电话对话标注成本高、隐私限制等挑战，首次构建结构化提取领域的合成数据基准。  <br/>2. **创新三阶段生成流程**：  <br/>   - 用LLM生成多场景结构化字段值；  <br/>   - 将字段值转化为真实对话特征（如歧义、中断、重叠）的自然语句；  <br/>   - 通过独立LLM验证合成语句的准确性，实现结构信息的还原。  <br/>3. **开发自动化提示优化方法**：基于DSPy的SIMBA优化器，无需人工干预即可生成高效提取提示，显著提升数值、名称、日期字段的抽取准确率（95% vs. 88-89%零样本）。  <br/>4. **验证合成到真实数据的泛化能力**：证明模型在包含背景噪声和领域术语的真实对话中表现优异，为商业场景的大规模电话分析提供可行方案。  <br/><br/>**总结（100字内）：**  <br/>论文提出LingVarBench合成数据生成框架，通过自动化验证和提示优化，显著提升电话对话结构化提取准确率，突破隐私与成本限制，推动合成数据在商业分析中的应用。|
|2508.15524v1|[The Enemy from Within: A Study of Political Delegitimization Discourse   in Israeli Political Speech](http://arxiv.org/abs/2508.15524v1)|总结：  <br/>本研究首次大规模分析政治去合法化话语，构建希伯来语语料库并提出两阶段分类模型，揭示其在民主话语中的长期演变趋势及社会特征。<br/><br/>贡献点：  <br/>1. **首次系统性研究**：提出针对政治去合法化话语（PDD）的首个大规模计算分析框架，定义其为对政治实体规范有效性的象征性攻击。  <br/>2. **多源语料库构建**：创建并手动标注包含10,410句希伯来语文本的新型语料库（议会演讲、社交媒体、新闻媒体），明确PDD实例（17.4%）及附加的强度、粗鲁性、目标类型和情感框架标注。  <br/>3. **两阶段分类方法**：设计结合微调编码器模型与解码器大语言模型（LLM）的分类流程，提升PDD识别与特征分类性能。  <br/>4. **模型性能验证**：提出DictaLM 2.0模型，在二分类PDD检测和多特征分类任务中分别达到F1值0.74和0.67，验证其有效性。  <br/>5. **跨平台与动态分析**：揭示PDD在30年间的显著增长趋势，对比社交媒体与议会辩论的差异，发现性别与意识形态相关的使用模式及选举/事件中的峰值现象。  <br/>6. **理论实践价值**：证明自动化PDD分析在研究民主话语中的可行性，为理解政治极化与话语演变提供新工具。|
|2508.15483v1|[HebID: Detecting Social Identities in Hebrew-language Political Text](http://arxiv.org/abs/2508.15483v1)|总结：  <br/>提出首个多标签希伯来语政治文本语料库HebID，验证调优后大模型在身份识别的优势，并通过跨场景分析揭示社会身份表达的动态差异与公众舆论关联。<br/><br/>贡献点：  <br/>1. 构建首个多标签希伯来语语料库（HebID），覆盖以色列政治人物的社交媒体和议会演讲文本，包含12种细致社会身份标签。  <br/>2. 验证生成式大语言模型（2B-9B参数）在希伯来语身份识别任务中的性能优势，提出宏F1值达0.74的评估结果。  <br/>3. 研究政治文本中社会身份表达的多维差异，包括流行度、时间趋势、聚类模式及性别关联性。  <br/>4. 将社会身份标注与国家级民意调查数据结合，对比精英话语与公众身份优先级的差异。  <br/>5. 提供非英语政治身份研究的标准化框架，为其他语言背景的类似研究奠定基础。|
|2508.14976v1|[Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal   CAPTCHA System](http://arxiv.org/abs/2508.14976v1)|总结（100字以内）:  <br/>Aura-CAPTCHA通过融合GANs、RL和LLMs，构建了可抵御OCR和对抗图像处理的多模态系统，实现动态难度调整和用户友好性，显著提升安全性与有效性，填补了现有CAPTCHA研究的漏洞。<br/><br/>贡献点:  <br/>1. **多模态安全设计**：结合视觉（3x3网格图像选择）和音频（随机数词组合）挑战，双重验证提升抗AI攻击能力。  <br/>2. **GANs动态生成挑战**：利用生成对抗网络创建复杂、动态的图像谜题，增强对传统OCR技术的防御。  <br/>3. **RL自适应难度控制**：基于尝试次数、响应时间和行为异常实时调整难度，优化用户体验与安全性平衡。  <br/>4. **LLMs生成语义内容**：通过大语言模型实现文本与音频提示的智能化生成，提高挑战的语义复杂度。  <br/>5. **实证效果显著**：在真实流量测试中达到92%人类通过率和10%机器人绕过率，超越现有CAPTCHA系统性能。|
|2508.14916v1|[Transsion Multilingual Speech Recognition System for MLC-SLM 2025   Challenge](http://arxiv.org/abs/2508.14916v1)|总结：  <br/>本文提出一种融合预训练模型与任务特定微调的多语言ASR系统，包含冻结Whisper-large-v3编码器、可训练适配器模块和冻结Qwen2.5-7B-Instruct模型加LoRA的解码架构，在11种语言评测中达到9.83%的WER/CER，排名全球第三。<br/><br/>贡献点：  <br/>1. **多语言ASR架构创新**：设计包含冻结预训练语音编码器、适配器模块和冻结大型语言模型的三阶段系统。  <br/>2. **高效对齐机制**：引入Linear-ReLU-Linear适配器模块优化语音与文本表示对齐。  <br/>3. **低成本优化策略**：采用冻结大语言模型（LLM）与LoRA微调结合，降低计算开销并提升上下文解码能力。  <br/>4. **跨语言性能验证**：在11种语言的评测集上实现9.83%的WER/CER，优于多数参赛方案。|
|2508.14764v1|[Investigation of the Inter-Rater Reliability between Large Language   Models and Human Raters in Qualitative Analysis](http://arxiv.org/abs/2508.14764v1)|总结：  <br/>该研究验证了GPT-4o和GPT-4.5在物理教育领域的定性分析可靠性，探索了优化参数与提示的方法，揭示了其在领域通用构念评估中的局限性，为大規模语音研究提供了可行性方案。<br/><br/>贡献点：  <br/>1. **首验证LLM可靠性**：通过Cohen's Kappa分析，首次系统评估ChatGPT-4o和ChatGPT-4.5在物理教育领域定性研究中的评分一致性，证明其可替代人类验证。  <br/>2. **优化模型表现**：提出通过调整提示词和超参数提升LLM对音频转录文本编码的准确性，实现更高效的分析流程。  <br/>3. **领域应用案例**：以实际物理教育研究场景（项目讨论策略）为例，展示LLM在具体主题（如问题解决方法）分析中的有效性。  <br/>4. **识别模型局限**：明确指出LLM在评估跨领域通用构念时仍存在局限，为后续改进提供方向。  <br/>5. **推动方法创新**：为语音领域定性研究提供可扩展的自动化工具方案，打破小数据集瓶颈。|
|2508.14706v1|[ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](http://arxiv.org/abs/2508.14706v1)|总结：  <br/>本研究提出首个面向TCM的多模态语言模型ShizhenGPT，构建全球最大TCM数据集并解决数据稀缺问题，创新性地实现多模态诊疗感知整合，通过权威评估验证其性能优势。<br/><br/>贡献点：  <br/>1. **首个TCM专用多模态LLM**：提出ShizhenGPT，首次将多模态能力应用于TCM领域。  <br/>2. **大规模多模态数据集**：构建包含100GB+文本和200GB+多模态数据（1.2M图像、200小时音频、生理信号）的全球最大TCM数据集。  <br/>3. **多模态感知融合**：实现跨模态统一感知（声、脉、嗅、视觉），突破传统LLM单模态局限。  <br/>4. **权威评估体系**：基于国家TCM资格考试建立视觉基准，评估模型在药识与视觉诊断能力。  <br/>5. **开源与可复现性**：开放数据、模型及代码，推动TCM领域研究发展。|
|2508.14130v1|[EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion   Recognition](http://arxiv.org/abs/2508.14130v1)|**总结（100字以内）：**  <br/>本文提出一种融合音频与文本的多模态情感识别方法，通过可学习接口模块和文本提示引导LLM，结合LoRA实现参数高效微调，在标准基准上取得优于多数模型的性能且参数量显著减少。  <br/><br/>---<br/><br/>**贡献点分点：**  <br/>1. **多模态融合框架创新**：首次将音频特征与文本表征结合，利用LLM进行语音情感预测，突破传统单模态或语言-语音分离的范式。  <br/>2. **跨模态映射机制设计**：提出可学习的接口模块，将音频特征高效映射至LLM的表示空间，解决异构模态对齐问题。  <br/>3. **任务引导输入方法**：引入文本提示（如情感标签描述）作为任务引导，增强模型对情感任务的语境理解能力。  <br/>4. **参数效率优化**：基于LoRA技术实现低参数量微调，有效降低模型规模的同时保持性能，提升部署可行性。  <br/>5. **性能验证与对比**：在主流情感识别基准上超越大部分Speech-Text LLMs，参数量减少至竞争方法的不到一半，证明方法有效性与效率。|
|2508.13769v1|[Can Large Language Models (LLMs) Describe Pictures Like Children? A   Comparative Corpus Study](http://arxiv.org/abs/2508.13769v1)|总结：  <br/>本研究通过比较LLM生成文本与儿童语言的特征，揭示其在模拟儿童语言表达上的局限性，并为LLM在教育与心理语言学领域的应用提供新视角。<br/><br/>贡献点：  <br/>1. **首次系统比较LLMs与儿童语言**：构建了基于相同图片故事和两种提示策略（零样本与少样本）的LLM语料库，对比分析其与德国儿童描述文本的异同。  <br/>2. **揭示LLMs语言生成的特征差异**：发现LLMs生成文本更长但词汇贫乏，依赖高频词且名词使用不足，语义相似性较低。  <br/>3. **评估多模态提示策略效果**：证明少样本提示可略微提升LLMs与儿童文本的相似性，但未能复制词汇和语义模式。  <br/>4. **为教育工具合适性提供依据**：提出LLMs生成语言在儿童导向教育工具中的适当性问题，为教育应用与研究提供警示。|
|2508.13603v1|[Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of   a Speech-LLM](http://arxiv.org/abs/2508.13603v1)|总结：  <br/>该研究通过说话人分配分析语音模型的性别偏见，构建性别相关数据集验证，指出Bark无系统偏见但存在意识倾向，揭示语音生成模型与文本模型在性别处理上的差异。<br/><br/>贡献点：  <br/>1. **提出新的分析框架**：首次将“说话人分配”作为语音模型性别偏见研究的显式分析工具，突破文本模型隐式编码的局限性。  <br/>2. **构建针对性数据集**：设计“职业（Professions）”与“性别化词汇（Gender-Colored Words）”两个数据集，系统评估语音模型对性别关联的识别与生成能力。  <br/>3. **揭示模型特性差异**：发现Speech-LLM在性别表达上与文本模型的显著差异，即其需显式生成性别化声音，导致说话人选择成为直接的偏见线索。  <br/>4. **实证分析结果**：通过实验验证Bark模型未展现系统性性别偏见，但存在性别意识和非系统性倾向，为语音模型的公平性研究提供新视角。|
|2508.12854v1|[E3RG: Building Explicit Emotion-driven Empathetic Response Generation   System with Multimodal Large Language Model](http://arxiv.org/abs/2508.12854v1)|总结：  <br/>提出E3RG系统，通过任务分解和多模态生成技术实现情感对话生成，无需额外训练即可生成自然且身份一致的响应，实验表现优异。<br/><br/>贡献点：  <br/>1. 提出E3RG系统，首次将多模态情感响应生成（MERG）分解为“情绪理解-记忆检索-响应生成”三个模块性任务。  <br/>2. 集成先进的语音与视频生成模型，实现无需额外训练的自然、情感丰富且身份一致的多模态响应生成。  <br/>3. 在零样本和少样本场景下验证系统有效性，于ACM MM 2025 Avatar-based Multimodal Empathy Challenge中取得Top-1成绩。|
|2508.12666v1|[Cryfish: On deep audio analysis with Large Language Models](http://arxiv.org/abs/2508.12666v1)|**贡献点分点：**  <br/>1. **提出Cryfish模型**：首个结合WavLM音频编码器与Qwen2语言模型的多模态架构，通过Transformer-based连接器实现跨模态整合。  <br/>2. **专门化训练策略**：设计针对听觉任务的训练方法，提升模型在复杂语音和声音识别任务上的泛化能力。  <br/>3. **基准测试与评估**：基于Dynamic SUPERB Phase-2新基准，系统性验证Cryfish在语音相关多任务上的性能。  <br/>4. **对比分析**：与公开的语音模型进行深入比较，突出Cryfish在听觉能力集成方面的优势。  <br/><br/>**总结（100字内）：**  <br/>本文提出Cryfish，将WavLM与Qwen2结合，通过Transformer连接器实现语音能力整合，并采用专门训练策略提升多任务性能，基于动态SUPERB基准验证模型效果，与现有模型对比分析其在听觉理解中的优势。|
|2508.12622v1|[Consiglieres in the Shadow: Understanding the Use of Uncensored Large   Language Models in Cybercrimes](http://arxiv.org/abs/2508.12622v1)|总结：  <br/>本文首次系统研究未审查大语言模型（ULLMs），构建知识图谱并运用图神经网络识别出超11,000个潜在恶意模型，揭示其广泛危害及地下论坛的滥用现象，强调需加强AI安全防护。<br/><br/>贡献点：  <br/>1. **首次系统性研究**：提出首个针对未审查大语言模型的系统性分析框架，填补研究空白。  <br/>2. **关系建模**：建立开源LLMs与相关数据（如微调、合并、压缩模型及有害数据集）的关联网络。  <br/>3. **知识图谱技术**：将模型关系转化为知识图谱，结合图神经网络实现高效识别。  <br/>4. **大规模发现**：基于少量标记样本和未审查数据，识别出超11,000个ULLMs。  <br/>5. **危害分析**：揭示ULLMs可生成仇恨言论、暴力内容、色情材料及恶意代码，并被集成至数百个恶意应用。  <br/>6. **地下活动证据**：发现黑客通过论坛分享技术构建廉价替代商业恶意LLMs的手段。  <br/>7. **安全呼吁**：强调LLM技术滥用的严重性，倡导制定针对性的防御策略。|
|2508.12438v1|[Express4D: Expressive, Friendly, and Extensible 4D Facial Motion   Generation Benchmark](http://arxiv.org/abs/2508.12438v1)|总结：本文提出Express4D数据集，结合自然语言指令与ARKit格式，解决现有数据集在表情精细度和采集成本上的不足，为文本到面部动态表情生成提供基准。  <br/><br/>贡献点：  <br/>1. **提出新型数据集Express4D**：包含细粒度面部运动序列与语义标注，解决现有数据集仅基于语音驱动或粗粒度情感标签的局限性。  <br/>2. **低成本采集方案**：利用消费级设备与LLM生成的自然语言指令高效采集数据，提升数据获取的便捷性与可扩展性。  <br/>3. **支持多对多映射研究**：通过数据集验证文本与面部表情之间的复杂关联，推动细粒度控制和跨模态学习。  <br/>4. **开源共享资源**：提供数据集、代码及视频示例，促进研究社区在该领域的进一步探索与应用。|
|2508.11873v1|[SimInterview: Transforming Business Education through Large Language   Model-Based Simulated Multilingual Interview Training System](http://arxiv.org/abs/2508.11873v1)|**总结**：  <br/>本文提出SimInterview系统，结合多语言模型与合成AI技术，实现跨语言个性化面试训练，提升职场竞争力，并通过实验验证其有效性，同时探索文化差异对面试策略的影响及AI伦理设计框架。<br/><br/>**贡献点**：  <br/>1. **多语言个性化面试训练系统**：开发SimInterview，基于LLM构建支持英语和日语的虚拟招聘官，实现实时、个性化的模拟面试场景。  <br/>2. **动态场景匹配机制**：利用RAG技术动态适配简历与职位要求，提升面试针对性和真实性。  <br/>3. **技术集成创新**：整合Whisper（语音识别）、GPT-SoVITS（语音合成）、Ditto（虚拟形象生成）和ChromaDB（知识库）等工具，增强系统交互性和效率。  <br/>4. **实验验证有效性**：通过大学候选人实验，证明系统能精准匹配岗位需求、保留简历信息，并获得高满意度评价，尤其突出Gemma 3模型的对话质量。  <br/>5. **文化差异研究**：分析标准化日语简历与多样化英语简历对文档检索的影响，揭示文化规范对提问策略的塑造作用。  <br/>6. **可争议AI设计框架**：提出可解释、可检测偏见且保留人类监督的AI设计，满足新兴法规对透明性和公平性的要求。|
|2508.11808v1|[Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](http://arxiv.org/abs/2508.11808v1)|**总结（100字以内）:**  <br/>本文提出了一种双路径方法，通过优化提示结构和引入多模态数据增强技术提升仇恨言论的检测效果，验证了提示设计与数据组成对模型性能的关键影响，并展示了生成中性数据以增强分类器可靠性的新方向。<br/><br/>**贡献点分点列出:**  <br/>1. **双路径方法论**：首次结合提示优化与数据增强策略，系统性提升多模态仇恨检测能力。  <br/>2. **提示优化框架**：提出动态调整提示结构、监督粒度与训练模式的框架，提升模型鲁棒性与效果（如InternVL2在二分类与缩放设置中表现最佳）。  <br/>3. **数据增强技术**：设计多模态数据增强流水线，通过隔离并重写仇恨内容生成2,479个中性meme，减少虚假关联并增强模型泛化能力。  <br/>4. **多代理协同机制**：利用多代理LLM-VLM系统实现更高效的虚假关联消除与数据生成。  <br/>5. **理论启示**：证明提示结构与数据配置对模型性能的决定性作用，颠覆传统依赖模型规模的假设。  <br/>6. **实践价值**：为训练可靠、公平的VLM提供合成数据生成的新范式，支持上下文敏感的仇恨检测。|
|2508.11434v1|[Online Anti-sexist Speech: Identifying Resistance to Gender Bias in   Political Discourse](http://arxiv.org/abs/2508.11434v1)|总结：  <br/>本研究揭示了LLMs在识别反性别歧视言论时的误判问题，提出需超越二元分类框架，结合人类审核与训练数据改进，以保障数字政治空间中的抵抗性言论。<br/><br/>贡献点：  <br/>1. 首次系统分析LLMs对反性别歧视、性别歧视及中性政治推文的分类偏差，尤其关注女性议员涉事的高敏感度事件。  <br/>2. 发现模型在政治化事件中易混淆"有害"与"抵抗"的修辞表达，导致反性别歧视言论被错误过滤。  <br/>3. 提出多维度改进方案：引入人机协同审核、在训练数据中明确包含反言论样本、重构危害判定标准。  <br/>4. 跨学科整合女性主义理论与模型评估方法，为数字平台内容治理提供社会技术视角的理论支持。  <br/>5. 强调算法偏见对弱势群体发声的结构性压制，推动算法伦理在政治言论监督中的实践应用。|
|2508.10949v1|[Perturbed Public Voices (P$^{2}$V): A Dataset for Robust Audio Deepfake   Detection](http://arxiv.org/abs/2508.10949v1)|总结：  <br/>提出P$^{2}$V数据集，系统评估现有音频深度伪造检测模型的脆弱性，并展示基于该数据集训练的模型在鲁棒性和泛化能力上的显著提升，推动深度伪造检测技术发展。<br/><br/>贡献点：  <br/>1. **构建首个针对性数据集**：开发了P$^{2}$V数据集，首次全面覆盖恶意音频深度伪造的三大核心特征（身份一致的转录文本、环境噪声与对抗干扰、先进语音克隆技术），为后续研究提供真实场景基准。  <br/>2. **揭示现有检测器的严重局限**：通过实验证明当前22种检测器在P$^{2}$V数据集上的性能显著下降（平均下降43%），并量化对抗扰动和高级克隆技术对检测效果的削弱程度（16%和20-30%）。  <br/>3. **提出增强鲁棒性的新方法**：基于P$^{2}$V训练的模型在抗攻击能力上优于传统方法，同时保持对现有数据集的泛化性，为音频深度伪造检测建立了新的性能基准。  <br/>4. **公开共享数据集**：承诺在论文被会议/期刊接受后公开P$^{2}$V，促进学术界共同研究和验证深度伪造检测技术的改进。|
|2508.10494v1|[A Unified Multi-Agent Framework for Universal Multimodal Understanding   and Generation](http://arxiv.org/abs/2508.10494v1)|总结：  <br/>本文提出MAGUS框架，通过解耦多模态理解和生成流程，实现灵活的any-to-any能力，超越现有模型并在多任务中表现优异。<br/><br/>贡献点：  <br/>1. **模块化架构设计**：首次提出解耦的Cognition（认知）和Deliberation（审慎）两阶段框架，兼顾多模态理解和生成的灵活性与可扩展性。  <br/>2. **多角色协作机制**：引入三种角色条件的LLM代理（Perceiver、Planner、Reflector），通过协同对话实现结构化信息理解和规划。  <br/>3. **协同生成策略**：设计Growth-Aware Search机制，结合LLM推理与扩散模型生成，形成相互增强的生成流程。  <br/>4. **无需联合训练**：支持plug-and-play扩展性、跨模态转换及语义对齐，无需多模态联合训练即可实现端到端功能。  <br/>5. **性能突破**：在MME等基准测试中超越闭源模型GPT-4o，验证了框架在多模态任务中的优越性。|
|2508.10027v2|[LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by   LLM-Generated Synthetic Data](http://arxiv.org/abs/2508.10027v2)|**贡献点总结（100字以内）**  <br/>提出融合Transformer嵌入与手crafted语言特征的模型，提升ADRD检测性能；利用LLM生成合成语音进行数据增广，显著改善分类效果；对比单模态与多模态LLM分类器，揭示多模态模型需进一步优化的价值。  <br/><br/>**分点贡献**  <br/>1. **融合模型设计**：首次将Transformer嵌入与110个词法衍生语言学特征结合，提升ADRD检测的F1值至83.3（AUC=89.5），优于纯语言或纯Transformer基线。  <br/>2. **合成语音数据增广**：通过LLM生成的合成语音（如MedAlpaca-7B）增强训练数据，使F1值进一步提高至85.7，验证了数据增强的有效性。  <br/>3. **多模态分类效能评估**：系统测试单模态（F1=78.5）与多模态（GPT-4o=70.2, Qwen=66.0）LLM分类器，揭示多模态模型性能不足的瓶颈。  <br/>4. **临床适用性验证**：证明经过临床微调的LLM可有效支持ADRD的语音-文本分类与数据增广，为实际应用提供方法论参考。  <br/>5. **性能关联分析**：发现增广效果与合成语音与真实语音的分布相似性相关，为语音生成与疾病检测的结合提供理论依据。|
|2508.10016v2|[Training-Free Multimodal Large Language Model Orchestration](http://arxiv.org/abs/2508.10016v2)|**贡献点：**  <br/>1. **无需额外训练**：提出多模态大语言模型编排框架（MLLM Orchestration），无需额外训练即可实现多模态交互系统，简化集成流程。  <br/>2. **三重核心技术创新**：  <br/>   - (1) **中央控制器LLM**：通过设计智能代理动态分配任务至专用模型，提升模态协调效率。  <br/>   - (2) **并行TTS架构**：支持全双工交互与无缝中断处理，优化对话流畅性与实时性。  <br/>   - (3) **跨模态记忆系统**：智能合成与检索多模态信息，保持上下文连贯性并减少冗余调用。  <br/>3. **性能验证**：在标准基准上实现性能提升（+7.8%）、延迟降低（-10.3%）及解释性增强，证明框架有效性。  <br/><br/>**总结（100字以内）**：  <br/>本文提出无需额外训练的多模态大语言模型编排方法，通过中央控制器、并行TTS架构和跨模态记忆系统实现高效交互，显著提升性能与解释性，降低延迟，为多模态AI系统提供新范式。|
|2508.09767v2|[UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in   Multilingual Text-to-Speech](http://arxiv.org/abs/2508.09767v2)|**贡献点：**  <br/>1. 提出轻量级UtterTune方法，通过低秩适配实现多语言TTS系统的语言特定发音控制。  <br/>2. 在无需显式G2P模块的情况下，解决直接处理最小编码文本（如字节对编码）的发音建模难题。  <br/>3. 实现日语语音的音素级发音和声调控制，同时保持零样本场景下的自然度与说话人相似性。  <br/>4. 通过客观与主观评估验证方法有效性，证明发音可控性提升与多语言性能兼容性。  <br/><br/>**总结：**  <br/>UtterTune通过低秩适配增强日语TTS发音可控性，兼顾多语言性能与零样本泛化能力，经验证有效。|
|2508.09767v1|[UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in   Multilingual Text-to-Speech](http://arxiv.org/abs/2508.09767v1)|**贡献点：**<br/>1. 提出UtterTune，一种轻量级适配方法，用于多语言TTS系统，无需显式G2P模块即可提升目标语言（日语）的发音可控性。<br/>2. 利用低秩微调技术，实现对日语音素级发音和音高重音的精准控制，同时保持零样本条件下其他语言的自然度与说话人相似性。<br/>3. 解决了直接处理简编码文本（如字节对编码）时的G2P建模与语调生成难题，提升多语言TTS系统的泛化能力。<br/>4. 通过客观和主观评估验证方法有效性，证明其在保持多语言性能的同时显著增强目标语言的可控性。<br/><br/>**总结：**<br/>该研究提出UtterTune方法，在无需显式G2P模块的前提下，通过低秩微调增强多语言TTS系统对日语音素级发音的控制，同时保持其他语言的自然度与零样本性能，经评估验证其有效性。|
|2508.09535v1|[AI Blob! LLM-Driven Recontextualization of Italian Television Archives](http://arxiv.org/abs/2508.09535v1)|**总结（100字以内）**  <br/>提出AI Blob!系统，整合ASR、语义嵌入与RAG技术，构建动态内容检索框架，实现档案电视素材的自动叙述生成与文化分析，推动跨学科媒体研究及AI技术应用。<br/><br/>**贡献点分点**  <br/>1. **开发AI Blob!系统**：首个将语义目录与LLMs结合用于档案电视素材检索与再语境化的实验系统。  <br/>2. **技术整合**：融合自动语音识别（ASR）、语义嵌入与检索增强生成（RAG）技术，实现音频文本化与语义化处理。  <br/>3. **动态数据处理**：采用动态内容检索机制，替代传统静态元数据，提升档案内容的语义关联性与可操作性。  <br/>4. **叙事生成创新**：通过算法筛选与结构化重组，生成具备讽刺拼贴与主题连贯性叙事片段，模拟编辑实践。  <br/>5. **提供开放资源**：公开数据集与方法论框架，支持后续跨学科研究与技术迭代。  <br/>6. **理论与实践结合**：回应媒体史与AI档案研究的学术争辩，拓展档案内容的分析维度与应用场景。|
|2508.08777v1|[Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge](http://arxiv.org/abs/2508.08777v1)|总结：  <br/>提出一种基于LLM的profile-aware框架，通过自然语言用户画像与播客匹配评估，解决传统推荐评估方法在曝光偏差和成本上的问题，实现高效、可解释的离线评估。<br/><br/>贡献点：  <br/>1. **提出新型评估框架**：首次将大语言模型（LLM）作为离线评估者，替代传统离线指标和高成本的在线A/B测试，提升播客推荐系统的评估效率与可扩展性。  <br/>2. **构建profile-aware用户画像**：从90天监听历史中提取自然语言用户画像，综合表征用户主题兴趣与行为模式，形成可解释的推荐质量分析依据。  <br/>3. **优化LLM输入机制**：通过用户画像提供高语义上下文，而非直接使用原始数据，降低输入复杂度并增强LLM对推荐与用户匹配度的推理能力。  <br/>4. **实现细粒度评估**：通过逐点（pointwise）和成对（pairwise）判断，量化用户画像与播客内容的匹配质量，提供更精确的推荐效果评估。  <br/>5. **实验验证有效性**：在47人对照研究中证明框架与人类判断高度一致，且优于仅使用原始监听数据的评估方法，验证其实际应用价值。|
|2508.08715v3|[MultiGen: Child-Friendly Multilingual Speech Generator with LLMs](http://arxiv.org/abs/2508.08715v3)|**贡献点总结（100字以内）：**  <br/>提出MultiGen多语言语音生成模型，针对低资源语言设计儿童友好交互，结合LLM架构与年龄适应性策略，支持新加坡普通话、马来语和泰米尔语，实验验证其优于基线方法。<br/><br/>**分点贡献：**<br/>1. **提出MultiGen模型**：首个针对低资源语言的多语言语音生成模型，支持新加坡普通话、马来语和泰米尔语三种语言。<br/>2. **儿童友好交互设计**：整合年龄适配的生成策略，优化语音内容以符合儿童语言学习需求。<br/>3. **LLM架构创新**：利用大语言模型架构实现语音生成，提升低资源语言的语料利用效率与生成质量。<br/>4. **文化相关性融合**：在生成过程中融入目标语言的文化背景，增强儿童与AI系统的自然交互体验。<br/>5. **实证效果验证**：通过客观指标和主观评估实验，证明MultiGen在低资源语言场景下的性能优势。|
|2508.08715v2|[MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation   Tutor with LLMs](http://arxiv.org/abs/2508.08715v2)|总结：  <br/>本研究提出MultiAiTutor，一种面向儿童教育的多语言生成式AI导师系统，通过LLM架构实现针对低资源语言（新加坡口音普通话、马来语、泰米尔语）的教育定制语音生成，并结合文化相关的图像描述任务验证其有效性。<br/><br/>贡献点：  <br/>1. **提出多语言儿童友好语音生成框架**：首次构建专门针对儿童语言学习的多语种生成式AI导师系统（MultiAiTutor），支持新加坡口音普通话、马来语和泰米尔三种低资源语言。  <br/>2. **整合年龄适配性与文化相关性**：通过设计适龄且具有文化背景的图像描述任务，提升儿童对多语言内容的理解与学习效果。  <br/>3. **优化LLM架构用于教育场景**：将大规模语言模型（LLM）技术应用于语音生成任务，定制化适配教育需求，改善低资源语言的生成质量。  <br/>4. **实验验证有效性**：通过客观与主观评估证明MultiAiTutor在低资源语言场景下的优越性能，为儿童语言教育提供新范式。|
|2508.08715v1|[MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation   Tutor with LLMs](http://arxiv.org/abs/2508.08715v1)|**贡献点**:  <br/>1. **提出MultiAiTutor模型**：开发首个面向儿童教育的多语言生成式AI导师系统，支持低资源语言（新加坡口音普通话、马来语、泰米尔语）的个性化语音生成。  <br/>2. **儿童友好设计**：集成年龄适应性和文化相关性，通过图像描述任务优化语音内容，提升儿童语言学习的沉浸感和实用性。  <br/>3. **多语言语音生成整合**：创新性地结合LLM架构，实现针对不同年龄段的多语言语音生成，解决低资源语言教育场景的挑战。  <br/>4. **文化相关任务应用**：在低资源语言中设计符合特定文化背景的图像描述任务，增强教育内容的本土化和情境真实感。  <br/>5. **实验验证有效性**：通过客观指标（如语音质量评估）和主观评价（如用户反馈）验证模型性能，证明其优于现有基线方法。  <br/><br/>**总结（100字以内）**:  <br/>本研究提出MultiAiTutor，一种多语言儿童友好的语音生成AI导师，解决低资源语言教育难题，通过文化相关任务和实验验证展示其优越性。|
|2508.08585v1|[Joint decoding method for controllable contextual speech recognition   based on Speech LLM](http://arxiv.org/abs/2508.08585v1)|总结：  <br/>该论文提出联合解码方法实现上下文信息的显式控制，提升语音识别性能，并扩展至敏感词抑制任务，证明未预训练的Speech LLM可通过此方法获得长上下文处理能力。<br/><br/>贡献点：  <br/>1. 提出联合解码方法，实现对上下文信息注入程度的显式控制，突破传统Prompt注入依赖模型内部注意力机制的局限性；  <br/>2. 构建支持敏感词抑制的语音识别框架，拓展了Speech LLM在隐私保护场景的应用潜力；  <br/>3. 验证该方法的有效性，证明即使非长上下文预训练的Speech LLM也能通过外部策略获得长上下文建模能力。|
|2508.08550v1|[Fine-grained Video Dubbing Duration Alignment with Segment Supervised   Preference Optimization](http://arxiv.org/abs/2508.08550v1)|**贡献点总结**  <br/>1. **问题建模创新**：首次将视频配音中的时长对齐问题转化为偏好优化框架，突破传统方法的局限性。  <br/>2. **方法提出**：设计Segment Supervised Preference Optimization (SSPO)方法，整合分段抽样策略与细粒度损失函数，针对性解决跨语言时长不匹配问题。  <br/>3. **技术优化**：通过分段抽样策略提升对局部语音片段的建模精度，结合细粒度损失函数精细化调整时长差异，增强音频与视频同步效果。  <br/>4. **实验验证**：在真实数据集上验证了SSPO方法的优越性能，显著提升视频配音中的时长对齐效果并改善观众体验。|
|2508.08131v1|[Optimal Transport Regularization for Speech Text Alignment in Spoken   Language Models](http://arxiv.org/abs/2508.08131v1)|总结：  <br/>提出OTReg方法，通过最优运输理论解决语音-文本模态间隙问题，提升多语言ASR模型泛化能力，具有轻量级和无需额外标注的特性。<br/><br/>贡献点：  <br/>1. **提出新方法OTReg**：将语音-文本对齐建模为最优运输问题，推导出用于改进SLM训练的正则化损失。  <br/>2. **解决模态间隙问题**：通过结构化对应关系减少语音嵌入的高变异性，促进跨数据集泛化。  <br/>3. **轻量级设计**：无需额外标签或可学习参数，直接集成到现有SLM训练流程中。  <br/>4. **实验证明有效性**：在多语言ASR任务中验证OTReg提升语音-文本对齐效果，显著增强模型泛化能力。|
|2508.08027v1|[Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking   Self-Supervised and Generative Approaches](http://arxiv.org/abs/2508.08027v1)|总结：  <br/>本研究系统评估了自监督ASR模型在语音障碍场景下的性能，提出基于LLM的解码方法提升可懂度，并分析了模型泛化能力和不同严重程度的识别错误，为改进dysarthric speech识别提供了新思路。<br/><br/>贡献点：  <br/>1. **系统性基准测试**：对Wav2Vec、HuBERT、Whisper等主流ASR架构在语音障碍场景下的表现进行系统性评估，明确其有效性瓶颈。  <br/>2. **LLM增强的解码方法**：首次引入BART、GPT-2、Vicuna等LLM用于ASR解码，通过语言约束优化音素恢复和语法纠错，提升可懂度。  <br/>3. **泛化能力分析**：对比不同数据集上的模型表现，揭示其在语音障碍场景中的泛化特性与局限性。  <br/>4. **识别错误分层研究**：针对语音障碍的严重程度分级，深入分析识别错误的分布规律，为错误定位与改进提供依据。|
|2508.08020v1|[EchoAid: Enhancing Livestream Shopping Accessibility for the DHH   Community](http://arxiv.org/abs/2508.08020v1)|总结：  <br/>本文提出EchoAid，整合语音转文字、RSVP与大语言模型技术，通过用户研究验证其有效性，显著改善聋哑用户直播购物体验，降低认知负担。<br/><br/>贡献点：  <br/>1. **提出针对DHH群体的语音购物辅助方案**：开发EchoAid移动应用，解决直播购物信息不畅与过载问题。  <br/>2. **整合多技术模块**：结合语音识别、RSVP视觉呈现及大语言模型，优化复杂信息流的处理。  <br/>3. **用户中心化设计**：基于8名聋哑用户的需求研究与3名参与者的反馈，迭代完善原型。  <br/>4. **系统验证与效果评估**：通过38人用户研究验证EchoAid的实用性，证实其对信息提取和认知负荷的改善作用。|
|2508.07781v1|[SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech   Translation](http://arxiv.org/abs/2508.07781v1)|**贡献点总结（100字以内）**  <br/>提出基于语法的分块策略与SASST端到端框架，融合冻结的Whisper模型与LLM，通过动态输出机制优化翻译时效性和内容质量，有效解决词序差异问题，验证了语法结构在多语言实时翻译中的关键作用。<br/><br/>**分点贡献**  <br/>1. **语法引导的分块策略**  <br/>   - 提出基于依存句法关系与标点特征的语义分块方法，精准分割音频输入为连贯语义单元，减少语义碎片化。  <br/><br/>2. **SASST框架设计**  <br/>   - 构建端到端框架，通过冻结Whisper编码器和解码器-only的大语言模型（LLM），实现高效的语法感知实时翻译。  <br/><br/>3. **动态输出优化机制**  <br/>   - 引入翻译token与<WAIT>符号的联合生成策略，同步优化翻译时机与内容质量，提升实时性。  <br/><br/>4. **目标端重排序技术**  <br/>   - 采用目标语言重排序模块，减少源语言与目标语言间词序差异带来的翻译误差。  <br/><br/>5. **跨语言翻译性能验证**  <br/>   - 在CoVoST2多语言语料库（En-De-Zh-Ja）上实验证明，该方法显著提升多语言实时翻译质量，验证语法结构对LLM驱动系统的有效性。|
|2508.07470v2|[AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning](http://arxiv.org/abs/2508.07470v2)|总结：  <br/>本文提出AURA基准，通过多模态推理评估揭示模型在因果性、同步性等六类任务中的逻辑缺陷，引入全新指标AuraScore以量化推理过程。<br/><br/>贡献点：  <br/>1. **提出AURA基准**：首个专注于跨模态（音频-视频）推理过程评估的基准，突破传统仅关注最终答案准确率的局限。  <br/>2. **六类认知任务设计**：涵盖因果性、音色音高、节奏同步、不可回答性、隐含干扰、技能画像等挑战性领域，需双模态联合推理。  <br/>3. **引入AuraScore指标**：首次分解推理为事实一致性（感知证据）和核心推理（逻辑有效性），量化评估推理质量而非仅结果。  <br/>4. **揭示模型推理缺陷**：实验发现SOTA模型虽准确率高（达92%），但推理过程得分低（<45%），证明其存在逻辑漏洞与幻觉问题。  <br/>5. **推动多模态评价研究**：指出现有基准的不足，为更全面的跨模态模型评估提供方法论和参考方向。|
|2508.07470v1|[AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning](http://arxiv.org/abs/2508.07470v1)|**贡献点总结**  <br/>该论文提出AURA基准，用于评估多模态模型的推理能力，而非仅答案准确率。引入AuraScore指标分解推理为事实一致性与逻辑有效性。发现当前SOTA模型虽高准确率，但推理得分低，揭示模型存在逻辑缺陷，推动更全面的多模态评估。  <br/><br/>**分点贡献**  <br/>1. 提出AURA基准：首个评估跨模态推理能力的多模态（音频-视频）基准，超越传统仅关注答案准确率的评测方法。  <br/>2. 设计多认知域问题：涵盖因果关系、音色音高、节奏同步、不可回答性、隐含干扰和技能分析六大领域，强制模型依赖多模态信息综合推理。  <br/>3. 创新AuraScore指标：首次将推理可信度拆解为“事实一致性”与“核心推理”两维度，提供更细致的模型性能评估工具。  <br/>4. 揭示推理缺陷：通过实验证明当前SOTA模型在推理得分上存在显著短板（<45%），提出改进多模态模型推理能力的研究方向。|
|2508.07302v2|[XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation](http://arxiv.org/abs/2508.07302v2)|**贡献点总结：**  <br/>1. 提出XEmoRAG框架，无需平行情感数据实现中泰零样本情感迁移。  <br/>2. 通过语言无关情感嵌入提取与情感匹配语句检索，实现可控情感生成。  <br/>3. 引入流匹配对齐模块，减少音高/时长差异以确保自然韵律。  <br/>4. 融合中文音色到泰语合成，提升节奏准确性及情感表达，保留说话人特征。  <br/>5. 实验证明在低资源场景下可有效跨语言传递情感，具灵活性与实用性。  <br/><br/>**精简版（100字内）:**  <br/>提出XEmoRAG框架，无需平行数据实现中泰零样本情感迁移，通过情感嵌入提取、流匹配对齐和音色融合技术，有效解决跨语言情感传递难题，实验验证其在低资源场景下的高效性和自然性。|
|2508.07302v1|[XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation](http://arxiv.org/abs/2508.07302v1)|总结（100字以内）:  <br/>XEmoRAG提出基于LLM的跨语言情感迁移框架，无需平行数据，通过语言无关情感嵌入和检索对齐模块生成自然泰语语音，有效提升情感表达与韵律一致性。<br/><br/>贡献点:  <br/>1. **开创性框架**：提出XEmoRAG，首次实现无需平行情感语料的中泰跨语言零样本情感迁移。  <br/>2. **语言无关情感表征**：通过LLM提取中文语音的情感嵌入，分离语言与情感特征，实现跨语言迁移。  <br/>3. **检索对齐机制**：从情感数据库中检索匹配情绪的泰语句子，无需显式情绪标签。  <br/>4. **韵律优化模块**：引入流程匹配对齐技术，减少音高/时长差异，提升自然度。  <br/>5. **音色融合技术**：将中文音色融入泰语合成，增强节奏准确性与情感表达，保留说话人特征。|
|2508.07295v1|[CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text   Factuality Evaluation](http://arxiv.org/abs/2508.07295v1)|总结（100字以内）:  <br/>提出跨语言-跨模态事实性评估基准CCFQA，揭示当前MLLMs在多语言语音任务中的缺陷，并开发少样本迁移策略提升性能，推动语音理解模型研究。  <br/><br/>贡献点:  <br/>1. **提出首个跨语言与跨模态事实性基准CCFQA**：包含8种语言的平行语音-文本事实性问题，系统性评估MLLMs在多语言语音输入下的事实性与可靠性。  <br/>2. **揭示当前多模态大模型的局限性**：实验表明现有模型在CCFQA中表现欠佳，凸显多语言语音处理能力不足的问题。  <br/>3. **设计高效少样本迁移策略**：将英语QA能力迁移至多语言SQA任务，仅需5样本即可达到与GPT-4o-mini-Audio相当的性能。  <br/>4. **开源代码与数据集**：公开CCFQA作为研究资源，为改进MLLMs的语音理解能力提供基础支持。|
|2508.07273v1|[Incorporating Contextual Paralinguistic Understanding in Large   Speech-Language Models](http://arxiv.org/abs/2508.07273v1)|总结：  <br/>本文提出两种方法提升语音大模型的情感理解能力，通过整合上下文与非语言信息，显著提高模型性能，并验证了LLM评估指标的可靠性。<br/><br/>贡献点：  <br/>1. 提出显式方法：直接引入情感元数据（如情绪标注）到模型训练，增强对非语言线索的感知。  <br/>2. 设计隐式方法：结合情感标注、语音转录及分类与维度标注，自动生成高质量QA对以优化训练数据。  <br/>3. 实验验证：隐式方法在基准测试中提升38.41%（LLM评分），与显式方法结合后达到46.02%，证明有效性。  <br/>4. 可靠性分析：通过LLM评分与分类指标的高相关性，论证评估方法的可信度。|
|2508.07173v1|[Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual   Large Language Models](http://arxiv.org/abs/2508.07173v1)|总结：  <br/>本文提出首个针对多模态大语言模型（OLLMs）的安全性综合评估基准Omni-SafetyBench，设计专用指标揭示模型在复杂输入下的安全缺陷，为提升OLLMs安全性提供基础。<br/><br/>贡献点：  <br/>1. 构建首个支持24种模态组合的OLLMs安全性平行基准Omni-SafetyBench，涵盖972个样本及专门的音频-视觉危害案例；  <br/>2. 提出Safety-score（基于条件攻击成功率C-ASR与拒绝率C-RR）和Cross-Modal Safety Consistency Score（CMSC-score）两类指标，分别解决模态理解失败与跨模态一致性评估问题；  <br/>3. 通过评估6个开源和4个闭源OLLMs，发现关键安全隐患：无模型在安全性和一致性上同时表现优异，复杂输入下安全防御显著弱化，部分模型在特定模态上得分极低；  <br/>4. 为OLLMs安全性研究提供系统性分析框架和改进方向，推动该领域安全评估体系的建立。|
|2508.06457v1|[ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](http://arxiv.org/abs/2508.06457v1)|**贡献点：**<br/>1. 提出ScamAgent：首个基于LLM的自主多轮对话诈骗代理，能生成模拟真实场景的流言语音剧本。<br/>2. 引入动态欺骗机制：通过对话记忆和多轮交互适应用户行为，结合欺骗性说服策略提升诈骗成功率。<br/>3. 暴露安全漏洞：证明现有LLM安全措施（如拒绝机制、内容过滤）对多轮代理框架下攻击无效。<br/>4. 实现自动化流程：利用现代TTS技术将文本剧本转化为逼真语音通话，构建完整诈骗链。<br/>5. 强调新安全需求：呼吁建立多轮对话安全审计体系，开发针对生成式AI对话欺骗的检测与干预方法。<br/><br/>**总结：**  <br/>该研究揭示了基于LLM的多轮对话诈骗代理ScamAgent对现有安全机制的突破，提出了自动化语音欺诈生成框架，并强调需构建新的对话层面安全防护方案。|
|2508.06284v1|[Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment](http://arxiv.org/abs/2508.06284v1)|总结（100字以内）:  <br/>提出基于大语言模型的两阶段训练策略，构建大规模伪标注数据集LibriAugmented，验证其在多语言和语音降质场景下的泛化能力，为语音质量评估提供低成本、可扩展的解决方案。<br/><br/>贡献点分点列出：  <br/>1. **构建大规模伪标注数据集**：提出LibriAugmented数据集（101,129条语音片段），利用微调的听觉LLM（Vicuna-7b-v1.5）生成模拟降质标注，解决SQA数据稀缺问题。  <br/>2. **提出两阶段训练框架**：设计“LLM预训练 + 人类标注微调”的混合策略，显著提升模型在DNSMOS Pro和DeePMOS上的泛化性能（如PCC提升0.08-0.12）。  <br/>3. **验证LLM作为伪评分者可行性**：通过跨语言和多降质场景的实验，证明LLM可有效替代人工标注，降低标注成本并提升系统可扩展性。|
|2508.06194v1|[Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak   Evaluation](http://arxiv.org/abs/2508.06194v1)|总结（100字以内）:  <br/>本文提出SceneJailEval框架，通过场景自适应多维评估解决现有方法的局限，构建涵盖14场景的高质量数据集，并在多个数据集上取得SOTA性能，显著提升jailbreak评估的精准度与适用性。<br/><br/>贡献点:  <br/>1. **场景自适应多维框架**：突破现有"一刀切"评估方式，支持灵活适配不同场景，增强方法扩展性。  <br/>2. **全面场景数据集**：构建包含14个场景、多样jailbreak变体及区域案例的基准数据集，填补高质量场景适配评估的空白。  <br/>3. **SOTA评估性能**：在全场景数据集上F1达0.917（+6%），在JBB数据集上F1达0.995（+3%），超越现有方法在异构场景中的准确率上限。|
|2508.06167v1|[Pragmatics beyond humans: meaning, communication, and LLMs](http://arxiv.org/abs/2508.06167v1)|总结：  <br/>该论文提出语用学作为语言与社会互动的动态接口，挑战传统理论框架，引入"上下文挫败"概念，强调人类在生成AI沟通中的主体性，并主张通过调整语用理论来适应机器学习模型的语用分析需求。<br/><br/>贡献点：  <br/>1. **理论重构**：提出语用学应被视为语言作为社会工具运作的动态接口，否定其作为"意义的第三维度"的传统定位。  <br/>2. **框架创新**：提出"人类-机器通信（HMC）"框架，替代传统符号三分法，强调连接主义LLM架构对语义等级结构的削弱。  <br/>3. **方法论转向**：主张概率语用学（如Rational Speech Act框架）取代Gricean语用学，以优化目标替代真理评估，适应LLM的预测机制。  <br/>4. **批判替换主义**：揭示LLM评估中存在的人类中心偏见，区分通用、语言和沟通三类替换主义现象，强调人类在语用主体中的不可替代性。  <br/>5. **概念提出**：引入"上下文挫败"（context frustration），描述LLM在海量上下文输入下出现的语用理解崩溃现象，突显用户在模型训练中的能动性角色。|
|2508.05880v1|[Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large   Language Models](http://arxiv.org/abs/2508.05880v1)|**贡献点：**  <br/>1. **提出CoRE基准**：构建首个大规模情感认知推理基准（CoRE），用于隐式评估LLMs在情感推理任务中所依赖的内部认知结构。  <br/>2. **基于认知评价理论**：将认知评价理论引入情感计算领域，系统分析LLMs在处理情感刺激时是否能生成连贯、合理的认知推理。  <br/>3. **设计关键实验问题**：通过三组实验，探索模型对特定认知维度的隐含依赖、情感特征的维度划分，以及情感类别表征的可解释性。  <br/>4. **揭示模型差异**：发现不同LLMs在情感认知推理中表现出显著的模式差异，为理解情感计算的模型特性提供实证依据。  <br/>5. **公开资源**：提供CoRE基准及代码，促进该领域的研究与复现。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个情感认知推理基准CoRE，基于认知评价理论分析LLMs的情感处理机制，揭示模型间差异，并公开资源推动研究进展。|
|2508.05473v1|[Embedding Alignment in Code Generation for Audio](http://arxiv.org/abs/2508.05473v1)|贡献点（分点）:  <br/>1. **提出音乐创作中代码生成的LLM应用潜力**：通过LLM辅助，用户可专注于结构动机而非语法细节，提升创意编程效率。  <br/>2. **揭示代码与音频嵌入空间的复杂映射关系**：发现代码和音频嵌入不存在简单线性关联，需通过建模学习嵌入对齐。  <br/>3. **构建代码-音频嵌入对齐预测模型**：开发新模型实现代码到音频嵌入的映射，增强生成结果的多样性与可解释性。  <br/><br/>总结（100字以内）:  <br/>本研究通过分析代码与音频嵌入空间的拓扑关系，构建预测模型实现代码到音频的对齐，提升LLM在创意编程中的多样性与效果。|
|2508.05149v1|[Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages](http://arxiv.org/abs/2508.05149v1)|**贡献点总结：**  <br/>1. 提出SLAM-ASR框架，通过可训练轻量级投影器连接语音编码器与LLM，解决低资源语言ASR问题。  <br/>2. 系统评估训练数据量需求，验证在低数据条件下达到Whisper-only性能的挑战与可行性。  <br/>3. 证明单语或多语投影器（基于高资源语言预训练）可显著缓解数据稀缺问题，提升小数据集效果。  <br/>4. 在多语言LLM（EuroLLM、Salamandra）与Whisper-large-v3-turbo结合的基准测试中，提供低资源语言与多语言优化的研究方向参考。  <br/><br/>**（总结：研究提出SLAM-ASR框架，通过轻量级投影器连接语音编码器与LLM，评估低数据环境下模型性能，并验证多语言预训练对低资源ASR的优化潜力，为相关研究提供实验依据与方向参考。）**|
|2508.04904v1|[Root Cause Analysis Training for Healthcare Professionals With   AI-Powered Virtual Simulation: A Proof-of-Concept](http://arxiv.org/abs/2508.04904v1)|**总结（100字以内）：**  <br/>提出AI驱动的3D模拟游戏用于提升医疗人员RCA能力，通过自然交互与虚拟角色、AI反馈、情感语音等技术，提供低成本、可扩展的培训方案，并计划实证评估其效果。<br/><br/>---<br/><br/>**贡献点分点列出：**  <br/>1. **开发新型RCA培训工具**：首创AI-powered 3D模拟游戏，替代传统高资源消耗的培训模式，提高训练效率与实施一致性。  <br/>2. **沉浸式交互场景**：构建模拟ICU死亡事件的虚拟环境，支持学习者通过对话与五位虚拟医护人员角色互动，提升情境理解。  <br/>3. **AI核心技术集成**：融合大语言模型（LLM）、情感文本语音合成与动画技术，实现自然、生动的交互体验。  <br/>4. **多维度反馈机制**：引入LLM提供形成性与总结性反馈，助力学习者持续改进RCA分析能力。  <br/>5. **实证研究规划**：提出系统的实证评估计划，为验证其临床培训价值奠定实验基础。|
|2508.04749v1|[Bridging Brains and Models: MoE-Based Functional Lesions for Simulating   and Rehabilitating Aphasia](http://arxiv.org/abs/2508.04749v1)|**贡献点：**  <br/>1. 提出基于模块化MoE语言模型的失语症模拟方法论，通过选择性禁用语言模块实现病理特征建模。  <br/>2. 验证模拟的失语症亚型（如布罗卡/维尔尼克失语）输出与真实患者语音高度相似。  <br/>3. 首次展示通过冻结受损模块并重新训练健康模块，可实现语言功能的计算修复。  <br/>4. 揭示模块化LLM在模拟语言障碍机制及探索治疗路径中的临床适用性与计算潜力。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过模块化MoE模型模拟失语症，验证病理输出与真实患者数据的一致性，并实现语言功能的计算修复，为语言障碍机制研究及治疗探索提供新框架。|
|2508.04531v1|[Unveiling the Landscape of Clinical Depression Assessment: From   Behavioral Signatures to Psychiatric Reasoning](http://arxiv.org/abs/2508.04531v1)|总结：  <br/>本研究提出C-MIND多模态数据集，分析行为特征与诊断相关性，评估多模态整合有效性，探索LLM在临床诊断中的局限并提出改进方法，构建数据与算法双重基础设施以推动抑郁症研究的可靠性。<br/><br/>贡献点：  <br/>1. **构建首个临床验证的多模态数据集**：C-MIND是首个基于真实医院访问收集的神经精神病学多模态诊断数据集，包含音频、视频、文本和fNIRS信号，为研究提供高质量临床数据支持。  <br/>2. **系统分析多模态任务与行为特征的诊断贡献**：通过经典模型量化不同任务（如结构化心理测试）和模态（音频、视频等）对诊断性能的影响，揭示其组合有效性。  <br/>3. **揭示LLM在临床诊断中的局限性及改进路径**：验证大语言模型在 psychiatric 推理中的不足，并提出通过整合临床专业知识提升其诊断性能（Macro-F1 提升10%）。  <br/>4. **建立临床抑郁症评估的基础设施框架**：从数据和算法角度提出系统性解决方案，推动心理健康评估研究的实用化和可靠性。|
|2508.04353v1|[LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for   Learned Thematic Significance Tracking in Multimedia Content](http://arxiv.org/abs/2508.04353v1)|**贡献点总结（100字以内）**：  <br/>提出LUST框架，结合视觉与语音多模态分析，通过LLMs实现分层的两阶段主题相关性评分，输出时序感知的视频注释与分析日志，提升用户定义重要性内容的量化评估能力。<br/><br/>**分点贡献**：  <br/>1. **多模态整合**：首次将视频视觉信息与自动语音识别（ASR）提取的文本描述结合，构建统一分析流程。  <br/>2. **分层评分机制**：设计“直接相关性”与“上下文相关性”双阶段模型，通过LLMs动态评估内容与主题的关联。  <br/>3. **时序感知分析**：引入时间维度，利用历史评分优化当前段落评估，实现对叙事演变的深度理解。  <br/>4. **可视化输出**：提供带有主题相关性标注的视频及详细分析日志，增强结果可解释性与实用性。|
|2508.04350v1|[Chain of Questions: Guiding Multimodal Curiosity in Language Models](http://arxiv.org/abs/2508.04350v1)|**贡献点总结（100字以内）：**  <br/>本文提出Chain of Questions (CoQ)框架，通过好奇心驱动的动态问题生成机制，提升多模态语言模型对复杂环境的感知模态选择与整合能力，验证了其在多模态任务中的有效性、可解释性及推理对齐性。<br/><br/>**分点贡献：**  <br/>1. **提出新框架CoQ**：首次引入基于好奇心驱动的推理方法，引导多模态模型动态生成环境相关问题，从而选择性激活视觉、听觉等感官模态。  <br/>2. **构建多模态基准数据集**：将WebGPT、ScienceQA、AVSD、ScanQA等多模态任务数据整合为新型基准，支持框架评估与对比。  <br/>3. **提升模型性能**：实验表明CoQ显著增强基础模型在复杂多模态任务中识别关键信息的能力，优化准确率、可解释性及推理过程与任务的对齐性。|
|2508.04240v1|[ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and   Neural Decoding during Reading and Listening](http://arxiv.org/abs/2508.04240v1)|贡献点：  <br/>1. **填补非英语语言数据空白**：首个针对中文的多模态神经解码基准数据集，解决非英语语料缺失问题。  <br/>2. **扩展模态覆盖**：基于原静默阅读数据集，新增朗读（RA）和被动听觉（PL）模态，覆盖更全面的语言任务。  <br/>3. **多模态数据整合**：包含高密度EEG信号、精确音频及预训练语言模型的语义嵌入，实现跨模态对齐。  <br/>4. **创新实验设计**：通过同步记录朗读EEG与音频、播放录音收集听觉EEG，建立语音时序和语义的跨模态对齐。  <br/>5. **支持联合学习**：与中文EEG结合，推动跨说话、听觉、阅读的联合语义对齐训练，促进脑-LLM协同。  <br/>6. **提供未来研究基础**：为下一代神经语义解码技术提供中文语言系统的基准数据。  <br/><br/>总结：本研究提出中文EEG-2数据集，涵盖朗读、听觉和阅读多模态，整合EEG、音频及语义嵌入，支持跨模态语义对齐学习，为中文神经解码提供基准。|
|2508.04206v1|[ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal   Movie Recommendation](http://arxiv.org/abs/2508.04206v1)|**贡献点：**  <br/>1. 提出ViLLA-MMBench，首个针对LLM增强的可复现、可扩展多模态电影推荐基准。  <br/>2. 集成MovieLens与MMTF-14K数据，实现音频、视觉、文本三模态密集嵌入对齐。  <br/>3. 利用LLM自动补全缺失/稀疏元数据，生成高质量电影剧情摘要。  <br/>4. 支持多种可配置文本编码器（Ada, LLaMA-2, Sentence-T5）生成多组嵌入数据。  <br/>5. 提供灵活的多阶段融合策略（早、中、晚融合）与多种推荐模型消融实验。  <br/>6. 实验通过单YAML文件声明式配置，简化流程并提高可操作性。  <br/>7. 引入综合评估体系，涵盖准确率（Recall, nDCG）与冷启动、覆盖率、新颖性、多样性、公平性等非传统指标。  <br/>8. 验证LLM增强对冷启动和覆盖率的显著提升，尤其在音频-视觉融合场景下表现突出。  <br/>9. 通过系统性基准揭示通用性与模型/指标特异性组合的差异性。  <br/><br/>**总结：**  <br/>提出ViLLA-MMBench，整合多模态数据并支持自动补全和灵活融合，建立全面评估体系，验证LLM增强对推荐效果的提升，开源促进多模态推荐研究。|
|2508.03360v1|[CogBench: A Large Language Model Benchmark for Multilingual Speech-Based   Cognitive Impairment Assessment](http://arxiv.org/abs/2508.03360v1)|贡献点总结：<br/>CogBench是首个跨语言跨临床场景评估LLMs语音领域泛化能力的基准，提出统一多模态评估框架，揭示思维链提示对模型性能的影响，以及LoRA微调提升泛化性的有效方法。<br/><br/>分点贡献：<br/>1. 提出CogBench基准：首个系统评估大语言模型在跨语言（英语/中文）和跨临床场景下语音认知障碍评估能力的基准框架。<br/>2. 建立统一评估流程：构建覆盖ADReSSo、NCMMSC2021-AD及新数据集CIR-E的多模态评估体系，支持多语言数据对比。<br/>3. 揭示模型泛化特性：证明传统深度学习模型在跨域迁移时性能显著下降，而思维链提示增强的LLMs展现出更好的适应性。<br/>4. 提出LoRA优化方案：通过低秩适配技术实现轻量级微调，有效提升目标领域泛化能力，为临床应用提供新思路。|
|2508.02958v1|[VRSight: An AI-Driven Scene Description System to Improve Virtual   Reality Accessibility for Blind People](http://arxiv.org/abs/2508.02958v1)|总结：  <br/>本研究提出VRSight系统和DISCOVR数据集，通过AI技术实现VR场景的无障碍访问，使盲人用户可通过空间音频反馈参与VR交互，验证了其在社交任务中的有效性。<br/><br/>贡献点：  <br/>1. **提出VRSight系统**：首个端到端VR场景后处理系统，结合AI模型（目标检测、深度估计、LLM氛围解析）生成基于音调的空间音频反馈，无需开发者干预即可增强盲人用户的VR交互能力。  <br/>2. **构建DISCOVR数据集**：首个针对VR环境的虚拟物体数据集（含30类虚拟物体，来自17款社交VR应用），替代传统现实数据集，解决VR场景建模的适配性问题。  <br/>3. **实证效果验证**：通过九名盲人用户测试VRSight在Rec Room中的应用，证明其在社交任务（如avatar感知、座位识别）中的实际有效性。|
|2508.02232v1|[Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities   via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults](http://arxiv.org/abs/2508.02232v1)|总结（100字以内）:  <br/>本研究提出Eye2Recall系统，通过整合眼动追踪与自然语言交互实现混合启动回忆体验，填补LLM驱动回忆中眼动-语音交互的研究空白，并验证其在提升老年人正向老龄化中的有效性。<br/><br/>贡献点:  <br/>1. **提出混合交互模型**：首次将眼动追踪与语音交互结合，构建基于LLM的混合启动（mixed-initiative）回忆对话框架。  <br/>2. **识别老年人挑战**：通过专家访谈系统化梳理老年人在照片回忆技术使用中的核心痛点与需求。  <br/>3. **开发Eye2Recall系统**：设计并实现支持视觉兴趣检测与自然语言交互的互动式回忆系统。  <br/>4. **实验验证有效性**：通过十人用户研究验证系统对提升老年人回忆体验、幸福感及技术适配性的实际效果。|
|2508.02228v1|[Guiding an Automatic Speech Recognition Decoder Using Large Language   Models](http://arxiv.org/abs/2508.02228v1)|**贡献点分点总结：**  <br/>1. **提出可分离的ASR框架**：通过分解MAP估计器，实现声学模型（AM）和语言模型（LM）的解耦整合，允许两者独立训练与优化。  <br/>2. **实验验证广泛性**：对比N-gram、GCNN、TransformerLM三类语言模型，在ALLSSTAR、WSJ0、TED-LIUM 3等多语种、多风格数据集上验证方法有效性。  <br/>3. **提升复杂场景性能**：方法在处理复杂句子、缩略语及领域术语等挑战性任务中表现尤为突出，显著增强ASR的泛化能力。  <br/>4. **模型兼容性**：兼容主流声学模型（wav2vec 2.0、HuBERT）与大语言模型（GPT-2、LLaMA 2、Falcon），实现技术路线的灵活适配。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种解耦ASR框架，通过迭代方法整合声学与语言模型，实现独立优化，显著提升复杂场景下的语音识别性能。|
|2508.01659v2|[From Contrast to Commonality: Audio Commonality Captioning for Enhanced   Audio-Text Cross-modal Understanding in Multimodal LLMs](http://arxiv.org/abs/2508.01659v2)|**贡献点：**  <br/>1. **提出Audio Commonality Captioning (ACC)**：作为Audio Difference Captioning (ADC)的替代方案，通过引导模型捕捉音频片段的共性语义而非差异细节，解决ADC中因语义差距导致的灾难性遗忘问题。  <br/>2. **验证ACC的鲁棒性**：实验表明ACC在保持多模态大模型通用能力的同时，显著提升了音频-文本跨模态理解性能，尤其在语音和音乐任务中表现更优。  <br/>3. **平衡泛化与任务表现**：ACC在多模态语言模型的预训练和微调中实现更优的跨模态对齐，有效协调了模型在多样化任务中的泛化能力与任务特异性表现。  <br/><br/>**总结（100字以内）：**  <br/>提出Audio Commonality Captioning (ACC)方法，弥补音频差异字幕（ADC）的不足，解决语义差距和灾难性遗忘问题。实验验证ACC在提升音频-文本理解的同时保持通用能力，有效平衡多模态大模型的泛化与任务表现。|
|2508.01274v1|[Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question   Answering in Taiwan](http://arxiv.org/abs/2508.01274v1)|总结：  <br/>本文提出首个面向传统中文的三模态评估基准Multi-TW，系统对比闭源与开源模型性能，揭示端到端多模态处理在延迟上的优势，强调传统中文微调与高效架构的重要性。<br/><br/>---<br/><br/>贡献点：  <br/>1. **首个中文三模态基准**：构建Multi-TW，填补传统中文领域对视觉、听觉、文本三模态联合评估的空白。  <br/>2. **权威数据来源**：采用由SC-TOP官方开发的考试题，确保基准的规范性和实用性。  <br/>3. **跨模态性能对比**：评估多种any-to-any模型与视觉语言模型，明确闭源模型在跨模态任务中的优势。  <br/>4. **延迟优化分析**：量化端到端多模态模型与分离音频转录方案的延迟差异，为实际部署提供参考。  <br/>5. **研究方向启示**：突出传统中文微调和高效多模态架构的必要性，推动领域相关技术改进。|
|2508.00760v1|[MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese   Hate Speech Detection under Cloaking Perturbations](http://arxiv.org/abs/2508.00760v1)|总结：本文提出MMBERT框架，通过多模态整合和三阶段训练提升中文仇恨言论检测的鲁棒性，显著优于现有文本和语言模型方法。<br/><br/>贡献点：<br/>1. 首次针对中文社交网络特点，提出融合文本、语音、视觉多模态的仇恨言论检测框架MMBERT；<br/>2. 创新性地设计渐进式三阶段训练范式，解决多模态MoE架构整合中的稳定性问题；<br/>3. 引入模态特定专家模块与路由器分配策略，构建兼具模态特异性与跨模态协作的混合架构；<br/>4. 在中文多模态数据集上验证了模型有效性，显著超越传统BERT、LLMs及上下文学习方法。|
|2508.00632v1|[Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](http://arxiv.org/abs/2508.00632v1)|**贡献点总结：**  <br/>1. 提出 AVR-Eval，首个用于评估交互式音视频内容质量的相对指标，通过音频-视觉记录实现多模态对比。  <br/>2. 构建 AVR-Agent 系统，基于多模态资产库生成 JavaScript 代码，结合自动评估与多智能体协作优化内容。  <br/>3. 通过实验验证 AVR-Agent 生成内容在对抗单次生成内容时表现优异，揭示当前模型在利用自定义资产和反馈上的局限。  <br/>4. 指出人机创作本质差异：人类高效利用高质量资产和音视频反馈，而现有模型尚未充分实现这一能力。  <br/><br/>**摘要总结（100字以内）：**  <br/>本研究提出 AVR-Eval 评估指标与 AVR-Agent 系统，解决 AI 生成交互式音视频内容的质量评估与复杂生成难题，揭示人机创作差异，为多模态内容生成提供新方法。|
|2507.23088v1|[Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for   Interoperative Surgical Assistance](http://arxiv.org/abs/2507.23088v1)|总结：  <br/>本论文提出了一种结合语音控制与视觉分割技术的新型感知代理，通过创新分割机制和记忆库，显著提升手术机器人实时辅助的灵活性与自然交互能力，验证了其在动态手术环境中的有效性。<br/><br/>贡献点：  <br/>1. **提出新型感知代理（Perception Agent）**  <br/>   - 首次将语音集成的prompt-engineered LLM（大语言模型）与SAM（Segment Anything Model）及任意点跟踪模型结合，实现动态手术场景中的实时人机交互。  <br/><br/>2. **开发创新分割机制**  <br/>   - 引入两种新型机制，支持对已知和未知手术元素（如仪器、假体、纱布）的灵活分割，突破传统固定类别限制。  <br/><br/>3. **构建记忆仓库系统**  <br/>   - 设计可记忆新元素的机制，使系统具备跨手术场景的知识迁移能力，推动手术机器人向人机共生方向发展。  <br/><br/>4. **实验验证有效性**  <br/>   - 通过公开数据集定量分析与定制数据集定性评估，证明其性能可媲美传统高强度人工提示方案，且具备更自然的交互特性。|
|2507.22367v1|[Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided   LLM Representations and Multimodal Apparent Behaviors](http://arxiv.org/abs/2507.22367v1)|贡献点列表（分点说明）:<br/><br/>1. **提出新颖框架**  <br/>   设计"Traits Run Deep"人格评估框架，首次融合心理学引导的提示与多模态信号处理，突破传统方法对表面特征的依赖。<br/><br/>2. **创新文本中心融合机制**  <br/>   引入文本-中心人格融合网络，实现跨模态异步信号的对齐与整合，提升多模态信息协同表征能力。<br/><br/>3. **构建多模块处理系统**  <br/>   开发包含Chunk-Wise Projector（降维）、Cross-Modal Connector（跨模态连接）、Text Feature Enhancer（文本增强）和ensemble regression head（泛化回归头）的层级融合模块。<br/><br/>4. **人格提示的首次应用**  <br/>   首次将人格特性驱动的提示应用于大语言模型（LLM），显著提升人格感知语义的表征质量。<br/><br/>5. **跨模态行为特征融合**  <br/>   通过提取并融合音频-视觉显性行为特征，提升人格评估的准确性，实验验证MSE降低45%。<br/><br/>6. **行业竞赛验证有效性**  <br/>   在AVI Challenge 2025中取得Personality Assessment赛道第一名，证明方法优越性。<br/><br/>总结（100字以内）:  <br/>提出"Traits Run Deep"框架，创新融合心理学提示与多模态信号处理，构建文本中心融合网络，首次应用人格引导的LLM技术，在AVI Challenge 2025中取得首名，显著提升人格评估准确性。|
|2507.21411v3|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v3)|总结（100字以内）:  <br/>该研究提出通过物理对象交互实现数据讲故事的新方法，开发出InSituTale原型系统，并通过实证研究验证其直观性与实用性，为增强叙事体验提供了创新的物理-数字融合交互范式。  <br/><br/>**贡献点分点列出:**  <br/>1. **提出新交互范式**：首次引入“augmented physical data storytelling”，通过物理对象操作融合视觉化与现实环境，突破传统基于语音/手势的控制方式。  <br/>2. **构建交互映射体系**：通过调研和专家工作坊，系统化梳理常用可视化命令及其与物理操作的映射关系，为设计提供理论依据。  <br/>3. **开发原型系统**：构建InSituTale系统，结合深度摄像头物体追踪与Vision-LLM事件检测，实现动态可视化命令执行与沉浸式叙事。  <br/>4. **实证有效性验证**：通过用户研究验证系统在直观性、实用性和参与度方面的优势，证明物理-数字混合交互的可行性。|
|2507.21411v2|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v2)|总结：  <br/>提出一种结合物理对象交互与可视化技术的新型数据讲故事方法，并通过用户研究验证其交互直观性和实用价值。<br/><br/>贡献点：  <br/>1. **提出新范式**：引入“增强物理数据讲故事”（Augmented Physical Data Storytelling），首次将物理对象操作与数据可视化深度融合，拓展传统基于手势或语音的交互方式。  <br/>2. **需求驱动设计**：通过文献调查和专家工作坊，系统化梳理数据驱动展示中的可视化指令及物理操作映射规则，为后续系统设计提供理论依据。  <br/>3. **原型开发**：设计InSituTale原型系统，集成深度摄像头对象追踪与Vision-LLM事件检测技术，实现物理交互实时触发可视化操作。  <br/>4. **实证验证**：通过用户实验验证系统有效性，证明其能够支持连贯的数据叙事体验，并提升交互的直观性、实用性和沉浸感。|
|2507.21411v1|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v1)|总结（100字以内）：  <br/>该研究提出通过物理对象交互实现增强数据叙事的新方法，结合深度摄像头追踪与AI技术开发原型系统，并通过用户研究验证其直观性和有效性，推动了数据可视化与物理环境的融合应用。<br/><br/>贡献点：  <br/>1. **提出新范式**：创新性地引入“增强物理数据叙事”，将物理对象交互作为控制可视化的核心手段，突破传统依赖肢体动作或语音的交互方式。  <br/>2. **系统化研究方法**：通过调研与专家工作坊，系统归纳数据驱动展示的常用命令，并建立物理操作与命令的映射关系。  <br/>3. **原型开发**：设计InSituTale系统，整合物体追踪（深度摄像头）与Vision-LLM（多模态AI），实现现实事件的实时检测与可视化响应。  <br/>4. **验证有效性**：通过用户实验证明系统在交互直观性、实用性和沉浸式叙事体验方面的显著优势，为跨模态数据叙事提供实证支持。|
|2507.20924v1|[FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech   Concept Bottleneck Models](http://arxiv.org/abs/2507.20924v1)|总结：  <br/>本文针对社交媒体中的性别歧视识别提出三种模型（SCBM、SCBMT和XLM-RoBERTa），通过结合可解释性瓶颈概念与Transformer技术，提升分类性能，并探索元数据对任务的影响，实现多语言场景下的有效分析。<br/><br/>贡献点：  <br/>1. **提出SCBM模型**：利用描述性形容词作为人类可解释的瓶颈概念，通过大语言模型编码文本特征，构建轻量分类器解决性别歧视识别任务。  <br/>2. **SCBMT模型创新**：融合形容词表示与Transformer上下文嵌入，平衡模型可解释性与分类性能，提升细粒度解释能力。  <br/>3. **多语言模型优化**：基于XLM-RoBERTa的微调方法，结合数据增强技术，在英语和西班牙语任务中取得优异排名。  <br/>4. **元数据应用研究**：探索标注者人口统计信息等元数据对性别歧视分析的辅助作用，提升模型对源意图的识别。  <br/>5. **跨子任务方案**：为性别识别、源意图分析和分类三个子任务设计统一框架，实现多任务协同处理与性能评估。|
|2507.20730v1|[Vocalize: Lead Acquisition and User Engagement through Gamified Voice   Competitions](http://arxiv.org/abs/2507.20730v1)|**贡献点：**  <br/>1. 提出Vocalize系统：首个端到端的**游戏化语音竞赛平台**，专门用于提升用户参与度和潜在客户获取。  <br/>2. 技术融合：结合**音频处理技术**与**大型语言模型（LLMs）**，实现语音交互体验的创新设计。  <br/>3. 技术架构描述：从**技术实现角度**详细阐述了系统的设计与运作机制。  <br/>4. 实验验证：通过在**4个现场活动**中的部署，验证了系统在提升用户参与度的实际效果。  <br/>5. 应用前景拓展：指出Vocalize在**营销及品牌推广**等领域的广泛应用潜力，强调其对客户忠诚度与品牌认知的促进作用。  <br/><br/>**总结（100字以内）：**  <br/>论文提出Vocalize语音竞赛系统，整合音频处理与LLMs技术，验证其提升用户参与和营销效果，为游戏化音频应用在品牌推广中的落地提供了新思路。|
|2507.20241v2|[Reframe Your Life Story: Interactive Narrative Therapist and Innovative   Moment Assessment with Large Language Models](http://arxiv.org/abs/2507.20241v2)|总结：  <br/>提出INT和IMA双模块框架，突破传统LLMs在模拟专业叙事治疗和疗效评估的局限，通过创新时刻追踪提升治疗质量，推动AI心理支持技术的实用化。<br/><br/>贡献点：  <br/>1. **开发INT模块**：模拟专业叙事治疗师，具备治疗阶段规划、反思引导及上下文适配的专家级对话生成能力。  <br/>2. **构建IMA评估体系**：基于“创新时刻”（IMs）的治疗进展量化方法，精准捕捉客户言语中的关键叙事转变。  <br/>3. **实验验证有效性**：在260个模拟客户和230个真实参与者数据上证明INT优于标准LLMs的治疗质量与深度。  <br/>4. **推动实际应用**：展示INT生成的高质量支持对话可促进AI在心理健康的社交场景落地。|
|2507.20241v1|[Reframe Your Life Story: Interactive Narrative Therapist and Innovative   Moment Assessment with Large Language Models](http://arxiv.org/abs/2507.20241v1)|总结：  <br/>本研究提出了一种结合INT与IMA的综合框架，通过模拟叙事治疗师和创新时刻评估提升治疗效果，验证其在治疗质量和深度上的优势，并推动心理健康支持的社交应用。<br/><br/>贡献点：  <br/>1. **提出综合框架**：设计包含INT（互动叙事治疗师）和IMA（创新时刻评估）的双组件系统，解决现有LLM在模拟专业心理治疗和量化治疗进展方面的不足。  <br/>2. **INT的核心功能**：实现对叙事治疗师行为的模拟，包括规划治疗阶段、引导反思层次和生成情境化、专业化的回应。  <br/>3. **IMA的评估方法**：引入“创新时刻”（IMs）作为关键指标，通过跟踪客户言语中的叙事转变量化治疗有效性。  <br/>4. **实验证明有效性**：在260个模拟客户和230个真实参与者上验证INT的显著优势，展示其在治疗质量和深度上超越标准LLMs的能力。  <br/>5. **实际应用潜力**：证明INT在合成高质量支持对话中的可行性，为心理健康领域的实际应用提供技术支持。|
|2507.20220v1|[Motion-example-controlled Co-speech Gesture Generation Leveraging Large   Language Models](http://arxiv.org/abs/2507.20220v1)|**贡献点总结**（100字以内）:  <br/>MECo提出基于动词示例和LLMs的共语手势生成框架，解决传统方法细节丢失问题，支持多模态输入与身体部位精细控制，在FGD、多样性、相似性等指标上达到SOTA，并开源代码与模型。<br/><br/>---  <br/>**分点贡献**:<br/>1. **提出新型控制方法**：通过动词示例而非预定义标签或伪标签控制手势生成，保留原始运动细节。  <br/>2. **融合多模态理解**：利用大语言模型（LLMs）微调能力，同时解析语音音频和动词示例，实现语义一致性与特征保留。  <br/>3. **显式上下文引导**：将动词示例作为显式查询上下文嵌入提示结构，取代隐式伪标签范式，提升生成可控性。  <br/>4. **多输入模态支持**：兼容动画片段、静态姿势、视频序列和文本描述等多种输入形式，增强框架通用性。  <br/>5. **SOTA性能表现**：在Fréchet Gesture Distance、运动多样性及示例-手势相似性三项指标上达到当前最优水平。  <br/>6. **开源实现**：提供代码、预训练模型和演示视频，便于研究与应用。|
|2507.20169v1|[Self-Improvement for Audio Large Language Model using Unlabeled Speech](http://arxiv.org/abs/2507.20169v1)|总结（100字以内）:  <br/>提出SI-SDA自改进方法，通过大模型解码信息评估伪标签并结合强化学习优化，实现无监督领域自适应，显著提升语音任务性能（WER、BLEU）且具备高数据效率，适用于实际部署。<br/><br/>贡献点分点:<br/>1. **提出SI-SDA方法**：首次设计无需标注数据的自提升框架，解决音频LLM在特定目标域性能下降的问题，实现高效领域自适应。  <br/>2. **伪标签质量评估机制**：创新性利用大模型解码过程中的信息，动态评估生成的伪标签质量，提升训练可靠性。  <br/>3. **强化学习优化策略**：结合强化学习优化领域适应过程，实现在多个ASR/SQA/S2TT数据集上超越现有基线的性能表现，并验证其高数据效率。|
|2507.20091v2|[ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in   Speech Language Models](http://arxiv.org/abs/2507.20091v2)|总结：  <br/>本文提出ProsodyLM，通过改进语音转文本的tokenization方式，保留更多prosody信息，使模型在预训练中能自主学习多样的语音韵律处理能力，提升情感理解、重音识别及长文本韵律一致性等任务效果。<br/><br/>贡献点：  <br/>1. **创新tokenization方案**：设计词级prosody tokens，将语音转写文本与韵律特征结合，保留更完整的语音韵律信息。  <br/>2. **揭示传统方法局限**：指出现有基于离散token的语音语言模型无法有效学习韵律，缺乏预训练阶段的韵律处理能力。  <br/>3. **实现多样化韵律能力**：证明ProsodyLM通过预训练可自主发展出对比焦点、情感识别、重音理解及长上下文韵律一致性等新兴能力。|
|2507.20091v1|[ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in   Speech Language Models](http://arxiv.org/abs/2507.20091v1)|总结：  <br/>该研究提出ProsodyLM，通过改进的语音分词方案保留完整韵律信息，使文本模型更易理解语音特征，并验证其在预训练下能自主学习多种韵律处理能力，如情感识别、焦点对比和长文本韵律一致性维护。<br/><br/>贡献点：  <br/>1. 提出ProsodyLM框架：设计基于语音转录与词级韵律标记的分词方案，直接整合语音内容与韵律特征；  <br/>2. 保留完整韵律信息：较传统语音分词方法更有效捕捉语音的连续韵律细节；  <br/>3. 实现自主学习能力：验证模型通过预训练即可发展出对比焦点、情感理解等多维度韵律处理能力；  <br/>4. 提升语音理解效果：优化后的分词方案显著增强文本模型对语音韵律特征的建模与推理能力。|
|2507.19616v1|[HITSZ's End-To-End Speech Translation Systems Combining   Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language   Model for IWSLT 2025 in Indic Track](http://arxiv.org/abs/2507.19616v1)|**贡献点总结:**<br/>1. 提出首个基于Whisper和Krutrim的端到端语音-文本翻译系统，专为低资源英语-印地语语言对设计  <br/>2. 验证端到端系统在双向翻译中达到行业领先BLEU得分（28.88/27.86）  <br/>3. 探索链式推理（CoT）方法在翻译任务中的应用潜力，发现其对解析输出的显著提升（+13.84 BLEU）  <br/>4. 指出CoT方法在格式一致性上的实施挑战，为后续研究提供方向  <br/><br/>**100字内总结:**  <br/>本文提出基于Whisper和Krutrim的端到端ST系统，达成了英语-印地语双向翻译的高水平BLEU得分，并系统评估了CoT方法在提升翻译质量中的潜力与挑战，为低资源语音翻译领域提供了新思路与实践参考。|