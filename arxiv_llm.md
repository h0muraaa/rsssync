|Source|Title|Summary|
|---|---|---|
|2511.11108v1|[Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108v1)|**贡献点总结（100字以内）**  <br/>提出针对美总统辩论中个人攻击的分析框架，构建2016-2024年跨周期人工标注数据集，对比微调transformer模型与通用LLM的检测效果，证实任务适配可提升模型对政治语境中有害语言的理解能力。  <br/><br/>**分点贡献**  <br/>1. **提出分析框架**：设计用于分析政治辩论中个人攻击的系统性方法。  <br/>2. **构建标注数据集**：手动标注涵盖2016、2020、2024三届选举的辩论转录文本，为研究提供实证基础。  <br/>3. **模型性能对比**：评估微调的transformer模型与通用LLM在检测个人攻击任务中的效能差异。  <br/>4. **任务适配价值**：验证任务特定模型适配对理解政治传播中隐含意图的重要性。|
|2511.10287v1|[OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models](https://arxiv.org/abs/2511.10287v1)|总结：  <br/>本文提出首个全面覆盖多模态内容安全的测试平台OutSafe-Bench，创新性地设计MCRS指标与FairScore框架，系统评估九个MLLMs的安全性漏洞，推动安全基准的完善。<br/><br/>贡献点：  <br/>1. **首个多模态安全基准**：构建包含文本、图像、音频、视频的综合性测试集（18,000+中英文提示、4,500图像、450音频/视频），覆盖九类关键安全风险。  <br/>2. **多维交叉风险指标（MCRS）**：提出新量化方法，评估多模态内容间风险的重叠与关联性。  <br/>3. **FairScore框架**：设计可解释的多审稿人加权聚合系统，动态选择高表现模型作为陪审团，降低单模型偏见提升评估可靠性。  <br/>4. **实证分析**：系统检验九个先进MLLMs的安全性缺陷，揭示多模态模型在安全事实推理中的显著漏洞。|
|2511.10059v1|[When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?](https://arxiv.org/abs/2511.10059v1)|总结：  <br/>提出AV-ConfuseBench基准测试，揭示MLLMs在音频-视觉混淆场景中的短板，设计RL-CoMM模型通过引入音频参考模型和置信度优化策略显著提升任务准确性。<br/><br/>贡献点：  <br/>1. **提出新型基准AV-ConfuseBench**：模拟"音频-视觉混淆"场景，通过修改视频中对象的音频信号（如静音）评估MLLMs对不存在音频的识别能力。  <br/>2. **揭示MLLMs的视觉主导缺陷**：发现现有模型（如Qwen2.5-Omni、Gemini 2.5）易受视觉信息干扰，难以准确捕捉非存在的音频特征。  <br/>3. **设计RL-CoMM协作框架**：基于Qwen2.5-Omni构建多模型协作系统，引入外部音频语言模型生成音频-only推理，并通过Step-wise Reward函数优化跨模态推理。  <br/>4. **提出答案中心置信度优化**：通过降低多模型推理差异的不确定性，提升答案预测的准确性和一致性。  <br/>5. **验证显著性能提升**：在少样本条件下，RL-CoMM在音频-视觉问答与幻觉检测任务中实现10-30%的精度提升。|
|2511.10045v1|[Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism](https://arxiv.org/abs/2511.10045v1)|**贡献点总结：**  <br/>1. 提出语音象征作为研究MLLMs听觉信息处理的探针，填补跨学科研究空白。  <br/>2. 构建首个大规模仿词数据集LEX-ICON（含8,052词及2,930伪词，覆盖4种语言与25语义维度）。  <br/>3. 开发音素级注意力分数分析方法，揭示模型分层信息处理机制。  <br/>4. 发现MLLMs在多语义维度上的语音直觉与语言学理论一致。  <br/>5. 识别出模型对象形音素的显著注意力模式，增强可解释性。  <br/>6. 实现AI与认知语言学的首次大规模量化关联，推动领域交叉探索。  <br/><br/>（100字以内）  <br/>研究提出语音象征作为探针，构建LEX-ICON数据集，通过音素级注意力分析揭示MLLMs对听觉信息的处理机制，发现其与语言学理论的契合性，并首次实现AI与认知语言学的大规模量化连接。|
|2511.10011v1|[Reinforcing Trustworthiness in Multimodal Emotional Support Systems](https://arxiv.org/abs/2511.10011v1)|总结：  <br/>提出MultiMood框架，通过整合多模态数据与强化学习提升情感支持系统的可靠性与专业性，实现情感预测与响应生成的协同优化。<br/><br/>贡献点：  <br/>1. **多模态整合**：首次同时利用视频、音频、文本多模态嵌入，预测情感成分并生成符合专业治疗标准的响应。  <br/>2. **心理标准优化**：引入心理学评估指标，结合强化学习（RL）对大语言模型进行优化，增强输出一致性与可信度。  <br/>3. **模型评估分析**：系统分析多款先进LLM的多模态情感支持能力，为领域发展提供基准参考。  <br/>4. **实验验证**：在MESC和DFEW数据集上取得SOTA性能，RL驱动的信任度改进通过人机双评价验证有效性。|
|2511.09915v2|[HI-TransPA: Hearing Impairments Translation Personal Assistant](https://arxiv.org/abs/2511.09915v2)|**贡献点总结：**  <br/>1. **引入Omni-Model范式**：首次将Omni-Model应用于助听技术，开发指令驱动的音频-视觉个人助手HI-TransPA。  <br/>2. **多模态融合创新**：结合模糊语音与唇部动态，实现语音翻译与对话理解的统一框架。  <br/>3. **定制化预处理流程**：设计多模态数据预处理和筛选管道，检测面部关键点、稳定唇部区域并量化样本质量。  <br/>4. **课程学习策略**：利用质量评分引导渐进式训练，先强化高置信度样本再提升模型鲁棒性。  <br/>5. **高效编码架构**：提出统一的3D-Resampler，优化唇部动态编码以提升语义解析准确性。  <br/>6. **性能验证**：在HI-Dialogue数据集上取得SOTA结果，验证了字面准确性和语义保真度的突破。  <br/><br/>**100字内总结**：  <br/>HI-TransPA通过融合模糊语音与唇部动态，结合定制预处理、课程学习和3D编码技术，在助听对话场景中实现高效多模态理解，为Omni-Model在辅助通信中的应用提供了创新框架与关键工具。|
|2511.09915v1|[HI-TransPA: Hearing Impairments Translation Personal Assistant](https://arxiv.org/abs/2511.09915v1)|**贡献点总结**  <br/>1. **提出HI-TransPA**：开发指令驱动的音频-视觉多模态个人助听助手，整合模糊语音与高帧率唇部动态，实现翻译与对话功能。  <br/>2. **多模态数据融合**：通过融合语音和唇动信息，提升听障者日常交流的准确性与语义连贯性。  <br/>3. **预处理与质量评估**：构建针对听障数据的预处理流水线，包括面部关键点检测、唇部区域分离与稳定化，并引入量化数据质量评分系统。  <br/>4. **课程学习策略**：基于质量评分设计渐进式训练方案，先训练干净样本再逐步引入困难案例，增强模型鲁棒性。  <br/>5. **编码器优化**：结合SigLIP编码器与统一3D重采样器，高效编码高帧率唇动数据，提升模型效率。  <br/>6. **SOTA性能验证**：在自建的HI-Dialogue数据集上验证模型效果，取得字面准确性和语义保真度的最先进成果，并为未来研究提供框架与工具。  <br/><br/>**总结（100字以内）**：  <br/>本研究提出HI-TransPA，融合模糊语音与高帧率唇动，构建定制化预处理流程并引入课程学习策略，结合SigLIP与3D重采样器优化编码，实现听障交流的SOTA性能，为多模态辅助技术提供新框架与工具。|
|2511.09690v1|[Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages](https://arxiv.org/abs/2511.09690v1)|总结（100字以内）:  <br/>论文提出Omnilingual ASR，首个大规模可扩展语音识别系统，支持1600种语言（含500种新语言），通过7B参数自监督预训练实现零样本泛化，开源模型与工具降低研究门槛，强调社区协作与伦理考量。<br/><br/>贡献点分点：  <br/>1. **首次实现大规模语言可扩展性**：构建首个支持超1600种语言（含500种此前未被服务）的ASR系统，突破传统架构对语言覆盖的限制。  <br/>2. **高效低资源语言支持**：仅需少量数据样本即可引入未服务语言，显著降低语言扩展成本。  <br/>3. **参数规模与鲁棒性提升**：采用7B参数自监督预训练，学习更鲁棒的语音表示，增强模型适应未知语言的能力。  <br/>4. **创新编码器-解码器架构**：引入LLM启发的解码器，实现零样本泛化，提升跨语言迁移效果。  <br/>5. **多源数据融合训练**：整合公共资源与社区协作录制数据，通过本地合作伙伴补偿机制扩大数据多样性。  <br/>6. **模型家族部署灵活性**：发布300M、7B等多种模型变体，适配低功耗设备与高精度场景。  <br/>7. **伦理与社会影响考量**：强调社区协作与开源共享，降低技术门槛并推动语言平等，促进多方参与与可持续发展。|
|2511.09448v1|[MCAD: Multimodal Context-Aware Audio Description Generation For Soccer](https://arxiv.org/abs/2511.09448v1)|总结：本研究提出首个无需人工标注的跨领域视频音频描述生成系统MCAD，并开发新型评估指标ARGE-AD，验证其在电影与足球视频中的应用效果，同时提供了专业标注的足球数据集。<br/><br/>贡献点：<br/>1. 提出首个无需依赖人工标注音频描述（AD）的跨领域端到端生成系统MCAD，拓展至体育领域（如足球比赛）。<br/>2. 通过微调电影AD数据集训练的VideoLLM，使模型掌握AD的叙事结构与通用规则，适应新领域。<br/>3. 构建多模态上下文融合机制，结合球员身份、比赛事件、动作及实时解说生成完整AD文本。<br/>4. 开发新型评估指标ARGE-AD，从人名使用、动作事件提及、长度规范、代词避免、内容重叠等五维度量化AD质量。<br/>5. 首次验证ARGE-AD在跨领域（电影/体育）AD生成中的有效性，展现其泛化评估能力。<br/>6. 公布由两位AD专家标注的100段足球比赛视频的描述数据集，推动体育领域AD研究。|
|2511.09282v1|[End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering](https://arxiv.org/abs/2511.09282v1)|**主要贡献**  <br/>1. 提出 **CLSR**（Contrastive Language‑Speech Retriever），一种端到端对比学习检索模型，能够在长音频中高效提取与提问相关的片段。  <br/>2. 引入 **中间文本化步骤**：先将声学特征映射为类文本表征，再与语言表征对齐，显著缩小语音–文本模态差距，提升检索质量。  <br/>3. 在四个跨模态检索基准上实现 **显著优势**，超过现有端到端语音检索模型以及“语音识别+文本检索”流水线方法。  <br/>4. 为 **长篇语音问答（Long‑form SQA）** 提供可靠的前置检索模块，推动实际应用落地。<br/><br/>**100字以内总结**  <br/>本文提出端到端对比检索模型CLSR，通过将声学特征转化为类文本表征再进行对齐，显著提升长音频的相关片段检索效果，实验表明其在多数据集上优于现有语音检索和识别+检索流水线，为长篇语音问答提供了强大前置支持。|
|2511.08247v1|[ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247v1)|总结（100字以内）:  <br/>提出ParliaBench议会演讲生成基准，构建UK议会语料库，引入结合计算指标与LLM评估的多维框架，设计政治光谱与政党对齐新指标，验证微调模型在政治维度评估上的显著提升。<br/><br/>贡献点：  <br/>1. **提出议会演讲生成的特殊挑战**：强调政治真实性与意识形态一致性，区别于常规文本生成任务。  <br/>2. **构建专用数据集与基准**：基于UK议会数据创建ParliaBench，支持系统化模型训练与评估。  <br/>3. **创新评估框架**：结合计算指标（如语义连贯性）与LLM人工评估，从语言质量、语义连贯性、政治真实性三维度考核生成效果。  <br/>4. **设计新型政治指标**：提出基于嵌入的"政治光谱对齐"与"政党对齐"，量化意识形态定位。  <br/>5. **验证微调有效性**：通过5模型微调实验，证明其在多数指标及政治维度上的显著提升效果。|
|2511.07989v1|[State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989v1)|**贡献点总结：**  <br/>1. **评估模型跨语言表现**：首次系统比较BERT-like模型与LLMs在南斯拉夫语等低资源语言上的文本分类效果。  <br/>2. **多任务多领域验证**：覆盖情感分类、主题分类和体裁识别三项任务，涉及议会演讲、新闻文章及网络文本三个领域。  <br/>3. **零样本性能突破**：发现LLMs在零样本设置中表现优异，甚至超过传统微调模型，凸显其泛化能力。  <br/>4. **语言泛化能力分析**：证明LLMs在南斯拉夫语与英语中的表现一致性，为低资源语言应用提供参考。  <br/>5. **局限性揭示**：指出LLMs存在输出不可预测、推理速度慢和计算成本高的问题，强调微调模型在实际场景中的优势。  <br/><br/>**总结（100字以内）：**  <br/>本研究对比BERT-like模型与LLMs在南斯拉夫语等低资源语言的文本分类表现，发现LLMs零样本性能优越，但存在输出不可控、效率低等缺陷，表明微调模型在大规模文本标注中仍更具实用性。|
|2511.07821v1|[SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech](https://arxiv.org/abs/2511.07821v1)|总结（100字以内）:  <br/>本论文提出多语言合成语音指令数据集SYNTTS-COMMANDS，通过先进TTS模型与说话人嵌入解决KWS数据稀缺问题，验证合成数据在低功耗边缘设备上的有效性，为TinyML领域提供隐私、高效且可扩展的语音交互解决方案。<br/><br/>贡献点：<br/>1. **提出新型合成语音指令数据集**：构建首个基于TTS合成技术的多语言（英语&中文）语音指令数据集SYNTTS-COMMANDS，突破传统人工录制数据的瓶颈。<br/>2. **创新数据生成方法**：结合CosyVoice 2模型与公开语料库的说话人嵌入，实现高质量、可扩展的指令语音合成。<br/>3. **验证合成数据有效性**：通过多种高效声学模型测试，证明合成数据在指令识别任务中可达99.5%（英语）和98%（中文）的准确率，替代人工数据。<br/>4. **推动TinyML应用**：为资源受限的边缘设备提供隐私保护、低延迟、节能的语音交互训练基础，解决实际部署中的数据瓶颈问题。|
|2511.07477v1|[The Polite Liar: Epistemic Pathology in Language Models](https://arxiv.org/abs/2511.07477v1)|**贡献点：**  <br/>1. **提出“礼貌的骗子”概念**：揭示大语言模型通过RLHF训练产生的自信虚构行为，将其归因于结构性的认知冷漠而非主动欺骗。  <br/>2. **理论框架创新**：借鉴弗兰克福“bullshit”理论，区分结构冷漠与欺骗，指出RLHF奖励架构的偏差（追求真诚感而非证据准确性）。  <br/>3. **诊断对齐方法缺陷**：批判现有对齐策略（如帮助性、礼貌性）忽视知识基础，导致模型优先用户满意度而非真相。  <br/>4. **多视角分析行为**：结合认识论美德理论、言语行为哲学与认知对齐，揭示RLHF下模型模仿自信但缺乏认知依据的机制。  <br/>5. **提出“认识论对齐”原则**：主张以“有依据的自信”替代“感知流利度”作为对齐目标，解决语言合作与认知完整性的深层矛盾。  <br/><br/>**总结（100字以内）**：  <br/>论文指出RLHF导致模型成为“礼貌的骗子”，通过结构冷漠而非欺骗制造虚假自信，提出以认识论对齐替代流利度优先，解决AI对齐中认知与语言合作的矛盾。|
|2511.07392v2|[Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction](https://arxiv.org/abs/2511.07392v2)|总结：  <br/>本研究提出基于LLM的语音导向手术代理协调平台SAOP，结合分层多智能体框架与多级评估指标，实现手术场景中多模态数据处理的高效性与鲁棒性，为微创手术提供语音交互支持。<br/><br/>贡献点：  <br/>1. **提出SAOP平台架构**：设计语音导向的手术代理协调系统，通过分层多智能体框架整合任务规划、细化、验证与推理功能。  <br/>2. **部署LLM驱动的三任务代理**：构建三个任务专用代理（临床信息检索、CT扫描操作、3D模型导航），实现复杂语音指令到手术任务的映射。  <br/>3. **创新多级评估指标MOEM**：引入命令级与类别级双维度评估体系，全面衡量系统性能与对语音错误的鲁棒性。  <br/>4. **验证高鲁棒性与有效性**：在240条语音指令测试中达成高准确率与成功率，证明系统可应对多样、模糊的自由形式命令。  <br/>5. **推动微创手术场景应用**：展示语音交互技术在da Vinci手术中的潜力，为实时多模态数据处理提供智能化解决方案。|
|2511.07011v1|[Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011v1)|**贡献点：**  <br/>1. 提出利用纵向口语数据结合线性混合效应模型（LMM）和机器学习（ML）方法，探索抑郁症症状严重程度的可解释词汇特征。  <br/>2. 首次在多语言（英语、荷兰、西班牙）背景下分析语音特征与MDD症状关联，揭示不同语言中关键词汇特征的差异（如英语中词汇多样性，荷兰中句法结构，西班牙无显著关联）。  <br/>3. 评估了词汇特征与高维向量嵌入的预测性能，发现其在所有语言中均接近随机水平，凸显当前方法在临床应用中的局限性。  <br/>4. 指出非英语语音数据研究的不足（小样本、NL工具限制），强调多语言、跨文化研究的重要性。  <br/>5. 呼吁未来研究结合更大型多语言样本和改进ML协议，以捕捉个体内外语言变异，推动语音在临床诊断中的实际应用。  <br/><br/>**总结（100字以内）：**  <br/>本研究分析多语言纵向语音数据与MDD症状严重度的关系，发现不同语言中关键词汇特征差异，但预测性能有限，揭示方法与工具的局限性，为未来跨语言临床语音研究提供方向。|
|2511.06519v1|[On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception](https://arxiv.org/abs/2511.06519v1)|总结：  <br/>本研究揭示LLMs中存在类似人脑处理词性标签的神经元机制，通过分析Llama 3激活模式，发现关键神经元可预测词性并构建分类器，表明模型内部存在专门捕获语法概念的子空间。<br/><br/>贡献点：  <br/>1. **发现LLMs的神经元机制与人脑相似**：首次证实LLMs中不同词性标签由特定神经元处理，模拟人脑对语法范畴的神经分化。  <br/>2. **关键神经元识别**：基于Llama 3提出方法，定位与词性预测相关的最显著神经元，为模型内部表征研究提供新视角。  <br/>3. **构建可预测的分类器**：利用关键神经元激活模式训练分类器，实现对新数据的高准确词性预测，验证其潜在应用价值。  <br/>4. **揭示词性子空间结构**：证明LLMs存在专门捕获词性概念的子空间，与神经科学中的脑损伤研究模式一致，推动语言模型与认知科学的交叉理解。|
|2511.06483v1|[SAR-LM: Symbolic Audio Reasoning with Large Language Models](https://arxiv.org/abs/2511.06483v1)|贡献点总结：  <br/>1. 提出SAR-LM，首次将音频转化为结构化、可读的符号特征（语音、声音事件、音乐），突破密集嵌入的局限性。  <br/>2. 结合caption-based方法优势，通过符号化输入提升模型对结构化音频任务的推理能力。  <br/>3. 支持透明错误分析，可追溯模型失败原因至具体符号特征，增强可解释性。  <br/>4. 在MMAU、MMAR、OmniBench三个基准上表现优异，验证了符号化音频推理的有效性。  <br/><br/>（总结：SAR-LM通过结构化符号特征解决音频模型可解释性不足问题，显著提升推理能力和错误分析透明度，表现优于现有方法。）|
|2511.06441v1|[Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models](https://arxiv.org/abs/2511.06441v1)|**贡献点总结**：  <br/>1. 提出统一模块化框架，智能路由不同查询至最优模型，平衡成本与质量。  <br/>2. 针对视觉任务设计两阶段开源高效管道，结合经典模型与现代方法。  <br/>3. 在MMLU/VQA等基准中超越或匹配高端模型性能，降低67%对昂贵模型依赖。  <br/>4. 支持多代理协作扩展，实现可扩展、资源高效的AI系统。  <br/><br/>**总结（100字以内）**：  <br/>本文提出模块化框架与智能路由网络，结合高效开源模型与经典方法，显著降低对昂贵模型的依赖，在多项基准测试中匹配高端模型性能，实现高性价比、可扩展的多模态AI系统。|
|2511.04914v3|[MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages](https://arxiv.org/abs/2511.04914v3)|**论文贡献（分点列出）**  <br/>1. **提出 MERaLiON‑SER**：面向英语及东南亚语系的强韧语音情感识别模型。  <br/>2. **混合训练目标**：将加权分类交叉熵（离散情感）与一致相关系数（CCC）损失（维度情感）联合优化，实现对情感类别和细粒度维度（唤醒、效价、支配）的双重建模。  <br/>3. **跨语言实验验证**：在新加坡多语种（English、Chinese、Malay、Tamil）以及公开情感基准上进行大规模评估，持续超越现有开源语音编码器和大型 Audio‑LLM。  <br/>4. **强调专用语音‑Only 模型的重要性**：证明针对语音的专属模型在副语言信息捕获与跨语言泛化方面优于通用多模态模型。  <br/>5. **提供情感感知框架**：为未来具备情感感知的交互式音频系统（agentic audio agents）奠定基础，促进更具共情性与情境自适应的多模态推理。<br/><br/>**100字以内总结**  <br/>MERaLiON‑SER 通过加权交叉熵＋CCC 双目标实现离散与维度情感双建模，且在英语、华语、马来语、泰米尔语等多语言测试中超越现有语音编码器和 Audio‑LLM，证实专用语音模型在情感识别和跨语言迁移上的优势，为情感感知的智能音频系统提供了可靠基石。|
|2511.01670v1|[SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670v1)|**论文主要贡献点**  <br/><br/>1. **首个面向东南亚语言的 LALM**：提出 SeaLLMs‑Audio，支持印尼语 (id)、泰语 (th)、越南语 (vi)、英语 (en) 与中文 (zh) 五种语言。  <br/>2. **多模态输入**：模型可接受纯音频、纯文本以及音频+文本的组合输入，实现灵活的音文交互。  <br/>3. **多任务能力**：覆盖音频理解与交互任务，包括 Audio Captioning、ASR、语音‑文本翻译、情感识别、语音问答、语音摘要等；还能进行基于语音的事实、数学与常识问答。  <br/>4. **专属基准 SeaBench‑Audio**：构建面向东南亚语言的多任务音频评测套件，实现 LALM 自动化评估。  <br/>5. **竞争性能**：在实验中，SeaLLMs‑Audio 在东南亚语言上的表现与现有 LALM 相当，证明其在该地区的可用性。  <br/>6. **促进地区生态**：为东南亚学术界和产业提供开源/可商用的音频大模型与评测资源，推动区域研究与应用落地。  <br/><br/>**简要总结（≤100字）**  <br/>提出首个支持印尼语、泰语、越南语等五语种的海量音频语言模型 SeaLLMs‑Audio，具备音频/文本/音文融合多模态输入，覆盖音频标注、ASR、翻译、情感识别、问答等多任务，并推出对应评测套件 SeaBench‑Audio，实验表明在东南亚语言上达至与其他 LALM 竞争的水平，推动地区研究与产业应用。|
|2511.01299v1|[Towards General Auditory Intelligence: Large Multimodal Models for Machine Listening and Speaking](https://arxiv.org/abs/2511.01299v1)|**主要贡献点**  <br/>1. 梳理并系统化了音频与大语言模型（LLM）结合的最新研究，划分为四大主题：音频理解、音频生成、基于语音的交互、音频‑视觉理解。  <br/>2. 阐明 LLM 在音频感知与推理中的作用，展示其如何提升对声音的语义、情感及情境层面的深度理解。  <br/>3. 探讨生成式音频技术在实现富表达、自然逼真声音输出方面的进展与方法。  <br/>4. 分析基于语音的人机交互框架，说明 LLM 如何实现更自然的对话式交互与指令执行。  <br/>5. 研究音频‑视觉跨模态融合，展示其在情境感知、跨模态推理以及多模态智能体中的潜力。  <br/>6. 归纳当前面临的关键挑战（如数据稀缺、对齐效率、实时性、安全隐私等）并提出未来研究方向，指向构建“音频原生”AGI 的路线图。<br/><br/>**100字以内总结**  <br/>本文综述了音频在大语言模型中的融合进展，系统阐述了音频理解、生成、语音交互及音视跨模态四大方向，分析了 LLM 如何提升声学语义推理与自然生成，并提出实现音频原生 AGI 的挑战与发展路径。|
|2510.20850v1|[Can large audio language models understand child stuttering speech? speech summarization, and source separation](https://arxiv.org/abs/2510.20850v1)|评估最新音频语言模型在儿童口语（含卡顿）中的源分离与仅儿童摘要任务，结合LLM评判、人类评分与BERTScore，揭示模型在混合音频下的可信度及失效情形，并提供提示与复现脚本，为临床教育应用提供指导。|
|2510.19670v2|[CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670v2)|总结（100字以内）:  <br/>CoSense-LLM提出边缘优先框架，将多模态传感流转化为可验证语义标记，结合LLM实现低延迟、隐私保护与高可靠性，在居家、办公等场景中实现高效部署与动态策略选择。<br/><br/>贡献点：  <br/>1. **多模态传感压缩**：SenseFusion将Wi-Fi CSI、IMU等传感器数据压缩为离散语义标记，实现跨模态对齐与语言压缩。  <br/>2. **本地混合检索（Edge-RAG）**：结合本地政策与场景知识，在边缘设备实现事实一致性的生成响应。  <br/>3. **动态生成策略（PromptRouter）**：根据成本与不确定性选择边缘生成、本地检索或云端升级，优化资源分配。  <br/>4. **数据最小化安全机制（Secure Execution）**：通过审计路径确保数据不离开设备，仅传输离散代码与脱敏元数据。  <br/>5. **边缘优化与泛化支持**：集成KV缓存、量化LoRA等技术，支持设备个性化与非IID数据下的联邦更新。  <br/>6. **性能验证**：在多场景实现亚秒级延迟、降低跨层级通信成本，并通过消融实验验证各模块的可靠性与隐私保护效果。|
|2509.20641v1|[Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641v1)|**本文的主要贡献**  <br/>1. **首次将 MM‑SHAP 引入 Audio LLM**：提供基于 Shapley 值的模态贡献度量方法。  <br/>2. **提出性能无关的模态贡献评估框架**：可量化音频与文本在模型预测中的相对作用。  <br/>3. **在 MuChoMusic 基准上进行系统实验**：对两种 Audio LLM 进行比较，发现更高准确率的模型更依赖文本信息。  <br/>4. **局部化分析揭示音频信息的实际作用**：即使整体音频贡献低，模型仍能成功定位关键声事件，表明音频并未被完全忽视。  <br/>5. **为音频大语言模型的可解释性提供基准和方法论**：为后续 Explainable AI 与音频研究奠定基础。  <br/><br/>**总结（100字以内）**  <br/>本文首次将 MM‑SHAP 用于 Audio LLM，量化音频与文本对预测的贡献，实验证明高精度模型更依赖文本，但仍能准确定位关键声事件，为音频大模型的可解释性研究奠定基础。|