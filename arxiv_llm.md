|Source|Title|Summary|
|---|---|---|
|2506.02742v1|[Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions](http://arxiv.org/abs/2506.02742v1)|总结（100字以内）:  <br/>本研究提出PUE方法，通过情感引导的提示学习实现零样本下的未见过情绪语音生成，利用LLM-TTS架构保持情绪一致性，并支持灵活调整情感比例以合成多风格混合情感语音。<br/><br/>贡献点:  <br/>1. **提出新型PUE框架**：首次设计Prompt-Unseen-Emotion（PUE）方法，突破传统情感TTS系统对固定分类情绪的依赖，支持生成任意未见过的情绪语音。  <br/>2. **情感一致性建模**：通过LLM-TTS架构将类别情感提示与语音输出进行对齐，确保生成语音与提示情绪在语义和情感特征上一致。  <br/>3. **情绪权重量化**：在训练阶段实现对每句话中不同情绪权重的量化建模，提升生成语音的情感表达的细腻度。  <br/>4. **混合情感合成能力**：在推理阶段支持通过动态调整情绪比例生成混合情感语音，扩展情感表达的多样性。  <br/>5. **零样本有效性验证**：在无需情绪训练数据的零样本场景下成功生成具有表达性的目标情绪语音，验证方法的泛化能力。|
|2506.02457v1|[SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant](http://arxiv.org/abs/2506.02457v1)|总结：  <br/>提出SOVA-Bench基准框架，系统评估语音LLMs的语义与声学生成能力，填补声学质量量化评估空白，推动语音交互系统发展方向。<br/><br/>贡献点：  <br/>1. 提出SOVA-Bench：首个系统性评估语音LLMs的基准框架，整合多维度能力测试。  <br/>2. 填补声学质量量化空白：首次对生成语音的声学特性进行量化评估，突破以往仅关注语义准确性的局限。  <br/>3. 综合多能力评估：同时衡量语音理解、语音识别、语义生成和声学生成能力，提供全面对比分析。  <br/>4. 指导技术发展：为语音交互系统的优化方向提供理论依据与实践参考，促进更自然的语音生成研究。|
|2506.00160v1|[Werewolf: A Straightforward Game Framework with TTS for Improved User   Engagement](http://arxiv.org/abs/2506.00160v1)|贡献点：  <br/>1. 提出基于LLM的新型Werewolf社会推理游戏系统，融合文本生成与语音交互技术，增强游戏沉浸感。  <br/>2. 设计调优后的文本到语音（TTS）模型，提升与多种LLM的兼容性，降低适配成本。  <br/>3. 通过简化系统结构（无需额外组件），实现更高效的用户参与度提升，反驳传统依赖微调或经验池的方案。  <br/>4. 强调LLM推理能力的持续提升将减少对辅助技术（如复杂提示工程）的依赖，指向未来研究方向。  <br/><br/>总结：  <br/>本文提出一种结合TTS与LLM的新型社会推理游戏系统，通过调优提升兼容性并简化架构，有效增强用户体验，同时指出LLM能力进步将减少对额外技术的依赖。|
|2505.22251v2|[Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in   Large Language Models for Speech Recognition](http://arxiv.org/abs/2505.22251v2)|摘要贡献点：  <br/>1. 揭示LibriSpeech和Common Voice数据集与公开LLM预训练语料存在显著重叠，质疑现有语音任务评估结果的可靠性。  <br/>2. 提出通过对比含/不含数据污染的LLM评估污染影响的方法，验证污染数据的存在性及对模型表现的影响。  <br/>3. 发现污染LLM的语音识别器虽在错误率上差异微小，但会显著提升对训练数据的转录概率，反映输出偏差。  <br/>4. 强调需使用独立数据评估LLM语音系统，以避免因数据污染导致的不准确结果。  <br/><br/>总结：  <br/>该研究揭露语音任务评估数据与LLM训练数据重叠问题，揭示污染对模型性能的潜在影响，并呼吁使用独立数据验证模型效果。|
|2505.24347v2|[Fewer Hallucinations, More Verification: A Three-Stage LLM-Based   Framework for ASR Error Correction](http://arxiv.org/abs/2505.24347v2)|总结:  <br/>本文提出RLLM-CF框架，通过错误预检测、迭代修正和推理验证三阶段解决LLM在语音识别中的hallucinations问题，无需额外数据或微调，实验证明在多个数据集上显著降低CER/WER。<br/><br/>贡献点:  <br/>1. **提出RLLM-CF框架**：设计包含错误预检测、链式思维子任务迭代修正和推理过程验证的三阶段校正流程，系统性解决LLM在ASR中的错误修正问题。  <br/>2. **无额外训练需求**：方法无需额外标注数据或模型微调，直接利用LLM能力进行端到端校正，降低应用门槛。  <br/>3. **抑制hallucinations**：通过多阶段验证机制有效避免LLM误改正确文本，保障修正结果的准确性。  <br/>4. **实验证明有效性**：在AISHELL-1、AISHELL-2和Librispeech数据集上验证，显示GPT-4o模型结合该框架后CER/WER分别降低21%/11%/9%/11.4%，具有实际应用价值。|
|2506.04711v1|[LLM-based phoneme-to-grapheme for phoneme-based speech recognition](http://arxiv.org/abs/2506.04711v1)|**贡献点：**<br/>1. **提出LLM-P2G解码框架**：首次将大语言模型（LLMs）引入基于音素的ASR系统，结合语音到音素（S2P）和音素到字符（P2G）的分步解码流程。  <br/>2. **解决级联信息损失问题**：针对S2P与P2G联用时的潜在信息损失，设计了两种训练策略：带噪声音素的数据增强（DANP）和随机化top-K边缘化训练与解码（TKM）。  <br/>3. **跨语言性能提升**：实验验证在波兰语和德语的跨语言ASR任务中，LLM-P2G相比传统WFST解码方法分别降低WER 3.6%和6.9%。  <br/>4. **简化解码流程**：通过LLMs替代WFST，减少了解码的复杂性，同时提升对多语言数据的适应能力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出LLM-P2G解码方法，结合S2P与P2G流程并引入数据增强和top-K边缘化策略，有效缓解信息损失问题，在波兰语和德语跨语言ASR中显著提升识别性能。|
|2505.18614v2|[MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song   Translation](http://arxiv.org/abs/2505.18614v2)|贡献点：  <br/>1. **提出首个多语言多模态基准**：构建了Multilingual Audio-Video Lyrics Benchmark (MAVL)，首次整合文本、音频与视频数据，为动画歌曲翻译提供综合评估标准。  <br/>2. **多模态数据增强翻译质量**：通过融合音频和视频信息，使翻译更贴近原作的旋律、节奏及风格，突破传统文本仅依赖语义的局限。  <br/>3. **创新音节约束模型结构**：提出SylAVL-CoT模型，结合链式推理（Chain-of-Thought）与音节约束机制，提升歌词的自然度与可唱性。  <br/>4. **验证多模态方法有效性**：实验表明该模型在可唱性和上下文准确性上显著优于文本基础模型，证明多模态、多语言框架对歌词翻译的价值。  <br/><br/>总结（100字以内）：  <br/>本研究提出多语言多模态基准MAVL与SylAVL-CoT模型，融合文本、音频和视频数据，通过音节约束提升歌词翻译的自然度和可唱性，验证了多模态方法在动画歌曲翻译中的优势。|
|2506.04586v1|[LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech   Foundational Models](http://arxiv.org/abs/2506.04586v1)|总结:  <br/>提出LESS框架，通过LLM优化伪标签并结合数据过滤策略提升语音任务性能，验证其跨语言和任务的适应性，为语音处理提供新方法。<br/><br/>贡献点:  <br/>1. **提出LESS框架**：首个结合大语言模型与半监督学习的语音处理框架，用于修正野外数据生成的伪标签。  <br/>2. **LLM伪标签修正**：利用LLM提升ASR/AST任务中伪标签的质量，显著降低词错误率（WER）。  <br/>3. **数据过滤策略**：设计数据增强机制优化LLM知识迁移效率，提升模型训练效果。  <br/>4. **跨语言/任务验证**：在中文ASR和西班牙语-英语AST任务中均取得显著性能提升，证明框架的通用性。  <br/>5. **消融研究分析**：通过不同LLM和提示配置的实验，揭示LLM衍生知识在语音处理中的关键作用与优化方向。|
|2506.05209v1|[The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly   Licensed Text](http://arxiv.org/abs/2506.05209v1)|**贡献点:**<br/>1. 提出Common Pile v0.1，首个面向LLM预训练的8TB开放授权文本数据集（覆盖30个来源，包括论文、代码、书籍等）；<br/>2. 通过训练7B参数模型验证数据集有效性，性能与基于未授权文本的LLM（如Llama 1/2 7B）相当；<br/>3. 释放数据集构建代码、训练混合策略及模型检查点，提升研究透明度与可复现性。<br/><br/>**总结（100字以内）:**  <br/>本文构建首个8TB开放授权文本数据集Common Pile v0.1，涵盖多领域内容。基于其训练的7B参数模型性能媲美未授权文本训练的LLM，并开源全流程代码和训练资源，推动合规模型研究发展。|
|2506.05191v1|[MokA: Multimodal Low-Rank Adaptation for MLLMs](http://arxiv.org/abs/2506.05191v1)|总结：  <br/>该论文提出MokA方法，通过结合单模态适配与跨模态交互，解决现有多模态微调方法忽视模态差异的问题，实验证明其有效性与普适性，为高效多模态模型适配提供新思路。<br/><br/>贡献点：  <br/>1. **揭示现有方法的局限性**：指出当前高效多模态微调方法直接套用LLM策略，忽视多模态场景的内在差异，影响模态协同利用。  <br/>2. **提出双适应理论框架**：论证单模态适配与跨模态适配是多模态模型微调的两个核心组成部分，强调两者的共同作用。  <br/>3. **设计MokA方法**：提出一种多模态感知的低秩微调策略，通过模态特异性参数压缩单模态信息，显式增强跨模态交互。  <br/>4. **多场景验证有效性**：在音频-视觉-文本、视觉-文本、语音-文本三种典型场景及多个大模型架构（如LLaMA2/3、Qwen2等）中验证方法的通用性与性能提升。  <br/>5. **系统评估方法优势**：通过消融实验与效率分析，全面验证MokA在参数压缩与跨模态增强上的效果。  <br/>6. **推动多模态研究**：认为MokA为多模态大模型高效适配提供更精准的解决方案，为后续研究奠定基础。|
|2506.05062v1|[Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation](http://arxiv.org/abs/2506.05062v1)|总结：  <br/>提出Debate Speech Evaluation新基准，系统分析LLM与人类在多维度辩论评估中的表现，揭示模型判断行为差异并评估生成能力，为LLM判决研究提供关键洞察。<br/><br/>贡献点：  <br/>1. **提出新基准**：构建首个用于评估大语言模型辩论判断能力的基准任务，填补LLM系统性基准领域的空白。  <br/>2. **多维评估框架**：提出综合考察论点强度、相关性、连贯性、风格适配等多层面的评价标准，系统性分析LLM的判断能力。  <br/>3. **大规模标注数据集**：利用包含600+条精细标注的辩论演讲数据集，首次实现对LLM在辩论任务上的大规模实验与验证。  <br/>4. **模型行为分析**：揭示大型模型在个体判断与整体行为层面与人类法官的显著差异，为LLM的局限性研究提供实证依据。  <br/>5. **生成能力评估**：验证前沿LLM生成具有说服力和观点性演讲的能力，证明其在特定任务上可能达到人类水平。|
|2505.20445v3|[In-context Language Learning for Endangered Languages in Speech   Recognition](http://arxiv.org/abs/2505.20445v3)|**总结（100字以内）:**  <br/>本文探索LLM通过上下文学习在低资源语言语音识别中的应用，证实相关文本样本能提升性能，概率方法优于传统指令方法，并展示ICL可使LLM达到或超越专用语言模型的ASR效果，同时保留原有能力。<br/><br/>**贡献点:**  <br/>1. **验证ICL在低资源语音识别中的可行性**：首次将上下文学习方法应用于语音识别领域，证明LLM能在未训练的低资源语言上实现有效学习。  <br/>2. **提出文本样本增强策略**：发现提供与任务更相关的文本样本能显著提升语言建模和ASR性能，为多语言模型优化提供新方向。  <br/>3. **对比方法效果与模型性能**：表明概率方法优于传统指令方法，且ICL使LLM在ASR任务中达到专用语言模型水平，同时保持其通用能力。|
|2506.04693v1|[Cracking the Code: Enhancing Implicit Hate Speech Detection through   Coding Classification](http://arxiv.org/abs/2506.04693v1)|总结：  <br/>提出新的隐性仇恨言论分类体系（codetypes），设计两种LLMs检测方法，验证其在中英文数据集上的有效性。<br/><br/>贡献点：  <br/>1. **构建隐性仇恨言论新分类框架**：首次提出六种编码策略（codetypes），为im-HS检测提供系统化的分类依据。  <br/>2. **创新性方法设计**：提出两种基于大语言模型的整合策略——直接提示分类与编码器嵌入codetypes，解决隐性HS检测挑战。  <br/>3. **跨语言有效性验证**：通过中英文数据集的实验结果，证明所提方法在不同语言环境下的普遍适用性与检测性能提升。|
|2506.04043v1|[Think Like a Person Before Responding: A Multi-Faceted Evaluation of   Persona-Guided LLMs for Countering Hate](http://arxiv.org/abs/2506.04043v1)|**贡献点（分点）:**  <br/>1. 提出首个针对大语言模型（LLM）生成的反叙事（CN）的多维度评估框架，涵盖角色框架、冗长性与可读性、情感基调及伦理稳健性。  <br/>2. 系统测试三种提示策略在MT-Conan和HatEval数据集上的表现，对比GPT-4o-Mini、CommandR-7B和LLaMA 3.1-70B等主流模型的生成效果。  <br/>3. 首次揭示LLM生成的CN存在可访问性不足的问题（如冗长、适配大学以上学历），限制其实际应用效果。  <br/>4. 量化分析情感引导提示策略在提升CN同理心与可读性方面的优势，同时指出其潜在安全与伦理风险。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出LLM生成反叙事的评估框架，系统分析三种提示策略及多模型表现，发现其冗长性与可访问性不足，情感引导策略能提升可读性但存在安全风险，为优化反仇恨言论技术提供关键见解。|
|2506.03099v1|[TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1)|**贡献点：**<br/><br/>1. **模型转化**：将预训练的SOTA图像-视频生成模型（DiT）改造为音频驱动的高参数（180亿）虚拟形象生成模型，实现实时对话动画。<br/>2. **无误差流技术**：通过双向教师模型向稀疏因果自回归学生模型的异步知识蒸馏，解决无限视频流中的误差累积问题。<br/>3. **高效推理优化**：设计高吞吐量、低延迟的推理管道，包含以下工程创新：  <br/>   - 分布式计算（DiT和VAE解码器分设设备）  <br/>   - CUDA流技术实现设备间通信与计算重叠  <br/>   - 消除冗余计算提升帧生成效率  <br/><br/>**总结（100字内）：**  <br/>本文提出TalkingMachines框架，将预训练视频生成模型转化为音频驱动的实时角色动画系统，通过模型优化与分布式推理技术，实现无误差无限视频流和高效生成性能。|
|2506.03009v1|[Conditioning Large Language Models on Legal Systems? Detecting   Punishable Hate Speech](http://arxiv.org/abs/2506.03009v1)|总结：  <br/>本文探讨了如何通过不同抽象层次的法律知识对齐LLMs，分析其在仇恨言论检测任务中的表现，揭示模型与法律专家在法律评估能力上的显著差距，并探讨抽象与具体法律知识对模型性能的不同影响。<br/><br/>贡献点：  <br/>1. 提出并研究了LLMs在法律系统不同抽象层级（宪法、法规、判例）的对齐方法，探索其法律问题评估能力。  <br/>2. 聚焦德国刑法框架下的煽动仇恨行为分类任务，构建具体应用案例分析。  <br/>3. 揭示LLMs在法律评估中的性能瓶颈：抽象法律知识模型缺乏任务理解，易出现矛盾与幻觉；具体法律知识模型虽能识别目标群体，但分类行为特征存在困难。  <br/>4. 为法律与AI交叉领域提供实证依据，指出模型与法律专家间的核心差距，并启发更精准的法律知识融入策略。|
|2506.02758v1|[Exploiting the English Vocabulary Profile for L2 word-level vocabulary   assessment with LLMs](http://arxiv.org/abs/2506.02758v1)|**总结（100字以内）：**  <br/>本研究提出结合大语言模型与英语词汇档案（EVP）的新型方法，实现对二语学习者写作中词汇使用的细粒度评估，解决多义性与上下文变化等挑战，并验证了其在词级与作文级熟练度相关性分析中的有效性。  <br/><br/>---<br/><br/>**贡献点：**  <br/>1. **提出新方法**：首次将大型语言模型（LLMs）与英语词汇档案（EVP）结合，实现基于句子语境的细粒度词汇评估。  <br/>2. **解决复杂问题**：有效应对二语词汇中的多义性（polysemy）、上下文差异（contextual variation）及多词表达（multi-word expressions）等评估难题。  <br/>3. **对比基准模型**：通过对比传统词性（PoS）基线，证明LLMs能利用更丰富的语义信息，提升词汇评分准确性。  <br/>4. **探索相关性**：首次分析词级语言能力与作文整体水平之间的关联，揭示词汇使用的层级性特征。  <br/>5. **验证EVP一致性**：应用该方法重新检验EVP的等级划分合理性，证实LLMs在词汇评估任务中的适用性与可靠性。|
|2504.08961v2|[A Fully Automated Pipeline for Conversational Discourse Annotation: Tree   Scheme Generation and Labeling with Large Language Models](http://arxiv.org/abs/2504.08961v2)|**贡献点总结：**  <br/>1. 提出基于LLM的全自动决策树构建与标注流水线，替代传统人工设计流程。  <br/>2. 首次将频率引导的决策树与LLM结合，提升语音功能标注性能。  <br/>3. 通过实验验证不同设计选择的效果，展示方法优于手动方案及人类标注。  <br/>4. 开源代码、标注方案及结果，促进语音领域对话标注研究。|
|2505.13338v2|[Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data   Condensation and Spoken QA Generation](http://arxiv.org/abs/2505.13338v2)|**贡献点分点列出：**  <br/>1. **提出首个融合上下文推理与语音伴随信息的框架**：首次设计结合两者的数据集生成方法，解决传统Speech-LLMs在综合理解上的不足。  <br/>2. **创新性的数据生成机制**：包含两阶段技术——基于伪语音伴随标签的野外语音数据压缩，以及LLM驱动的上下文语音伴随问答（CPQA）生成。  <br/>3. **验证框架有效性**：通过Qwen2-Audio-7B-Instruct在自动生成与人工标注CPQA数据集上的强相关性评估，证明其能力。  <br/>4. **揭示Speech-LLMs的局限性**：明确指出模型在共情推理任务中的缺陷，强调需针对性数据集与更优模型。  <br/>5. **潜在应用价值**：为训练具备语音伴随推理能力的鲁棒Speech-LLMs提供基础，推动语音理解研究发展。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出首个融合上下文推理与语音伴随信息的数据集生成框架，揭示Speech-LLMs在共情任务中的局限，验证框架对模型训练的有效性，为提升语音AI能力提供新方向。|
|2505.17536v2|[Multimodal Conversation Structure Understanding](http://arxiv.org/abs/2505.17536v2)|总结：  <br/>本研究提出对话角色归属与线程划分任务框架，构建大规模人工标注数据集，评估多模态模型在理解对话结构上的性能差异，揭示关键影响因素，为改进多模态语言模型的对话理解能力提供基础。<br/><br/>贡献点：  <br/>1. **构建多模态对话结构标注数据集**：提供首个包含4,398条对话角色标注、5,755条收件人信息、3,142条旁观者信息的语料库，涵盖多参与者、多模态场景。  <br/>2. **定义关键任务框架**：提出针对对话角色分配（说话人、收件人、旁观者）和对话线程构建（语句关联与聚类）的系统性任务，结合会话分析与社会语言学理论。  <br/>3. **评估模型性能差异**：对比音频-视觉LLM与视觉-语言模型，发现前者在说话人/收件人识别上更优，但匿名化参与者时性能显著下降。  <br/>4. **揭示关键影响因素**：通过实验发现对话参与者数量是角色识别的主要负向预测因子，而声学清晰度（音调、频谱质心）和面部覆盖情况与性能呈正相关。  <br/>5. **推动未来研究方向**：为多模态LLM在对话结构建模和推理能力的提升提供基准与启示。|
|2506.01808v1|[NAVER LABS Europe Submission to the Instruction-following Track](http://arxiv.org/abs/2506.01808v1)|**贡献点总结（100字以内）:**  <br/>本文提出一种多任务语音处理系统，整合语音到LLM嵌入投影器与LoRA适配器，通过指令微调实现跨语言（中、意、德）的ASR、ST、SQA任务联合处理，优化了多语言和多模态数据下的模型性能。<br/><br/>**分点贡献：**  <br/>1. **多任务联合处理**：开发可同步执行语音识别（ASR）、语音翻译（ST）和语音问答（SQA）的系统，支持英语输入到中文、意大利语和德语的跨语言转换。  <br/>2. **模块化架构**：采用两个预训练模块：(1) 语音到LLM的嵌入投影器（基于SeamlessM4T-v2-large语音编码器）；(2) LoRA适配器（基于Llama-3.1-8B-Instruct语言模型）。  <br/>3. **指令微调策略**：联合加载模块后，在多语言和多模态数据上进行1K步指令优化，提升对复杂指令的响应能力。  <br/>4. **高效训练方法**：通过预训练模块与微调结合，简化多任务模型的训练流程，可能提升推理效率和泛化性能。|
|2506.01683v1|[Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection   Using Speech and Large Language Models](http://arxiv.org/abs/2506.01683v1)|**贡献点：**  <br/>1. 提出结合语音和语言模型的CoT推理方法，用于阿尔茨海默病（AD）与非AD分类。  <br/>2. 引入监督微调（SFT）与CoT提示策略，增强模型分类能力。  <br/>3. 设计线性层作为分类模块，提升模型对语音文本的判别性能。  <br/>4. 实验显示方法在无CoT策略的对比基线中实现16.7%的相对性能提升。  <br/>5. 达到当前CoT方法在阿尔茨海默病诊断领域的最先进性能水平。  <br/><br/>**总结：**  <br/>该研究提出一种基于链式思维的语音-语言模型联合框架，通过监督微调与提示策略显著提升痴呆症分类准确率，达到领域内最先进水平。|
|2505.09439v2|[Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](http://arxiv.org/abs/2505.09439v2)|总结：  <br/>本文提出Omni-R1，通过GRPO强化学习微调Qwen2.5-Omni，在MMAU和MMAR基准上取得SOTA性能，揭示了文本推理能力对音频任务的关键作用，并意外发现文本数据微调可提升音频表现。<br/><br/>贡献点：  <br/>1. **提出Omni-R1模型**：通过GRPO强化学习方法对Qwen2.5-Omni进行微调，聚焦音频问答任务。  <br/>2. **SOTA性能突破**：在MMAU和MMAR基准测试中达到当前最优结果，尤其在声音、音乐、语音及总体平均类别均表现最佳。  <br/>3. **因果分析**：验证了性能提升主要源于增强的文本推理能力，而非单纯依赖音频数据。  <br/>4. **意外发现**：发现仅以文本数据集进行微调也能有效提升模型的音频表现，为多模态训练提供新思路。|
|2506.01484v2|[LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech   Detoxification](http://arxiv.org/abs/2506.01484v2)|总结（100字以内）:  <br/>该研究提出基于LLM的自动化detoxification框架，构建首个大规模hatespeech平行数据集ParaDeHate，并验证其在提升模型性能方面的有效性，为替代人工标注提供可扩展解决方案。<br/><br/>贡献点分点列出:<br/>1. **提出LLM-in-the-loop自动化流程**：设计一种利用GPT-4o-mini替代人工标注的新型管道，实现对有害语言的自动改写，降低标注成本。<br/>2. **构建领域专用数据集ParaDeHate**：创建包含8K对仇恨/非仇恨文本的平行数据集，填补hatespeech detoxification的高质量数据缺口。<br/>3. **验证LLM生成数据的有效性**：通过实验表明，基于ParaDeHate微调的BART等模型在风格准确性、内容保留和流畅度方面显著优于现有方法。<br/>4. **建立基准与方法对比**：发布ParaDeHate作为评估基准，并系统比较多种基线模型表现，推动该领域的研究进展。|
|2506.01133v1|[From Words to Waves: Analyzing Concept Formation in Speech and   Text-Based Foundation Models](http://arxiv.org/abs/2506.01133v1)|贡献点（分点）:<br/>1. 首次验证语音模型是否能像文本模型一样获得抽象语义概念<br/>2. 系统比较单模态（语音/文本）与多模态联合训练模型的语义结构差异<br/>3. 提出并应用Latent Concept Analysis方法分析跨模态语义形成机制<br/>4. 开源实验代码与资源提升研究可复现性<br/><br/>总结: 本研究通过无监督分析方法，揭示语音与文本模型在语义抽象形成上的异同，验证多模态训练对语义理解的增强作用，并开放资源促进学术研究复现。|
|2506.01111v1|[FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal   Contextual Fusion](http://arxiv.org/abs/2506.01111v1)|**贡献点分点：**  <br/>1. **两阶段音频描述生成方法**：提出基于人类听觉感知启发的自动化框架，结合多模态信息提取（语音、音乐、环境声音、视频）和大语言模型（LLM）的上下文融合，实现细粒度、语境感知的音频描述。  <br/>2. **FusionAudio数据集**：构建包含120万条高质量音频描述和60万问答对的大型数据集，为跨模态研究提供标注资源。  <br/>3. **改进的音频模型**：开发基于CLAP的音频编码器，提升音频-文本对齐能力与指令遵循效果，增强模型对复杂音频环境的理解。  <br/><br/>**总结（100字以内）：**  <br/>提出两阶段音频描述生成框架与FusionAudio数据集，结合多模态信息和大语言模型提升描述质量，优化CLAP编码器实现更精准的音频-文本对齐与理解。|
|2506.01077v1|[TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans](http://arxiv.org/abs/2506.01077v1)|总结：  <br/>本论文提出TRiMM框架，解决数字人类实时手势生成与长文本理解难题，包含跨模态注意力、长上下文建模及大规模动作匹配系统，实现120fps推理速度，并在消费级GPU上保持低延迟，代码开源。<br/><br/>贡献点：  <br/>1. **提出TRiMM框架**  <br/>   - 首次结合多模态技术实现实时3D手势生成，同时解决长文本理解与实时合成的挑战。  <br/><br/>2. **设计三大核心模块**  <br/>   - **跨模态注意力机制**：实现语音与手势的精确定时对齐。  <br/>   - **长上下文自回归模型**：采用滑动窗口机制高效建模长序列，增强语义连贯性。  <br/>   - **大规模动作匹配系统**：构建原子动作库，支持实时检索生成高质量手势。  <br/><br/>3. **轻量级Unreal Engine实现**  <br/>   - 开发轻量级实验流水线，实现实时推理速度（120 fps）与低句级延迟（0.15秒）。  <br/><br/>4. **全面评估验证效果**  <br/>   - 在ZEGGS和BEAT数据集上完成主观与客观评估，证明其性能优于当前SOTA方法。  <br/><br/>5. **开源代码促进应用**  <br/>   - 提供完整代码库，推动LLM驱动数字人类研究的可复现性与实际部署。|
|2506.00955v1|[Leveraging Large Language Models for Sarcastic Speech Annotation in   Sarcasm Detection](http://arxiv.org/abs/2506.00955v1)|总结：  <br/>提出基于大语言模型的语音讽刺标注方法，构建首个大规模单模态讽刺语音数据集PodSarc，验证其有效性并展示73.63%的检测性能，为该领域研究提供新基准。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的单模态标注流程**：首次利用GPT-4o和LLaMA 3等大语言模型生成语音讽刺标注数据，解决语音领域数据稀缺问题。  <br/>2. **构建大规模语音讽刺数据集PodSarc**：通过人机协作验证，创建高质量单模态讽刺语音数据集，填补语音讽刺研究的数据空白。  <br/>3. **验证方法有效性**：采用协作门控架构对比标注质量与检测性能，证明所生成数据集的可靠性及研究价值。  <br/>4. **提供基准性能指标**：检测模型在公开数据集上达到73.63% F1分数，为后续研究提供量化评估标准。|
|2506.00304v1|[Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion   with LLMs](http://arxiv.org/abs/2506.00304v1)|总结：  <br/>本研究提出一种无需配对语音数据的EMG适配器模块，使LLMs能处理无声EMG信号，实现低错误率的EMG-to-text转换，并在小数据量下显著优于专用模型。<br/><br/>贡献点：  <br/>1. **首次提出EMG适配器模块**：创新性地设计模块，将无声EMG特征映射至LLM输入空间，无需依赖配对的有声信号或语音数据。  <br/>2. **实现高效转换性能**：在封闭词汇任务中达成平均WER 0.49，表明模型对无声EMG信号的识别能力。  <br/>3. **小数据量优势**：仅需6分钟无声EMG数据，性能超越专用模型近20%，验证方法的泛化能力与实用性。  <br/>4. **拓展LLMs应用边界**：探索LLMs在理解发声生物信号（如无声EMG）中的潜力，为跨模态语音识别提供新方向。|
|2505.24869v1|[SiLVR: A Simple Language-based Video Reasoning Framework](http://arxiv.org/abs/2505.24869v1)|总结（100字以内）:  <br/>提出SiLVR框架，通过双阶段语言表征与推理实现复杂视频语言理解，利用多感官输入提升性能，并验证强LLM无需视频训练即可有效处理多模态信息，达到多个基准数据集的最佳结果。<br/><br/>**贡献点分点列出**:<br/>1. **双阶段框架设计**：创新性地构建SiLVR框架，将复杂视频理解拆分为语言表征提取与推理两阶段，简化多模态处理流程。<br/>2. **多感官输入融合**：引入短片标题、音频/语音字幕等多模态数据作为语言表征的输入，增强对视频内容的描述能力。<br/>3. **自适应token削减方案**：提出动态调整时间粒度的token减少方法，有效处理长上下文多模态输入的效率与精度问题。<br/>4. **训练自由的模组化架构**：框架无需额外训练，依赖推理时的语言模型能力，提升灵活性和易用性。<br/>5. **多任务性能突破**：在Video-MME、Video-MMMU、Video-MMLU、CGBench、EgoLife等基准数据集上取得当前最优性能。<br/>6. **跨模态推理验证**：实验证明，强LLM无需视频领域训练即可高效聚合多感官信息，支持复杂时序、因果、长上下文及知识推理任务。|
|2505.24691v1|[Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing   Cross-Lingual Transfer in Low-Resource Scenarios](http://arxiv.org/abs/2505.24691v1)|总结：提出融合音素表征与Chain-of-Thought框架的S2TT方法，通过课程学习策略提升低资源与零资源场景的翻译性能，为跨语言语音翻译的普及提供新思路。<br/><br/>贡献点：<br/>1. 提出将音素表征整合入CoT框架，构建新型S2TT系统  <br/>2. 引入音素识别作为跨语言迁移的中间步骤，实现零资源翻译能力  <br/>3. 基于多语言LLM开发语音-文本联合处理架构  <br/>4. 设计渐进式课程学习策略，优化多任务训练过程  <br/>5. 证实音素增强的CoT方法在低资源场景显著提升译质，且具备可扩展性|
|2505.24493v1|[MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging   LLM Embedded Knowledge](http://arxiv.org/abs/2505.24493v1)|总结：  <br/>该研究通过GPT-4o构建首个全自动生成的多模态情感数据集MELT，并验证其在自监督学习中的有效性，显著提升语音情感识别性能。<br/><br/>贡献点：  <br/>1. **提出无监督情感标注方法**：首次利用GPT-4o纯文本标注能力，基于情景喜剧Friends构建多模态情感数据集MELT，解决传统人工标注成本高和不一致的难题。  <br/>2. **构建全自动生成数据集**：MELT是首个完全依赖大型语言模型（LLM）生成标签的多模态情感数据集，无需人工监督或多模态输入。  <br/>3. **验证SSL模型效果**：通过微调四个自监督学习（SSL）基线模型，评估MELT在语音情感识别任务中的适用性与性能提升。  <br/>4. **实验证明性能优势**：主观实验结果表明，MELT显著改善了语音情感识别（SER）的准确性与一致性。|
|2505.24458v1|[SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social   Engineering Behaviors](http://arxiv.org/abs/2505.24458v1)|**贡献点：**  <br/>1. **首个整合AR与多模态LLM的社会工程数据集**：SEAR是首个专门针对AR增强现实和多模态大语言模型驱动的社会工程攻击的多模态数据集。  <br/>2. **多场景对抗性对话数据**：包含60名参与者在模拟会议、课堂、社交活动等场景中的180条标注对话，涵盖多样化的社会工程情境。  <br/>3. **多模态同步数据采集**：整合AR捕捉的同步视觉/音频线索（如面部表情、语音语调）、环境信息及用户社交媒体资料，实现全面行为分析。  <br/>4. **主观攻击效果评估**：引入信任评分与易受性评估等主观指标，量化攻击对用户心理的影响。  <br/>5. **高攻击效能实证结果**：揭示SEAR在诱导用户点击钓鱼链接（93.3%）、接受电话（85%）、提升信任度（76.7%）等任务中的显著效果。  <br/>6. **伦理合规保障**：通过匿名化处理与IRB（伦理审查委员会）批准，确保数据集的负责任使用。  <br/>7. **开源开放共享**：数据集通过GitHub平台公开，支持学术研究与技术开发。  <br/><br/>**总结（100字以内）**：  <br/>SEAR Dataset是首个结合AR与多模态LLM的社会工程攻击数据集，包含多场景、多模态数据及主观评估指标，揭示攻击高效能，为检测与防御研究提供资源，同时保障伦理合规与公开共享。|
|2505.24016v1|[BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech   Translation System](http://arxiv.org/abs/2505.24016v1)|总结:  <br/>本研究提出BeaverTalk级联系统，结合VAD分段器、Whisper Large V2和Gemma 3 12B，通过LoRAs微调和单句记忆机制实现高效实时翻译，显著提升英德、英中翻译性能。<br/><br/>贡献点:  <br/>1. **系统架构创新**：设计级联系统BeaverTalk，集成VAD分段器、Whisper Large V2语音识别模型和Gemma 3 12B语言模型，实现端到端语音到文本翻译。  <br/>2. **微调方法优化**：采用低秩适配器（LoRAs）技术对翻译LLM进行轻量级微调，结合对话提示策略利用单一源语言前句记忆库提升上下文建模能力。  <br/>3. **延迟与语言方向支持**：系统兼容低延迟（StreamLAAL 1837.86）和高延迟（StreamLAAL 3343.73）运行模式，在英德（BLEU 24.64/27.83）与英中（BLEU 34.07/37.23）任务中均取得突出性能。  <br/>4. **实际部署效果**：在IWSLT 2025真实任务中验证系统有效性，为多语言实时翻译提供可落地的解决方案。|
|2505.15670v2|[Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](http://arxiv.org/abs/2505.15670v2)|总结：  <br/>本文提出首个无需语音预训练的双工S2S模型，通过连续输入输出与信道融合实现实时对话，降低比特率并提升推理、回合管理及打断处理能力，同时减少数据需求，开源代码促进复现。<br/><br/>贡献点：  <br/>1. **双工S2S架构设计**：支持连续用户输入与同步代理输出，引入信道融合机制实现真实场景下的实时交互（如用户打断）。  <br/>2. **无需语音预训练**：首次提出仅依赖流式编码器的双工模型，消除对专用语音预训练的需求。  <br/>3. **高效编码与微调**：采用独立的代理和用户建模架构，支持编码微调以优化代理语音质量，将比特率降至0.6kbps。  <br/>4. **性能提升**：在推理能力、回合控制和打断处理等关键指标上超越现有双工模型。  <br/>5. **数据需求降低**：跳过语音预训练环节，显著减少所需语音数据量，简化模型构建流程。  <br/>6. **开源与可复现性**：首个公开完整训练与推理代码的双工S2S模型，推动领域研究复现与验证。|
|2506.05796v1|[Diarization-Aware Multi-Speaker Automatic Speech Recognition via Large   Language Models](http://arxiv.org/abs/2506.05796v1)|**贡献点总结（100字以内）**：  <br/>提出融合说话人聚类与大语言模型的多说话人语音识别系统，保留绝对时间信息，提升多语言对话与复杂多说话人场景的识别性能，验证了LLM作为统一后端在联合发言分割和转录中的潜力。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新框架**：设计了一种结合说话人聚类（diarization）与大语言模型（LLM）的多说话人语音识别（MS-ASR）系统，解决重叠语音转录难题。  <br/>2. **保留时序信息**：不同于传统序列化输出训练（SOT）方法，通过整合帧级说话人和语义嵌入，保留绝对时间信息以适应时间敏感场景。  <br/>3. **多模态输入处理**：框架同时处理结构化diarization输入与帧级嵌入，实现段级转录输出，提升识别粒度与准确性。  <br/>4. **多语言与复杂场景适应性**：实验表明系统在多语言对话及高重叠多说话人会议场景中均表现优异，验证其鲁棒性与实用性。  <br/>5. **统一后端潜力**：强调LLM作为统一后端在联合发言分割与转录任务中的优势，推动语音处理与自然语言处理的融合。|
|2506.05706v1|[Bridging the Modality Gap: Softly Discretizing Audio Representation for   LLM-based Automatic Speech Recognition](http://arxiv.org/abs/2506.05706v1)|总结（100字以内）:  <br/>本文提出将向量量化整合到LLM的ASR系统中，通过软离散化方法提升模型对跨域音频的处理能力，揭示了其作为模态桥梁的潜力。<br/><br/>贡献点:  <br/>1. **提出VQ与LLM结合的方法**：解决音频连续性与LLM离散token范式之间的鸿沟，实现音频表示与语言模型的对齐。  <br/>2. **构建基于LLM嵌入表的VQ codebook**：利用预训练LLM的嵌入表作为量化字典，降低模型训练复杂度并增强跨模态一致性。  <br/>3. **设计软离散化技术**：通过动态更新codebook和加权求和策略，生成更符合语言结构的离散音频representation。  <br/>4. **验证有效性与泛化性**：实验表明该方法在out-of-domain场景下显著优于基线，为LLM-based ASR提供新思路。|
|2506.05671v1|[Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](http://arxiv.org/abs/2506.05671v1)|总结：  <br/>本研究提出文本仅微调策略，通过未配对文本实现低资源领域自适应，保持语音-文本对齐并提升模型泛化能力，为ASR领域提供有效新方法。<br/><br/>贡献点：  <br/>1. 提出无需额外音频的文本仅微调框架，解决低资源环境下配对语音-文本数据不足的问题。  <br/>2. 引入实时评估机制，确保微调过程中语音-文本对齐性得以保留。  <br/>3. 实验证明方法在多个数据集上可保持与全音频-文本微调相近的性能，且性能退化最小。  <br/>4. 有效提升模型对新领域的泛化能力，避免灾难性遗忘问题。  <br/>5. 为低资源领域适应性ASR提供可扩展的文本驱动优化方案。|
|2506.06113v1|[Bridging the Gap: In-Context Learning for Modeling Human Disagreement](http://arxiv.org/abs/2506.06113v1)|**贡献点分点总结**  <br/>1. **探索LLMs在主观任务中的多视角建模能力**：首次系统研究LLMs是否能通过上下文学习（ICL）捕捉多角度观点，并反映主观注释中存在的分歧（如仇恨言论检测）。  <br/>2. **提出多标签建模策略的对比实验**：对比了聚合硬标签与拆分硬标签/软标签在零样本和小样本场景下的效果，揭示了不同标签策略对模型表现的影响。  <br/>3. **分析小样本提示优化方法**：评估了基于文本相似度（BM25、PLM）、注释分歧（熵）和组合排名的演示选择方法，以及随机与课程式（curriculum-based）示例排序策略的有效性。  <br/>4. **揭示LLMs建模主观性的局限**：发现零样本设置下多视角生成可行，但小样本场景难以全面体现人类判断，提示设计和演示选择对性能具有显著影响。  <br/>5. **强调改进方向**：指出需构建更视角敏感、具备社会智能的LLMs，以更好应对主观性任务中的注释分歧问题。  <br/><br/>**总结（100字以内）**：  <br/>该研究探讨LLMs在主观任务中捕捉多视角与注释分歧的潜力，通过对比多种标签策略和提示优化方法，揭示模型在零样本与小样本场景下的差异，并提出改进LLMs社会智能性的方向。|
|2506.06066v1|[Conversational Interfaces for Parametric Conceptual Architectural   Design: Integrating Mixed Reality with LLM-driven Interaction](http://arxiv.org/abs/2506.06066v1)|总结：  <br/>本论文提出一种基于对话的MR界面，整合语音、手势与多智能体LLM，实现参数化建模的直观操作，降低设计门槛，推动MR向生成设计平台发展。<br/><br/>贡献点：  <br/>1. **提出新型对话式MR交互框架**：首次将语音输入、手势识别与多智能体大语言模型（LLM）系统结合，形成面向参数化建模的自然交互范式。  <br/>2. **动态参数状态管理机制**：通过对话与上下文提示解决命令歧义，实现参数的实时动态调整与状态跟踪。  <br/>3. **降低设计认知与操作壁垒**：简化参数化工作流程，使无编程背景的设计师可高效探索和优化设计空间。  <br/>4. **扩展MR作为生成设计平台**：将MR环境从空间交互工具升级为支持程序化思维的创造性设计平台。|
|2506.05538v1|[SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful   Deepfake Content on Social Media Platforms](http://arxiv.org/abs/2506.05538v1)|总结：  <br/>提出SocialDF数据集及多模态LLM检测方法，解决社交媒体深伪内容的识别难题。<br/><br/>贡献点：  <br/>1. 构建SocialDF数据集：首个涵盖社交媒体真实场景的深伪挑战数据集，包含多来源高保真合成媒体。  <br/>2. 多模态检测框架：融合面部识别、语音转录与多智能体LLM，实现音频-视觉线索的交叉验证。  <br/>3. 语言行为分析：引入 linguistic、behavioral 和 contextual 多维度分析，提升检测的鲁棒性和准确性。|
|2506.05414v1|[SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and   Hearing](http://arxiv.org/abs/2506.05414v1)|总结：  <br/>本研究提出首个支持动态场景中3D空间推理的基准SAVVY-Bench，并设计无需训练的推理框架SAVVY，通过多模态轨迹聚合与坐标对齐提升音频-视觉大语言模型的性能，推动动态3D场景理解。<br/><br/>贡献点：  <br/>1. 提出首个专注于动态、音频-视觉环境中3D空间推理的基准SAVVY-Bench，引入同步空间音频数据。  <br/>2. 设计无训练的推理框架SAVVY，包含两个阶段：基于视角的物体轨迹估计和动态全局地图构建。  <br/>3. 引入细粒度时序定位、一致3D定位及多模态标注，提升场景理解的复杂性与准确性。  <br/>4. 实验证明SAVVY显著提升现有AV-LLMs性能，为动态3D空间推理研究建立新标准。|
|2505.05335v2|[FLAM: Frame-Wise Language-Audio Modeling](http://arxiv.org/abs/2505.05335v2)|总结：  <br/>本文提出FLAM，解决帧级音频理解与开放词汇定位难题，通过logit调整和大规模数据集提升模型性能，保持全局检索能力。<br/><br/>贡献点：  <br/>1. **首次提出开放词汇帧级音频定位模型**：突破传统模型对预定义类别的依赖，实现对真实场景中未见事件的泛化定位。  <br/>2. **设计记忆高效且校准的帧级目标函数**：引入logit调整机制，有效缓解训练中的虚假相关（如事件依赖、标签不平衡）。  <br/>3. **构建多源帧级监督数据集**：结合LLM生成字幕与模拟数据，提供多样化、细粒度的音频事件标注。  <br/>4. **验证模型多任务能力**：在保持文本-音频检索性能的同时，显著提升帧级定位效果及下游任务表现。|
|2506.07726v1|[Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription   with RAG-based Correction and Predicted BLEU](http://arxiv.org/abs/2506.07726v1)|**贡献点：**  <br/>1. 构建了首个包含801小时、高质量的长文本形式瑞士议会语料库（751小时通过质量控制），支持瑞士德语多小时辩论会的语音-文本对齐。  <br/>2. 提出分两阶段的GPT-4o校正流程：第一阶段修正ASR误识别（如专有名词），第二阶段评估语义完整性，提升文本准确性。  <br/>3. 引入基于BLEU分数和LLM评估的自动化过滤机制，优化数据质量并确保语义一致性。  <br/>4. 验证了结合高精度ASR、LLM校正与数据驱动过滤方法对低资源、领域专用语音语料库的显著效果（BLEU提升6分）。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了大尺寸瑞士议会长文本语音语料库，通过优化ASR与LLM校正流程提升数据质量，验证了多阶段校正与过滤方法对低资源语料的有效性，实现BLEU显著提升，为瑞士德语语音研究提供重要资源。|
|2506.07707v1|[Interaction Analysis by Humans and AI: A Comparative Perspective](http://arxiv.org/abs/2506.07707v1)|总结：  <br/>本研究通过比较MR与2D视频会议对儿童手势猜谜游戏交流的影响，揭示了MR在增强互动质量与协作学习中的潜力，同时探讨了LLMs在儿童互动分析中的效率与局限性。<br/><br/>贡献点：  <br/>1. **对比研究**：首次系统分析MR（HoloLens）与2D视频会议（Zoom）在儿童协作任务中的交互差异。  <br/>2. **LLMs应用**：开发基于大语言模型的自动化分析框架，实现注释、翻译及迭代校正，提升数据处理效率。  <br/>3. **交互特征发现**：发现MR促进更丰富的非语言交流（如情感表达），而Zoom则以简洁性和可访问性为优势。  <br/>4. **跨平台洞见**：为分布式教育场景下的协作学习设计提供实证依据，强调MR在增强沉浸感与参与度方面的潜力。|
|2506.06775v1|[They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse](http://arxiv.org/abs/2506.06775v1)|总结：  <br/>本研究首次构建意大利政治演讲的IMPAQTS语料库，系统评估LLMs在处理隐含内容（预设与暗示）上的局限性，并提出改进方向，同时开源数据与代码。<br/><br/>贡献点：  <br/>1. **首次构建专用语料库**：创建包含意大利政治演讲及操纵性隐含内容标注的IMPAQTS语料库，填补该领域研究空白。  <br/>2. **设计双重评估任务**：通过多项选择和开放生成任务，全面测试LLMs对隐含内容的解析能力。  <br/>3. **揭示模型局限性**：证明当前LLMs在政治语境下的隐含内容理解存在显著缺陷，识别其缺失的关键pragmatic能力。  <br/>4. **提出改进方向**：指出增强模型对高度隐含语言处理能力的潜在研究路径。  <br/>5. **开放数据与代码**：提供数据集和实现代码，促进该领域的进一步研究与应用。|
|2506.08967v1|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v1)|**贡献点：**  <br/>1. **提出全端到端的AQAA模型（Step-Audio-AQAA）**：首次设计专门针对音频查询-音频回答（AQAA）任务的端到端大型音频语言模型，克服传统模型依赖文本输出的局限。  <br/>2. **双代码本音频分词器设计**：引入语言与语义双重编码机制，有效提取语音中的结构化与高层次语义特征。  <br/>3. **大参数量骨干模型与神经声码器结合**：采用1300亿参数的LLM作为主干，并搭配神经声码器，实现高保真语音合成与控制。  <br/>4. **混合训练策略优化**：通过文本与音频交错输出增强语义连贯性，并结合Direct Preference Optimization（DPO）与模型合并技术提升整体性能。  <br/>5. **语音控制任务的性能突破**：在StepEval-Audio-360基准测试中，显著优于现有SOTA模型，验证了模型在语音控制方向的有效性。  <br/>6. **突出token-based声码器的关键作用**：强调声码器在端到端AQAA任务中的核心地位，为未来研究提供新的技术视角。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Step-Audio-AQAA，通过双代码本分词器、大参数模型和神经声码器实现端到端音频对话，结合交错训练与DPO优化，并在语音控制任务中超越现有SOTA模型。|
|2506.08633v1|[Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs](http://arxiv.org/abs/2506.08633v1)|**贡献点：**<br/>1. 提出基于小连接模块（connector module）的跨模态桥接方法，整合语音编码器（如WavLM-large）与大语言模型（如OLMo/LLaMA）的表示空间。<br/>2. 推动全开源和开放数据方案，采用WavLM-large和OLMo等公开模型实现端到端DST系统。<br/>3. 系统性消融实验分析了不同微调策略（全量/LoRA适配器）与对话轮次影响，验证了关键组件的有效性。<br/>4. 引入基于模糊匹配的输出后处理技术，显著提升对话槽值中的命名实体识别性能。<br/>5. 创新性结合SpokenWOZ数据集与Speech-Aware MultiWOZ数据集，提升训练数据多样性与模型表现。<br/>6. 在 SpokenWOZ 标准测试集上取得 34.66% JGA 的 SOTA 结果，并通过 Gemma-2-9B-instruct 实现更高精度（42.17% JGA）。<br/><br/>**总结（100字以内）：**  <br/>本研究通过连接模块融合语音与语言模型，结合开源组件与数据增强策略，系统性优化DST方法。实验证明其在SpokenWOZ数据集上达到SOTA性能，并进一步通过大模型提升至更高水平。|
|2506.08593v1|[Hateful Person or Hateful Model? Investigating the Role of Personas in   Hate Speech Detection by Large Language Models](http://arxiv.org/abs/2506.08593v1)|总结（100字以内）:  <br/>本文首次系统研究MBTI性格特质对LLM仇恨言论分类的系统性影响，发现人格特征显著改变模型输出，并提出优化标注流程的建议，强调对公平性与价值观对齐的重要性。<br/><br/>贡献点:  <br/>1. **首次研究框架**：提出首个基于MBTI人格特质的个性提示（persona prompt）系统性框架，用于分析其对仇恨言论分类的影响。  <br/>2. **人类标注验证**：通过大规模人类标注调查，实证证明MBTI维度显著影响标注行为，揭示人格特质在主观任务中的关键作用。  <br/>3. **多模型跨数据集评估**：在三个主流仇恨言论数据集上，评估四个开源LLM的输出差异，验证人格提示对模型表现的调控效应。  <br/>4. **识别偏差现象**：发现人格驱动导致的三类问题：与真实标签不一致、跨人格标注分歧、logit层级的隐性偏差，凸显模型与人类价值观的潜在差距。  <br/>5. **实践指导意义**：提出需谨慎设计人格提示以提升LLM标注流程的公平性，为伦理引导和模型对齐提供理论依据。|
|2506.08147v1|[Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models](http://arxiv.org/abs/2506.08147v1)|总结：  <br/>本研究构建首个乌尔都语-英语-西班牙语三语仇恨言论检测数据集，提出融合注意力机制与大语言模型的方法，显著提升多语言分类性能，为全球数字社区安全提供有效解决方案。<br/><br/>贡献点：  <br/>1. **构建首个三语仇恨言论数据集**  <br/>   - 收集10,193条跨语言推文（英语3,834；乌尔都语3,197；西班牙语3,162），通过关键词过滤确保平衡标注（4,849 Hate/5,344 Not-Hate）。  <br/>   - 弥补乌尔都语在仇恨言论研究中的空白，尤其缺乏基于翻译的语料支持。  <br/><br/>2. **提出混合模型方法论**  <br/>   - 结合注意力层（预处理）与大语言模型（如GPT-3.5 Turbo、Qwen 2.5 72B），优化多语言特征提取。  <br/>   - 明确区分传统模型（TF-IDF + SVM）与Transformer模型（BERT/RoBERTa）的对比实验。  <br/><br/>3. **验证高性能结果**  <br/>   - 在英语、西班牙语及乌尔都语中分别实现87%、85%、81%的F1得分，多语言联合模型达88%，均优于SVM基线（提升8.75%-7.32%）。  <br/>   - 通过Fleiss' Kappa（0.821）确保标注一致性，提升数据可靠性。  <br/><br/>4. **推动多语言检测应用**  <br/>   - 提供可扩展框架，支持跨语言仇恨言论识别，促进全球社交媒体安全治理。|
|2506.09707v1|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v1)|总结（100字以内）:  <br/>提出基于预训练模型和LoRA技术的自动PE治疗遵循定位方法，实现高精度识别与可扩展的临床应用。<br/><br/>贡献点:  <br/>1. **方法创新**：首次将LoRA技术应用于预训练音频-语言模型（Qwen2-Audio）的微调，实现对PE治疗关键环节的自动时间定位。  <br/>2. **标签生成**：通过LLM提示和人工验证结合，为三个核心协议阶段（P1/P2/P3）生成高质量的遵循标签。  <br/>3. **软监督策略**：设计任务特定提示引导的软监督框架，提升边界预测的准确性（MAE为5.3秒）。  <br/>4. **参数分析**：系统研究窗口大小与LoRA秩对性能的影响，揭示上下文粒度与模型适配的关键作用。  <br/>5. **实际应用**：构建可扩展的框架，支持PE治疗的临床培训、监督与质量评估，解决传统人工评估的效率问题。|
|2506.09983v1|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v1)|总结（100字以内）:  <br/>提出一种基于逐步指令策略的依存解析方法，结合通用词性标注和简化输出格式，在17种语言中实现SOTA性能，同时通过多语言微调提升跨语言泛化能力，验证了显式推理步骤对LLM解析的有效性。<br/><br/>贡献点:  <br/>1. **创新的逐步指令策略**：首次将通用词性标注作为先验步骤，结合句法头和依存关系预测，提升解析的结构有效性与准确性。  <br/>2. **简化输出格式设计**：采用类似CoNLL-U的轻量化输出格式，减少冗余信息干扰，避免模型生成错误或污染（hallucination）。  <br/>3. **多语言性能突破**：在17种语言的Universal Dependencies数据集上均取得领先的准确率，证明方法的普适性。  <br/>4. **跨语言泛化优化**：通过多语言微调，显著提升模型在不同语言间的迁移能力，增强通用性。  <br/>5. **方法对比与可扩展性**：提出一种格式一致、可扩展的替代方案，优于传统基于括号的解析方法，推动下游任务的兼容性。|
|2506.09391v1|[Comparing human and LLM politeness strategies in free production](http://arxiv.org/abs/2506.09391v1)|总结：  <br/>本研究揭示大语言模型在礼貌策略上虽能复制人类偏好并被偏好，但存在过度依赖消极策略、导致误解的偏差，突显AI系统在语用对齐中的关键挑战。<br/><br/>贡献点：  <br/>1. 系统分析LLM与人类在礼貌策略使用上的差异，验证大规模模型（≥70B参数）可复制计算语用学关键偏好。  <br/>2. 发现人类评估者在开放性任务中更倾向接受LLM生成的礼貌回应，表明模型在部分场景下的表现接近人类。  <br/>3. 指出LLM在积极语境中过度依赖消极礼貌策略，可能引发语用误解，揭示模型行为的局限性。  <br/>4. 强调语用对齐在AI系统中的重要性，为未来研究提供方向性启示。|
|2506.09349v1|[OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment](http://arxiv.org/abs/2506.09349v1)|总结（100字以内）:  <br/>提出OmniDRCA并行语音-文本模型，采用双分辨率表示和对比对齐，实现SOTA性能并在全双工对话场景中展示应用潜力。<br/><br/>贡献点：  <br/>1. 提出OmniDRCA模型，基于联合自回归建模实现并行生成语音和文本，突破传统分离式生成的局限。  <br/>2. 引入双分辨率语音表示（高低频特征分开编码），提升对语音细节和整体语义的理解能力。  <br/>3. 设计对比交叉模态对齐机制，增强语音与文本之间的对应关系和互模态感知。  <br/>4. 在口语问答基准测试中取得优于现有平行模型的SOTA性能，并验证其与交织模型的竞争力。  <br/>5. 探索框架在全双工对话场景中的扩展性，推动语音生成技术向更复杂交互任务迁移。|
|2506.09301v1|[$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for   Figurative Language Understanding](http://arxiv.org/abs/2506.09301v1)|贡献点：<br/>1. 提出$(RSA)^2$框架，首次将修辞策略纳入RSA模型以处理隐喻语言<br/>2. 实现无需建模非字面表达动机的意图理解机制，突破传统RSA框架限制<br/>3. 开发PragMega+新数据集并验证框架有效性，取得LLMs在讽刺识别任务上的SOTA性能<br/>4. 建立可扩展的语音理解范式，实现字面与意图语义的兼容性解读<br/><br/>总结：该研究提出$(RSA)^2$框架，通过建模修辞策略实现无需动机建模的隐喻理解，在新数据集上取得LLMs讽刺识别的最先进性能。|
|2506.10789v1|[FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](http://arxiv.org/abs/2506.10789v1)|**贡献点总结**（100字以内）:  <br/>本研究提出首个美国社会背景下的Neo-fascist文本编码方案，结合NLP与政治科学，构建大规模标注数据集并开发分类模型，揭示该言论在论坛中的广泛存在，强调社会背景的重要性，呼吁持续对抗以维护民主。<br/><br/>**分点贡献**:<br/>1. **首创编码方案**：提出首个针对美国社会语境的Neo-fascist话语编码体系，由政治科学学者主导，填补了NLP与政治学在该领域的交叉研究空白。  <br/>2. **跨学科方法论**：首次将政治学与NLP结合，构建系统性框架以识别和分析Neo-fascist言论，推动多学科协作研究。  <br/>3. **大规模标注数据**：通过众包技术标注1000条文本，创建首个公共可用的Neo-fascist话语标注数据集，为后续研究提供基础。  <br/>4. **模型验证与对比**：对SLMs和LLMs分别进行微调与测试，开发首个可用于Neo-fascist文本分类的模型，验证不同规模模型的效果差异。  <br/>5. **现象分析与警示**：揭示Neo-fascist言论在特定论坛中的普遍性，为理解其传播特征和对民主社会的威胁提供实证依据。  <br/>6. **研究伦理说明**：明确研究仅针对文本内容，不涉及对个人或组织的标签化，强调方法的中立性与合规性。|
|2506.10779v1|[Improving Named Entity Transcription with Contextual LLM-based Revision](http://arxiv.org/abs/2506.10779v1)|**贡献点：**  <br/>1. **提出LLM修订机制**：设计基于大语言模型（LLM）的命名实体修正方法，结合LLM的推理能力和局部上下文信息（如课程笔记）改进ASR对命名实体的识别。  <br/>2. **引入NER-MIT-OpenCourseWare数据集**：发布包含45小时MIT课程音频的开放数据集，支持命名实体识别任务的开发与测试。  <br/>3. **显著性能提升**：在所提出数据集上，该技术使命名实体的词错误率（WER）降低最高达30%，验证了方法的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出LLM驱动的命名实体修正方法，结合上下文信息提升ASR准确性，并发布专用数据集，实验证明在命名实体识别任务中WER降低30%。|
|2506.10504v1|[Beyond Single-User Dialogue: Assessing Multi-User Dialogue State   Tracking Capabilities of Large Language Models](http://arxiv.org/abs/2506.10504v1)|**贡献点分点总结：**  <br/>1. **构建多用户对话数据集**：基于言语行为理论，扩展现有DST数据集，生成第二用户的话语，模拟真实多用户交互场景。  <br/>2. **提出可控评估框架**：系统性地将第二用户话语融入对话，设计方法论以评估LLMs在多用户DST中的鲁棒性。  <br/>3. **揭示性能局限性**：实验表明，LLMs在多用户DST任务中表现显著下降，凸显其在处理多重说话者时的不足。  <br/>4. **指导未来研究方向**：强调需优化LLMs以应对多用户场景，推动更真实、鲁棒的对话状态跟踪模型发展。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过构建多用户DST数据集并设计可控评估框架，揭示了LLMs在处理复杂多用户对话中的性能局限，为未来改进多用户对话理解模型提供了方向。|
|2506.10245v1|[ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in   Portuguese](http://arxiv.org/abs/2506.10245v1)|**贡献点总结（100字以内）**  <br/>1. 构建首个大规模葡萄牙语细粒度仇恨言论语料库，覆盖九个法律保护少数群体。  <br/>2. 提出四阶段合成数据生成流程，包含人工种子、少样本扩展、改写增强和领域平衡。  <br/>3. 实现跨领域泛化能力，验证在多种任务和数据集上的有效性。  <br/>4. 公开数据促进低资源环境下合成数据和仇恨言论检测研究。|
|2506.10202v1|[Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual   Text-to-Video Retrieval](http://arxiv.org/abs/2506.10202v1)|**贡献点：**<br/>1. 提出Q2E方法：首个针对复杂现实事件的零样本多语言文本到视频检索框架，通过LLMs和VLMs提取隐式参数知识实现查询-事件分解。  <br/>2. 多模态输入兼容：支持视觉和语音输入的联合处理，展示跨模态知识融合的有效性。  <br/>3. 熵融合策略：引入熵基融合评分机制，提升零样本多模态信息整合能力。  <br/>4. 音频增强效果：验证音频信息在文本到视频检索中的显著提升作用。  <br/>5. 可泛化性：方法可适应不同数据集、领域及模型架构（LLMs/VLMs）。  <br/>6. 开源贡献：公开代码与数据，推动后续研究。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Q2E，通过LLMs和VLMs提取隐式知识实现零样本多语言文本到视频检索，支持视觉/语音输入融合，并验证音频信息的增强效果。方法具备良好的泛化性，已开源促进研究。|
|2506.08967v2|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v2)|总结（100字以内）:  <br/>提出全端到端LALM Step-Audio-AQAA，结合双代码库分词、130B参数模型与声码器，创新训练方法，显著提升语音控制能力，并建立新基准测试验证效果。  <br/><br/>**贡献点分项:**  <br/>1. **端到端语音交互模型**：首次设计直接生成自然语音响应的AQAA专用LALM (Step-Audio-AQAA)，突破传统文本依赖的瓶颈。  <br/>2. **双代码库音频分词器**：融合语言与语义特征提取，增强对音频内容的理解与建模能力。  <br/>3. **超大规模模型架构**：采用1300亿参数LLM作为骨干，显著提升生成质量与复杂任务处理能力。  <br/>4. **训练方法创新**：提出交错token输出机制，结合DPO与模型合并技术，优化语义连贯性与语音合成效果。  <br/>5. **基准测试与性能验证**：开发StepEval-Audio-360基准，证明模型在语音控制等关键指标上优于现有SOTA方法，凸显声码器的必要性。|
|2506.11125v1|[ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone   Scams](http://arxiv.org/abs/2506.11125v1)|**贡献点分点列出：**  <br/>1. **识别关键漏洞**：首次定位语音钓鱼（vishing）攻击链中的ASR转录环节为最大薄弱点，指出其对攻击成功的关键作用。  <br/>2. **提出ASRJam框架**：设计了一种主动防御系统，通过向目标音频注入对抗扰动，干扰攻击者的ASR并切断攻击反馈循环。  <br/>3. **改进对抗扰动方法**：提出EchoGuard，利用自然语音现象（如回声、混响）作为扰动源，实现对ASR的破坏性干扰，同时保持人类语音可懂性。  <br/>4. **实证评估**：通过39人用户实验验证EchoGuard的实用性，对比现有攻击技术，证明其在ASR干扰与用户体验平衡上的最优性。  <br/><br/>**总结（100字以内）：**  <br/>论文提出针对语音钓鱼的双防御策略：ASRJam主动干扰ASR转录，EchoGuard利用自然扰动阻断攻击而不影响人声理解。通过用户实验验证了EchoGuard在攻防效果与可用性上的优越性。|
|2506.11842v1|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v1)|总结：  <br/>本文提出PACE-ADS框架，通过整合语音等认知指令与心理状态感知，实现人机协同的自动驾驶系统，提升舒适度与安全性，展示大语言模型在人车交互中的应用潜力。<br/><br/>贡献点：  <br/>1. **提出跨模态人车交互框架**：首次构建融合外部交通环境感知与内部乘客心理/认知状态理解的人机协同自动驾驶系统（PACE-ADS），填补自动化驾驶与人类需求的gap。  <br/>2. **设计语音认知代理模块**：开发专门处理乘客语音指令（如语义理解）与心理信号（如EEG、心率）的Psychologist Agent，实现多模态意图解析。  <br/>3. **实现行为级自主决策**：通过Coordinator Agent整合感知信息，支持自主驾驶逻辑生成与安全恢复策略，无需替代传统模块。  <br/>4. **验证语音引导有效性**：在交通信号、行人、施工区等场景的仿真测试中，证明语音交互可提升驾驶风格适应性与用户舒适度。  <br/>5. **推动LLM应用落地**：探索基于大语言模型（LLM）的框架在自动驾驶领域的人类中心化驱动潜力，为未来人车协作提供新思路。  <br/><br/>（注：原文实际属于自动驾驶领域，但按语音视角提炼了其中与语音指令处理、多模态交互、LLM应用等相关内容作为贡献）|
|2506.11558v1|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v1)|**贡献点总结**（100字以内）:  <br/>提出DaMO数据高效视频语言模型，创新性采用分层双流时序感知架构与全局残差机制，设计四阶段渐进训练方法，构建增强数据集，并在时序任务中实现性能突破。  <br/><br/>**分点贡献**:  <br/>1. **模型设计**：提出DaMO，专为准确时序推理和多模态理解优化，提升数据效率。  <br/>2. **架构创新**：首创Temporal-aware Fuseformer，采用分层双流结构，分别捕捉视频/音频时序动态，有效融合多模态信息。  <br/>3. **效率优化**：引入全局残差机制，减少空间冗余的同时保留关键语义信息。  <br/>4. **训练方法**：提出四阶段渐进式训练范式，逐步强化多模态对齐、语义 grounding 和时序推理能力。  <br/>5. **数据贡献**：构建多个增强数据集，基于现有数据集添加GPT生成的时序相关问答对，支持时序监督任务。  <br/>6. **实验验证**：在时序定位和视频问答基准测试中，DaMO显著优于现有方法，尤其在精确时序对齐任务中表现突出。|