|Source|Title|Summary|
|---|---|---|
|2511.05399v1|[Robust Neural Audio Fingerprinting using Music Foundation Models](http://arxiv.org/abs/2511.05399v1)||
|2511.04995v1|[Enhancing Public Speaking Skills in Engineering Students Through AI](http://arxiv.org/abs/2511.04995v1)||
|2511.04366v1|[Towards Aligning Multimodal LLMs with Human Experts: A Focus on   Parent-Child Interaction](http://arxiv.org/abs/2511.04366v1)|总结：  <br/>本研究通过分析言语语言治疗师的观察流程，探索多模态大语言模型在亲子交互联合注意分析中的对齐方法，揭示其在观察与判断阶段的差异及应用潜力。<br/><br/>贡献点：  <br/>1. **首次将MLLMs与SLPs对齐**：提出将多模态大语言模型应用于亲子互动中联合注意分析的探索性研究，填补了该领域专家-AI协作的空白。  <br/>2. **构建专家观察流程框架**：通过访谈和视频标注，系统化分析SLPs对眼神、动作、语音等观察线索的推理机制。  <br/>3. **设计分阶段提示方法**：创新提出两阶段提示策略，分离“观察”与“判断”模块以模拟SLPs的工作流。  <br/>4. **揭示对齐差异性**：发现模型在观察层（共性描述）与判断层（差异标准）对齐效果不同，为优化模型提供理论依据。  <br/>5. **案例研究定位**：将该工作视为专家-AI对齐的案例探针，突出其在复杂社会行为分析中的可行性与挑战。|
|2511.04080v1|[Caption Injection for Optimization in Generative Search Engine](http://arxiv.org/abs/2511.04080v1)|**贡献点：**<br/><br/>1. 提出首个多模态生成搜索引擎优化方法 Caption Injection。  <br/>2. 将图像描述（caption）注入文本内容，融合视觉语义以提升内容主观可见性。  <br/>3. 在 MRAMG 基准上系统评估，验证了多模态集成对 G-SEO 的有效性。  <br/>4. 实验证明 Caption Injection 在 G-Eval 指标下显著优于纯文本 G-SEO 基线。  <br/><br/>**总结（100字以内）:**  <br/>本文提出首个多模态 G-SEO 方法 Caption Injection，通过融合图像描述提升生成搜索结果的主观可见性，实验表明其在多模态场景下效果显著优于传统文本优化方法。|
|2511.02834v2|[Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything](http://arxiv.org/abs/2511.02834v2)|总结：  <br/>提出Agent-Omni框架，通过主-代理系统实现多模态模型的灵活协作与无缝集成，无需微调，在跨模态任务中取得SOTA性能，并具备高扩展性。<br/><br/>贡献点：  <br/>1. **提出Agent-Omni框架**：首次构建支持文本、图像、音频、视频等全模态协作的系统架构，无需对模型进行重新训练。  <br/>2. **主-代理协作机制**：通过主代理解析意图并分配子任务，实现多模态模型的协同推理与结果整合。  <br/>3. **跨模态性能优势**：在文本、图像、音频、视频及全模态基准测试中均表现出色，尤其在复杂跨模态推理任务中达到SOTA。  <br/>4. **模块化与可扩展性**：设计模块化结构，便于集成新模型并适应多样化输入，同时保持透明性和可解释性。|
|2511.02378v1|[Revisiting put-that-there, context aware window interactions via LLMs](http://arxiv.org/abs/2511.02378v1)|**贡献点分点总结：**  <br/>1. **融合传统与现代技术**：将Bolt的“Put-That-There”概念与XR传感器技术栈结合，引入大语言模型（LLMs）提升交互智能化。  <br/>2. **多模态环境建模**：整合语义分割的3D环境、实时应用元数据及用户语言、手势、头部注视等多源信息，形成全面的上下文理解。  <br/>3. **三级交互方式**：支持明确命令（如放置应用）、指涉性语言与手势（如“Put that there”）、以及高层次目标（如发送消息）的多样化操作映射。  <br/>4. **动态推理与布局优化**：通过目标导向推理，LLM可动态推断相关应用及布局决策，包括工具间的关联关系，减少人为干预。  <br/>5. **无缝意图驱动体验**：在沉浸式XR环境中实现无需手动操作的自动窗口管理，提升交互流畅性与效率。  <br/><br/>**总结（100字以内）**：  <br/>本研究将大语言模型与XR技术结合，提出新型语音-手势交互框架，支持多模态信息融合与目标驱动的动态布局，实现沉浸式环境中无需手动操作的智能空间管理。|
|2511.02371v1|[LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming   Alignment](http://arxiv.org/abs/2511.02371v1)|总结：  <br/>LUMA-RAG提出了一种实用的多模态RAG架构，通过动态内存分层、跨模态对齐桥梁和稳定性保障机制，解决了索引时效性、跨模态语义一致性和多模态数据处理的挑战。<br/><br/>贡献点：  <br/>1. **动态多层级记忆系统**：引入热HNSW层与压缩IVFPQ层的流式迁移机制，在有限内存预算下保持索引新鲜度，避免高昂重索引成本。  <br/>2. **跨模态对齐桥梁**：设计CLAP到CLIP的流式对齐方法，通过增量正交Procrustes更新维持多模态语义一致性。  <br/>3. **稳定性感知检索监控**：构建联合约束框架，通过控制对齐漂移和量化误差，提供Safe@k稳定性保证。  <br/>4. **实验验证性能**：在文本-图像检索任务中达到Recall@10=0.94，在音频-图像排序中实现Safe@1=1.0，证明框架的实用性与鲁棒性。|
|2511.01892v1|[Retrieval-Augmented Multimodal Depression Detection](http://arxiv.org/abs/2511.01892v1)|**贡献点总结（100字以内）：**  <br/>提出一种基于RAG框架的抑郁检测方法，通过情感数据检索增强情感表示，提升模型可解释性与性能，在AVEC 2019数据集上达到SOTA，优于传统迁移学习和多任务学习方法。<br/><br/>---<br/><br/>**分点贡献：**<br/><br/>1. **提出新的RAG框架**：首次将Retrieval-Augmented Generation应用于抑郁检测任务，通过检索情感数据增强模型的情感理解能力。<br/><br/>2. **引入Emotion Prompt机制**：利用LLM生成情感提示（Emotion Prompt），作为辅助模态提升情感表示的丰富性和可解释性。<br/><br/>3. **解决传统方法局限**：有效缓解了高计算成本、领域不匹配和静态知识不足等问题，提升模型泛化与情感理解能力。<br/><br/>4. **达到SOTA性能**：在AVEC 2019数据集上取得优异结果，CCC达到0.593，MAE为3.95，优于现有迁移学习与多任务学习方法。|
|2511.01716v1|[SemBench: A Benchmark for Semantic Query Processing Engines](http://arxiv.org/abs/2511.01716v1)|**贡献点总结：**<br/><br/>1. 提出面向语义查询处理引擎的新型基准测试，涵盖多种场景、模态和操作符。  <br/>2. 包含电影评论分析、医学问答等多领域场景，支持图像、音频、文本等多模态数据。  <br/>3. 设计了多样化的语义操作符，如过滤、连接、映射、排序和分类。  <br/>4. 在三个学术系统和一个工业系统（Google BigQuery）上进行了评估，揭示当前系统的能力与局限。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出面向语义查询处理引擎的新基准，涵盖多种场景、模态与操作符，评估了多个系统的表现，为未来研究提供了关键方向。|
|2511.01670v1|[SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](http://arxiv.org/abs/2511.01670v1)|**贡献点**：  <br/>1. 提出首个面向东南亚多语言（印尼、泰、越）及英文、中文的大型音频语言模型（SeaLLMs-Audio），填补区域语音AI空白。  <br/>2. 实现多模态支持（纯音频、纯文本、音频融合文本）与多任务能力（涵盖音频分析、语音对话等）。  <br/>3. 构建专用评估基准SeaBench-Audio，推动东南亚LALM的自动化评测与技术发展。  <br/>4. 通过实验验证SeaLLMs-Audio在东南亚语言任务上的竞争力，优于现有模型。  <br/><br/>**总结**：  <br/>开发首个支持东南亚多语言的音频语言模型SeaLLMs-Audio，并构建专用评估基准，推动区域语音技术进步。|
|2510.26974v1|[Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction   from Doctor-Patient Consultations](http://arxiv.org/abs/2510.26974v1)|**贡献点总结：**<br/><br/>1. 提出了首个从医患对话中提取医疗指令的共享任务 MEDIQA-OE 2025。  <br/>2. 收集并提供了用于该任务的语料数据集。  <br/>3. 汇总了六支队伍的参与情况及所采用的多种方法。  <br/>4. 对比了封闭式和开放式大型语言模型在该任务中的表现。  <br/>5. 公布了最终的排行榜结果，为后续研究提供基准。|
|2510.26830v1|[SmoothGuard: Defending Multimodal Large Language Models with Noise   Perturbation and Clustering Aggregation](http://arxiv.org/abs/2510.26830v1)|**总结（100字以内）**：  <br/>本文提出 SmoothGuard，一种轻量且通用的 MLLM 防御框架，通过噪声注入和聚类预测聚合提升模型鲁棒性，实验表明其在多个基准上有效抵御对抗攻击，同时保持性能竞争力。  <br/><br/>**贡献点**：  <br/>1. **提出通用对抗图像生成方法**：在 HuggingFace 生态系统中推广对抗样本生成技术。  <br/>2. **设计轻量模型无关防御框架 SmoothGuard**：通过随机噪声注入和聚类预测聚合增强 MLLMs 的鲁棒性。  <br/>3. **实验验证有效性**：在 POPE、LLaVA-Bench（In-the-Wild）和 MM-SafetyBench 上展示对抗攻击下的稳定性与性能。  <br/>4. **确定最优噪声范围**：通过消融实验发现噪声范围 0.1-0.2 可平衡鲁棒性与实用性。|
|2510.25760v2|[Multimodal Spatial Reasoning in the Large Model Era: A Survey and   Benchmarks](http://arxiv.org/abs/2510.25760v2)|总结：  <br/>本论文系统综述了多模态空间推理任务，分类MLLMs进展，引入开放基准，探索3D空间与具身AI应用，分析音频与第一视角视频等新兴模态的作用，并提供代码与资源。  <br/><br/>贡献点：  <br/>1. **系统性综述**：首次对多模态空间推理任务进行全面总结，涵盖视觉、听觉等多模态感知与推理框架。  <br/>2. **进展分类**：系统梳理MLLMs在多模态空间推理领域的最新技术进展，分类归纳其方法和应用。  <br/>3. **开放基准引入**：提出可评估的开放基准，为研究者提供标准化测试平台。  <br/>4. **3D任务扩展**：突破传统2D任务，深入探讨3D空间中的视觉问答、定位及场景理解。  <br/>5. **具身AI研究**：总结具身AI领域进展，包括视觉-语言导航与动作模型。  <br/>6. **新兴模态分析**：分析音频和第一人称视频等新模态对空间理解的贡献，推动跨模态研究。  <br/>7. **资源开源**：提供论文引用信息、代码及基准实现，促进社区共享与复现。|
|2510.25577v1|[Lost in Phonation: Voice Quality Variation as an Evaluation Dimension   for Speech Foundation Models](http://arxiv.org/abs/2510.25577v1)|**贡献点：**<br/><br/>1. 提出首个针对语音质量（如沙哑声、气声）的非词性语音理解评估方法。  <br/>2. 引入合成语音质量数据集，用于评估语音基础模型对不同音质的敏感性。  <br/>3. 采用开放生成和情感识别任务，而非传统的多项选择题，更准确地探索语音质量对模型行为的影响。  <br/>4. 揭示语音质量在语音理解中的重要性，拓展语音基础模型的应用边界。  <br/><br/>**总结（100字以内）：**  <br/>本文首次系统探讨语音基础模型对语音质量的敏感性，提出新型合成数据集，并通过开放任务评估模型行为，揭示语音质量对情感识别和语言理解的影响。|
|2510.25502v2|[TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time   Series Forecasting](http://arxiv.org/abs/2510.25502v2)|**总结**：<br/>本文提出TempoPFN，一种基于线性RNN的无监督时间序列基础模型，通过改进架构实现高效并行训练，在zero-shot任务中超越合成数据与真实数据训练模型，开源数据生成与训练代码推动研究复现。<br/><br/>**贡献点**：<br/>1. **提出TempoPFN模型**：首个基于线性RNN的单变量时间序列基础模型，专为零样本预测设计。  <br/>2. **GatedDeltaProduct架构**：结合state-weaving技术，突破传统序列长度限制，实现完全并行化训练与推理，无需窗口划分或数据压缩。  <br/>3. **统一合成数据生成框架**：整合SDE、高斯过程、音频合成等多种生成器，并引入新颖的数据增强方法，提升数据多样性与模型泛化能力。  <br/>4. **超越现有方法性能**：在Gift-Eval基准上，显著优于所有合成数据方法及大多数真实数据训练模型，同时保持更高的计算效率。  <br/>5. **开源可复现性**：公开完整的数据生成流程与训练代码，为后续研究提供可复现的基础。|
|2510.25150v1|[Explainable Disentanglement on Discrete Speech Representations for   Noise-Robust ASR](http://arxiv.org/abs/2510.25150v1)|**贡献点：**  <br/>1. 提出在Whisper嵌入空间中分离语义语音内容与背景噪声的方法。  <br/>2. 使用代码本令牌表示干净语音，通过轻量分类器监督提取噪声向量。  <br/>3. 在保持Whisper冻结的情况下，实现82%的错误率降低。  <br/>4. 在VBDemand测试集上比基线方法提升35%的ASR性能。  <br/>5. 学习的令牌空间对常见和未见过的声学条件均有良好泛化能力。  <br/><br/>**总结：**  <br/>本论文提出一种分离语音内容与噪声的端到端模型，提升ASR性能与噪声鲁棒性。|
|2510.25069v1|[TOPol: Capturing and Explaining Multidimensional Semantic Polarity   Fields and Vectors](http://arxiv.org/abs/2510.25069v1)|**贡献点总结**：<br/><br/>1. **提出多维语义极性框架TOPol**：首次将语义极性建模为多维叙事场，突破传统一维情感分析的局限性。<br/>2. **结合半监督与HoTL机制**：通过人类参与定义上下文边界（CBs），实现动态、可交互的语义极性分析。<br/>3. **引入多模态技术组件**：集成tLLM（文本嵌入）、UMAP（方向向量投影）与Leiden聚类（主题分割），提升方法的多维建模能力。<br/>4. **量化微观语义位移**：利用方向向量计算不同话语体系间的语义偏移，捕捉非情感性语义变化。  <br/>5. **提供可解释性分析工具**：通过对比标签生成和覆盖范围估计，增强模型结果的可理解性。  <br/>6. **验证方法稳定性**：论证CB定义为唯一显著影响因素，体现方法在参数调整下的鲁棒性。  <br/>7. **跨数据集验证通用性**：在宏微观经济数据（CBs）和产品评论（NRC情感）中均表现优异，证明框架的普适性。|
|2510.24817v2|[Towards a Method for Synthetic Generation of Persons with Aphasia   Transcripts](http://arxiv.org/abs/2510.24817v2)|总结：  <br/>该研究提出两种生成合成失语症转录本的方法，并验证其在不同严重程度下的表现，发现Mistral 7b Instruct更准确地模拟了失语症的语言退化特征，为构建大规模语料库和改进AAC系统提供新思路。<br/><br/>贡献点：  <br/>1. **提出合成数据生成方法**：开发两种方法（程序化生成与基于LLM的生成）模拟失语症患者的语言表现，解决AphasiaBank数据稀缺问题。  <br/>2. **多维度生成失语症特征**：通过词删除、填充词插入和词替换等技术，生成涵盖轻度至极重度四种语言退化程度的合成转录本。  <br/>3. **验证LLM模拟效果**：发现Mistral 7b Instruct在非定向词（NDW）、词数和词长等关键指标上优于程序化方法，更真实反映失语症语言模式。  <br/>4. **提出应用方向建议**：建议构建更大语料库、微调模型以提高失语症表征精度，并由SLPs评估合成数据的实用性和真实性。|
|2510.24180v1|[V-SAT: Video Subtitle Annotation Tool](http://arxiv.org/abs/2510.24180v1)|**贡献点：**  <br/>1. 提出V-SAT，首个统一的视频字幕质量检测与修正框架。  <br/>2. 结合LLMs、VLMs、图像处理与ASR，利用多模态上下文信息。  <br/>3. 显著提升字幕质量，SUBER得分从9.6降至3.54。  <br/>4. 图像模式问题的F1-score达到约0.80。  <br/>5. 引入人机协同验证机制，确保输出高质量。  <br/><br/>**总结：**  <br/>V-SAT是首个多模态字幕质量统一解决方案，结合多种技术提升准确性与同步性，并引入人工验证确保质量。|
|2510.24178v1|[MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations](http://arxiv.org/abs/2510.24178v1)|总结（100字以内）:  <br/>本研究提出首个德语多模态讽刺检测数据集MuSaG，包含33分钟电视节目内容，涵盖文本、音频与视频。通过多模型基准测试，揭示人类与模型在检测任务中的差异，推动更合适的模型开发，并公开数据集以促进未来研究。<br/><br/>贡献点:  <br/>1. **首个德语多模态讽刺检测数据集**：MuSaG是首个针对德语环境的多模态（文本/音频/视频）讽刺语料库，涵盖33分钟电视节目内容，填补了该领域的数据空白。  <br/>2. **多模态对齐标注**：提供精确对齐的跨模态数据，支持单模态和多模态场景下的模型评估。  <br/>3. **跨模型性能对比**：系统性地对九种文本、音频、视觉及多模态模型进行基准测试，对比其与人类标注的表现差异。  <br/>4. **揭示人类与模型行为差异**：发现人类在对话场景中高度依赖音频线索，而模型在文本分析上更优，指出当前多模态模型的不足。  <br/>5. **促进研究与模型对齐**：公开数据集以推动未来关于多模态讽刺检测及人机对齐的研究。|
|2510.23896v1|[AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for   African Languages](http://arxiv.org/abs/2510.23896v1)|**贡献点：**<br/><br/>1. 提出AfriMTEB，扩展MMTEB至59种非洲语言，涵盖14个新任务和38个数据集。  <br/>2. 新增六套数据集，覆盖14-56种非洲语言，弥补非洲语言在多语言评估中的不足。  <br/>3. 引入新任务类型，如仇恨言论检测、意图检测和情感分类。  <br/>4. 开发AfriE5模型，基于mE5进行跨语言对比蒸馏，适应非洲语言。  <br/>5. 实验表明AfriE5在多个任务上优于Gemini-Embeddings和mE5等基线模型。|
|2510.23763v3|[RoboOmni: Proactive Robot Manipulation in Omni-modal Context](http://arxiv.org/abs/2510.23763v3)|**贡献点分点总结：**<br/><br/>1. 提出跨模态上下文指令新范式，意图由语音、环境音和视觉线索推断，而非显式指令。  <br/>2. 设计RoboOmni框架，融合感知、思考、对话与执行功能，实现意图识别与行动执行一体化。  <br/>3. 引入时空融合机制，增强语音与视觉信号的意图识别能力。  <br/>4. 构建OmniAction数据集，包含大量真实场景数据，用于训练机器人主动意图识别模型。  <br/>5. 在仿真与真实环境中验证，RoboOmni在成功率、推理速度与主动辅助能力上优于传统文本和语音识别基线。  <br/><br/>**总结（100字以内）：**  <br/>本文提出跨模态上下文指令框架RoboOmni，并构建OmniAction数据集，实现机器人通过语音、声音和视觉线索主动理解用户意图，显著提升任务成功率与交互效率。|
|2510.23252v2|[Are ASR foundation models generalized enough to capture features of   regional dialects for low-resource languages?](http://arxiv.org/abs/2510.23252v2)|**贡献点：**<br/><br/>1. 构建了一个包含78小时标注的孟加拉语语音到文本（STT）语料库Ben-10，用于研究方言对语音识别的影响。  <br/>2. 发现现有的语音基础模型在方言语音识别中表现不佳，无论是否进行微调。  <br/>3. 证明方言特定的模型训练能有效缓解方言语音识别的困难。  <br/>4. 提供了一个可用于语音识别算法在资源受限环境下测试的分布外（OOD）数据集。  <br/>5. 数据集和代码已公开，便于后续研究和应用。|
|2510.18915v3|[UNO-Bench: A Unified Benchmark for Exploring the Compositional Law   Between Uni-modal and Omni-modal in Omni Models](http://arxiv.org/abs/2510.18915v3)|**贡献点**：  <br/>1. **提出统一基准框架**：设计首个高质量、全面覆盖单模态与多模态能力的统一评估基准UNO-Bench，整合44种任务类型和5种模态组合。  <br/>2. **构建双数据集体系**：包含1250个跨模态可解决性达98%的高质量人工标注样本，及2480个增强单模态样本，自动压缩数据集实现90%速度提升且保持98%一致性。  <br/>3. **创新评估形式**：引入多步骤开放式问题格式，突破传统多选题限制，增强对复杂推理能力的考察。  <br/>4. **通用评分模型**：开发支持6种问题类型的自动化评分系统，准确率达95%，实现高效评估。  <br/>5. **揭示性能关联规律**：通过实验验证多模态性能与单模态能力的“组合定律”，发现弱模型中多模态为瓶颈，强模型中呈现协同促进效应。  <br/><br/>**总结**（100字以内）：  <br/>本研究提出UNO-Bench基准，统一评估单模态与多模态能力，创新多步骤开放式问题形式，并揭示多模态性能与单模态的组合规律，为多模态模型发展提供关键分析工具与理论支持。|
|2509.19902v2|[WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction](http://arxiv.org/abs/2509.19902v2)|总结：  <br/>WEST是一个基于LLM的语音工具包，具备全栈支持、简单易用和开源可复现性，提供两种实验方案，助力语音理解与交互研究。<br/><br/>贡献点：  <br/>1. **全LLM架构**：复用成熟大模型架构、生态（如Hugging Face）和方法（如序列打包），提升语音处理能力。  <br/>2. **全栈功能**：集成语音识别、合成、理解、对话及多模态能力，支持开源模型扩展。  <br/>3. **易用性设计**：采用简单直接的接口，降低使用门槛，实现通用化应用。  <br/>4. **双实验方案**：提供完全开源可复现的基线方案和大规模数据训练的高性能方案，满足不同需求。|
|2509.16648v3|[FESTA: Functionally Equivalent Sampling for Trust Assessment of   Multimodal LLMs](http://arxiv.org/abs/2509.16648v3)|总结：  <br/>提出FESTA方法，通过等效与互补输入采样实现无监督黑盒信任评估，显著提升多模态大语言模型在视觉和音频任务中的选择性预测性能，并开源代码。<br/><br/>贡献点：  <br/>1. **提出FESTA方法**：首次设计基于等效与互补输入采样的信任评估框架，用于量化多模态大语言模型生成预测的不确定性。  <br/>2. **黑盒无监督设计**：仅依赖模型输入输出接口，无需真实标签（ground truth），适用于实际部署中的模型评估场景。  <br/>3. **任务保持性采样**：通过扩展输入空间，同时检测模型的稳定性（一致性）和敏感性，提升对预测可靠性的分析能力。  <br/>4. **显著性能提升**：实验表明在视觉和音频推理任务中，FESTA分别实现33.3%和29.6%的相对性能提升（基于AUROC指标）。  <br/>5. **开源实现**：提供可复现的代码，促进方法的广泛应用和进一步研究。|