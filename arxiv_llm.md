|Source|Title|Summary|
|---|---|---|
|2507.07877v1|[Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models](http://arxiv.org/abs/2507.07877v1)|总结：  <br/>本文系统评估了八种先进PTQ方法在Whisper和Moonshine模型上的性能，揭示量化对效率与准确性的权衡，为低功耗边缘设备的ASR优化提供实用指导。<br/><br/>贡献点：  <br/>1. **系统性基准测试**：对八种SOTA PTQ方法在Whisper和Moonshine两个主流边缘ASR模型家族进行全面评估，覆盖七类开放数据集。  <br/>2. **统一框架构建**：基于LLM压缩工具包扩展，开发集成量化算法、校准与评估流程、以及分析工具的框架，提升边缘ASR模型部署效率。  <br/>3. **量化影响分析**：量化研究权重与激活的性能影响，揭示不同位宽配置（如3-bit）对高容量模型的可行性，证明其在高级PTQ技术下的有效性。  <br/>4. **实际应用指导**：为低功耗、持续运行的边缘设备优化ASR模型提供关键理论依据与实用策略，促进模型轻量化与资源约束场景下的部署。|
|2507.06329v1|[MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in   Music Mixing](http://arxiv.org/abs/2507.06329v1)|总结：  <br/>提出MixAssist数据集，填补音乐创作协作研究空白，支持AI在音乐混音中的教学与辅助功能。<br/><br/>贡献点：  <br/>1. **填补协作创作研究空白**：首次关注音乐生产中专家与业余制作者的互动教学维度，突破传统端到端自动化研究框架。  <br/>2. **构建多模态对话数据集**：创建包含431个音频-语言对的MixAssist，记录真实混音场景下的多轮协作对话，强调上下文关联性。  <br/>3. **提供模型训练与评估资源**：为音频语言模型开发提供独特数据支持，推动理解复杂音乐制作指令的能力提升。  <br/>4. **验证模型实证效果**：通过对比实验表明，基于MixAssist微调的Qwen-Audio在生成混音建议方面显著优于其他模型。  <br/>5. **推动创意辅助工具发展**：聚焦音频语境下的协作教学，推动构建支持艺术家创意过程的智能AI混音助手。|
|2507.05609v1|[MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses](http://arxiv.org/abs/2507.05609v1)|**贡献点：**  <br/>1. **提出Tri-Mamba混合块**：通过三通道Mamba架构实现原始波形级多麦克风音频融合，支持流式处理兼容性。  <br/>2. **设计帧级分割Mamba层**：增强帧级背景语音抑制能力，提升Whisper模型的微调效率。  <br/>3. **开发多尺度分组相对策略优化（GRPO）**：联合优化帧级和语句级背景语音抑制，提升整体抑制效果。  <br/><br/>**总结：**  <br/>该研究提出MMW框架，通过Tri-Mamba混合块、帧级分割Mamba层和多尺度分组相对策略优化，显著提升智能眼镜在嘈杂环境下的语音识别性能，降低词错误率4.95%。|
|2507.05591v1|[MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models](http://arxiv.org/abs/2507.05591v1)|**贡献点总结：**  <br/>1. 提出多模态大语言模型 MLlm-DR，整合小规模 LLM 与轻量级查询模块（LQ-former）实现抑郁症诊断。  <br/>2. 构建针对面试场景的多模态训练数据集，提升模型在领域任务中的逻辑推理能力。  <br/>3. 实现可解释性诊断，通过生成抑郁评分及评估理由增强临床适用性。  <br/>4. 在 CMDC 和 E-DAIC-WOZ 两个基准数据集上达到当前最优性能，验证方法有效性。  <br/><br/>**摘要总结（100字以内）：**  <br/>提出 MLlm-DR 模型，结合小 LLM 和轻量查询模块，通过多模态数据生成可解释的抑郁评分，在两个基准数据集上实现SOTA效果。|
|2507.03875v1|[Demystifying ChatGPT: How It Masters Genre Recognition](http://arxiv.org/abs/2507.03875v1)|总结：  <br/>该研究通过MovieLens-100K数据集，验证了ChatGPT在电影类型预测中的优越性，提出结合音频和视觉信息的多模态方法，提升了模型表现。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统分析ChatGPT在电影类型预测任务中的能力与局限，推动语音领域多模态应用研究。  <br/>2. **对比模型性能**：在未经微调的条件下，验证ChatGPT在类型预测任务上优于其他LLMs，并通过微调实现最佳性能。  <br/>3. **创新提示方法**：设计基于电影预告片音频转录/字幕的零样本与少量样本提示框架，覆盖1682部电影及18种类型（多重标签）。  <br/>4. **引入视觉信息**：结合IMDb电影海报与VLM，通过视觉语义增强LLM提示，探索多模态输入对类型预测的提升作用。  <br/>5. **多模态整合潜力**：证明ChatGPT在融合语音与视觉信息后仍具强大类型预测能力，凸显其在内容分析场景的应用价值。|
|2507.03343v2|[SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.03343v2)|总结：  <br/>本文提出一种结合并行语音编码器与大语言模型的多语言语音识别框架，创新性采用三阶段训练策略和语言感知提示，显著提升识别性能且不增加训练数据，优于官方基线。<br/><br/>贡献点：  <br/>1. **多语言架构创新**：首次将Whisper-large-v3与mHuBERT-147并行语音编码器结合，通过拼接输出嵌入形成统一框架，融合声学与语言知识。  <br/>2. **三阶段联合训练策略**：设计新型训练方法，同步优化语音编码器的低秩适配模块及LLM的投影参数，提升模型协同能力。  <br/>3. **语言感知提示机制**：引入针对LLM输入的语言提示，增强语言特异性文本生成，改善多语言识别效果。  <br/>4. **高效性能提升**：在盲测试集上实现11.76%的CER/WER，比官方基线高8.41，且未增加训练数据量。|
|2507.03343v1|[SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.03343v1)|总结：  <br/>本文提出一种多语言对话语音识别系统，通过并行语音编码器与大语言模型的融合，结合三阶段训练策略和语言感知提示，显著提升识别性能且无需增加训练数据。<br/><br/>贡献点：  <br/>1. **创新架构**：提出将并行语音编码器（Whisper-large-v3与mHuBERT-147）与大语言模型（LLM）结合的统一多语言ASR框架。  <br/>2. **互补知识融合**：通过拼接两个编码器的输出嵌入，使模型同时利用声学与语言知识，提升多语言识别能力。  <br/>3. **三阶段训练策略**：设计联合优化低秩适配模块、投影参数及LLM的分阶段训练方法，增强模型整体性能。  <br/>4. **语言感知提示**：在LLM输入端引入语言感知提示，针对不同语言优化文本生成效果。  <br/>5. **高效性能提升**：在盲测数据集上实现11.76%的CER/WER，超越官方基线8.41个百分点，且不依赖额外训练数据。|
|2507.03043v1|[K-Function: Joint Pronunciation Transcription and Feedback for   Evaluating Kids Language Function](http://arxiv.org/abs/2507.03043v1)|**贡献点：**<br/><br/>1. **提出K-Function框架**  <br/>   设计了一种统一的语音评估框架，专门针对儿童语言的高音调、长音和数据稀疏问题，整合了子词转录、客观评分和可操作反馈功能。<br/><br/>2. **创新Kids-WFST模型架构**  <br/>   将Wav2Vec2语音编码器与基于音素相似性（phoneme-similarity）的Dysfluent-WFST结合，实现儿童语音错误的精准捕捉，同时保持解释性。<br/><br/>3. **显著提升语音识别性能**  <br/>   在MyST和Multitudes数据集上，Kids-WFST分别达到1.39%和8.61%的音素错误率，较贪心解码器有10.47和7.06个百分点的绝对提升。<br/><br/>4. **构建LLM语言评估系统**  <br/>   利用高保真转录数据开发了LLM，可评估语言能力、里程碑、阅读及理解，与人类评估者一致，并提供可视化工具（唇舌动态）和具体建议。<br/><br/>5. **实现诊断-反馈闭环**  <br/>   通过精准音素识别建立了完整的诊断-反馈循环，推动临床适用的、可扩展的语言评估解决方案。|
|2507.02982v1|[We Need Knowledge Distillation for Solving Math Word Problems](http://arxiv.org/abs/2507.02982v1)|**贡献点分点总结：**  <br/>1. 提出基于BERT嵌入压缩的LLMs模型蒸馏方法，实现参数减少至1/12且性能保持90%以上。  <br/>2. 证明确保模型在数学问题解决任务中的高精度与强泛化能力，且蒸馏过程不依赖特定任务。  <br/>3. 揭示词性信息对数学问题处理的关键作用，突破传统认为实体识别主导的认知。  <br/>4. 为智能辅导系统提供高效、低成本的数学解题模型，推动智能教育领域发展。  <br/><br/>**总结（100字以内）：**  <br/>本文通过压缩BERT嵌入向量和模型蒸馏，构建高效数学解题模型，显著降低计算成本，同时保持高精度与泛化能力，为智能教育系统提供实用解决方案。|
|2507.02904v1|[Enhancing Sports Strategy with Video Analytics and Data Mining:   Assessing the effectiveness of Multimodal LLMs in tennis video analysis](http://arxiv.org/abs/2507.02904v1)|**贡献点：**<br/>1. **评估MLLMs在体育视频分析中的应用**：首次系统评估多模态大语言模型（MLLMs）对网球视频的处理能力，探索其在动作识别和事件理解上的有效性。  <br/>2. **填补事件序列识别的空白**：针对现有网球分析研究中缺乏对rally事件顺序识别的问题，提出模型在这一领域的改进方向。  <br/>3. **动作分类与序列分析**：验证MLLMs在分类网球动作及识别复杂动作序列（如rally过程）中的潜力，为其他体育分析任务提供参考。  <br/>4. **性能优化方法**：探讨通过调整训练策略及结合传统模型提升MLLMs性能的途径，推动多模态模型在运动分析中的实际应用。  <br/><br/>**总结（100字以内）**：  <br/>本研究评估MLLMs在网球视频分析中的应用，聚焦于事件序列识别与动作分类，提出填补研究空白的改进方法，并探索训练策略优化与模型融合路径。|
|2507.02858v1|[Requirements Elicitation Follow-Up Question Generation](http://arxiv.org/abs/2507.02858v1)|总结：  <br/>本研究提出了一种基于常见面试错误类型的框架，利用GPT-4o生成实时访谈问题，通过实验证明其在清晰度、相关性和信息性方面不亚于人类，并在指导框架下表现更优，为提升需求获取效率提供了新方法。<br/><br/>贡献点：  <br/>1. **提出框架识别常见面试错误类型**：构建了针对面试者常见失误的分类体系，为后续生成优化问题提供理论基础。  <br/>2. **开发基于语音的实时问题生成方法**：结合访谈者的口语输入，动态生成更贴合需求的后续问题，提升访谈针对性。  <br/>3. **设计双实验验证LLM生成效果**：  <br/>   - 第一实验对比LLM生成与人类撰写问题的质量（清晰度、相关性、信息性）；  <br/>   - 第二实验验证在使用错误类型框架指导下LLM生成问题的优化效果。  <br/>4. **实验证明LLM生成问题的有效性**：结果表明，LLM生成的问题在关键指标上不逊于人类，且指导后表现更优，证明其在需求获取中的潜力。  <br/>5. **为实时访谈辅助工具提供新思路**：展示了LLM在动态支持面试者提高访谈质量中的应用价值，推动语音技术与软件工程的结合。|
|2507.02530v1|[Open-Source System for Multilingual Translation and Cloned Speech   Synthesis](http://arxiv.org/abs/2507.02530v1)|总结（100字以内）:  <br/>该研究提出了多语言翻译与语音再生的开源系统，集成Whisper和VAD技术，结合双LLM处理流程与语音克隆TTS模块，实现高效、自然的跨语言沟通解决方案，并通过灵活部署与社区共享提升应用可及性。<br/><br/>贡献点分点列出:<br/>1. 开源系统设计：构建支持多语言翻译与语音再生的集成化系统，解决跨语言沟通和无障碍访问问题。  <br/>2. 技术整合创新：融合Whisper语音识别、VAD说话段检测与双LLM处理（分割+翻译）技术，形成端到端流程。  <br/>3. 语音再生技术：采用带语音克隆功能的TTS模块，实现保留原说话人身份和自然性的语音重构。  <br/>4. 部署灵活性：支持本地运行和API调用，适用于Zoom实时翻译、公共广播、蓝牙播放等多种场景。  <br/>5. 社区共享与验证：发布开源项目并提供系统性能分析（延迟、词准确率），验证其在现实多语言环境中的适用性。|
|2507.02282v1|[Content filtering methods for music recommendation: A review](http://arxiv.org/abs/2507.02282v1)|**贡献点：**  <br/>1. 指出协作过滤在音乐推荐中的局限性，特别是因用户-歌曲互动稀疏导致的推荐效果下降。  <br/>2. 系统梳理内容过滤在解决协作过滤偏差中的作用，强调其在音乐领域的重要性。  <br/>3. 提出基于大语言模型（LLMs）的歌词分析与音频信号处理的歌曲分类方法，拓展内容过滤技术路径。  <br/>4. 分析不同分析方法（文本与音频）间的潜在冲突，并提供融合策略或模型优化的解决方向。  <br/><br/>**总结：**  <br/>该文探讨音乐推荐系统中协作过滤的局限性，强调内容过滤在缓解偏差中的作用，提出了结合LLMs和音频处理的分类方法，并分析了不同方法间的冲突及解决途径。|
|2507.00672v1|[Toward Edge General Intelligence with Multiple-Large Language Model   (Multi-LLM): Architecture, Trust, and Orchestration](http://arxiv.org/abs/2507.00672v1)|总结：  <br/>该综述系统总结了多大模型在边缘计算中的应用，提出关键使能技术与多模态架构，强调可信性，并指出未来研究方向。<br/><br/>贡献点：  <br/>1. **提出多LLMs整合框架**：首次系统探讨多大模型在边缘计算场景中协作处理复杂动态任务的技术路径，弥补单模型在资源约束下的性能缺陷。  <br/>2. **梳理技术演进路线**：从传统边缘AI到单LLM部署，再到多LLM系统的演变过程，为研究提供历史背景与发展方向。  <br/>3. **关键使能技术分析**：总结动态编排、资源调度、跨域知识迁移等核心技术，解决多LLM系统在边缘端的实现难题。  <br/>4. **多模态架构设计**：提出针对文本、图像、音频等多模态数据的多LLM协同机制，提升综合分析能力。  <br/>5. **可信性系统研究**：聚焦边缘环境中隐私与可靠性需求，构建可信多LLM系统的理论框架与实践方向。|
|2506.23930v1|[Leveraging the Potential of Prompt Engineering for Hate Speech Detection   in Low-Resource Languages](http://arxiv.org/abs/2506.23930v1)|**贡献点总结（100字以内）**：  <br/>提出针对低资源语言的6种提示策略，创新性设计"比喻提示"突破LLM安全机制；在Bengali语言分析并验证方法有效性，拓展至Hindi、English和German语言；结合多模型和嵌入方法对比实验，引入环境影响因素（CO₂排放、能耗）作为评估指标。<br/><br/>**分点贡献**：  <br/>1. **提出6种提示策略**：系统研究零样本提示、拒绝抑制、分类器引导、多样本提示、角色提示及创新性"比喻提示"，针对低资源语言的仇恨言论检测难题。  <br/>2. **突破LLM安全机制**：首次通过"比喻提示"绕过LLM内置安全机制，区别于现有破解方法，提升模型对敏感内容的识别能力。  <br/>3. **多语言验证有效性**：在Bengali语言构建实验框架后，扩展验证至Hindi（低资源）、English和German（高资源），证明方法通用性。  <br/>4. **多模型对比分析**：综合评估Llama2-7B模型与MLP、CNN、BiGRU深度学习模型的性能，结合GloVe、Word2Vec、FastText词嵌入方法。  <br/>5. **引入环境评估指标**：首次将F1分数与环境影响因子（CO₂排放、能耗、计算时间）结合，量化仇恨言论检测的绿色计算需求。|
|2506.23774v1|[Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate   Incidents Management](http://arxiv.org/abs/2506.23774v1)|**贡献点总结：**<br/>- 提出基于LLM的多智能体系统，结合检索增强提示与人格建模，模拟真实仇恨情境以提升教师培训效果。<br/>- 系统具备识别仇恨言论模式、预测潜在升级及提出干预策略的功能，提供情境化学习支持。<br/>- 突破传统培训的时空局限与成本障碍，实现教师对冲突动态的理解与应对能力的提升。<br/>- 通过试点验证教师对标注分歧和上下文作用的认知深化，进而制定更有效课堂应对策略。<br/>- 探索LLM在教育领域（仇恨管理）的创新应用，为计算机辅助教师培训提供新范式。<br/><br/>**摘要总结（100字以内）：**  <br/>本研究开发了基于多智能体大语言模型的系统，用于模拟仇恨事件管理情境，提升教师应对能力。系统结合检索增强与人格建模，支持动态分析和策略制定，解决了传统培训的时空与成本限制，并通过试点验证其有效性。|
|2506.23714v1|[Towards an Automated Multimodal Approach for Video Summarization:   Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](http://arxiv.org/abs/2506.23714v1)|总结（100字以内）:  <br/>该论文提出多模态视频摘要框架，通过融合文本、音频与视觉信息识别关键语义与情感时刻，并引入"bonus words"概念提升摘要质量，实验验证在文本和视频指标上均显著优于传统方法。<br/><br/>贡献点分点:<br/>1. 创新性提出行为感知的多模态整合方法，综合文本、音频、视觉多模态特征生成时序对齐的视频摘要<br/>2. 首次定义"bonus words"概念，识别跨模态强调的关键词以增强摘要的语义相关性和表达清晰度<br/>3. 构建伪真实标签评估体系，采用LLM提取方法生成基准对比，验证框架效果显著优于传统单模态方法<br/>4. 实验结果展示多模态技术在视频摘要任务中的优势，文本指标ROUGE-1提升31.7%，BERTScore提升3.8%，视频F1-Score提升23%|
|2506.23094v1|[TOMI: Transforming and Organizing Music Ideas for Multi-Track   Compositions with Full-Song Structure](http://arxiv.org/abs/2506.23094v1)|总结：  <br/>提出TOMI框架，结合指令调优基础模型与四维空间表示，实现音乐概念层次化生成与人机交互创作，实验表明在结构连贯性与生成质量上优于基线方法。<br/><br/>贡献点：<br/>1. **引入概念层次结构建模**：首次将音乐创作中的“概念层次”（如音乐动机生成、转化与组织）作为核心研究点，突破传统时间结构分析，强调跨时空的音乐组织逻辑。  <br/>2. **提出TOMI框架**：设计TOMI（Transforming and Organizing Music Ideas）新方法，结合深度学习与语言模型（LLM），实现多轨电子音乐的结构化生成。  <br/>3. **构建四维空间表示**：形式化多轨道音乐生成过程为稀疏的四维空间（clip-片段、section-时间位置、track-音轨、transformation-转化方法），提供结构化建模基础。  <br/>4. **实现人机协同创作**：将TOMI模型集成至REAPER数字音频工作站，支持交互式的人工与AI协作音乐创作。  <br/>5. **验证生成效果**：通过实验验证TOMI在电子音乐生成中显著提升结构连贯性与整体质量，较基线方法表现更优。|
|2506.23049v1|[AURA: Agent for Understanding, Reasoning, and Automated Tool Use in   Voice-Driven Tasks](http://arxiv.org/abs/2506.23049v1)|总结：  <br/>AURA是首个集成语音、多轮对话与工具使用的开源系统，通过模块化设计和开放权重模型实现高效任务执行，性能超越现有开源系统并接近GPT-4o。<br/><br/>贡献点：  <br/>1. **填补技术空白**：首次提出支持全语音到语音、多轮对话、工具调用及智能推理的开源系统AURA。  <br/>2. **级联架构整合**：将开放权重的ASR、TTS和LLM串联构建端到端语音对话系统，实现复杂任务处理。  <br/>3. **模块化扩展能力**：支持通过自然语言提示和动作类别灵活集成新工具（如日历、搜索、邮件等）。  <br/>4. **性能优势**：在VoiceBench测试中，OpenBookQA得分92.75%（高于所有开源系统），AlpacaEval得分4.39（与开源系统竞争）。  <br/>5. **实际有效性验证**：人类评估显示在复杂多轮语音任务中达成90%任务成功率，证明系统实用性。|
|2506.22679v1|[Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions](http://arxiv.org/abs/2506.22679v1)|**修改后的贡献点总结（100字以内）**：  <br/>本文验证了LLM在团队对话中检测微行为的可行性，发现解码器-only模型（如Llama-3.1）优于编码器-only模型，并提出指令微调策略提升性能，对高风险环境下的团队沟通分析具有应用价值。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **验证LLM在团队对话微行为检测中的可行性**：基于模拟任务的转录数据，探索了LLM在识别微行为（如负面语言）中的潜力。  <br/>2. **系统比较不同微调策略**：对比了零样本分类、传统微调、同义词增强微调（编码器-only）与少样本文本生成（解码器-only）方法的效果差异。  <br/>3. **发现模型架构差异的影响**：编码器-only模型（如RoBERTa）在检测低频微行为（如负面语言）时表现不佳，而指令微调的解码器-only模型（Llama-3.1）性能更优。  <br/>4. **提出改进方案**：通过指令微调策略显著提升解码器-only模型的微行为分类能力，达到3-way分类44%和binary分类68%的F1分数。  <br/>5. **强调实际应用价值**：为高风险场景（如航天任务）中基于文本数据的团队沟通分析技术提供了新的研究方向和方法参考。|
|2506.22554v2|[Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale   Dataset](http://arxiv.org/abs/2506.22554v2)|**总结（100字以内）:**  <br/>本研究提出Seamless Interaction Dataset，开发多模态模型以生成符合人类语音的双人行为，并引入可控变体和评估方法，推动虚拟代理与人机交互技术的发展。<br/><br/>**贡献点分点列出:**  <br/>1. **提出Seamless Interaction Dataset**  <br/>   - 构建首个大规模多模态人脸交互数据集（4,000小时视频，4,000名参与者），涵盖多样场景，为研究双人行为动态提供基础资源。  <br/><br/>2. **开发双人动态生成模型**  <br/>   - 创新性结合语音与视觉行为，设计模型生成与人类对话同步的运动手势和面部表情，支持跨模态输入。  <br/><br/>3. **引入可控模型变体**  <br/>   - 提出可调节情感响应和表达强度的模型，增强生成动作的语义相关性与个性化适配能力。  <br/><br/>4. **建立模型评估体系**  <br/>   - 提出针对双人动态生成模型的评估方法，推动更自然、智能化的人机交互技术发展。|
|2506.22554v1|[Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale   Dataset](http://arxiv.org/abs/2506.22554v1)|总结：  <br/>本研究提出首个大规模双人交互数据集，开发多模态生成模型及可控变体，建立评估体系，推动虚拟代理与人机交互技术发展。<br/><br/>贡献点：  <br/>1. **数据集构建**：创建Seamless Interaction数据集，包含超过4,000小时多场景真实面对面互动视频（4,000名参与者）。  <br/>2. **模型生成**：设计可生成与语音对齐的双人动作手势和面部表情的多模态模型，支持输入对话者的行为数据。  <br/>3. **可控性增强**：开发可调节情感响应、表达强度及生成语义相关手势的模型变体，提升交互灵活性。  <br/>4. **评估方法**：提出针对双人运动模型的质量评估方法，促进更自然、高效的AI交互系统实现。|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|**贡献点（分点列出）：**  <br/>1. **提出综合语音对话评估基准**：构建首个针对端到端语音交互的专用基准，填补多模态LLM在语音领域评估的空白。  <br/>2. **增强数据集多样性**：系统收集真实场景对话数据，涵盖多样的语音属性、声学条件及语音特有的现象（如韵律、同音词、结巴）。  <br/>3. **设计查询感知评估方法**：引入定制化评估检查清单与提示，提升自动评估的准确性与场景适配性。  <br/>4. **揭示模型性能差异**：通过全面测试主流语音模型，量化其在不同语音场景下的表现差异。  <br/>5. **提供开发指导**：基准可为语音模型的优化与实际应用中的用户体验提升提供关键参考。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个端到端语音交互评估基准，通过多样化真实数据和查询感知方法，揭示主流语音模型在不同场景下的性能差异，为模型优化与实际应用提供科学指导。|
|2506.21864v2|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v2)|**总结（100字以内）**  <br/>DeepTalk通过MoE架构实现自适应模态专家学习，解决了语音-文本数据不足导致的模型性能下降问题，在保持低延迟的同时，有效降低性能损失，与模块化模型相当，且开源代码便于复用。<br/><br/>**贡献点分点**  <br/>1. **提出DeepTalk框架**：基于MoE架构设计适应性模态专家学习方法，缓解语音-文本数据不足对模型训练的制约。  <br/>2. **性能优化**：仅导致5.5%的性能损失，显著优于传统native MLLMs（如GLM-4-Voice）的平均20%以上降损。  <br/>3. **模态协同训练**：通过分阶段的单模态训练与联合多模态协作训练，保留丰富的副语言特征（情感、语调）。  <br/>4. **低延迟交互**：端到端对话延迟控制在0.5秒内，提升语音交互的实时性与流畅性。  <br/>5. **开源与可复用性**：公开代码和模型，为语音领域研究提供参考和实践基础。|
|2506.21864v1|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v1)|总结：  <br/>提出DeepTalk框架，通过自适应多模态专家学习与MoE架构有效缓解语音-文本数据不足导致的模型性能下降，实现与原始LLM相近的效率并保持低延迟对话体验。<br/><br/>贡献点：  <br/>1. **提出新的框架**：基于MoE架构设计DeepTalk，实现自适应多模态专家学习，解决传统Native MLLMs因数据不足导致的灾难性遗忘问题。  <br/>2. **分阶段训练策略**：通过模态负载区分专家，结合单模态特训与多模态协作训练，优化模型对语音和文本的联合处理能力。  <br/>3. **性能提升**：相比传统Native MLLMs（如GLM-4-Voice）性能下降仅5.5%，显著优于平均20%的水平，且与模块化方法性能相当。  <br/>4. **低延迟交互**：端到端对话延迟控制在0.5秒以内，提升语音交互的实时性与流畅度。  <br/>5. **开源实现**：提供代码和模型，促进方法复现与行业应用。|
|2506.20666v2|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v2)|总结：  <br/>该研究提出基于认知模型的框架，系统分析LLMs中价值权衡的动态变化，揭示其在推理努力与RL训练下的不同偏好，并为模型训练与行为调控提供理论指导与实践启示。<br/><br/>贡献点：  <br/>1. **引入认知模型框架**：首次将认知科学中的效用权衡理论应用于LLMs，分析其在复杂社交情境中的价值决策机制。  <br/>2. **系统性评估模型设置**：针对前沿黑盒模型的推理“努力”程度和开源模型的RL后训练动态，设计双场景实验框架。  <br/>3. **揭示效用权衡差异**：发现推理模型更偏向信息效用，开源模型在数学推理表现更强，突显LLMs在不同任务下的价值倾向。  <br/>4. **分析训练动态的影响**：指出模型早期训练存在显著效用值变化，且基础模型和预训练数据的选择对长期表现有持久影响。  <br/>5. **提供方法论应用价值**：方法适配LLM快速演变的生态，为构建高阶行为假设、优化训练策略和控制价值权衡提供新思路。|
|2506.20666v1|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v1)|总结：  <br/>本研究提出利用认知模型分析LLMs在价值权衡中的表现，揭示了不同训练策略下效用分配差异，为理解模型行为、优化训练机制及控制价值冲突提供新视角。<br/><br/>贡献点：  <br/>1. **引入认知模型框架**：首次将认知科学中的价值权衡模型（如礼貌语言模型）应用于LLMs，系统分析其在社交情境中处理矛盾目标的能力。  <br/>2. **双维度模型评估**：针对前沿黑盒模型的推理"努力"程度与开源模型的RL对齐动态，提出统一的评估方法，探索不同场景下的效用函数分配机制。  <br/>3. **发现效用偏向规律**：揭示推理模型更注重信息效用而非社会效用，同时指出开源模型在数学推理上的优势，为模型能力差异提供实证依据。  <br/>4. **揭示训练动态影响**：证明模型训练早期存在显著的效用值变化，且基础模型和预训练数据选择对效用分配具有持续影响，优于反馈数据或对齐方法的作用。  <br/>5. **方法泛化与应用价值**：提出的方法可适配LLMs快速演进的生态，为推测高阶行为、设计训练策略及平衡模型价值冲突提供理论支持和实践指导。|
|2506.19835v1|[MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration](http://arxiv.org/abs/2506.19835v1)|**总结（100字以内）:**  <br/>提出MAM框架，通过模块化多智能体分工提升多模态医疗诊断效率与性能，实现知识更新优化和跨模态协同，实验证明其显著优于基线模型。<br/><br/>**贡献点分点:**  <br/>1. **提出 Modular Multi-Agent Framework (MAM) 框架**  <br/>   - 首次将多模态医疗诊断任务分解为可协作的模块化智能体（General Practitioner, Specialist Team, Radiologist, Medical Assistant, Director），提升系统灵活性与可扩展性。<br/><br/>2. **优化知识更新与利用**  <br/>   - 通过模块化设计降低知识更新成本，有效整合现有医疗LLM和知识库，支持动态知识迭代。<br/><br/>3. **跨模态诊断性能提升**  <br/>   - 在文本、图像、音频、视频等多模态数据集上，MAM显著超越模态特定LLM的性能（提升幅度18%-365%），验证其跨模态协同优势。<br/><br/>4. **量化实验验证有效性**  <br/>   - 基于广泛公开数据集的对比实验，证明MAM在多模态医疗诊断任务中的优越性，为领域研究提供基准。<br/><br/>5. **开放代码促进复现与研究**  <br/>   - 开源实现（GitHub链接），便于学术界和工业界验证与改进该框架，推动技术应用。|
|2506.19732v1|[Who Does What in Deep Learning? Multidimensional Game-Theoretic   Attribution of Function of Neural Units](http://arxiv.org/abs/2506.19732v1)|总结（100字以内）:  <br/>本文提出MSA框架，通过博弈论方法量化神经网络单元对高维输出的贡献，适用于不同规模模型，揭示了正则化、语言专家分布及GAN的倒置生成层级，为深度模型的解释、编辑与压缩提供新工具。<br/><br/>贡献点：<br/>1. **提出模型无关的解释框架**：开发Multiperturbation Shapley-value Analysis (MSA)，首次将Shapley值理论扩展至量化神经网络单元对高维输出（如语音、图像、文本）的贡献。<br/>2. **实现输出维度对齐的贡献分析**：生成Shapley Modes，提供与模型输出（像素、token、logits）完全一致维度的单元贡献地图。<br/>3. **跨模型尺度验证有效性**：将方法应用于多层感知机（MLP）、大型语言模型（Mixtral-8x7B）和生成对抗网络（GAN），展示其广泛适用性。<br/>4. **揭示神经网络结构特征**：通过实验证明正则化集中计算资源、语言模型中存在语言特定专家、GAN中的生成层级存在倒置现象。<br/>5. **拓展应用潜力**：为深度模型的**解释、编辑与压缩**提供理论支持和实用工具，推动模型可理解性研究。|
|2506.19502v1|[MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications](http://arxiv.org/abs/2506.19502v1)|总结：  <br/>本文提出MATE多模态无障碍MAS系统，支持灵活模态转换和本地部署，提升残疾人数字交互体验，并引入精准的ModCon-Task-Identifier模型，实现跨领域应用与隐私保护。<br/><br/>贡献点：  <br/>1. **开发开源多模态MAS系统**：MATE支持模态转换（如图像转音频描述），解决传统系统封闭设计导致的定制化不足问题。  <br/>2. **跨领域灵活性**：兼容多种模型（LLM API、自定义ML分类器）与硬件，适应不同行业（如医疗）和用户需求。  <br/>3. **本地运行保障隐私**：确保敏感信息在本地处理，保密性强，适用于机构技术集成（如医疗系统）。  <br/>4. **任务识别模型创新**：提出ModCon-Task-Identifier，可精准提取用户输入的模态转换任务，实验表现优于其他模型。  <br/>5. **开源代码与数据**：提供公开资源，促进研究复现与应用拓展。|
|2506.19073v1|[MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral   Reasoning of LLMs through Hate Speech Multi-hop Explanation](http://arxiv.org/abs/2506.19073v1)|总结：  <br/>提出多语言道德推理评估数据集MFTCXplain，揭示当前LLMs在道德情感预测和跨文化解释上的局限性，推动更透明、文化包容的道德评估研究。<br/><br/>贡献点：  <br/>1. **多语言道德推理基准**：构建包含葡萄牙语、意大利语、波斯语和英语的3,000条推文数据集，支持跨文化道德能力评估。  <br/>2. **细粒度标注体系**：引入二元仇恨言论标签、道德类别及文本片段级解释，提升评估的透明度与可解释性。  <br/>3. **理论驱动设计**：基于道德基础理论（MFT）设计任务，明确道德推理的评估框架。  <br/>4. **揭示模型局限性**：通过实证发现LLMs在道德情感预测（F1<0.35）和非主流语言解释对齐上表现显著不足，强调其内化人类道德推理的能力有限。|
|2506.18576v1|[A Modular Taxonomy for Hate Speech Definitions and Its Impact on   Zero-Shot LLM Classification Performance](http://arxiv.org/abs/2506.18576v1)|**贡献点总结（100字以内）**  <br/>本文提出仇恨言论定义的分类体系，分析其14个核心要素，并通过零样本实验评估不同定义对LLM性能的影响，揭示定义差异对模型效果的非一致性影响。<br/><br/>**分点贡献：**  <br/>1. **理论贡献**  <br/>   - 构建了一个包含14个概念元素的仇恨言论定义分类体系（Taxonomy），系统整理了文献中对仇恨言论的多维度表述（如目标类型、后果等），解决其定义模糊问题。  <br/><br/>2. **实验贡献**  <br/>   - 首次在三种仇恨言论数据集（合成、人工参与、真实场景）上，对三类大语言模型（LLMs）进行基于不同定义的零样本性能评估，发现定义的特异性程度对模型效果存在显著但非一致的影响。|
|2506.18274v1|[Leveraging Large Language Models for Information Verification -- an   Engineering Approach](http://arxiv.org/abs/2506.18274v1)|总结：  <br/>提出基于GPT-4o的自动化多媒体新闻源验证方法，整合多模态数据处理与交叉验证技术，减少人工干预。<br/><br/>贡献点：  <br/>1. **多模态数据整合**：首次将图像、视频、音频等多类型数据纳入新闻源验证流程，提升信息全面性。  <br/>2. **LLM为核心架构**：采用GPT-4o作为验证管道的骨干模型，实现端到端自动化处理。  <br/>3. **流程自动化**：通过提示工程完全自动化元数据生成、数据分帧、特征筛选及交叉验证等步骤。  <br/>4. **关键帧筛选机制**：开发基于重要性排序的帧选择策略，优化视频分析效率。  <br/>5. **轻量人工干预**：仅需人类参与最终结果验证，显著降低人工成本与操作复杂性。|
|2506.17715v1|[Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource   Medieval Romance Languages](http://arxiv.org/abs/2506.17715v1)|总结：  <br/>本研究系统分析中世纪罗曼语词性标注的挑战，评估多种技术方法，揭示LLMs在历史语言处理上的局限性，并探索有效解决方案，涵盖多领域与多语言文本。<br/><br/>贡献点分点：  <br/>1. **系统性研究历史语言变体挑战**：首次对中世纪奥克语、西班牙语、法语文本的词性标注进行多维度分析，揭示历时性语言演变、拼写非标准化和数据稀缺等问题对模型性能的影响。  <br/>2. **多技术方法对比实验**：系统评估微调、提示工程、模型架构、解码策略及跨语言迁移学习等技术对词性标注准确率的影响，提供方法选择的实证依据。  <br/>3. **多领域与多语言覆盖**：基于包含宗教、圣徒传、医学、饮食等领域的跨语言语料库（Medieval Occitan、Spanish、French），验证技术的泛化能力与适应性。  <br/>4. **提出针对性解决方案**：发现特定技术（如跨语言迁移学习）在低资源历史语言场景中的有效性，为优化历史语言处理提供新思路。|
|2506.17410v1|[Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A   Feasibility Study](http://arxiv.org/abs/2506.17410v1)|**贡献点总结：**  <br/>1. 提出利用生成式AI分析真实数学辅导的可行性与可扩展性方法；  <br/>2. 识别并评估"有效表扬"和"纠正学生数学错误"两项关键辅导行为；  <br/>3. 验证多模型（GPT-4系列、Gemini-1.5-pro、LearnLM）在辅导行为检测与评估中的可靠性；  <br/>4. 设计成本效益高的提示策略以提升AI在教育评估中的实用性；  <br/>5. 促进LLM在AI支持学习研究中的可复现性与实际应用价值。<br/><br/>**研究摘要总结（100字以内）：**  <br/>开发生成式AI方法分析真实数学辅导行为，验证多模型在检测表扬与纠错效果上的可靠性，提出高效提示策略，推动AI在教育评估中的可扩展应用与研究创新。|
|2506.16575v1|[Advancing Harmful Content Detection in Organizational Research:   Integrating Large Language Models with Elo Rating System](http://arxiv.org/abs/2506.16575v1)|**贡献点总结（100字内）：**  <br/>提出基于Elo评分的改进方法，提升LLM在有害内容分析中的性能，通过两个数据集验证其在准确性、精确率与F1分数上的优势，减少误报并增强可扩展性，支持职场骚扰检测与构建安全工作环境等组织应用。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：设计Elo评分机制，优化LLM对有害内容（如微歧视、仇恨言论）的分析能力，解决传统系统过度审查的问题。  <br/>2. **数据集验证**：在微歧视检测和仇恨言论分析的两个数据集上验证方法有效性，证明其优于传统提示技术与常规机器学习模型。  <br/>3. **性能提升**：显著提高关键指标（准确率、精确率、F1分数）表现，减少误报并增强结果可靠性。  <br/>4. **可扩展性**：支持大规模数据集处理，提升实际应用的可行性。  <br/>5. **应用价值**：推动组织研究中职场骚扰识别、毒性沟通评估及安全环境构建等实际场景的应用。|
|2506.16528v1|[Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility   Metrics Using Phonetic, Semantic, and NLI Approaches](http://arxiv.org/abs/2506.16528v1)|**贡献点：**  <br/>1. **提出新的ASR评估指标**：整合自然语言推理（NLI）评分、语义相似性和语音相似性，克服传统指标（如WER、CER）对语音可懂度的不足。  <br/>2. **验证有效性**：在Speech Accessibility Project数据集上实现与人类判断的0.890高相关性，证明该指标优于现有方法。  <br/>3. **强调评估方向**：指出应优先关注语音内容的可懂度而非单纯依赖错误率，为语音识别系统优化提供理论依据。  <br/><br/>**总结：**  <br/>本研究提出一种结合NLI、语义和语音相似性的新型ASR评估指标，显著提升与人类判断的相关性，推动语音可懂度评估从错误率向内容理解转变。|
|2506.16008v1|[ChatAR: Conversation Support using Large Language Model and Augmented   Reality](http://arxiv.org/abs/2506.16008v1)|**贡献点总结：**  <br/>本研究提出了一种结合HMD-AR与LLMs的实时对话支持系统，通过关键词识别、信息生成与可视化增强交流效率；创新性地引入眼动控制策略，避免暴露用户阅读行为；实验验证了该系统在提升对话平衡性与互动兴奋度方面的有效性。  <br/><br/>**分点贡献：**  <br/>1. **系统设计**：构建集成HMD-AR与LLMs的实时对话辅助系统，实现关键词识别、信息生成及动态可视化呈现。  <br/>2. **眼动优化方法**：开发基于自然眼动模式的信息展示策略，降低用户阅读行为被对话方察觉的风险。  <br/>3. **实验验证**：通过两组对照实验，证实所提方法能提升对话平衡性与用户感知的交流兴奋度。|
|2506.14973v1|[Thinking in Directivity: Speech Large Language Model for Multi-Talker   Directional Speech Recognition](http://arxiv.org/abs/2506.14973v1)|**贡献点总结**  <br/>（100字以内）：  <br/>提出directional-SpeechLlama模型，结合智能眼镜麦克风阵列实现定向语音识别、声源定位和旁瓣干扰抑制，创新性引入S-DOT和CDDA技术，有效提升空间音频理解能力。  <br/><br/>**分点贡献**  <br/>1. **首个结合麦克风阵列的定向Speech LLM**：  <br/>   利用智能眼镜的多通道麦克风阵列，首次实现针对声源方向的语音识别、定位及旁瓣干扰抑制，突破传统Speech LLM对空间信息处理的局限。<br/><br/>2. **创新性训练方法S-DOT**：  <br/>   提出串联方向输出训练（Serialized Directional Output Training），通过结构化方向信息增强模型对文本与空间音频关系的建模能力。<br/><br/>3. **对比方向数据增强技术CDDA**：  <br/>   引入对比方向数据增强（Contrastive Direction Data Augmentation），通过对比学习策略提升模型在复杂声学环境下的鲁棒性与泛化能力。<br/><br/>4. **实验验证性能优势**：  <br/>   在语音识别和声源定位任务中，模型表现出显著性能提升，证明了文本线索与空间音频关系建模的有效性。|
|2506.14903v1|[DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct   Preference Optimization](http://arxiv.org/abs/2506.14903v1)|总结（100字以内）：  <br/>本文提出DPO-Kernels，通过混合损失、核化表示和分歧选择三个方向提升T2I模型的对齐性能，并构建首个包含社会偏见数据的DETONATE基准，引入AQI指标评估对齐质量，验证HT-SR机制的有效性，公开数据与代码。<br/><br/>贡献点分点：  <br/>1. **DPO-Kernels框架**：首次将DPO思想扩展至T2I系统，提出混合损失（结合嵌入与概率目标）、核化表示（RBF、多项式、小波核）和分歧选择（Wasserstein与Rényi分歧）三重改进，提升生成内容的对齐性与鲁棒性。  <br/>2. **DETONATE基准**：构建首个大规模T2I对齐基准，包含10万对精心筛选的图像对，系统性涵盖种族、性别、残疾三种社会偏见类别，用于评估模型公平性与安全性的隐性漏洞。  <br/>3. **AQI指标**：提出基于几何度量的对齐质量指数（Alignment Quality Index），量化潜在空间中安全/危险图像激活的分离程度，揭示T2I模型的隐藏偏见问题。  <br/>4. **HT-SR机制**：通过实验证明DPO-Kernels结合重尾自正则化（Heavy-Tailed Self-Regularization）可维持强泛化边界，并公开完整代码与数据集以促进研究复现。|
|2506.14028v2|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v2)|总结：  <br/>提出首个多语言多模态金融基准MultiFinBen，包含新颖的跨语言与OCR嵌入任务，设计动态难度选择机制，并揭示现有LLMs在复杂金融场景下的局限性。<br/><br/>贡献点：  <br/>1. **构建首个全球化金融多模态基准**：MultiFinBen是首个支持多语言（中、英、西等）和多模态（文本、视觉、音频）的金融领域基准，全面评估LLMs的跨模态与跨语言能力。  <br/>2. **引入双跨任务**：设计PolyFiQA-Easy/PolyFiQA-Expert（多语言复杂推理任务）和EnglishOCR/SpanishOCR（OCR嵌入的金融问答任务），填补金融领域多语言与多模态任务的空白。  <br/>3. **动态难度筛选机制**：提出基于难度感知的样本选择方法，而非简单合并现有数据集，确保基准数据集的平衡性与有效性。  <br/>4. **揭示模型局限性**：通过评估22个SOTA模型，发现即便具备多模态能力的LLMs仍难以处理金融领域的复杂跨语言和跨模态任务。  <br/>5. **开放数据促进研究**：公开MultiFinBen数据集，推动金融NLP和语音应用研究的透明性、可复现性与全球化发展。|
|2506.14028v1|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v1)|总结：  <br/>本文提出首个多语言多模态金融基准MultiFinBen，包含新任务与难度评估机制，为金融领域模型评估提供全面且挑战性的测试平台。<br/><br/>贡献点：  <br/>1. **首创多语言多模态基准框架**：构建首个覆盖文本、视觉、音频三模态及多语言场景的金融领域基准MultiFinBen，填补现有单语单模态研究的不足。  <br/>2. **引入双新颖任务**：  <br/>   - *PolyFiQA*（易/专家级）：首次提出多语言金融复杂推理任务，支持混合语言输入；  <br/>   - *EnglishOCR/SpanishOCR*：首次设计OCR嵌入的金融问答任务，要求模型从视觉文本中提取与推理信息。  <br/>3. **动态难度选择机制**：提出基于模型能力的难度自适应筛选方法，确保基准的挑战性与公平性。  <br/>4. **精炼平衡数据集**：通过精细化数据筛选而非简单数据集拼接，构建紧凑且跨领域平衡的测试集合。  <br/>5. **揭示模型能力瓶颈**：实验证明主流模型在多模态与跨语言金融任务中存在显著性能局限，为后续研究提供方向。|
|2506.13894v1|[EmoNews: A Spoken Dialogue System for Expressive News Conversations](http://arxiv.org/abs/2506.13894v1)|**总结**  <br/>该论文提出了一种基于上下文情感调节的新闻对话语音系统，结合LLM与PromptTTS技术，设计新评估指标，并开源代码。<br/><br/>**贡献点**  <br/>1. **开发整合情感调控的SDS**：首次将情感分析与任务导向的语音对话系统结合，通过上下文线索生成更具同理心的新闻对话。  <br/>2. **创新情感语音合成技术**：提出基于PromptTTS的上下文匹配情感语音生成方法，提升对话的情感适配性。  <br/>3. **构建主观评估体系**：设计专门用于情绪调控的主观评价量表，填补社会目标评估的空白，并验证系统效果。  <br/>4. **开源实现与可复现性**：提供完整代码库，支持相关研究的复现与扩展。|
|2506.13252v1|[Vector Ontologies as an LLM world view extraction method](http://arxiv.org/abs/2506.13252v1)|总结：  <br/>首次提出并验证向量本体方法，构建音乐领域的高维几何框架，并证明LLM内部可解析的结构化音乐知识与真实数据的对齐性及提示语相关性。<br/><br/>贡献点：  <br/>1. **首个实证验证**：首次通过实验验证向量本体框架（vector ontologies）可将LLM的高维神经表示转化为可解释的几何结构。  <br/>2. **构建音乐领域向量本体**：基于Spotify音频特征构建8维音乐流派向量本体，提供具体领域的应用案例。  <br/>3. **内部分布一致性分析**：证明LLM的音乐世界模型在不同语言提示下仍保持高空间一致性，且与真实音频特征分布高度对齐。  <br/>4. **提示语与空间变化的关联**：揭示提示语措辞直接影响LLM推理的向量本体空间位置，证明方法的可控性和可验证性。  <br/>5. **方法应用前景**：提出向量本体作为提取和分析LLM内部结构化知识的有效工具，推动可解释AI在语音领域的应用。|
|2506.12936v1|[CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team   Reflection in Action During Clinical Operation](http://arxiv.org/abs/2506.12936v1)|**总结（100字以内）：**  <br/>本研究提出CliniDial数据集，包含多模态临床操作数据，通过实验揭示现有大模型在处理真实临床场景中面临的挑战，推动相关算法发展，并开源代码促进研究。<br/><br/>**贡献点：**  <br/>1. **构建多模态临床操作数据集**：首次整合手术模拟中的音频、转录、生理信号及双视角视频，涵盖标签不平衡、自然交互等复杂特性。  <br/>2. **提出行为代码标注框架**：基于现有方法对团队协作过程进行结构化标注，为分析临床对话与行为模式提供标准化工具。  <br/>3. **评估LLMs在临床场景中的表现**：系统测试现有大模型对CliniDial数据的处理能力，揭示其在真实临床任务中的局限性。  <br/>4. **开源数据与代码**：推动数据共享，促进语音与多模态技术在医疗领域的研究与应用。|
|2506.12744v1|[Rethinking Hate Speech Detection on Social Media: Can LLMs Replace   Traditional Models?](http://arxiv.org/abs/2506.12744v1)|总结（100字以内）:  <br/>本文提出IndoHateMix数据集，验证了大语言模型在处理多语言、混合语言仇恨言论检测中的优势，挑战传统BERT模型的主导地位，并探讨未来研究应侧重模型优化还是数据增强。<br/><br/>贡献点分点列出：<br/>1. **构建多语言仇恨言论数据集**：提出IndoHateMix，首次系统性收录印度语境下的印地语-英语混合语言（code-mixing）和转写（transliteration）文本，填补了该领域的高质量标注数据空白。<br/>2. **验证大语言模型性能优势**：实验证明LLMs（如LLaMA-3.1）在少量数据微调下仍显著优于专门训练的BERT模型，展现更强的泛化能力与跨语言适应性。<br/>3. **揭示模型发展新方向**：提出未来研究需在模型优化与数据多样性之间权衡的思考，引发对通用LLMs替代领域专用模型的讨论。<br/>4. **推动复杂场景下的NLP应用**：为评估多语言环境下仇恨言论检测的鲁棒性提供现实基准，促进应对非正式、文化嵌入性语言的NLP技术发展。|
|2506.12537v1|[Speech-Language Models with Decoupled Tokenizers and Multi-Token   Prediction](http://arxiv.org/abs/2506.12537v1)|总结：  <br/>本文提出解耦分词器设计、多token预测机制及speaker-aware生成范式，优化语音-文本跨模态对齐与生成效率，建立RoleTriviaQA基准测试，显著提升语音语言模型的性能与说话人一致性。<br/><br/>贡献点：  <br/>1. **系统分析SLM关键组件**：首次系统研究语音分词器、语音头、说话人建模等组件对LLM-centric语音模型的影响。  <br/>2. **解耦分词器设计**：提出耦合、半解耦、全解耦分词器对比实验，证明解耦分词可显著提升跨模态对齐与语音合成质量。  <br/>3. **多token预测机制**：引入MTP解决语音与文本信息密度差异，实现12倍解码速度提升与3.01的词错误率降低。  <br/>4. **Speaker-aware生成范式**：构建RoleTriviaQA大型基准测试，涵盖多样说话人身份，提升知识理解与说话人一致性。|
|2506.12502v1|[Towards Fairness Assessment of Dutch Hate Speech Detection](http://arxiv.org/abs/2506.12502v1)|**贡献点：**  <br/>1. **构建荷兰语社会群体术语列表**：首次系统梳理反映荷兰社会背景的歧视性社会群体术语，为后续研究提供基础数据。  <br/>2. **生成反事实数据**：利用大语言模型（LLMs）和MGS、SLL策略构建荷兰语仇恨言论的反事实数据集，解决语言特定挑战。  <br/>3. **微调模型提升检测性能**：通过反事实数据微调Transformer模型，验证其在荷兰语仇恨言论检测任务中的有效性。  <br/>4. **提出综合公平性评估框架**：引入CTF与群体公平性指标（equality of odds、demographic parity），量化模型在反事实公平性与群体层面的公平表现。  <br/><br/>**总结：**  <br/>该研究聚焦荷兰语仇恨言论检测，提出反事实公平性评估方法，构建语言特定数据集，验证模型性能，填补文献空白并提供改进建议。|
|2506.11842v2|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v2)|**贡献点总结（100字以内）：**  <br/>提出PACE-ADS框架，通过多代理协作实现自动驾驶系统的人机双向通信，提升舒适度、安全性和个性化体验，可扩展集成于现有平台，弥补技术自主与人本需求的鸿沟。<br/><br/>**分点贡献：**  <br/>1. **提出人机中心自主框架**：PACE-ADS通过整合心理学与认知模型，解决自动驾驶车辆缺乏双向人机交互的问题，提升个性化体验和应对复杂场景的能力。  <br/>2. **三代理协同机制**：Driver Agent处理外部环境，Psychologist Agent解析心理信号（如EEG、心率）与语音指令，Coordinator Agent整合信息生成高阶决策，增强系统响应性。  <br/>3. **分层架构设计**：在语义规划层部署，保留低级控制权给原系统，确保兼容性与灵活性，避免对现有模块的重构。  <br/>4. **多场景验证**：通过闭环模拟测试，在复杂交通场景（如交叉路口、行人交互）中验证框架的有效性，展示提升的舒适度与安全性。  <br/>5. **最小化集成改造**：框架可无缝集成至现有自动驾驶平台，减少对原有系统的调整，提高实际部署的可行性。|
|2506.11842v1|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v1)|总结：  <br/>本文提出PACE-ADS框架，通过整合语音等认知指令与心理状态感知，实现人机协同的自动驾驶系统，提升舒适度与安全性，展示大语言模型在人车交互中的应用潜力。<br/><br/>贡献点：  <br/>1. **提出跨模态人车交互框架**：首次构建融合外部交通环境感知与内部乘客心理/认知状态理解的人机协同自动驾驶系统（PACE-ADS），填补自动化驾驶与人类需求的gap。  <br/>2. **设计语音认知代理模块**：开发专门处理乘客语音指令（如语义理解）与心理信号（如EEG、心率）的Psychologist Agent，实现多模态意图解析。  <br/>3. **实现行为级自主决策**：通过Coordinator Agent整合感知信息，支持自主驾驶逻辑生成与安全恢复策略，无需替代传统模块。  <br/>4. **验证语音引导有效性**：在交通信号、行人、施工区等场景的仿真测试中，证明语音交互可提升驾驶风格适应性与用户舒适度。  <br/>5. **推动LLM应用落地**：探索基于大语言模型（LLM）的框架在自动驾驶领域的人类中心化驱动潜力，为未来人车协作提供新思路。  <br/><br/>（注：原文实际属于自动驾驶领域，但按语音视角提炼了其中与语音指令处理、多模态交互、LLM应用等相关内容作为贡献）|
|2506.11558v2|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v2)|**贡献点：**<br/>1. **提出Temporal-aware Fuseformer架构**：采用分层双流结构，逐步捕捉多模态的时间动态信息，并有效融合视觉与听觉特征。  <br/>2. **设计全局残差机制**：减少空间冗余的同时保留关键语义信息，提升计算效率与模型性能。  <br/>3. **四阶段渐进训练方法**：分步骤增强多模态对齐、语义接地和时间推理能力，系统性优化模型表现。  <br/>4. **构建时序语义数据集**：通过GPT生成带时序对齐的问答对，扩展现有数据集用于时间监督任务。  <br/>5. **实验证明优越性**：在视频问答与时间定位任务中超越现有方法，尤其在精确时间对齐与推理上表现突出，为高效视频语言建模提供新方向。  <br/><br/>**总结：**  <br/>DaMO通过分层双流与全局残差结构、四阶段训练及GPT增强数据集，在视频语言任务中显著提升时间推理能力，推动数据高效模型发展。|
|2506.11558v1|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v1)|**贡献点总结**（100字以内）:  <br/>提出DaMO数据高效视频语言模型，创新性采用分层双流时序感知架构与全局残差机制，设计四阶段渐进训练方法，构建增强数据集，并在时序任务中实现性能突破。  <br/><br/>**分点贡献**:  <br/>1. **模型设计**：提出DaMO，专为准确时序推理和多模态理解优化，提升数据效率。  <br/>2. **架构创新**：首创Temporal-aware Fuseformer，采用分层双流结构，分别捕捉视频/音频时序动态，有效融合多模态信息。  <br/>3. **效率优化**：引入全局残差机制，减少空间冗余的同时保留关键语义信息。  <br/>4. **训练方法**：提出四阶段渐进式训练范式，逐步强化多模态对齐、语义 grounding 和时序推理能力。  <br/>5. **数据贡献**：构建多个增强数据集，基于现有数据集添加GPT生成的时序相关问答对，支持时序监督任务。  <br/>6. **实验验证**：在时序定位和视频问答基准测试中，DaMO显著优于现有方法，尤其在精确时序对齐任务中表现突出。|
|2506.11125v1|[ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone   Scams](http://arxiv.org/abs/2506.11125v1)|**贡献点分点列出：**  <br/>1. **识别关键漏洞**：首次定位语音钓鱼（vishing）攻击链中的ASR转录环节为最大薄弱点，指出其对攻击成功的关键作用。  <br/>2. **提出ASRJam框架**：设计了一种主动防御系统，通过向目标音频注入对抗扰动，干扰攻击者的ASR并切断攻击反馈循环。  <br/>3. **改进对抗扰动方法**：提出EchoGuard，利用自然语音现象（如回声、混响）作为扰动源，实现对ASR的破坏性干扰，同时保持人类语音可懂性。  <br/>4. **实证评估**：通过39人用户实验验证EchoGuard的实用性，对比现有攻击技术，证明其在ASR干扰与用户体验平衡上的最优性。  <br/><br/>**总结（100字以内）：**  <br/>论文提出针对语音钓鱼的双防御策略：ASRJam主动干扰ASR转录，EchoGuard利用自然扰动阻断攻击而不影响人声理解。通过用户实验验证了EchoGuard在攻防效果与可用性上的优越性。|
|2506.10789v1|[FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](http://arxiv.org/abs/2506.10789v1)|**贡献点总结**（100字以内）:  <br/>本研究提出首个美国社会背景下的Neo-fascist文本编码方案，结合NLP与政治科学，构建大规模标注数据集并开发分类模型，揭示该言论在论坛中的广泛存在，强调社会背景的重要性，呼吁持续对抗以维护民主。<br/><br/>**分点贡献**:<br/>1. **首创编码方案**：提出首个针对美国社会语境的Neo-fascist话语编码体系，由政治科学学者主导，填补了NLP与政治学在该领域的交叉研究空白。  <br/>2. **跨学科方法论**：首次将政治学与NLP结合，构建系统性框架以识别和分析Neo-fascist言论，推动多学科协作研究。  <br/>3. **大规模标注数据**：通过众包技术标注1000条文本，创建首个公共可用的Neo-fascist话语标注数据集，为后续研究提供基础。  <br/>4. **模型验证与对比**：对SLMs和LLMs分别进行微调与测试，开发首个可用于Neo-fascist文本分类的模型，验证不同规模模型的效果差异。  <br/>5. **现象分析与警示**：揭示Neo-fascist言论在特定论坛中的普遍性，为理解其传播特征和对民主社会的威胁提供实证依据。  <br/>6. **研究伦理说明**：明确研究仅针对文本内容，不涉及对个人或组织的标签化，强调方法的中立性与合规性。|
|2506.10779v1|[Improving Named Entity Transcription with Contextual LLM-based Revision](http://arxiv.org/abs/2506.10779v1)|**贡献点：**  <br/>1. **提出LLM修订机制**：设计基于大语言模型（LLM）的命名实体修正方法，结合LLM的推理能力和局部上下文信息（如课程笔记）改进ASR对命名实体的识别。  <br/>2. **引入NER-MIT-OpenCourseWare数据集**：发布包含45小时MIT课程音频的开放数据集，支持命名实体识别任务的开发与测试。  <br/>3. **显著性能提升**：在所提出数据集上，该技术使命名实体的词错误率（WER）降低最高达30%，验证了方法的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出LLM驱动的命名实体修正方法，结合上下文信息提升ASR准确性，并发布专用数据集，实验证明在命名实体识别任务中WER降低30%。|
|2506.10504v1|[Beyond Single-User Dialogue: Assessing Multi-User Dialogue State   Tracking Capabilities of Large Language Models](http://arxiv.org/abs/2506.10504v1)|**贡献点分点总结：**  <br/>1. **构建多用户对话数据集**：基于言语行为理论，扩展现有DST数据集，生成第二用户的话语，模拟真实多用户交互场景。  <br/>2. **提出可控评估框架**：系统性地将第二用户话语融入对话，设计方法论以评估LLMs在多用户DST中的鲁棒性。  <br/>3. **揭示性能局限性**：实验表明，LLMs在多用户DST任务中表现显著下降，凸显其在处理多重说话者时的不足。  <br/>4. **指导未来研究方向**：强调需优化LLMs以应对多用户场景，推动更真实、鲁棒的对话状态跟踪模型发展。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过构建多用户DST数据集并设计可控评估框架，揭示了LLMs在处理复杂多用户对话中的性能局限，为未来改进多用户对话理解模型提供了方向。|
|2506.10245v1|[ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in   Portuguese](http://arxiv.org/abs/2506.10245v1)|**贡献点总结（100字以内）**  <br/>1. 构建首个大规模葡萄牙语细粒度仇恨言论语料库，覆盖九个法律保护少数群体。  <br/>2. 提出四阶段合成数据生成流程，包含人工种子、少样本扩展、改写增强和领域平衡。  <br/>3. 实现跨领域泛化能力，验证在多种任务和数据集上的有效性。  <br/>4. 公开数据促进低资源环境下合成数据和仇恨言论检测研究。|
|2506.10202v1|[Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual   Text-to-Video Retrieval](http://arxiv.org/abs/2506.10202v1)|**贡献点：**<br/>1. 提出Q2E方法：首个针对复杂现实事件的零样本多语言文本到视频检索框架，通过LLMs和VLMs提取隐式参数知识实现查询-事件分解。  <br/>2. 多模态输入兼容：支持视觉和语音输入的联合处理，展示跨模态知识融合的有效性。  <br/>3. 熵融合策略：引入熵基融合评分机制，提升零样本多模态信息整合能力。  <br/>4. 音频增强效果：验证音频信息在文本到视频检索中的显著提升作用。  <br/>5. 可泛化性：方法可适应不同数据集、领域及模型架构（LLMs/VLMs）。  <br/>6. 开源贡献：公开代码与数据，推动后续研究。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Q2E，通过LLMs和VLMs提取隐式知识实现零样本多语言文本到视频检索，支持视觉/语音输入融合，并验证音频信息的增强效果。方法具备良好的泛化性，已开源促进研究。|
|2506.09983v2|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v2)|**贡献点总结**  <br/>（100字以内）  <br/>提出分步指令策略与简化输出格式，显著提升多语言依赖分析性能，避免幻觉污染；揭示显式推理步骤对跨语言泛化的重要性，为LLM解析提供可扩展、格式一致的替代方案。<br/><br/>**详细贡献点**  <br/>1. **分步推理策略**：首次将通用词性标注作为预处理步骤，优先预测句法主语和依存标签，提升结构有效性与准确性。  <br/>2. **输出格式优化**：设计简化的CoNLL-U格式，避免传统基于括号的解析方法的污染与冗余问题。  <br/>3. **多语言微调增强**：证明多语言微调可同时提升跨语言泛化性能，扩大模型适用范围。  <br/>4. **SOTA性能验证**：在17种语言的Universal Dependencies数据集上实现当前最优的解析准确率，无幻觉或污染。  <br/>5. **可扩展性方案**：提供格式一致、易于扩展的解析框架，替代传统基于括号的Approach，推动LLM在语音领域的应用。|
|2506.09983v1|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v1)|总结（100字以内）:  <br/>提出一种基于逐步指令策略的依存解析方法，结合通用词性标注和简化输出格式，在17种语言中实现SOTA性能，同时通过多语言微调提升跨语言泛化能力，验证了显式推理步骤对LLM解析的有效性。<br/><br/>贡献点:  <br/>1. **创新的逐步指令策略**：首次将通用词性标注作为先验步骤，结合句法头和依存关系预测，提升解析的结构有效性与准确性。  <br/>2. **简化输出格式设计**：采用类似CoNLL-U的轻量化输出格式，减少冗余信息干扰，避免模型生成错误或污染（hallucination）。  <br/>3. **多语言性能突破**：在17种语言的Universal Dependencies数据集上均取得领先的准确率，证明方法的普适性。  <br/>4. **跨语言泛化优化**：通过多语言微调，显著提升模型在不同语言间的迁移能力，增强通用性。  <br/>5. **方法对比与可扩展性**：提出一种格式一致、可扩展的替代方案，优于传统基于括号的解析方法，推动下游任务的兼容性。|
|2506.09707v2|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v2)|**总结（100字以内）:**  <br/>本研究提出一种基于预训练模型和LoRA微调的自动PE疗法依从性评估方法，通过音频-文本分析精准定位治疗关键阶段，显著提升效率，具有临床培训与质量监控的应用潜力。<br/><br/>**贡献点：**  <br/>1. **方法创新**：首次将LoRA技术应用于PE疗法依从性评估，通过30秒音频-转录窗口微调Qwen2-Audio模型，实现关键环节的自动时间定位。  <br/>2. **标签生成机制**：结合LLM提示与人工评分者验证，生成并验证三阶段（P1/P2/P3）依从性标签，提升标注准确性。  <br/>3. **软监督策略**：提出基于任务提示的软监督训练框架，直接预测归一化边界偏移，优化模型适应性。  <br/>4. **参数分析研究**：系统评估窗口大小及LoRA秩对性能的影响，揭示上下文粒度与模型调优的重要性。  <br/>5. **应用价值**：构建可扩展的PE疗法依从性追踪框架，为临床培训、监督及质量保证提供自动化工具。|
|2506.09707v1|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v1)|总结（100字以内）:  <br/>提出基于预训练模型和LoRA技术的自动PE治疗遵循定位方法，实现高精度识别与可扩展的临床应用。<br/><br/>贡献点:  <br/>1. **方法创新**：首次将LoRA技术应用于预训练音频-语言模型（Qwen2-Audio）的微调，实现对PE治疗关键环节的自动时间定位。  <br/>2. **标签生成**：通过LLM提示和人工验证结合，为三个核心协议阶段（P1/P2/P3）生成高质量的遵循标签。  <br/>3. **软监督策略**：设计任务特定提示引导的软监督框架，提升边界预测的准确性（MAE为5.3秒）。  <br/>4. **参数分析**：系统研究窗口大小与LoRA秩对性能的影响，揭示上下文粒度与模型适配的关键作用。  <br/>5. **实际应用**：构建可扩展的框架，支持PE治疗的临床培训、监督与质量评估，解决传统人工评估的效率问题。|
|2506.09391v1|[Comparing human and LLM politeness strategies in free production](http://arxiv.org/abs/2506.09391v1)|总结：  <br/>本研究揭示大语言模型在礼貌策略上虽能复制人类偏好并被偏好，但存在过度依赖消极策略、导致误解的偏差，突显AI系统在语用对齐中的关键挑战。<br/><br/>贡献点：  <br/>1. 系统分析LLM与人类在礼貌策略使用上的差异，验证大规模模型（≥70B参数）可复制计算语用学关键偏好。  <br/>2. 发现人类评估者在开放性任务中更倾向接受LLM生成的礼貌回应，表明模型在部分场景下的表现接近人类。  <br/>3. 指出LLM在积极语境中过度依赖消极礼貌策略，可能引发语用误解，揭示模型行为的局限性。  <br/>4. 强调语用对齐在AI系统中的重要性，为未来研究提供方向性启示。|
|2506.09349v1|[OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment](http://arxiv.org/abs/2506.09349v1)|总结（100字以内）:  <br/>提出OmniDRCA并行语音-文本模型，采用双分辨率表示和对比对齐，实现SOTA性能并在全双工对话场景中展示应用潜力。<br/><br/>贡献点：  <br/>1. 提出OmniDRCA模型，基于联合自回归建模实现并行生成语音和文本，突破传统分离式生成的局限。  <br/>2. 引入双分辨率语音表示（高低频特征分开编码），提升对语音细节和整体语义的理解能力。  <br/>3. 设计对比交叉模态对齐机制，增强语音与文本之间的对应关系和互模态感知。  <br/>4. 在口语问答基准测试中取得优于现有平行模型的SOTA性能，并验证其与交织模型的竞争力。  <br/>5. 探索框架在全双工对话场景中的扩展性，推动语音生成技术向更复杂交互任务迁移。|
|2506.09301v1|[$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for   Figurative Language Understanding](http://arxiv.org/abs/2506.09301v1)|贡献点：<br/>1. 提出$(RSA)^2$框架，首次将修辞策略纳入RSA模型以处理隐喻语言<br/>2. 实现无需建模非字面表达动机的意图理解机制，突破传统RSA框架限制<br/>3. 开发PragMega+新数据集并验证框架有效性，取得LLMs在讽刺识别任务上的SOTA性能<br/>4. 建立可扩展的语音理解范式，实现字面与意图语义的兼容性解读<br/><br/>总结：该研究提出$(RSA)^2$框架，通过建模修辞策略实现无需动机建模的隐喻理解，在新数据集上取得LLMs讽刺识别的最先进性能。|
|2506.08967v2|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v2)|总结（100字以内）:  <br/>提出全端到端LALM Step-Audio-AQAA，结合双代码库分词、130B参数模型与声码器，创新训练方法，显著提升语音控制能力，并建立新基准测试验证效果。  <br/><br/>**贡献点分项:**  <br/>1. **端到端语音交互模型**：首次设计直接生成自然语音响应的AQAA专用LALM (Step-Audio-AQAA)，突破传统文本依赖的瓶颈。  <br/>2. **双代码库音频分词器**：融合语言与语义特征提取，增强对音频内容的理解与建模能力。  <br/>3. **超大规模模型架构**：采用1300亿参数LLM作为骨干，显著提升生成质量与复杂任务处理能力。  <br/>4. **训练方法创新**：提出交错token输出机制，结合DPO与模型合并技术，优化语义连贯性与语音合成效果。  <br/>5. **基准测试与性能验证**：开发StepEval-Audio-360基准，证明模型在语音控制等关键指标上优于现有SOTA方法，凸显声码器的必要性。|
|2506.08967v1|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v1)|**贡献点：**  <br/>1. **提出全端到端的AQAA模型（Step-Audio-AQAA）**：首次设计专门针对音频查询-音频回答（AQAA）任务的端到端大型音频语言模型，克服传统模型依赖文本输出的局限。  <br/>2. **双代码本音频分词器设计**：引入语言与语义双重编码机制，有效提取语音中的结构化与高层次语义特征。  <br/>3. **大参数量骨干模型与神经声码器结合**：采用1300亿参数的LLM作为主干，并搭配神经声码器，实现高保真语音合成与控制。  <br/>4. **混合训练策略优化**：通过文本与音频交错输出增强语义连贯性，并结合Direct Preference Optimization（DPO）与模型合并技术提升整体性能。  <br/>5. **语音控制任务的性能突破**：在StepEval-Audio-360基准测试中，显著优于现有SOTA模型，验证了模型在语音控制方向的有效性。  <br/>6. **突出token-based声码器的关键作用**：强调声码器在端到端AQAA任务中的核心地位，为未来研究提供新的技术视角。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Step-Audio-AQAA，通过双代码本分词器、大参数模型和神经声码器实现端到端音频对话，结合交错训练与DPO优化，并在语音控制任务中超越现有SOTA模型。|
|2506.08633v1|[Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs](http://arxiv.org/abs/2506.08633v1)|**贡献点：**<br/>1. 提出基于小连接模块（connector module）的跨模态桥接方法，整合语音编码器（如WavLM-large）与大语言模型（如OLMo/LLaMA）的表示空间。<br/>2. 推动全开源和开放数据方案，采用WavLM-large和OLMo等公开模型实现端到端DST系统。<br/>3. 系统性消融实验分析了不同微调策略（全量/LoRA适配器）与对话轮次影响，验证了关键组件的有效性。<br/>4. 引入基于模糊匹配的输出后处理技术，显著提升对话槽值中的命名实体识别性能。<br/>5. 创新性结合SpokenWOZ数据集与Speech-Aware MultiWOZ数据集，提升训练数据多样性与模型表现。<br/>6. 在 SpokenWOZ 标准测试集上取得 34.66% JGA 的 SOTA 结果，并通过 Gemma-2-9B-instruct 实现更高精度（42.17% JGA）。<br/><br/>**总结（100字以内）：**  <br/>本研究通过连接模块融合语音与语言模型，结合开源组件与数据增强策略，系统性优化DST方法。实验证明其在SpokenWOZ数据集上达到SOTA性能，并进一步通过大模型提升至更高水平。|
|2506.08593v1|[Hateful Person or Hateful Model? Investigating the Role of Personas in   Hate Speech Detection by Large Language Models](http://arxiv.org/abs/2506.08593v1)|总结（100字以内）:  <br/>本文首次系统研究MBTI性格特质对LLM仇恨言论分类的系统性影响，发现人格特征显著改变模型输出，并提出优化标注流程的建议，强调对公平性与价值观对齐的重要性。<br/><br/>贡献点:  <br/>1. **首次研究框架**：提出首个基于MBTI人格特质的个性提示（persona prompt）系统性框架，用于分析其对仇恨言论分类的影响。  <br/>2. **人类标注验证**：通过大规模人类标注调查，实证证明MBTI维度显著影响标注行为，揭示人格特质在主观任务中的关键作用。  <br/>3. **多模型跨数据集评估**：在三个主流仇恨言论数据集上，评估四个开源LLM的输出差异，验证人格提示对模型表现的调控效应。  <br/>4. **识别偏差现象**：发现人格驱动导致的三类问题：与真实标签不一致、跨人格标注分歧、logit层级的隐性偏差，凸显模型与人类价值观的潜在差距。  <br/>5. **实践指导意义**：提出需谨慎设计人格提示以提升LLM标注流程的公平性，为伦理引导和模型对齐提供理论依据。|
|2506.08147v1|[Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models](http://arxiv.org/abs/2506.08147v1)|总结：  <br/>本研究构建首个乌尔都语-英语-西班牙语三语仇恨言论检测数据集，提出融合注意力机制与大语言模型的方法，显著提升多语言分类性能，为全球数字社区安全提供有效解决方案。<br/><br/>贡献点：  <br/>1. **构建首个三语仇恨言论数据集**  <br/>   - 收集10,193条跨语言推文（英语3,834；乌尔都语3,197；西班牙语3,162），通过关键词过滤确保平衡标注（4,849 Hate/5,344 Not-Hate）。  <br/>   - 弥补乌尔都语在仇恨言论研究中的空白，尤其缺乏基于翻译的语料支持。  <br/><br/>2. **提出混合模型方法论**  <br/>   - 结合注意力层（预处理）与大语言模型（如GPT-3.5 Turbo、Qwen 2.5 72B），优化多语言特征提取。  <br/>   - 明确区分传统模型（TF-IDF + SVM）与Transformer模型（BERT/RoBERTa）的对比实验。  <br/><br/>3. **验证高性能结果**  <br/>   - 在英语、西班牙语及乌尔都语中分别实现87%、85%、81%的F1得分，多语言联合模型达88%，均优于SVM基线（提升8.75%-7.32%）。  <br/>   - 通过Fleiss' Kappa（0.821）确保标注一致性，提升数据可靠性。  <br/><br/>4. **推动多语言检测应用**  <br/>   - 提供可扩展框架，支持跨语言仇恨言论识别，促进全球社交媒体安全治理。|
|2506.07726v1|[Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription   with RAG-based Correction and Predicted BLEU](http://arxiv.org/abs/2506.07726v1)|**贡献点：**  <br/>1. 构建了首个包含801小时、高质量的长文本形式瑞士议会语料库（751小时通过质量控制），支持瑞士德语多小时辩论会的语音-文本对齐。  <br/>2. 提出分两阶段的GPT-4o校正流程：第一阶段修正ASR误识别（如专有名词），第二阶段评估语义完整性，提升文本准确性。  <br/>3. 引入基于BLEU分数和LLM评估的自动化过滤机制，优化数据质量并确保语义一致性。  <br/>4. 验证了结合高精度ASR、LLM校正与数据驱动过滤方法对低资源、领域专用语音语料库的显著效果（BLEU提升6分）。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了大尺寸瑞士议会长文本语音语料库，通过优化ASR与LLM校正流程提升数据质量，验证了多阶段校正与过滤方法对低资源语料的有效性，实现BLEU显著提升，为瑞士德语语音研究提供重要资源。|
|2506.07707v1|[Interaction Analysis by Humans and AI: A Comparative Perspective](http://arxiv.org/abs/2506.07707v1)|总结：  <br/>本研究通过比较MR与2D视频会议对儿童手势猜谜游戏交流的影响，揭示了MR在增强互动质量与协作学习中的潜力，同时探讨了LLMs在儿童互动分析中的效率与局限性。<br/><br/>贡献点：  <br/>1. **对比研究**：首次系统分析MR（HoloLens）与2D视频会议（Zoom）在儿童协作任务中的交互差异。  <br/>2. **LLMs应用**：开发基于大语言模型的自动化分析框架，实现注释、翻译及迭代校正，提升数据处理效率。  <br/>3. **交互特征发现**：发现MR促进更丰富的非语言交流（如情感表达），而Zoom则以简洁性和可访问性为优势。  <br/>4. **跨平台洞见**：为分布式教育场景下的协作学习设计提供实证依据，强调MR在增强沉浸感与参与度方面的潜力。|
|2506.06775v1|[They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse](http://arxiv.org/abs/2506.06775v1)|总结：  <br/>本研究首次构建意大利政治演讲的IMPAQTS语料库，系统评估LLMs在处理隐含内容（预设与暗示）上的局限性，并提出改进方向，同时开源数据与代码。<br/><br/>贡献点：  <br/>1. **首次构建专用语料库**：创建包含意大利政治演讲及操纵性隐含内容标注的IMPAQTS语料库，填补该领域研究空白。  <br/>2. **设计双重评估任务**：通过多项选择和开放生成任务，全面测试LLMs对隐含内容的解析能力。  <br/>3. **揭示模型局限性**：证明当前LLMs在政治语境下的隐含内容理解存在显著缺陷，识别其缺失的关键pragmatic能力。  <br/>4. **提出改进方向**：指出增强模型对高度隐含语言处理能力的潜在研究路径。  <br/>5. **开放数据与代码**：提供数据集和实现代码，促进该领域的进一步研究与应用。|
|2506.06113v1|[Bridging the Gap: In-Context Learning for Modeling Human Disagreement](http://arxiv.org/abs/2506.06113v1)|**贡献点分点总结**  <br/>1. **探索LLMs在主观任务中的多视角建模能力**：首次系统研究LLMs是否能通过上下文学习（ICL）捕捉多角度观点，并反映主观注释中存在的分歧（如仇恨言论检测）。  <br/>2. **提出多标签建模策略的对比实验**：对比了聚合硬标签与拆分硬标签/软标签在零样本和小样本场景下的效果，揭示了不同标签策略对模型表现的影响。  <br/>3. **分析小样本提示优化方法**：评估了基于文本相似度（BM25、PLM）、注释分歧（熵）和组合排名的演示选择方法，以及随机与课程式（curriculum-based）示例排序策略的有效性。  <br/>4. **揭示LLMs建模主观性的局限**：发现零样本设置下多视角生成可行，但小样本场景难以全面体现人类判断，提示设计和演示选择对性能具有显著影响。  <br/>5. **强调改进方向**：指出需构建更视角敏感、具备社会智能的LLMs，以更好应对主观性任务中的注释分歧问题。  <br/><br/>**总结（100字以内）**：  <br/>该研究探讨LLMs在主观任务中捕捉多视角与注释分歧的潜力，通过对比多种标签策略和提示优化方法，揭示模型在零样本与小样本场景下的差异，并提出改进LLMs社会智能性的方向。|
|2506.06066v1|[Conversational Interfaces for Parametric Conceptual Architectural   Design: Integrating Mixed Reality with LLM-driven Interaction](http://arxiv.org/abs/2506.06066v1)|总结：  <br/>本论文提出一种基于对话的MR界面，整合语音、手势与多智能体LLM，实现参数化建模的直观操作，降低设计门槛，推动MR向生成设计平台发展。<br/><br/>贡献点：  <br/>1. **提出新型对话式MR交互框架**：首次将语音输入、手势识别与多智能体大语言模型（LLM）系统结合，形成面向参数化建模的自然交互范式。  <br/>2. **动态参数状态管理机制**：通过对话与上下文提示解决命令歧义，实现参数的实时动态调整与状态跟踪。  <br/>3. **降低设计认知与操作壁垒**：简化参数化工作流程，使无编程背景的设计师可高效探索和优化设计空间。  <br/>4. **扩展MR作为生成设计平台**：将MR环境从空间交互工具升级为支持程序化思维的创造性设计平台。|
|2506.05796v1|[Diarization-Aware Multi-Speaker Automatic Speech Recognition via Large   Language Models](http://arxiv.org/abs/2506.05796v1)|**贡献点总结（100字以内）**：  <br/>提出融合说话人聚类与大语言模型的多说话人语音识别系统，保留绝对时间信息，提升多语言对话与复杂多说话人场景的识别性能，验证了LLM作为统一后端在联合发言分割和转录中的潜力。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新框架**：设计了一种结合说话人聚类（diarization）与大语言模型（LLM）的多说话人语音识别（MS-ASR）系统，解决重叠语音转录难题。  <br/>2. **保留时序信息**：不同于传统序列化输出训练（SOT）方法，通过整合帧级说话人和语义嵌入，保留绝对时间信息以适应时间敏感场景。  <br/>3. **多模态输入处理**：框架同时处理结构化diarization输入与帧级嵌入，实现段级转录输出，提升识别粒度与准确性。  <br/>4. **多语言与复杂场景适应性**：实验表明系统在多语言对话及高重叠多说话人会议场景中均表现优异，验证其鲁棒性与实用性。  <br/>5. **统一后端潜力**：强调LLM作为统一后端在联合发言分割与转录任务中的优势，推动语音处理与自然语言处理的融合。|
|2506.05706v1|[Bridging the Modality Gap: Softly Discretizing Audio Representation for   LLM-based Automatic Speech Recognition](http://arxiv.org/abs/2506.05706v1)|总结（100字以内）:  <br/>本文提出将向量量化整合到LLM的ASR系统中，通过软离散化方法提升模型对跨域音频的处理能力，揭示了其作为模态桥梁的潜力。<br/><br/>贡献点:  <br/>1. **提出VQ与LLM结合的方法**：解决音频连续性与LLM离散token范式之间的鸿沟，实现音频表示与语言模型的对齐。  <br/>2. **构建基于LLM嵌入表的VQ codebook**：利用预训练LLM的嵌入表作为量化字典，降低模型训练复杂度并增强跨模态一致性。  <br/>3. **设计软离散化技术**：通过动态更新codebook和加权求和策略，生成更符合语言结构的离散音频representation。  <br/>4. **验证有效性与泛化性**：实验表明该方法在out-of-domain场景下显著优于基线，为LLM-based ASR提供新思路。|
|2506.05671v1|[Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](http://arxiv.org/abs/2506.05671v1)|总结：  <br/>本研究提出文本仅微调策略，通过未配对文本实现低资源领域自适应，保持语音-文本对齐并提升模型泛化能力，为ASR领域提供有效新方法。<br/><br/>贡献点：  <br/>1. 提出无需额外音频的文本仅微调框架，解决低资源环境下配对语音-文本数据不足的问题。  <br/>2. 引入实时评估机制，确保微调过程中语音-文本对齐性得以保留。  <br/>3. 实验证明方法在多个数据集上可保持与全音频-文本微调相近的性能，且性能退化最小。  <br/>4. 有效提升模型对新领域的泛化能力，避免灾难性遗忘问题。  <br/>5. 为低资源领域适应性ASR提供可扩展的文本驱动优化方案。|
|2506.05538v1|[SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful   Deepfake Content on Social Media Platforms](http://arxiv.org/abs/2506.05538v1)|总结：  <br/>提出SocialDF数据集及多模态LLM检测方法，解决社交媒体深伪内容的识别难题。<br/><br/>贡献点：  <br/>1. 构建SocialDF数据集：首个涵盖社交媒体真实场景的深伪挑战数据集，包含多来源高保真合成媒体。  <br/>2. 多模态检测框架：融合面部识别、语音转录与多智能体LLM，实现音频-视觉线索的交叉验证。  <br/>3. 语言行为分析：引入 linguistic、behavioral 和 contextual 多维度分析，提升检测的鲁棒性和准确性。|
|2506.05414v1|[SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and   Hearing](http://arxiv.org/abs/2506.05414v1)|总结：  <br/>本研究提出首个支持动态场景中3D空间推理的基准SAVVY-Bench，并设计无需训练的推理框架SAVVY，通过多模态轨迹聚合与坐标对齐提升音频-视觉大语言模型的性能，推动动态3D场景理解。<br/><br/>贡献点：  <br/>1. 提出首个专注于动态、音频-视觉环境中3D空间推理的基准SAVVY-Bench，引入同步空间音频数据。  <br/>2. 设计无训练的推理框架SAVVY，包含两个阶段：基于视角的物体轨迹估计和动态全局地图构建。  <br/>3. 引入细粒度时序定位、一致3D定位及多模态标注，提升场景理解的复杂性与准确性。  <br/>4. 实验证明SAVVY显著提升现有AV-LLMs性能，为动态3D空间推理研究建立新标准。|
|2506.05209v1|[The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly   Licensed Text](http://arxiv.org/abs/2506.05209v1)|**贡献点:**<br/>1. 提出Common Pile v0.1，首个面向LLM预训练的8TB开放授权文本数据集（覆盖30个来源，包括论文、代码、书籍等）；<br/>2. 通过训练7B参数模型验证数据集有效性，性能与基于未授权文本的LLM（如Llama 1/2 7B）相当；<br/>3. 释放数据集构建代码、训练混合策略及模型检查点，提升研究透明度与可复现性。<br/><br/>**总结（100字以内）:**  <br/>本文构建首个8TB开放授权文本数据集Common Pile v0.1，涵盖多领域内容。基于其训练的7B参数模型性能媲美未授权文本训练的LLM，并开源全流程代码和训练资源，推动合规模型研究发展。|
|2506.05191v1|[MokA: Multimodal Low-Rank Adaptation for MLLMs](http://arxiv.org/abs/2506.05191v1)|总结：  <br/>该论文提出MokA方法，通过结合单模态适配与跨模态交互，解决现有多模态微调方法忽视模态差异的问题，实验证明其有效性与普适性，为高效多模态模型适配提供新思路。<br/><br/>贡献点：  <br/>1. **揭示现有方法的局限性**：指出当前高效多模态微调方法直接套用LLM策略，忽视多模态场景的内在差异，影响模态协同利用。  <br/>2. **提出双适应理论框架**：论证单模态适配与跨模态适配是多模态模型微调的两个核心组成部分，强调两者的共同作用。  <br/>3. **设计MokA方法**：提出一种多模态感知的低秩微调策略，通过模态特异性参数压缩单模态信息，显式增强跨模态交互。  <br/>4. **多场景验证有效性**：在音频-视觉-文本、视觉-文本、语音-文本三种典型场景及多个大模型架构（如LLaMA2/3、Qwen2等）中验证方法的通用性与性能提升。  <br/>5. **系统评估方法优势**：通过消融实验与效率分析，全面验证MokA在参数压缩与跨模态增强上的效果。  <br/>6. **推动多模态研究**：认为MokA为多模态大模型高效适配提供更精准的解决方案，为后续研究奠定基础。|
|2506.05062v1|[Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation](http://arxiv.org/abs/2506.05062v1)|总结：  <br/>提出Debate Speech Evaluation新基准，系统分析LLM与人类在多维度辩论评估中的表现，揭示模型判断行为差异并评估生成能力，为LLM判决研究提供关键洞察。<br/><br/>贡献点：  <br/>1. **提出新基准**：构建首个用于评估大语言模型辩论判断能力的基准任务，填补LLM系统性基准领域的空白。  <br/>2. **多维评估框架**：提出综合考察论点强度、相关性、连贯性、风格适配等多层面的评价标准，系统性分析LLM的判断能力。  <br/>3. **大规模标注数据集**：利用包含600+条精细标注的辩论演讲数据集，首次实现对LLM在辩论任务上的大规模实验与验证。  <br/>4. **模型行为分析**：揭示大型模型在个体判断与整体行为层面与人类法官的显著差异，为LLM的局限性研究提供实证依据。  <br/>5. **生成能力评估**：验证前沿LLM生成具有说服力和观点性演讲的能力，证明其在特定任务上可能达到人类水平。|
|2506.04711v1|[LLM-based phoneme-to-grapheme for phoneme-based speech recognition](http://arxiv.org/abs/2506.04711v1)|**贡献点：**<br/>1. **提出LLM-P2G解码框架**：首次将大语言模型（LLMs）引入基于音素的ASR系统，结合语音到音素（S2P）和音素到字符（P2G）的分步解码流程。  <br/>2. **解决级联信息损失问题**：针对S2P与P2G联用时的潜在信息损失，设计了两种训练策略：带噪声音素的数据增强（DANP）和随机化top-K边缘化训练与解码（TKM）。  <br/>3. **跨语言性能提升**：实验验证在波兰语和德语的跨语言ASR任务中，LLM-P2G相比传统WFST解码方法分别降低WER 3.6%和6.9%。  <br/>4. **简化解码流程**：通过LLMs替代WFST，减少了解码的复杂性，同时提升对多语言数据的适应能力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出LLM-P2G解码方法，结合S2P与P2G流程并引入数据增强和top-K边缘化策略，有效缓解信息损失问题，在波兰语和德语跨语言ASR中显著提升识别性能。|
|2506.04693v1|[Cracking the Code: Enhancing Implicit Hate Speech Detection through   Coding Classification](http://arxiv.org/abs/2506.04693v1)|总结：  <br/>提出新的隐性仇恨言论分类体系（codetypes），设计两种LLMs检测方法，验证其在中英文数据集上的有效性。<br/><br/>贡献点：  <br/>1. **构建隐性仇恨言论新分类框架**：首次提出六种编码策略（codetypes），为im-HS检测提供系统化的分类依据。  <br/>2. **创新性方法设计**：提出两种基于大语言模型的整合策略——直接提示分类与编码器嵌入codetypes，解决隐性HS检测挑战。  <br/>3. **跨语言有效性验证**：通过中英文数据集的实验结果，证明所提方法在不同语言环境下的普遍适用性与检测性能提升。|
|2506.04586v1|[LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech   Foundational Models](http://arxiv.org/abs/2506.04586v1)|总结:  <br/>提出LESS框架，通过LLM优化伪标签并结合数据过滤策略提升语音任务性能，验证其跨语言和任务的适应性，为语音处理提供新方法。<br/><br/>贡献点:  <br/>1. **提出LESS框架**：首个结合大语言模型与半监督学习的语音处理框架，用于修正野外数据生成的伪标签。  <br/>2. **LLM伪标签修正**：利用LLM提升ASR/AST任务中伪标签的质量，显著降低词错误率（WER）。  <br/>3. **数据过滤策略**：设计数据增强机制优化LLM知识迁移效率，提升模型训练效果。  <br/>4. **跨语言/任务验证**：在中文ASR和西班牙语-英语AST任务中均取得显著性能提升，证明框架的通用性。  <br/>5. **消融研究分析**：通过不同LLM和提示配置的实验，揭示LLM衍生知识在语音处理中的关键作用与优化方向。|
|2506.04043v1|[Think Like a Person Before Responding: A Multi-Faceted Evaluation of   Persona-Guided LLMs for Countering Hate](http://arxiv.org/abs/2506.04043v1)|**贡献点（分点）:**  <br/>1. 提出首个针对大语言模型（LLM）生成的反叙事（CN）的多维度评估框架，涵盖角色框架、冗长性与可读性、情感基调及伦理稳健性。  <br/>2. 系统测试三种提示策略在MT-Conan和HatEval数据集上的表现，对比GPT-4o-Mini、CommandR-7B和LLaMA 3.1-70B等主流模型的生成效果。  <br/>3. 首次揭示LLM生成的CN存在可访问性不足的问题（如冗长、适配大学以上学历），限制其实际应用效果。  <br/>4. 量化分析情感引导提示策略在提升CN同理心与可读性方面的优势，同时指出其潜在安全与伦理风险。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出LLM生成反叙事的评估框架，系统分析三种提示策略及多模型表现，发现其冗长性与可访问性不足，情感引导策略能提升可读性但存在安全风险，为优化反仇恨言论技术提供关键见解。|
|2506.03099v1|[TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1)|**贡献点：**<br/><br/>1. **模型转化**：将预训练的SOTA图像-视频生成模型（DiT）改造为音频驱动的高参数（180亿）虚拟形象生成模型，实现实时对话动画。<br/>2. **无误差流技术**：通过双向教师模型向稀疏因果自回归学生模型的异步知识蒸馏，解决无限视频流中的误差累积问题。<br/>3. **高效推理优化**：设计高吞吐量、低延迟的推理管道，包含以下工程创新：  <br/>   - 分布式计算（DiT和VAE解码器分设设备）  <br/>   - CUDA流技术实现设备间通信与计算重叠  <br/>   - 消除冗余计算提升帧生成效率  <br/><br/>**总结（100字内）：**  <br/>本文提出TalkingMachines框架，将预训练视频生成模型转化为音频驱动的实时角色动画系统，通过模型优化与分布式推理技术，实现无误差无限视频流和高效生成性能。|
|2506.03009v1|[Conditioning Large Language Models on Legal Systems? Detecting   Punishable Hate Speech](http://arxiv.org/abs/2506.03009v1)|总结：  <br/>本文探讨了如何通过不同抽象层次的法律知识对齐LLMs，分析其在仇恨言论检测任务中的表现，揭示模型与法律专家在法律评估能力上的显著差距，并探讨抽象与具体法律知识对模型性能的不同影响。<br/><br/>贡献点：  <br/>1. 提出并研究了LLMs在法律系统不同抽象层级（宪法、法规、判例）的对齐方法，探索其法律问题评估能力。  <br/>2. 聚焦德国刑法框架下的煽动仇恨行为分类任务，构建具体应用案例分析。  <br/>3. 揭示LLMs在法律评估中的性能瓶颈：抽象法律知识模型缺乏任务理解，易出现矛盾与幻觉；具体法律知识模型虽能识别目标群体，但分类行为特征存在困难。  <br/>4. 为法律与AI交叉领域提供实证依据，指出模型与法律专家间的核心差距，并启发更精准的法律知识融入策略。|
|2506.02758v1|[Exploiting the English Vocabulary Profile for L2 word-level vocabulary   assessment with LLMs](http://arxiv.org/abs/2506.02758v1)|**总结（100字以内）：**  <br/>本研究提出结合大语言模型与英语词汇档案（EVP）的新型方法，实现对二语学习者写作中词汇使用的细粒度评估，解决多义性与上下文变化等挑战，并验证了其在词级与作文级熟练度相关性分析中的有效性。  <br/><br/>---<br/><br/>**贡献点：**  <br/>1. **提出新方法**：首次将大型语言模型（LLMs）与英语词汇档案（EVP）结合，实现基于句子语境的细粒度词汇评估。  <br/>2. **解决复杂问题**：有效应对二语词汇中的多义性（polysemy）、上下文差异（contextual variation）及多词表达（multi-word expressions）等评估难题。  <br/>3. **对比基准模型**：通过对比传统词性（PoS）基线，证明LLMs能利用更丰富的语义信息，提升词汇评分准确性。  <br/>4. **探索相关性**：首次分析词级语言能力与作文整体水平之间的关联，揭示词汇使用的层级性特征。  <br/>5. **验证EVP一致性**：应用该方法重新检验EVP的等级划分合理性，证实LLMs在词汇评估任务中的适用性与可靠性。|
|2506.02457v1|[SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant](http://arxiv.org/abs/2506.02457v1)|总结：  <br/>提出SOVA-Bench基准框架，系统评估语音LLMs的语义与声学生成能力，填补声学质量量化评估空白，推动语音交互系统发展方向。<br/><br/>贡献点：  <br/>1. 提出SOVA-Bench：首个系统性评估语音LLMs的基准框架，整合多维度能力测试。  <br/>2. 填补声学质量量化空白：首次对生成语音的声学特性进行量化评估，突破以往仅关注语义准确性的局限。  <br/>3. 综合多能力评估：同时衡量语音理解、语音识别、语义生成和声学生成能力，提供全面对比分析。  <br/>4. 指导技术发展：为语音交互系统的优化方向提供理论依据与实践参考，促进更自然的语音生成研究。|
|2506.01808v1|[NAVER LABS Europe Submission to the Instruction-following Track](http://arxiv.org/abs/2506.01808v1)|**贡献点总结（100字以内）:**  <br/>本文提出一种多任务语音处理系统，整合语音到LLM嵌入投影器与LoRA适配器，通过指令微调实现跨语言（中、意、德）的ASR、ST、SQA任务联合处理，优化了多语言和多模态数据下的模型性能。<br/><br/>**分点贡献：**  <br/>1. **多任务联合处理**：开发可同步执行语音识别（ASR）、语音翻译（ST）和语音问答（SQA）的系统，支持英语输入到中文、意大利语和德语的跨语言转换。  <br/>2. **模块化架构**：采用两个预训练模块：(1) 语音到LLM的嵌入投影器（基于SeamlessM4T-v2-large语音编码器）；(2) LoRA适配器（基于Llama-3.1-8B-Instruct语言模型）。  <br/>3. **指令微调策略**：联合加载模块后，在多语言和多模态数据上进行1K步指令优化，提升对复杂指令的响应能力。  <br/>4. **高效训练方法**：通过预训练模块与微调结合，简化多任务模型的训练流程，可能提升推理效率和泛化性能。|
|2506.01683v1|[Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection   Using Speech and Large Language Models](http://arxiv.org/abs/2506.01683v1)|**贡献点：**  <br/>1. 提出结合语音和语言模型的CoT推理方法，用于阿尔茨海默病（AD）与非AD分类。  <br/>2. 引入监督微调（SFT）与CoT提示策略，增强模型分类能力。  <br/>3. 设计线性层作为分类模块，提升模型对语音文本的判别性能。  <br/>4. 实验显示方法在无CoT策略的对比基线中实现16.7%的相对性能提升。  <br/>5. 达到当前CoT方法在阿尔茨海默病诊断领域的最先进性能水平。  <br/><br/>**总结：**  <br/>该研究提出一种基于链式思维的语音-语言模型联合框架，通过监督微调与提示策略显著提升痴呆症分类准确率，达到领域内最先进水平。|
|2506.01484v2|[LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech   Detoxification](http://arxiv.org/abs/2506.01484v2)|总结（100字以内）:  <br/>该研究提出基于LLM的自动化detoxification框架，构建首个大规模hatespeech平行数据集ParaDeHate，并验证其在提升模型性能方面的有效性，为替代人工标注提供可扩展解决方案。<br/><br/>贡献点分点列出:<br/>1. **提出LLM-in-the-loop自动化流程**：设计一种利用GPT-4o-mini替代人工标注的新型管道，实现对有害语言的自动改写，降低标注成本。<br/>2. **构建领域专用数据集ParaDeHate**：创建包含8K对仇恨/非仇恨文本的平行数据集，填补hatespeech detoxification的高质量数据缺口。<br/>3. **验证LLM生成数据的有效性**：通过实验表明，基于ParaDeHate微调的BART等模型在风格准确性、内容保留和流畅度方面显著优于现有方法。<br/>4. **建立基准与方法对比**：发布ParaDeHate作为评估基准，并系统比较多种基线模型表现，推动该领域的研究进展。|
|2506.01133v1|[From Words to Waves: Analyzing Concept Formation in Speech and   Text-Based Foundation Models](http://arxiv.org/abs/2506.01133v1)|贡献点（分点）:<br/>1. 首次验证语音模型是否能像文本模型一样获得抽象语义概念<br/>2. 系统比较单模态（语音/文本）与多模态联合训练模型的语义结构差异<br/>3. 提出并应用Latent Concept Analysis方法分析跨模态语义形成机制<br/>4. 开源实验代码与资源提升研究可复现性<br/><br/>总结: 本研究通过无监督分析方法，揭示语音与文本模型在语义抽象形成上的异同，验证多模态训练对语义理解的增强作用，并开放资源促进学术研究复现。|
|2506.01111v1|[FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal   Contextual Fusion](http://arxiv.org/abs/2506.01111v1)|**贡献点分点：**  <br/>1. **两阶段音频描述生成方法**：提出基于人类听觉感知启发的自动化框架，结合多模态信息提取（语音、音乐、环境声音、视频）和大语言模型（LLM）的上下文融合，实现细粒度、语境感知的音频描述。  <br/>2. **FusionAudio数据集**：构建包含120万条高质量音频描述和60万问答对的大型数据集，为跨模态研究提供标注资源。  <br/>3. **改进的音频模型**：开发基于CLAP的音频编码器，提升音频-文本对齐能力与指令遵循效果，增强模型对复杂音频环境的理解。  <br/><br/>**总结（100字以内）：**  <br/>提出两阶段音频描述生成框架与FusionAudio数据集，结合多模态信息和大语言模型提升描述质量，优化CLAP编码器实现更精准的音频-文本对齐与理解。|
|2506.01077v1|[TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans](http://arxiv.org/abs/2506.01077v1)|总结：  <br/>本论文提出TRiMM框架，解决数字人类实时手势生成与长文本理解难题，包含跨模态注意力、长上下文建模及大规模动作匹配系统，实现120fps推理速度，并在消费级GPU上保持低延迟，代码开源。<br/><br/>贡献点：  <br/>1. **提出TRiMM框架**  <br/>   - 首次结合多模态技术实现实时3D手势生成，同时解决长文本理解与实时合成的挑战。  <br/><br/>2. **设计三大核心模块**  <br/>   - **跨模态注意力机制**：实现语音与手势的精确定时对齐。  <br/>   - **长上下文自回归模型**：采用滑动窗口机制高效建模长序列，增强语义连贯性。  <br/>   - **大规模动作匹配系统**：构建原子动作库，支持实时检索生成高质量手势。  <br/><br/>3. **轻量级Unreal Engine实现**  <br/>   - 开发轻量级实验流水线，实现实时推理速度（120 fps）与低句级延迟（0.15秒）。  <br/><br/>4. **全面评估验证效果**  <br/>   - 在ZEGGS和BEAT数据集上完成主观与客观评估，证明其性能优于当前SOTA方法。  <br/><br/>5. **开源代码促进应用**  <br/>   - 提供完整代码库，推动LLM驱动数字人类研究的可复现性与实际部署。|
|2506.00955v1|[Leveraging Large Language Models for Sarcastic Speech Annotation in   Sarcasm Detection](http://arxiv.org/abs/2506.00955v1)|总结：  <br/>提出基于大语言模型的语音讽刺标注方法，构建首个大规模单模态讽刺语音数据集PodSarc，验证其有效性并展示73.63%的检测性能，为该领域研究提供新基准。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的单模态标注流程**：首次利用GPT-4o和LLaMA 3等大语言模型生成语音讽刺标注数据，解决语音领域数据稀缺问题。  <br/>2. **构建大规模语音讽刺数据集PodSarc**：通过人机协作验证，创建高质量单模态讽刺语音数据集，填补语音讽刺研究的数据空白。  <br/>3. **验证方法有效性**：采用协作门控架构对比标注质量与检测性能，证明所生成数据集的可靠性及研究价值。  <br/>4. **提供基准性能指标**：检测模型在公开数据集上达到73.63% F1分数，为后续研究提供量化评估标准。|
|2506.00304v1|[Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion   with LLMs](http://arxiv.org/abs/2506.00304v1)|总结：  <br/>本研究提出一种无需配对语音数据的EMG适配器模块，使LLMs能处理无声EMG信号，实现低错误率的EMG-to-text转换，并在小数据量下显著优于专用模型。<br/><br/>贡献点：  <br/>1. **首次提出EMG适配器模块**：创新性地设计模块，将无声EMG特征映射至LLM输入空间，无需依赖配对的有声信号或语音数据。  <br/>2. **实现高效转换性能**：在封闭词汇任务中达成平均WER 0.49，表明模型对无声EMG信号的识别能力。  <br/>3. **小数据量优势**：仅需6分钟无声EMG数据，性能超越专用模型近20%，验证方法的泛化能力与实用性。  <br/>4. **拓展LLMs应用边界**：探索LLMs在理解发声生物信号（如无声EMG）中的潜力，为跨模态语音识别提供新方向。|
|2506.00160v1|[Werewolf: A Straightforward Game Framework with TTS for Improved User   Engagement](http://arxiv.org/abs/2506.00160v1)|贡献点：  <br/>1. 提出基于LLM的新型Werewolf社会推理游戏系统，融合文本生成与语音交互技术，增强游戏沉浸感。  <br/>2. 设计调优后的文本到语音（TTS）模型，提升与多种LLM的兼容性，降低适配成本。  <br/>3. 通过简化系统结构（无需额外组件），实现更高效的用户参与度提升，反驳传统依赖微调或经验池的方案。  <br/>4. 强调LLM推理能力的持续提升将减少对辅助技术（如复杂提示工程）的依赖，指向未来研究方向。  <br/><br/>总结：  <br/>本文提出一种结合TTS与LLM的新型社会推理游戏系统，通过调优提升兼容性并简化架构，有效增强用户体验，同时指出LLM能力进步将减少对额外技术的依赖。|
|2505.24869v1|[SiLVR: A Simple Language-based Video Reasoning Framework](http://arxiv.org/abs/2505.24869v1)|总结（100字以内）:  <br/>提出SiLVR框架，通过双阶段语言表征与推理实现复杂视频语言理解，利用多感官输入提升性能，并验证强LLM无需视频训练即可有效处理多模态信息，达到多个基准数据集的最佳结果。<br/><br/>**贡献点分点列出**:<br/>1. **双阶段框架设计**：创新性地构建SiLVR框架，将复杂视频理解拆分为语言表征提取与推理两阶段，简化多模态处理流程。<br/>2. **多感官输入融合**：引入短片标题、音频/语音字幕等多模态数据作为语言表征的输入，增强对视频内容的描述能力。<br/>3. **自适应token削减方案**：提出动态调整时间粒度的token减少方法，有效处理长上下文多模态输入的效率与精度问题。<br/>4. **训练自由的模组化架构**：框架无需额外训练，依赖推理时的语言模型能力，提升灵活性和易用性。<br/>5. **多任务性能突破**：在Video-MME、Video-MMMU、Video-MMLU、CGBench、EgoLife等基准数据集上取得当前最优性能。<br/>6. **跨模态推理验证**：实验证明，强LLM无需视频领域训练即可高效聚合多感官信息，支持复杂时序、因果、长上下文及知识推理任务。|
|2505.24691v1|[Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing   Cross-Lingual Transfer in Low-Resource Scenarios](http://arxiv.org/abs/2505.24691v1)|总结：提出融合音素表征与Chain-of-Thought框架的S2TT方法，通过课程学习策略提升低资源与零资源场景的翻译性能，为跨语言语音翻译的普及提供新思路。<br/><br/>贡献点：<br/>1. 提出将音素表征整合入CoT框架，构建新型S2TT系统  <br/>2. 引入音素识别作为跨语言迁移的中间步骤，实现零资源翻译能力  <br/>3. 基于多语言LLM开发语音-文本联合处理架构  <br/>4. 设计渐进式课程学习策略，优化多任务训练过程  <br/>5. 证实音素增强的CoT方法在低资源场景显著提升译质，且具备可扩展性|
|2505.24458v1|[SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social   Engineering Behaviors](http://arxiv.org/abs/2505.24458v1)|**贡献点：**  <br/>1. **首个整合AR与多模态LLM的社会工程数据集**：SEAR是首个专门针对AR增强现实和多模态大语言模型驱动的社会工程攻击的多模态数据集。  <br/>2. **多场景对抗性对话数据**：包含60名参与者在模拟会议、课堂、社交活动等场景中的180条标注对话，涵盖多样化的社会工程情境。  <br/>3. **多模态同步数据采集**：整合AR捕捉的同步视觉/音频线索（如面部表情、语音语调）、环境信息及用户社交媒体资料，实现全面行为分析。  <br/>4. **主观攻击效果评估**：引入信任评分与易受性评估等主观指标，量化攻击对用户心理的影响。  <br/>5. **高攻击效能实证结果**：揭示SEAR在诱导用户点击钓鱼链接（93.3%）、接受电话（85%）、提升信任度（76.7%）等任务中的显著效果。  <br/>6. **伦理合规保障**：通过匿名化处理与IRB（伦理审查委员会）批准，确保数据集的负责任使用。  <br/>7. **开源开放共享**：数据集通过GitHub平台公开，支持学术研究与技术开发。  <br/><br/>**总结（100字以内）**：  <br/>SEAR Dataset是首个结合AR与多模态LLM的社会工程攻击数据集，包含多场景、多模态数据及主观评估指标，揭示攻击高效能，为检测与防御研究提供资源，同时保障伦理合规与公开共享。|
|2505.24347v2|[Fewer Hallucinations, More Verification: A Three-Stage LLM-Based   Framework for ASR Error Correction](http://arxiv.org/abs/2505.24347v2)|总结:  <br/>本文提出RLLM-CF框架，通过错误预检测、迭代修正和推理验证三阶段解决LLM在语音识别中的hallucinations问题，无需额外数据或微调，实验证明在多个数据集上显著降低CER/WER。<br/><br/>贡献点:  <br/>1. **提出RLLM-CF框架**：设计包含错误预检测、链式思维子任务迭代修正和推理过程验证的三阶段校正流程，系统性解决LLM在ASR中的错误修正问题。  <br/>2. **无额外训练需求**：方法无需额外标注数据或模型微调，直接利用LLM能力进行端到端校正，降低应用门槛。  <br/>3. **抑制hallucinations**：通过多阶段验证机制有效避免LLM误改正确文本，保障修正结果的准确性。  <br/>4. **实验证明有效性**：在AISHELL-1、AISHELL-2和Librispeech数据集上验证，显示GPT-4o模型结合该框架后CER/WER分别降低21%/11%/9%/11.4%，具有实际应用价值。|
|2505.24016v1|[BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech   Translation System](http://arxiv.org/abs/2505.24016v1)|总结:  <br/>本研究提出BeaverTalk级联系统，结合VAD分段器、Whisper Large V2和Gemma 3 12B，通过LoRAs微调和单句记忆机制实现高效实时翻译，显著提升英德、英中翻译性能。<br/><br/>贡献点:  <br/>1. **系统架构创新**：设计级联系统BeaverTalk，集成VAD分段器、Whisper Large V2语音识别模型和Gemma 3 12B语言模型，实现端到端语音到文本翻译。  <br/>2. **微调方法优化**：采用低秩适配器（LoRAs）技术对翻译LLM进行轻量级微调，结合对话提示策略利用单一源语言前句记忆库提升上下文建模能力。  <br/>3. **延迟与语言方向支持**：系统兼容低延迟（StreamLAAL 1837.86）和高延迟（StreamLAAL 3343.73）运行模式，在英德（BLEU 24.64/27.83）与英中（BLEU 34.07/37.23）任务中均取得突出性能。  <br/>4. **实际部署效果**：在IWSLT 2025真实任务中验证系统有效性，为多语言实时翻译提供可落地的解决方案。|
|2505.23990v2|[Multi-RAG: A Multimodal Retrieval-Augmented Generation System for   Adaptive Video Understanding](http://arxiv.org/abs/2505.23990v2)|总结：  <br/>提出Multi-RAG多模态检索增强生成系统，通过整合视频、音频、文本多源信息提升情境理解与决策效率，验证其在动态场景中优于现有模型且资源占用更少的优势。  <br/><br/>贡献点：  <br/>1. 提出Multi-RAG系统，解决动态场景下人机协作的认知负担问题。  <br/>2. 首次整合视频、音频、文本多模态信息流进行联合推理与生成。  <br/>3. 在MMBench-Video基准测试中表现优于现有视频大语言模型（Video-LLM）和视觉语言模型（LVLM）。  <br/>4. 以更少资源和输入数据实现高效性能，具有实际应用潜力。|
|2505.22251v2|[Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in   Large Language Models for Speech Recognition](http://arxiv.org/abs/2505.22251v2)|摘要贡献点：  <br/>1. 揭示LibriSpeech和Common Voice数据集与公开LLM预训练语料存在显著重叠，质疑现有语音任务评估结果的可靠性。  <br/>2. 提出通过对比含/不含数据污染的LLM评估污染影响的方法，验证污染数据的存在性及对模型表现的影响。  <br/>3. 发现污染LLM的语音识别器虽在错误率上差异微小，但会显著提升对训练数据的转录概率，反映输出偏差。  <br/>4. 强调需使用独立数据评估LLM语音系统，以避免因数据污染导致的不准确结果。  <br/><br/>总结：  <br/>该研究揭露语音任务评估数据与LLM训练数据重叠问题，揭示污染对模型性能的潜在影响，并呼吁使用独立数据验证模型效果。|
|2505.22029v2|[Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection](http://arxiv.org/abs/2505.22029v2)|**贡献点：**  <br/>1. **构建首个全面覆盖词和音素层级的11类不流畅性合成语料库**：LLM-Dys解决了现有数据集中语调不自然和上下文多样性不足的问题，提供更真实的语音不流畅性样本。  <br/>2. **提出改进的端到端检测框架**：基于LLM-Dys数据集优化模型，实现语音不流畅性检测的最先进性能。  <br/>3. **开源全部资源**：公开数据、模型与代码，促进领域研究和应用。  <br/><br/>**总结（100字以内）：**  <br/>本文构建了首个全面覆盖词/音素层级的不流畅性合成数据集LLM-Dys，改进端到端检测框架达SOTA，所有数据及代码开源，推动语音不流畅性研究。|
|2505.20445v3|[In-context Language Learning for Endangered Languages in Speech   Recognition](http://arxiv.org/abs/2505.20445v3)|**总结（100字以内）:**  <br/>本文探索LLM通过上下文学习在低资源语言语音识别中的应用，证实相关文本样本能提升性能，概率方法优于传统指令方法，并展示ICL可使LLM达到或超越专用语言模型的ASR效果，同时保留原有能力。<br/><br/>**贡献点:**  <br/>1. **验证ICL在低资源语音识别中的可行性**：首次将上下文学习方法应用于语音识别领域，证明LLM能在未训练的低资源语言上实现有效学习。  <br/>2. **提出文本样本增强策略**：发现提供与任务更相关的文本样本能显著提升语言建模和ASR性能，为多语言模型优化提供新方向。  <br/>3. **对比方法效果与模型性能**：表明概率方法优于传统指令方法，且ICL使LLM在ASR任务中达到专用语言模型水平，同时保持其通用能力。|
|2505.18614v2|[MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song   Translation](http://arxiv.org/abs/2505.18614v2)|贡献点：  <br/>1. **提出首个多语言多模态基准**：构建了Multilingual Audio-Video Lyrics Benchmark (MAVL)，首次整合文本、音频与视频数据，为动画歌曲翻译提供综合评估标准。  <br/>2. **多模态数据增强翻译质量**：通过融合音频和视频信息，使翻译更贴近原作的旋律、节奏及风格，突破传统文本仅依赖语义的局限。  <br/>3. **创新音节约束模型结构**：提出SylAVL-CoT模型，结合链式推理（Chain-of-Thought）与音节约束机制，提升歌词的自然度与可唱性。  <br/>4. **验证多模态方法有效性**：实验表明该模型在可唱性和上下文准确性上显著优于文本基础模型，证明多模态、多语言框架对歌词翻译的价值。  <br/><br/>总结（100字以内）：  <br/>本研究提出多语言多模态基准MAVL与SylAVL-CoT模型，融合文本、音频和视频数据，通过音节约束提升歌词翻译的自然度和可唱性，验证了多模态方法在动画歌曲翻译中的优势。|
|2505.18110v2|[Watch and Listen: Understanding Audio-Visual-Speech Moments with   Multimodal LLM](http://arxiv.org/abs/2505.18110v2)|总结：  <br/>本文提出TriSense三模态模型，结合视觉、音频与语音信号，引入Query-Based Connector实现模态自适应融合，构建TriSense-2M数据集支持多模态分析，验证了模型的有效性，并公开代码与数据集。<br/><br/>贡献点：  <br/>1. **提出TriSense模型**：首个整合视觉、音频和语音三模态的大型语言模型，用于全面提升视频内容的时间理解能力。  <br/>2. **设计Query-Based Connector**：通过自适应调整模态权重，支持在模态缺失情况下的鲁棒性，并实现灵活的多模态输入组合。  <br/>3. **构建TriSense-2M数据集**：包含200万样本的高质量数据集，采用自动化生成流程，涵盖长视频及多样化的多模态组合。  <br/>4. **实验验证与公开资源**：在多个基准上验证模型效果，证明其对多模态视频分析的潜力，并开放代码和数据集供研究复现。|
|2505.17536v2|[Multimodal Conversation Structure Understanding](http://arxiv.org/abs/2505.17536v2)|总结：  <br/>本研究提出对话角色归属与线程划分任务框架，构建大规模人工标注数据集，评估多模态模型在理解对话结构上的性能差异，揭示关键影响因素，为改进多模态语言模型的对话理解能力提供基础。<br/><br/>贡献点：  <br/>1. **构建多模态对话结构标注数据集**：提供首个包含4,398条对话角色标注、5,755条收件人信息、3,142条旁观者信息的语料库，涵盖多参与者、多模态场景。  <br/>2. **定义关键任务框架**：提出针对对话角色分配（说话人、收件人、旁观者）和对话线程构建（语句关联与聚类）的系统性任务，结合会话分析与社会语言学理论。  <br/>3. **评估模型性能差异**：对比音频-视觉LLM与视觉-语言模型，发现前者在说话人/收件人识别上更优，但匿名化参与者时性能显著下降。  <br/>4. **揭示关键影响因素**：通过实验发现对话参与者数量是角色识别的主要负向预测因子，而声学清晰度（音调、频谱质心）和面部覆盖情况与性能呈正相关。  <br/>5. **推动未来研究方向**：为多模态LLM在对话结构建模和推理能力的提升提供基准与启示。|
|2505.15670v2|[Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](http://arxiv.org/abs/2505.15670v2)|总结：  <br/>本文提出首个无需语音预训练的双工S2S模型，通过连续输入输出与信道融合实现实时对话，降低比特率并提升推理、回合管理及打断处理能力，同时减少数据需求，开源代码促进复现。<br/><br/>贡献点：  <br/>1. **双工S2S架构设计**：支持连续用户输入与同步代理输出，引入信道融合机制实现真实场景下的实时交互（如用户打断）。  <br/>2. **无需语音预训练**：首次提出仅依赖流式编码器的双工模型，消除对专用语音预训练的需求。  <br/>3. **高效编码与微调**：采用独立的代理和用户建模架构，支持编码微调以优化代理语音质量，将比特率降至0.6kbps。  <br/>4. **性能提升**：在推理能力、回合控制和打断处理等关键指标上超越现有双工模型。  <br/>5. **数据需求降低**：跳过语音预训练环节，显著减少所需语音数据量，简化模型构建流程。  <br/>6. **开源与可复现性**：首个公开完整训练与推理代码的双工S2S模型，推动领域研究复现与验证。|
|2505.13338v2|[Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data   Condensation and Spoken QA Generation](http://arxiv.org/abs/2505.13338v2)|**贡献点分点列出：**  <br/>1. **提出首个融合上下文推理与语音伴随信息的框架**：首次设计结合两者的数据集生成方法，解决传统Speech-LLMs在综合理解上的不足。  <br/>2. **创新性的数据生成机制**：包含两阶段技术——基于伪语音伴随标签的野外语音数据压缩，以及LLM驱动的上下文语音伴随问答（CPQA）生成。  <br/>3. **验证框架有效性**：通过Qwen2-Audio-7B-Instruct在自动生成与人工标注CPQA数据集上的强相关性评估，证明其能力。  <br/>4. **揭示Speech-LLMs的局限性**：明确指出模型在共情推理任务中的缺陷，强调需针对性数据集与更优模型。  <br/>5. **潜在应用价值**：为训练具备语音伴随推理能力的鲁棒Speech-LLMs提供基础，推动语音理解研究发展。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出首个融合上下文推理与语音伴随信息的数据集生成框架，揭示Speech-LLMs在共情任务中的局限，验证框架对模型训练的有效性，为提升语音AI能力提供新方向。|
|2505.09439v2|[Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](http://arxiv.org/abs/2505.09439v2)|总结：  <br/>本文提出Omni-R1，通过GRPO强化学习微调Qwen2.5-Omni，在MMAU和MMAR基准上取得SOTA性能，揭示了文本推理能力对音频任务的关键作用，并意外发现文本数据微调可提升音频表现。<br/><br/>贡献点：  <br/>1. **提出Omni-R1模型**：通过GRPO强化学习方法对Qwen2.5-Omni进行微调，聚焦音频问答任务。  <br/>2. **SOTA性能突破**：在MMAU和MMAR基准测试中达到当前最优结果，尤其在声音、音乐、语音及总体平均类别均表现最佳。  <br/>3. **因果分析**：验证了性能提升主要源于增强的文本推理能力，而非单纯依赖音频数据。  <br/>4. **意外发现**：发现仅以文本数据集进行微调也能有效提升模型的音频表现，为多模态训练提供新思路。|
|2505.05335v2|[FLAM: Frame-Wise Language-Audio Modeling](http://arxiv.org/abs/2505.05335v2)|总结：  <br/>本文提出FLAM，解决帧级音频理解与开放词汇定位难题，通过logit调整和大规模数据集提升模型性能，保持全局检索能力。<br/><br/>贡献点：  <br/>1. **首次提出开放词汇帧级音频定位模型**：突破传统模型对预定义类别的依赖，实现对真实场景中未见事件的泛化定位。  <br/>2. **设计记忆高效且校准的帧级目标函数**：引入logit调整机制，有效缓解训练中的虚假相关（如事件依赖、标签不平衡）。  <br/>3. **构建多源帧级监督数据集**：结合LLM生成字幕与模拟数据，提供多样化、细粒度的音频事件标注。  <br/>4. **验证模型多任务能力**：在保持文本-音频检索性能的同时，显著提升帧级定位效果及下游任务表现。|
|2504.20007v3|[Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from   Police Body-Worn Camera Footage](http://arxiv.org/abs/2504.20007v3)|总结：  <br/>本研究提出一种跨学科框架，整合AI与机器学习技术分析警用摄像头数据，通过多模态方法提取警民互动行为动态，构建结构化总结系统，并建立评估流程以提升执法审查与知识发现效率。<br/><br/>贡献点：  <br/>1. **跨学科框架创新**：首次将人工智能（AI）与统计机器学习（ML）技术结合，专门针对警用摄像头（BWC）视频进行模式分析，突破传统单一技术应用的局限。  <br/>2. **多模态数据融合**：综合图像、音频和自然语言处理（NLP）技术，实现对警民互动场景的全面分析，提取尊重/不尊重、冲突升级等复杂行为动态。  <br/>3. **结构化总结生成**：引入说话人分离、转录及大语言模型（LLMs），构建可解释的警民接触事件摘要，提升数据分析的可操作性。  <br/>4. **定制化评估体系**：开发适用于高风险执法场景的自动评估流程，量化转录质量与行为检测准确率，支持实际应用验证。  <br/>5. **执法应用场景落地**：方法论与实证结果为执法部门提供审查、培训及问责的实用工具，推动AI技术在警务实践中的落地应用。  <br/>6. **知识发现前沿推进**：通过系统化分析复杂BWC数据，拓展法律与技术交叉领域的研究边界，为公共安全数据分析提供新范式。|
|2504.08961v2|[A Fully Automated Pipeline for Conversational Discourse Annotation: Tree   Scheme Generation and Labeling with Large Language Models](http://arxiv.org/abs/2504.08961v2)|**贡献点总结：**  <br/>1. 提出基于LLM的全自动决策树构建与标注流水线，替代传统人工设计流程。  <br/>2. 首次将频率引导的决策树与LLM结合，提升语音功能标注性能。  <br/>3. 通过实验验证不同设计选择的效果，展示方法优于手动方案及人类标注。  <br/>4. 开源代码、标注方案及结果，促进语音领域对话标注研究。|