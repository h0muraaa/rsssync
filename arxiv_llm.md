|Source|Title|Summary|
|---|---|---|
|2511.02378v1|[Revisiting put-that-there, context aware window interactions via LLMs](http://arxiv.org/abs/2511.02378v1)|**总结（100字以内）:**  <br/>本研究提出结合LLM与XR技术的新型交互系统，通过多模态数据融合和目标导向推理，实现无手动操作的无缝全景工作区管理。<br/><br/>**贡献点分点列出:**  <br/>1. **经典概念的现代化延伸**：将Bolt的"Put-That-There"理念与现代头戴式显示设备结合，利用LLM增强空间交互能力。  <br/>2. **多模态环境融合**：整合（i）语义分割3D环境、（ii）实时应用元数据、（iii）用户语言、手势和头部注视信息，形成全面感知系统。  <br/>3. **三类交互模式支持**：实现通过显式指令（如"放置地图"）、指称性语言加手势（如"Put that there"）和高层目标（如"发送消息"）的多样化操作方式。  <br/>4. **目标驱动推理机制**：引入目标中心化逻辑，使LLM能动态推断应用程序关联性与布局决策，提升上下文理解能力。  <br/>5. **无缝意图驱动交互**：在沉浸式XR环境中消除手动窗口管理需求，实现自然流畅的交互体验。|
|2511.01716v1|[SemBench: A Benchmark for Semantic Query Processing Engines](http://arxiv.org/abs/2511.01716v1)|**总结**：本文提出一个针对语义查询处理引擎的基准测试，涵盖多种场景、模态和操作符，评估了多个系统，揭示其当前优劣势，为未来研究提供方向。<br/><br/>**贡献点**：<br/><br/>1. 提出首个针对语义查询处理引擎的基准测试，填补了该领域的空白。  <br/>2. 覆盖多种应用场景（如电影评论分析、医疗问答），提升基准的普适性。  <br/>3. 包含多模态数据（图像、音频、文本），支持更丰富的语义操作。  <br/>4. 引入多样化的语义操作符（过滤、连接、映射、排序、分类），增强查询复杂度。  <br/>5. 评估了3个学术系统和1个工业系统（Google BigQuery），揭示当前系统的性能差异与发展方向。|
|2511.01670v1|[SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](http://arxiv.org/abs/2511.01670v1)|总结（100字以内）:  <br/>本文提出SeaLLMs-Audio，首个支持印尼、泰、越、英、中五种语言的多模态音频-语言模型，覆盖音频分析与语音交互多任务，并构建了东南亚专用基准SeaBench-Audio，助力区域研究与产业发展。<br/><br/>**分点贡献：**  <br/>1. **首创性多语言支持**：首次开发面向东南亚多语言（印尼语、泰语、越南语）的LALM，兼容英语和中文，填补区域语言模型空白。  <br/>2. **多模态输入融合**：支持纯音频、纯文本及音频与文本结合的输入，提升模型在跨模态任务中的通用性。  <br/>3. **多任务能力拓展**：涵盖音频分析（如语音识别、情感识别、摘要）与语音交互（问答、对话）六大类任务，实现功能多样化。  <br/>4. **区域基准构建**：提出SeaBench-Audio，为东南亚LALM评估提供标准化测试框架，推动自动化评测体系发展。|
|2510.26974v1|[Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction   from Doctor-Patient Consultations](http://arxiv.org/abs/2510.26974v1)|总结：  <br/>本文提出首个医生-患者对话中提取医疗指令的共享任务（MEDIQA-OE 2025），组织多方参与并评估了多种模型方法，推动语音技术在医疗文档生成中的应用。<br/><br/>贡献点：  <br/>1. **提出首个医疗指令提取任务**：首次设立MEDIQA-OE 2025共享任务，聚焦从临床对话中自动提取医疗订单，填补语音领域在医疗场景应用的空白。  <br/>2. **构建标准化数据集**：通过任务定义与数据集发布，为后续研究提供统一的评估基准和基准数据。  <br/>3. **多模型方法对比实验**：组织六支团队实验闭源与开源大语言模型（LLMs）的多样化解决方案，验证不同技术路径的可行性。  <br/>4. **共享任务成果分析**：汇总最终排行榜及参与者技术方案，系统总结任务进展与挑战，指导未来研究方向。|
|2510.24817v2|[Towards a Method for Synthetic Generation of Persons with Aphasia   Transcripts](http://arxiv.org/abs/2510.24817v2)|总结：  <br/>该研究提出两种生成合成转录本的方法以解决 aphasia 数据稀缺问题，验证其在模拟语言退化特征上的有效性，并为构建更大数据集和改进模型提供方向。<br/><br/>贡献点：  <br/>1. **方法创新**：开发两种生成合成转录本的方案（程序化方法与基于 Mistral 7b Instruct 和 Llama 3.1 8b Instruct 的 LLM 方法），用于模拟 aphasic 语言特征。  <br/>2. **多维度数据生成**：通过词删除、填充词插入、换词替换等操作，生成覆盖 Mild/Moderate/Severe/Very Severe 四种严重程度级别的合成数据。  <br/>3. **模型性能评估**：比较不同方法生成的合成数据，发现 Mistral 7b Instruct 在 NDW（非词性词汇）、词数和词长等关键指标上最贴近真实 aphasia 语言退化模式。  <br/>4. **实践应用指导**：为后续构建更大规模 aphasia 数据集、模型微调和 SLPs 对合成数据的评估应用提供理论依据和操作建议。|
|2510.24817v1|[Towards a Method for Synthetic Generation of PWA Transcripts](http://arxiv.org/abs/2510.24817v1)|**贡献点总结：**<br/><br/>1. 提出两种生成合成语音转录的方法，用于解决语音领域中数据稀缺的问题。  <br/>2. 方法基于不同的技术路线：程序化生成与大型语言模型（Mistral 7b Instruct、Llama 3.1 8b Instruct）的应用。  <br/>3. 合成数据覆盖四种失语严重等级，通过词删除、填充词插入和同音替代等手段模拟真实语言退化。  <br/>4. 评估显示，Mistral 7b Instruct 在捕捉失语语言关键特征方面表现最佳。  <br/>5. 为未来构建更大规模失语语料库、优化模型及由言语治疗师评估合成数据的实用性提供了方向。  <br/><br/>**总结（100字以内）：**  <br/>本文提出两种生成失语语音转录的合成数据方法，借助大型语言模型更真实地模拟失语语言特征，为解决数据稀缺问题提供新思路。|
|2510.24180v1|[V-SAT: Video Subtitle Annotation Tool](http://arxiv.org/abs/2510.24180v1)|总结（100字以内）:  <br/>V-SAT通过整合LLMs、VLMs、图像处理与ASR技术，提出首个统一字幕质量检测框架，有效解决同步、文本错误、格式和速度等问题，显著提升SUBER和F1评分，并引入人机协同验证机制，提供全面可靠的字幕生成解决方案。<br/><br/>贡献点：  <br/>1. 提出首个统一框架V-SAT，全面检测并纠正字幕质量多维度问题  <br/>2. 首次融合LLMs、VLMs、图像处理与ASR技术，实现跨模态上下文感知  <br/>3. 实现SUBER得分从9.6降至3.54，图像模式F1达0.80的显著改进  <br/>4. 引入人类反馈验证机制，确保生成字幕的高质量与可靠性  <br/>5. 突破传统方法单点优化局限，减少后期人工编辑成本|
|2510.23896v1|[AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for   African Languages](http://arxiv.org/abs/2510.23896v1)|总结：该论文提出AfriMTEB框架扩展MMTEB覆盖非洲语言，新增6个数据集和14项任务，并开发AfriE5模型在非洲语言上达到SOTA性能，填补了非洲语言文本嵌入研究的空白。<br/><br/>贡献点：<br/>1. 构建AfriMTEB多语言评估基准：首次系统覆盖59种非洲语言，包含14项任务和38个数据集（含6个全新数据集），突破传统翻译基准任务局限。<br/>2. 开发跨语言对比蒸馏模型AfriE5：基于mE5模型进行指令调优和多语言对比蒸馏，实现非洲语言文本嵌入的适配与优化。<br/>3. 引入新型语言理解任务：新增仇恨言论检测、意图识别、情感分类等任务，拓展语音文本嵌入的应用场景。<br/>4. 实现性能突破：AfriE5在非洲语言基准上超越Gemini-Embeddings和mE5等主流模型，验证了区域化扩展的有效性。|
|2510.23763v3|[RoboOmni: Proactive Robot Manipulation in Omni-modal Context](http://arxiv.org/abs/2510.23763v3)|**贡献点：**  <br/>1. **引入新交互范式**：提出基于跨模态上下文指令的Vision-Language-Action（VLA）设置，通过语音对话、环境声和视觉线索而非显式指令推断用户意图。  <br/>2. **设计RoboOmni框架**：构建Perceiver-Thinker-Talker-Executor架构，利用端到端多模态大语言模型统一意图识别、交互确认与动作执行。  <br/>3. **创建OmniAction数据集**：提出包含140k场景、5k+说话者、2.4k事件声音等数据的训练集，填补机器人操作中主动意图识别的数据空白。  <br/>4. **验证性能优势**：在仿真与实际环境中，实验表明RoboOmni在成功率、推理速度及意图识别等指标上显著优于文本与ASR基线模型。  <br/><br/>**总结（100字以内）：**  <br/>提出跨模态上下文指令的VLA新范式，设计端到端RoboOmni框架，构建大规模OmniAction数据集，验证其在机器人操作中的高效意图识别与执行能力。|
|2510.23763v2|[RoboOmni: Proactive Robot Manipulation in Omni-modal Context](http://arxiv.org/abs/2510.23763v2)|总结：  <br/>本研究提出基于跨模态上下文指令的新范式，设计RoboOmni多模态框架并构建OmniAction数据集，实验证明其在机器人操作中的意图识别与主动辅助效果优于传统方法。<br/><br/>贡献点：  <br/>1. **新研究范式**：提出跨模态上下文指令（cross-modal contextual instructions），通过语音对话、环境声音和视觉线索联合推断用户意图，替代传统明确指令模式。  <br/>2. **跨模态框架RoboOmni**：开发Perceiver-Thinker-Talker-Executor架构，实现意图识别、交互确认与行动执行的端到端统一，支持实时语音交互。  <br/>3. **大规模数据集OmniAction**：构建包含14万场景、2400个背景、5000+说话者及六类上下文指令的多模态数据集，解决主动意图识别的训练数据不足问题。  <br/>4. **多模态融合技术**：通过时空对齐方法融合听觉与视觉信号，提升意图识别的鲁棒性与效率。  <br/>5. **实验验证有效性**：在仿真与真实场景中验证性能，证明RoboOmni在成功率、推理速度及主动辅助能力上优于文本和语音识别（ASR）基线。|
|2510.23763v1|[RoboOmni: Proactive Robot Manipulation in Omni-modal Context](http://arxiv.org/abs/2510.23763v1)|总结：  <br/>本研究提出跨模态上下文指令的新交互范式，设计了端到端的RoboOmni框架，构建了大规模OmniAction数据集，验证了其在意图识别和主动辅助方面优于文本与ASR基线。  <br/><br/>贡献点：  <br/>1. **提出新交互范式**：引入跨模态上下文指令（spoken dialogue、环境声音、视觉线索），替代传统显式指令，更贴近真实人机协作场景。  <br/>2. **设计RoboOmni框架**：基于多模态LLM，整合意图识别、交互确认与动作执行，实现端到端的Proactive Vision-Language-Action协作。  <br/>3. **开发OmniAction数据集**：构建包含14万场景、2400种声音、640背景和6种指令类型的大规模数据集，解决主动意图识别训练数据不足问题。  <br/>4. **实验证明优越性**：在仿真与实际环境中，RoboOmni在成功率、推理速度和主动辅助能力上显著优于文本及ASR基础模型。|
|2510.23585v1|[Hope Speech Detection in Social Media English Corpora: Performance of   Traditional and Transformer Models](http://arxiv.org/abs/2510.23585v1)|总结：  <br/>本研究对比了传统机器学习模型与Transformer模型在希望语识别任务中的表现，发现后者在小数据集上具有更高的精度和召回率，为后续研究提供了方向。<br/><br/>贡献点：  <br/>1. **任务定义**：明确将希望语识别确立为NLP领域的重要任务，强调其在社交媒体情感分析中的应用价值。  <br/>2. **模型对比**：系统评估传统机器学习（SVM、Logistic Regression、Naïve Bayes）与细粒度Transformer模型（如BERT）的性能差异。  <br/>3. **数据集验证**：基于预分割的希望语数据集（train/dev/test），量化不同模型的指标表现（宏观F1、加权F1、准确率等）。  <br/>4. **结果分析**：揭示Transformer架构在捕捉希望语的隐含语义（如细微情感表达）上的优势，尤其在小数据集上表现更优。  <br/>5. **未来方向**：提出更大规模的Transformer和大语言模型（LLMs）可能进一步提升希望语识别效果的潜力。|
|2510.23357v1|[Large language model-based task planning for service robots: A review](http://arxiv.org/abs/2510.23357v1)|总结：  <br/>本文系统综述了大语言模型在服务机器人中的整合应用，重点分析其在任务规划中的关键作用，梳理了LLMs的技术基础与多模态输入应用进展，同时指出了当前研究的不足与未来发展方向。<br/><br/>贡献点：  <br/>1. **系统性综述**：全面总结LLMs与服务机器人整合的现状，强调其在复杂环境下的任务规划价值。  <br/>2. **技术梳理**：详细回顾LLMs的核心技术（预训练、微调、RAG、提示工程）及在机器人任务规划中的应用逻辑。  <br/>3. **多模态分析**：探讨LLMs在文本、视觉、音频及多模态输入下的任务规划最新进展与技术挑战。  <br/>4. **未来方向**：提出针对家庭等非结构化场景的LLMs任务规划研究瓶颈与改进路径。|
|2510.22961v1|[Adapting Speech Foundation Models with Large Language Models for Unified   Speech Recognition](http://arxiv.org/abs/2510.22961v1)|总结（100字以内）:<br/>本文提出UASR-LLM框架，通过视觉注入模块和解码器-only LLM的结合，在统一模型中实现VSR、ASR和AVSR任务。采用两阶段训练策略，冻结语音模型参数并优化视觉注入与LLM适配器，验证了方法在多模态场景下的通用性和性能优势。<br/><br/>贡献点：<br/>1. 提出UASR-LLM框架：首次将冻结的语音基础模型(SFM)与大语言模型(LLM)结合，构建统一的跨模态语音识别系统，支持视觉、听觉及视听任务处理。<br/>2. 设计视觉注入模块：创新性地在多层SFM中嵌入视觉表征，实现多模态输入处理和跨模态隐藏状态统一。<br/>3. 实现两阶段训练策略：采用"视觉注入预训练+语音识别微调"的分段训练方法，保持SFM参数冻结，仅优化视觉模块和LLM的LoRA参数。<br/>4. 验证方法通用性：通过消融实验证明该框架对不同SFM和LLM的兼容性，同时在多种环境（干净/嘈杂）下的任务表现优于SOTA基准模型。|
|2510.22860v1|[Far from the Shallow: Brain-Predictive Reasoning Embedding through   Residual Disentanglement](http://arxiv.org/abs/2510.22860v1)|总结：本文提出残差解纠缠方法，分离语言模型中词法、句法、语义及推理成分，并应用于脑记录数据，揭示推理的神经标记具有独特预测性、时间差异及更深的认知贡献。<br/><br/>贡献点：<br/>1. **提出残差解纠缠方法**：通过计算方法将语言模型中的多层级语言特征（词法、句法、语义、推理）进行解耦，生成四组接近正交的嵌入表示。  <br/>2. **揭示推理的时空特征**：发现推理嵌入具有独特预测能力，能解释传统语言特征无法解释的神经活动变异，并扩展至视觉区域。同时，推理信号在时间上显著滞后（~350-400ms），符合层级处理理论。  <br/>3. **验证传统模型的局限性**：证明未解纠缠的LLM嵌入可能掩盖深层认知过程的贡献，其预测效果主要依赖浅层语言特征，而非深层推理机制。|
|2510.22610v1|[Everything counts: the managed omnirelevance of speech in 'human - voice   agent' interaction](http://arxiv.org/abs/2510.22610v1)|总结：  <br/>该研究首次从互动视角揭示语音代理对人类对话行为的影响，提出"人类语音全相关性"概念，并论证现代语音技术增强了这一现象。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次从实证角度分析语音代理对人类对话者的互动约束，突破传统技术视角研究局限。  <br/>2. **构建核心理论**：提出"人类语音全相关性"（omnirelevance）概念，揭示人类在对话中始终存在的"触发风险感知"机制。  <br/>3. **技术影响分析**：论证近期语音采集技术进步使人工智能代理对人类行为的干预效能显著增强，重塑人机交互模式。|
|2510.20867v1|[Incentivizing Consistent, Effective and Scalable Reasoning Capability in   Audio LLMs via Reasoning Process Rewards](http://arxiv.org/abs/2510.20867v1)||
|2510.20508v1|[Assessing the Political Fairness of Multilingual LLMs: A Case Study   based on a 21-way Multiparallel EuroParl Dataset](http://arxiv.org/abs/2510.20508v1)|**总结**：  <br/>该研究提出基于多语言翻译公平性的新框架评估LLMs政治偏见，构建了包含政治归属信息的扩展EuroParl数据集，并揭示主流政党演讲翻译质量优于边缘政党。<br/><br/>**贡献点**：  <br/>1. **方法创新**：提出通过多语言翻译公平性原则评估LLMs政治偏见的替代框架，而非传统英语调查方法。  <br/>2. **数据集构建**：创建首个21-way多平行版本的EuroParl数据集，整合欧盟议会发言及演讲者政治归属信息。  <br/>3. **规模与多样性**：数据集涵盖1.5M句子、40M词和249M字符，包含7国、12欧盟政党、25委员会及1000+演讲者，时间跨度三年。  <br/>4. **实证发现**：发现主流政党（左、中、右）的演讲在翻译质量上显著优于边缘政党，揭示潜在政治偏见关联性。|
|2510.20154v1|[Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](http://arxiv.org/abs/2510.20154v1)|**贡献点**：<br/>1. 首次系统评估大语言模型在立场检测任务中的偏见，揭示其在零样本场景下对特定社会群体存在显著刻板印象。<br/>2. 提出通过标注方言/口音与文本复杂度分析模型偏见的新方法，扩展了传统立场检测的评估维度。<br/>3. 发现模型将非裔美国人方言与反对特朗普立场、支持大麻观点与低文本复杂度等错误关联，暴露偏见的具体表现形式。<br/>4. 强调立场检测任务的敏感性，为后续公平性改进提供实证依据和方向指引。<br/><br/>**总结**：  <br/>该研究首次揭示LLMs在零样本立场检测中存在刻板印象，通过方言与文本复杂度分析，发现模型对社会群体的错误关联，指出立场检测任务的公平性问题。|
|2510.20113v1|[SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment   Assistance](http://arxiv.org/abs/2510.20113v1)|**贡献点总结：**  <br/>1. 提出针对语音障碍用户的辅助通信系统SpeechAgent，整合大语言模型（LLM）与语音处理模块，实现自适应支持。  <br/>2. 开发结构化部署管道，支持移动及边缘设备的实时语音处理，显著降低延迟并保持高准确率与语音质量。  <br/>3. 通过真实语音障碍数据集和边缘设备延迟测试，验证系统在实际场景下的有效性和用户友好性。  <br/><br/>**摘要总结（100字以内）：**  <br/>该研究提出SpeechAgent，是一款整合LLM与语音处理的移动辅助通信系统，通过优化部署实现低延迟实时处理，经实际数据验证，为语音障碍用户提供了高效、个性化的日常沟通解决方案。|
|2510.19782v2|[Adapting Multilingual Models to Code-Mixed Tasks via Model Merging](http://arxiv.org/abs/2510.19782v2)|总结：  <br/>本研究提出通过模型融合提升代码混合NLP任务性能，证明融合比传统微调和持续预训练更有效，并验证了跨语言迁移能力，为低资源代码混合场景提供新的适应策略。<br/><br/>贡献点：  <br/>1. **提出模型融合策略**：将持续预训练（CPT）与模型融合结合，形成新的代码混合NLP适应框架，替代传统微调方法。  <br/>2. **验证融合效果**：在En-Hi和En-Es任务中，融合模型显著优于全微调（FT）和CPT→FT，F1提升2-5点。  <br/>3. **跨语言迁移能力**：通过En-Hi训练的融合模型在En-Ta和En-Ml任务上表现更优，优于单语基线模型。  <br/>4. **数据策略指导**：总结适用于不同数据条件（仅标注、标注+未标注、迁移场景）的适应方案，并讨论其扩展局限与规模化考量。|
|2510.19670v1|[CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware   Cloud-Edge Cooperation](http://arxiv.org/abs/2510.19670v1)|总结：  <br/>CoSense-LLM提出了一种边缘优先框架，通过多模态传感器数据压缩、本地检索与生成协同、动态资源选择及隐私保护机制，实现在干扰环境下的高效语义理解和隐私安全，支持多场景部署。  <br/><br/>贡献点：  <br/>1. **多模态传感数据压缩**：设计SenseFusion模块，将连续的Wi-Fi CSI、IMU、音频、RFID、轻量视觉等传感器流转化为紧凑可验证的语义token，降低传输和计算成本。  <br/>2. **本地混合检索增强**：引入Edge-RAG，结合场景特定策略与本地检索信息，提升生成内容的事实一致性并减少矛盾。  <br/>3. **动态资源调度机制**：通过PromptRouter实现基于成本和不确定性的策略，支持边缘独占生成、边缘+检索混合模式或云级联的高效决策。  <br/>4. **端到端隐私保护**：Secure Execution模块确保原始波形不离开设备，仅传输离散代码和脱敏元数据，满足隐私要求。  <br/>5. **边缘优先系统优化**：兼容分页/流式KV缓存、FlashAttention、推测解码、量化LoRA等技术，优化延迟与能耗，支持非IID漂移下的本地个性化和联邦学习更新。  <br/>6. **严格服务等级目标**：在家庭、办公室、诊所部署中实现亚秒级边缘延迟、跨层级成本降低及隐私保障，验证边缘优先设计的有效性。|
|2510.19641v1|[Style Attack Disguise: When Fonts Become a Camouflage for Adversarial   Intent](http://arxiv.org/abs/2510.19641v1)|**贡献点:**  <br/>1. **提出人类-模型感知差异问题**：揭示风格化字体和emoji在人类可读性与模型token化处理间的矛盾，指出其对NLP模型的潜在威胁。  <br/>2. **设计新型攻击方法SAD**：基于文本风格提出Style Attack Disguise，通过干扰模型token化机制实现攻击。  <br/>3. **双版本攻击框架**：支持轻量级（高效查询）与强力级（高性能攻击）两种模式，适应不同应用场景需求。  <br/>4. **多任务验证有效性**：在情感分类、机器翻译及商业服务中验证SAD的强攻击能力，覆盖传统模型与LLMs。  <br/>5. **扩展攻击范围**：首次将风格化攻击威胁延伸至多模态任务（如文本到图像/语音生成），凸显跨模态风险。  <br/><br/>**总结:**  <br/>该研究提出Style Attack Disguise（SAD），通过风格化文本干扰NLP模型token化处理，验证其对情感分类、机器翻译及多模态任务的有效攻击，揭示人类与模型在文本感知上的差异，为模型鲁棒性研究提供新方向。|
|2510.19358v1|[M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large   Language Models](http://arxiv.org/abs/2510.19358v1)|总结：  <br/>提出M3-SLU多模态基准，填补多说话者对话理解研究空白，整合四个公开语料库构建12,000+验证数据集，设计两个创新任务并提供基线结果，揭示模型在说话者属性识别上的关键缺陷。<br/><br/>贡献点：  <br/>1. **提出新的多模态基准**：M3-SLU是首个针对多说话者、多轮对话中说话者属性推理的MLLM基准，推动该领域发展。  <br/>2. **构建大规模多模态数据集**：整合CHiME-6、MELD、MultiDialog和AMI四个语料库，包含12,000+验证实例，提供音频、文本与元数据的多模态配对。  <br/>3. **定义双重任务框架**：设计"说话者属性问答"与"通过语句匹配确定说话者"两个核心任务，明确研究方向。  <br/>4. **提供基线评估结果**：对比级联管道与端到端模型的性能，采用LLM-as-Judge与准确率指标进行系统性评估。  <br/>5. **揭示技术瓶颈**：通过实验发现模型在说话者身份识别上的显著不足，指出现有的多模态理解研究关键缺口。|
|2510.19331v1|[Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate   Speech Detection](http://arxiv.org/abs/2510.19331v1)|**贡献点：**  <br/>1. 探索了人格化（Persona-LLMs）对仇恨言论检测中身份偏见的影响，揭示了标注者与目标身份关联对模型敏感性的关键作用。  <br/>2. 提出并对比了两种人格提示方法：浅层提示与基于RAG的深层情境化人格构建，验证其对模型性能的差异化效果。  <br/>3. 将心理学中的群体认同理论与NLP技术结合，构建了融入社会人口统计属性的仇恨言论检测框架。  <br/>4. 通过实验分析不同社会群体中模型的检测公平性，明确了人格化方法在减少偏见中的潜力与局限。  <br/>5. 为开发更公正的仇恨言论检测系统提供了理论依据与实证指导，推动技术与社会价值观的融合。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过人格化方法优化大语言模型的仇恨言论检测，结合心理学与NLP技术，揭示身份认同对模型偏见的影响，评估不同人格构建策略的公平性，为提升检测系统的公正性提供新思路。|
|2510.19326v1|[Slot Filling as a Reasoning Task for SpeechLLMs](http://arxiv.org/abs/2510.19326v1)|总结:  <br/>该论文提出将推理机制引入语音大语言模型，通过链式推理框架和混合模型设计提升槽填充任务性能，并评估不同领域基础模型的适用性。<br/><br/>贡献点:  <br/>1. **首次将推理框架应用于语音LLMs**：引入chain-of-thought（CoT）方法，将槽填充任务分解为多步推理流程。  <br/>2. **构建专用推理数据集**：针对语音任务设计包含推理步骤的标注数据集，并通过监督微调策略训练模型。  <br/>3. **对比分析基础模型领域适配性**：发现通用文本基础模型（如数学、逻辑类）在语音任务中存在性能局限。  <br/>4. **提出混合语音LLM架构**：结合混合文本基础模型与多模式微调策略（保留直接推理与链式推理），提升任务表现。|
|2510.19144v1|[Tibetan Language and AI: A Comprehensive Survey of Resources, Methods   and Challenges](http://arxiv.org/abs/2510.19144v1)|总结：本文系统梳理藏语AI领域现状与挑战，识别关键瓶颈，探讨跨语言迁移、多模态学习及社区协作策略，为低资源语言的AI研究提供参考。<br/><br/>贡献点：<br/>1. **全面覆盖藏语AI研究领域**：系统总结了文本与语音数据资源、NLP任务、机器翻译、语音识别及LLMs的最新进展。  <br/>2. **方法分类与性能评估**：对现有数据集、工具进行系统分类，并对不同任务的方法和性能进行对比分析。  <br/>3. **关键瓶颈识别**：明确指出藏语AI发展中的主要挑战，如数据稀疏性、正字法差异、缺乏统一评估标准。  <br/>4. **跨模态与迁移学习建议**：探讨跨语言迁移、多模态学习等可扩展方法在藏语AI中的应用潜力。  <br/>5. **社区协作框架倡导**：强调社区驱动的资源建设与合作机制，推动构建包容可持续的低资源语言AI生态。|
|2510.19031v1|[CLiVR: Conversational Learning System in Virtual Reality with AI-Powered   Patients](http://arxiv.org/abs/2510.19031v1)|贡献点总结（100字以内）：本研究提出CLiVR系统，整合LLMs、语音处理与3D虚拟患者，动态生成病例并反馈沟通情感，替代传统标准化患者训练。通过专家评估验证其可用性、真实性和教育价值，为医学教育提供可扩展、沉浸式的低成本解决方案。|
|2510.18871v1|[How Do LLMs Use Their Depth?](http://arxiv.org/abs/2510.18871v1)|总结：  <br/>该研究通过追踪开源模型的中间表示，揭示LLM深度使用模式，提出Guess-then-Refine框架，并通过三个案例分析说明不同任务中各层的预测行为，为提升计算效率提供理论依据。<br/><br/>贡献点：  <br/>1. 提出"Guess-then-Refine"框架，系统解释LLM在推理中分层计算的结构化机制。  <br/>2. 发现早期层主要输出高频token作为统计猜测，后续层通过上下文信息进行修正，证明预测过程具有动态修正特性。  <br/>3. 通过三个案例研究揭示：功能词最早被预测、多token答案首token需更深计算、多选题格式识别发生在前半层而非最终层。  <br/>4. 为优化Transformer模型计算效率提供新的分析视角和理论基础。|
|2510.18053v1|[Adaptive Divergence Regularized Policy Optimization for Fine-tuning   Generative Models](http://arxiv.org/abs/2510.18053v1)|总结（100字以内）:  <br/>本文提出ADRPO方法，动态调整生成模型微调中的正则化强度，解决探索与利用的平衡问题，实现在文本到图像生成和多模态音频推理中超越大模型的性能表现。<br/><br/>贡献点:<br/>1. **提出ADRPO算法**：设计基于优势估计的自适应分歧正则化机制，通过动态调整正则化强度解决传统固定正则化在探索与利用间的矛盾。<br/>2. **提升生成质量**：在文本-图像生成任务中，采用Wasserstein-2正则化实现流匹配生成模型，优于DPO等离线方法和ORW-CFM-W2等固定正则化在线方法。<br/>3. **跨模态适用性**：兼容KL正则化的文本模型和多模态推理模型，扩展到LLM微调与音频推理场景，实现更广泛的适用性。<br/>4. **小模型超越大模型**：在属性绑定、语义一致性等任务中，2B参数模型效果显著优于4.8B和12B参数模型，验证了方法效率。<br/>5. **对比工业级模型**：在多模态音频推理中，7B模型性能超越Gemini 2.5 Pro和GPT-4o Audio等大参数商业模型，提供可插拔的解决方案。|
|2510.17633v1|[SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated   Refusal Steering](http://arxiv.org/abs/2510.17633v1)|**总结（100字以内）:**  <br/>本文提出SARSteer，首个针对音频语言模型的推理时安全防御框架，通过文本引导的拒绝策略与分解安全空间消融技术，有效解决音频输入引发的有害响应问题，同时保持对无害语音的正常响应。<br/><br/>**贡献点分点:**  <br/>1. **揭示LALMs安全性挑战**：首次系统分析音频输入在安全对齐中面临的关键问题，指出文本引导机制在音频处理中的局限性（分布差异）及提示防御的过度拒绝缺陷。  <br/>2. **提出SARSteer框架**：设计首个针对LALMs的推理时安全防御方法，通过文本衍生的拒绝策略避免直接修改音频输入，结合分解的safe-space消融技术降低过度拒绝风险。  <br/>3. **实验验证有效性**：通过大量实验证明SARSteer在抑制有害查询响应方面效果显著，同时有效保留了对无害语音的正常生成能力。  <br/>4. **推动安全对齐发展**：为LALMs的安全对齐提供理论和实践依据，填补了语音领域安全防御方法的研究空白。|
|2510.15870v2|[OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding   LLM](http://arxiv.org/abs/2510.15870v2)|总结（100字以内）:  <br/>本文提出开源多模态大模型OmniVinci，通过三方面架构创新和24M跨模态对话数据构建，在感知与推理协同性、性能效率及下游应用效果上均显著优于Qwen2.5-Omni。<br/><br/>贡献点分点列出:  <br/>1. 引入OmniVinci开源多模态大模型，首次实现跨模态感知与推理能力的统一建模  <br/>2. 提出三大模型架构创新：OmniAlignNet（视觉-音频对齐）、时序嵌入分组（相对时序对齐）、受限旋转时间嵌入（绝对时序编码）  <br/>3. 构建24M规模跨模态对话数据集，实现单模态与多模态数据的高效合成  <br/>4. 验证多模态协同效应，模型在DailyOmni（跨模态理解）、MMAR（音频）、Video-MME（视觉）三项任务上分别提升19.05/1.7/3.9  <br/>5. 以0.2T训练数据量（仅Qwen2.5-Omni的1/6）实现性能突破，显著降低大模型训练成本  <br/>6. 首次系统展示多模态模型在机器人、医疗AI、智能制造等多领域的应用优势|
|2510.15870v1|[OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding   LLM](http://arxiv.org/abs/2510.15870v1)|**贡献点（分点）：**<br/>1. **提出多模态架构创新**：设计三种关键模块（OmniAlignNet、Temporal Embedding Grouping、Constrained Rotary Time Embedding）以增强跨模态感知与推理能力。<br/>2. **构建大规模多模态数据集**：开发数据合成管道生成24M单模态和多模态对话数据，支持跨模态学习。<br/>3. **显著性能提升**：在DailyOmni、MMAR、Video-MME等任务上分别超越Qwen2.5-Omni达19.05、1.7、3.9个百分点，且训练令牌减少至0.2T（仅为Qwen2.5-Omni的1/6）。<br/>4. **验证多模态优势**：在机器人、医疗AI、智能制造等下游应用场景中证明了OmniVinci的实效性。<br/>5. **开源与跨模态协同效应**：强调模型开源特性，并揭示模态间协同增强感知与推理能力的机制。<br/><br/>**总结（100字内）：**  <br/>OmniVinci通过多模态架构创新、大规模数据生成与优化，显著提升跨模态理解性能，同时降低训练成本，并在机器人、医疗等场景验证了其应用价值，推动多模态AI发展。|
|2510.15685v1|[Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate   Speech Detection](http://arxiv.org/abs/2510.15685v1)|**贡献点（要点）:**  <br/>1. 提出基于LLM动态知识库的上下文生成新方法，用于提升文本和多模态仇恨言论检测性能。  <br/>2. 设计两种上下文生成策略：命名实体聚焦与全文提示法。  <br/>3. 对比四种上下文整合方式：文本拼接、嵌入拼接、分层Transformer融合、LLM文本增强。  <br/>4. 验证上下文信息及整合方法对检测效果的关键影响，在文本和多模态任务中分别提升F1值3和6个百分点。  <br/><br/>**总结（100字内）:**  <br/>本文提出利用LLM生成动态背景上下文的HSD新方法，设计两种生成策略及四种整合方式，并在文本和多模态数据集上验证其有效性，显著提升检测性能。|
|2510.15406v1|[VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to   Disfluency](http://arxiv.org/abs/2510.15406v1)|总结（100字以内）：  <br/>本研究提出VocalBench-DF框架系统评估语音不流畅性，揭示主流Speech-LLMs在处理帕金森等疾病相关语音时性能显著下降，指出音素级处理与长上下文建模是主要瓶颈，并呼吁改善模型鲁棒性以构建包容性语音系统。<br/><br/>贡献点：  <br/>1. **提出VocalBench-DF框架**：首个系统性评估语音不流畅性的多维度框架，覆盖多种语音障碍场景（如帕金森病）。  <br/>2. **揭示现有模型缺陷**：通过大规模评估22个Speech-LLMs，发现其在实际语音干扰场景下的性能严重退化。  <br/>3. **定位关键瓶颈**：识别音素级处理能力和长上下文建模能力为导致不流畅性处理失败的核心原因。  <br/>4. **提出改进方向**：建议通过增强底层识别与高阶推理能力，提升模型对语音干扰的鲁棒性。  <br/>5. **强调包容性需求**：呼吁语音模型研究关注真实用户需求，推动更适用的无障碍语音技术发展。|
|2510.15231v1|[Extending Audio Context for Long-Form Understanding in Large   Audio-Language Models](http://arxiv.org/abs/2510.15231v1)|贡献点总结：<br/>1. 提出Partial YaRN：基于RoPE的无训练音频上下文扩展方法，仅修改音频标记位置以突破LALMs的短音频上下文限制，保留文本处理能力。<br/>2. 开发VLAT训练策略：通过训练时的位置增强技术，模拟多样化的音频长度，提升模型对超长未见过音频的泛化能力与鲁棒性。<br/>3. 实证验证：在SALMONN和Qwen2-Audio数据集上验证了方法有效性，证明Partial YaRN显著优于原始模型，VLAT进一步提升长音频理解性能。|
|2510.14628v1|[RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF](http://arxiv.org/abs/2510.14628v1)|贡献点：<br/>1. 提出RLAIF-SPA框架，首次将AI反馈（ASR和LLM）作为直接奖励机制，优化语音情感表达与可懂度<br/>2. 建立四维细粒度评估体系（结构/情感/速度/语气），联合优化语义准确性和韵律情感对齐<br/>3. 创新性整合ASR评估语义准确性、LLM评估韵律情感匹配，解决情感标注成本高的问题<br/>4. 实现显著性能提升：在Libri Speech数据集上，WER降低26.1%，SIM-O提升9.1%，人工评估超过10%<br/><br/>总结：该研究提出RLAIF-SPA框架，通过AI反馈机制和四维联合优化，有效提升语音情感表达与可懂度，在基准数据集上取得显著效果。|
|2510.13829v1|[A Linguistics-Aware LLM Watermarking via Syntactic Predictability](http://arxiv.org/abs/2510.13829v1)|分点贡献：  <br/>1. 提出STELA框架，首次将水印强度与语言本身的语言自由度（linguistic degrees of freedom）对齐，解决文本质量与检测鲁棒性之间的平衡问题。  <br/>2. 引入基于词性（POS）n-gram建模的语言不确定性作为动态信号调节机制，实现语法约束下减弱水印、自由上下文中增强水印的自适应策略。  <br/>3. 设计无需访问模型logits的检测器，突破了现有方法对模型特定信号的依赖，支持公开可验证的水印检测。  <br/>4. 通过跨语言实验（分析英语、中文、韩语等语言类型），验证STELA在检测鲁棒性上的显著优势，证明其普适性与有效性。  <br/><br/>总结（100字以内）：  <br/>STELA通过语言不确定性动态调节水印强度，设计无需模型logits的检测器，解决了文本质量与检测鲁棒性的矛盾，显著提升多语言环境下的水印检测性能。|
|2510.13632v1|[Closing the Gap Between Text and Speech Understanding in LLMs](http://arxiv.org/abs/2510.13632v1)|总结：本文提出SALAD方法，通过结合跨模态蒸馏与合成数据主动选择，有效缓解语音模型的文本-语音理解差距，显著降低训练数据需求，在公开语料上实现高效性能提升。<br/><br/>贡献点：<br/>1. 提出"文本-语音理解差距"概念，揭示语音适配模型相较于文本模型的系统性性能缺陷<br/>2. 首次从"文本能力遗忘"和"跨模态对齐偏差"两个维度分析语音理解性能下降的根本原因<br/>3. 创新性融合主动数据选择策略与跨模态蒸馏技术，构建SALAD框架实现高效对齐优化<br/>4. 在无需大规模专有数据的条件下，通过公开语料训练即能获得接近强模型的跨模态理解能力<br/>5. 在3B/7B参数规模下验证方法有效性，实现跨模态对齐性能提升与训练数据量级的显著降低|
|2510.13351v1|[Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise   LLM Systems](http://arxiv.org/abs/2510.13351v1)|总结：  <br/>提出多模态安全防护模型Protect，解决实时监控、多模态数据处理和可解释性难题，超越现有模型，在四个安全维度实现最先进性能，为生产环境安全系统提供可靠基础。<br/><br/>贡献点：  <br/>1. **首个多模态防护系统**：提出Protect，是首个集成文本、图像、音频多模态处理的防护模型，突破传统单模态方案限制。  <br/>2. **多维度安全框架**：基于毒性、性别歧视、数据隐私和提示注入四大安全维度设计防护机制，覆盖全面。  <br/>3. **高效微调方法**：采用Low-Rank Adaptation (LoRA)技术对多模态数据进行分类适配器训练，提升模型泛化与效率。  <br/>4. **教师辅助标注流程**：通过结合推理与解释痕迹生成高保真、上下文感知的标签，增强标注质量与可解释性。  <br/>5. **性能超越现有方案**：在实验中展现state-of-the-art表现，优于WildGuard、LlamaGuard-4、GPT-4等开放和商业模型。  <br/>6. **生产级部署能力**：构建可审计、可信的系统架构，支持企业级大规模应用场景的安全防护需求。|
|2510.13281v1|[Two Heads Are Better Than One: Audio-Visual Speech Error Correction with   Dual Hypotheses](http://arxiv.org/abs/2510.13281v1)|总结：提出DualHyp框架和RelPrompt机制，显著提升音频-视觉语音识别的错误纠正效果，并开源相关代码与数据集。<br/><br/>贡献点：  <br/>1. **提出DualHyp框架**：首次在语言空间直接建模模态特异性证据，通过融合ASR和VSR的N-best假设实现跨模态错误纠正。  <br/>2. **设计RelPrompt机制**：引入噪声感知的模态引导提示，动态平衡音频与视觉信息的时间可靠性，提升模型纠偏能力。  <br/>3. **实验证明有效性**：在LRS2数据集上实现57.7%的错误率提升（对比标准ASR基线），远超传统单流GER方法的10%提升。  <br/>4. **开源资源支持**：公开代码与包含ASR/VSR假设的数据集，便于复现与进一步研究。|
|2510.13194v1|[StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis   Preservation](http://arxiv.org/abs/2510.13194v1)|**贡献点（分项）：**<br/><br/>1. **提出Stress-Aware S2ST系统**：首次结合大语言模型（LLMs）实现跨语言重音转换，通过生成目标语言重音标签引导可控TTS模型，保留源语言词汇重音信息。  <br/>2. **解决数据稀缺问题**：开发自动对齐训练数据生成流程，提升数据效率，减少依赖人工标注。  <br/>3. **创新评估方法**：引入"LLM-as-Judge"机制，利用LLM评估翻译中重音保留的准确性与自然度。  <br/>4. **验证有效性**：实验表明，在保持翻译质量、说话人意图和自然度的同时，显著优于基线方法保强调重点。  <br/><br/>**总结：**  <br/>本研究提出了一种基于LLM的stress-aware S2ST系统，通过自动数据生成和新型评估方法，有效保留跨语言重音信息，同时确保翻译质量与自然度。|
|2510.13105v1|[EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs   via Egocentric Social Interaction Perception](http://arxiv.org/abs/2510.13105v1)|总结：  <br/>本文提出EgoSocial数据集和EgoSoD方法，通过整合多模态上下文线索解决AI在社交互动中干预时机判断不足的问题，显著提升模型性能，并将开源数据集和代码。<br/><br/>贡献点：  <br/>1. **构建EgoSocial数据集**：提供包含13,500个社交视频-问题对的基准数据集，用于评估AI在社交互动感知中的干预能力。  <br/>2. **分析现有OLLMs局限性**：系统评估多模态大模型在检测社交语境线索中的不足，揭示干预时机识别的准确率瓶颈（如Gemini 2.5 Pro仅14.4%）。  <br/>3. **提出EgoSoD方法**：设计端到端方案，基于社交思维图动态建模参与者及互动，精准识别何时应干预。  <br/>4. **提升模型性能**：在干预时机任务中，EgoSoD将Phi-4提升45.6%、Gemini 2.5 Pro提升9.9%；整体社交互动任务中分别提升20.4%和6.9%。  <br/>5. **开源数据与代码**：计划发布数据集和代码，推动社交感知AI的进一步研究和应用。|
|2510.12386v1|[Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in   Dashboard Onboarding](http://arxiv.org/abs/2510.12386v1)|总结（100字以内）:  <br/>本文提出DIANA系统，通过多模态交互（聊天、音频、鼠标）实现可视化仪表盘的自助引导，突破传统文本依赖模式，将解释直接集成至界面，并通过用户研究验证不同模态在任务中的适用性。  <br/><br/>**贡献点分点列出：**  <br/>1. **提出多模态交互框架**：首次整合聊天、音频和鼠标操作，支持用户通过多种交互方式自助理解仪表盘功能。  <br/>2. **界面内解释机制**：区别于传统LLM的纯文本交互，直接在仪表盘界面提供交互式解释，增强导航与分析效率。  <br/>3. **自适应引导策略**：允许用户按需选择或组合不同交互模态，动态匹配任务复杂度与学习需求。  <br/>4. **实证研究支持**：通过定性用户研究，分析不同模态在上手任务中的作用及复杂性，为系统设计提供依据。|
|2510.12316v1|[Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech   Generation](http://arxiv.org/abs/2510.12316v1)|总结：  <br/>本文提出基于知识引导的反言论生成框架，整合RAG技术与多源知识库，显著提升生成内容的可信度和适用性，为应对仇恨言论等场景提供更可靠方案。<br/><br/>贡献点：  <br/>1. **创新框架**：首次将反言论生成建模为知识引导的文本生成任务，而非单纯依赖大语言模型（LLM）或专家知识。  <br/>2. **RAG整合**：引入先进检索增强生成（RAG）流水线，增强生成内容的可靠性和连贯性。  <br/>3. **多源知识库**：构建覆盖联合国、欧盟法规及人权机构的32,792文本知识库，支持8类仇恨言论目标群体（女性、少数族裔、残疾人、移民、穆斯林、犹太人、LGBT群体等）。  <br/>4. **多维度评估**：基于MultiTarget-CONAN数据集，通过标准指标（JudgeLM）与人工评估验证框架有效性，超越现有基线与竞争方法。  <br/>5. **通用性拓展**：框架可推广至其他需要可信反言论生成的场景，推动伦理AI与社会责任应用研究。|
|2510.12195v1|[DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech   Translation](http://arxiv.org/abs/2510.12195v1)|贡献点总结（100字以内）：  <br/>提出基于DPO的LLM分段框架，实现偏好对齐，提升实时翻译的自然分割与质量，验证在多个语言对上的优势，推动人类对齐的自适应实时语音翻译。<br/><br/>分点贡献：  <br/>1. **方法创新**：首次将直接偏好优化（DPO）应用于语音分段任务，构建基于LLM的分段框架，通过人类偏好对齐提升分割自然度。  <br/>2. **性能提升**：实验表明，DPO微调的LLM在分割准确率、翻译质量（BLEU/COMET）及延迟（Average Lagging）上均优于SHAS模型。  <br/>3. **跨语言验证**：在英语-日语、中英、德英三组语言对上系统验证，证明方法的泛化能力。  <br/>4. **基线对比**：结合IWSLT基准数据，实现与传统方法的直接对比，凸显偏好对齐的优势。  <br/>5. **应用价值**：为实时语音翻译提供更高效、自然的分段方案，推动自适应与人类对齐的进展。|
|2510.11389v1|[Beyond Survival: Evaluating LLMs in Social Deduction Games with   Human-Aligned Strategies](http://arxiv.org/abs/2510.11389v1)|**总结（100字以内）:**  <br/>本文构建多模态狼人杀数据集，提出基于策略对齐的双阶段评估框架，系统分析LLM在社交游戏中的表现，揭示欺骗与反事实推理的不足，推动语言、推理与策略研究的结合。<br/><br/>**贡献点分点:**  <br/>1. **构建高质量多模态数据集**：首次创建包含100小时视频、32.4M发言标记及15规则变体的人工验证狼人杀数据集，全面覆盖社交游戏的复杂性。  <br/>2. **提出双阶段策略对齐评估方法**：将评价分为语音评估（多选任务测试五个社交维度）与决策评估（分析投票及对手角色推断），填补现有粗粒度指标的不足。  <br/>3. **揭示模型能力差距**：实验表明SOTA LLM在欺骗和反事实推理上表现分化，近半模型得分低于0.50，为社交智能研究提供实证依据。  <br/>4. **推动多智能体交互研究**：通过数据集与评估框架，为语言、推理与策略协同的研究提供基准，促进真实社交场景的模型训练与评估。|
|2510.11129v1|[video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via   Memory](http://arxiv.org/abs/2510.11129v1)|总结（100字以内）:  <br/>本文提出video-SALMONN S，首次实现固定内存下3小时、1 FPS、360p高清视频流处理。通过测试时训练模块与提示相关内存读取器，攻克长视频理解中信息丢失与内存约束难题，在多个基准测试中表现优于传统离线和流式模型。<br/><br/>贡献点:  <br/>1. **首个支持固定内存的高分辨率视频流处理**：实现3小时视频以1 FPS和360p分辨率处理，突破现有模型对帧率和分辨率的限制。  <br/>2. **创新测试时训练（TTT）内存模块**：通过持续更新token表示捕捉长程依赖，替代传统token合并策略，提升信息保留能力。  <br/>3. **提示依赖性内存读取器设计**：基于任务提示选择性检索关键上下文，优化流式处理中的信息检索效率。  <br/>4. **高效优化方法TTT_HF**：采用Hessian-free共轭梯度算法，显著降低TTT模块的计算开销，支持实时适应。  <br/>5. **多基准表现验证**：在Video-MME、LVBench、VideoEvalPro等长视频数据集上验证性能，8B参数模型取得67.8%的Video-MME长分割准确率，超越离线与流式基线。|
|2510.10329v1|[End-to-end Automatic Speech Recognition and Speech Translation:   Integration of Speech Foundational Models and LLMs](http://arxiv.org/abs/2510.10329v1)|总结：本文提出结合预训练语音编码器与大语言模型的端到端ST架构，在英德语对上超越SeamlessM4T，媲美级联系统，COMET_DA22指标提升8%。<br/><br/>贡献点：<br/>1. 提出融合预训练语音编码器与LLMs的端到端ST架构，实现ASR与ST的联合处理<br/>2. 在英德语对测试中，模型性能优于当前大型端到端多模态翻译模型SeamlessM4T<br/>3. 与级联系统（Whisper+NLLB）表现相当，且在COMET_DA22指标上取得8%的提升|
|2510.09424v1|[The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue   State Tracking Approach](http://arxiv.org/abs/2510.09424v1)|**贡献点：**  <br/>1. 系统性比较三种上下文管理策略（传统多模态、完整语音历史、压缩语音历史）在端到端语音对话状态追踪中的效果。  <br/>2. 实验证明完整语音对话输入在模型性能上显著优于传统方法（基于SpokenWOZ语料库）。  <br/>3. 提出基于注意力池的语音历史压缩方法，实现上下文体积减少与性能保持的平衡。  <br/>4. 通过分析揭示性能提升的核心机制：更有效的上下文利用。  <br/><br/>**总结（100字以内）：**  <br/>论文提出三种上下文管理策略，证明完整语音对话输入在性能上优于传统方法，同时展示注意力池压缩的有效权衡，揭示其改进源于更高效的上下文利用机制。|
|2510.09316v1|[Large Language Model Prompt Datasets: An In-depth Analysis and Insights](http://arxiv.org/abs/2510.09316v1)|**贡献点：**  <br/>1. **首次系统性编译多源提示数据集**：收集来自GitHub、社交媒体等多渠道的广泛提示数据集，涵盖多种下游任务、语言及模态。  <br/>2. **系统分析提示构建特征**：选取代表性数据集，揭示不同类别提示的共性与差异，区分其与其他文本语料库（如文献、网页）的特性。  <br/>3. **提出基于句法嵌入的优化方法**：利用词性与依存结构的句法嵌入，通过识别提示的中心表示并引导LLM重写提示，提升输出有意义性。  <br/>4. **开放数据与代码资源**：公开数据集和实现代码，促进学术研究与应用实践。  <br/><br/>**总结（100字以内）：**  <br/>本文首次系统整理多源提示数据集，分析其与文本语料库的差异，并提出基于句法嵌入的优化方法，提升LLM输出质量，开放数据和代码以促进研究。|
|2510.08608v1|[MMA-ASIA: A Multilingual and Multimodal Alignment Framework for   Culturally-Grounded Evaluation](http://arxiv.org/abs/2510.08608v1)|总结：  <br/>提出首个跨文本、图像、语音三模态对齐的亚洲文化理解数据集MMA-ASIA，构建五维评估体系，创新文化知识验证方法，揭示模型文化差异原因，推动构建文化可靠型多模态大模型。<br/><br/>贡献点：  <br/>1. **首个跨模态对齐数据集**：创建涵盖8个亚洲国家、10种语言、27,000题的MMA-ASIA基准，实现文本、图像与语音输入模态的对齐，支持跨模态迁移学习研究。  <br/>2. **五维文化评估框架**：提出文化意识差异、跨语言一致性、跨模态一致性、文化知识泛化及接地有效性五个维度，系统量化LLM在亚洲文化场景中的表现。  <br/>3. **文化知识验证模块**：设计"快捷学习"检测机制，通过验证文化知识与答案的关联性确保评估严谨性，避免模型依赖浅层特征。  <br/>4. **多维度模型分析方法**：结合模型对比、注意力追踪与VPR技术，揭示跨语言/模态差异的底层原因，为文化可靠模型开发提供改进方向。  <br/>5. **推动文化意识研究**：聚焦亚洲多语境、多模态文化理解，填补LLM在非西方、低资源场景的评估空白，推动多模态模型的文化适应性优化。|
|2510.08373v1|[DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching](http://arxiv.org/abs/2510.08373v1)|**贡献点总结（100字以内）：**  <br/>提出DialoSpeech双轨架构结合LLM与Chunked Flow Matching，支持中英文及跨语言对话合成，解决双轨数据稀缺及交互动态难题，实现自然连贯的多轮对话，实验表现优于基线。<br/><br/>**分点贡献：**  <br/>1. **创新架构**：设计DialoSpeech框架，整合大语言模型与Chunked Flow Matching技术，提升对话语音的表达力与交互自然性。  <br/>2. **数据构建**：开发双轨对话数据处理流程，构建支持多轮对话的数据集，解决数据稀缺问题并增强训练可扩展性。  <br/>3. **多语言支持**：实现中英文及跨语言对话语音合成，覆盖更广泛的使用场景与语言需求。  <br/>4. **实验验证**：通过多维度实验验证模型优越性，对比基线方法在自然性、连贯性及交互动态（如轮流、重叠、一致性）上表现更优。|
|2510.07497v1|[Can Speech LLMs Think while Listening?](http://arxiv.org/abs/2510.07497v1)|**贡献点总结：**  <br/>1. **提出CoT微调方法**：首次将链式推理微调应用于多流语音LLMs，显著提升语音推理任务的准确性（平均2.4倍）。  <br/>2. **设计熵基指标**：引入“问题完整性”（基于熵的指标），指导模型在用户查询结束前启动推理，优化准确率-延迟权衡。  <br/>3. **结合DPO与拒绝采样**：通过直接偏好优化（DPO）和基于拒绝采样的偏好数据，进一步提升模型性能，实现延迟降低70%且准确率不下降。  <br/>4. **实验证明有效性**：在ARC-Easy任务中验证方法有效性，等效延迟下准确率提升4%，证实了多流语音LLMs的推理优化潜力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出基于熵的“问题完整性”指标和DPO优化方法，显著提升多流语音LLMs的推理准确率与效率，在等效延迟下实现4%准确率提升，且总延迟降低70%，为语音交互系统的实时性与智能性提供了新方案。|
|2510.07458v3|[Populism Meets AI: Advancing Populism Research with LLMs](http://arxiv.org/abs/2510.07458v3)|总结：  <br/>本研究提出一种基于评分标准和链式推理的新方法，利用全球民粹主义数据库训练大语言模型，证明其可有效分类民粹主义内容，媲美专家水平，解决了传统文本分析的局限性。<br/><br/>贡献点：  <br/>1. **提出新型提示策略**：设计了结合评分标准（rubric）和锚点（anchor）的链式推理（CoT）方法，模拟人类编码员的训练过程，提升模型对复杂概念的理解。  <br/>2. **构建跨语言的评估框架**：通过全球民粹主义数据库（GPD）实现多语言、多语境和大规模语料的测试，克服传统方法在语言扩展上的限制。  <br/>3. **验证模型性能**：在GPD标注数据的基础上，对比多个专有和开源模型的分类准确率，证明该策略能与专家人类编码员达到同等水平。  <br/>4. **推动民粹主义研究**：为语音领域（或相关文本分析）提供可扩展、客观的自动化工具，降低人工标注成本，提升研究效率。|
|2510.07458v2|[Populism Meets AI: Advancing Populism Research with LLMs](http://arxiv.org/abs/2510.07458v2)|总结（100字以内）:  <br/>本研究提出基于评分表和锚点引导的链式思维提示方法，利用全球民粹主义数据库测试不同模型，证明LLM在民粹主义分类中可达到专家水平，解决传统方法的高成本和跨语言扩展难题。<br/><br/>贡献点:  <br/>1. **创新性方法**：设计链式思维（CoT）提示框架，模拟人类编码员的培训流程，实现对民粹主义语义的自动化评估。  <br/>2. **跨语言适用性**：通过统一标注的数据集（GPD）验证方法的可迁移性，突破传统文本分析的多语言局限。  <br/>3. **模型效能验证**：对比多个专有与开源模型在GPD上的表现，证明LLM可达到专家级分类准确度。  <br/>4. **降低分析成本**：提供高效、可扩展的自动化解决方案，替代人工标注的高成本与低效率问题。  <br/>5. **理论与实践结合**：建立可复用的标注标准，为后续跨语言、跨语境的民粹主义研究提供方法论基础。|
|2510.07458v1|[Populism Meets AI: Advancing Populism Research with LLMs](http://arxiv.org/abs/2510.07458v1)|总结：  <br/>本研究提出一种基于评分表和链式推理的新型方法，利用全球民粹主义数据库验证大语言模型在分类准确性上可与专家人类编码员媲美，展示了其处理民粹主义复杂性与语境敏感性的能力。<br/><br/>贡献点：  <br/>1. **创新方法**：设计基于评分表和锚点引导的链式推理提示策略，模拟人类编码员的训练过程，实现对民粹主义内容的自动化测量。  <br/>2. **数据集利用**：依托Global Populism Database（GPD），通过结构化文档引导LLM推理，验证模型在真实标注数据上的表现。  <br/>3. **模型验证**：在GPD框架下测试多种专有与开源大模型，证明其分类能力可达到专家水平，推动民粹主义分析的技术替代。  <br/>4. **跨领域应用**：解决传统文本分析方法在语言、语境和大规模语料中的扩展难题，为语音领域或其他多模态研究提供可迁移的框架。  <br/>5. **启发未来研究**：揭示LLM在处理复杂社会议题（如民粹主义）中的潜力，为构建更精准的自动化编码系统提供实践路径。|
|2510.07249v2|[TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video   Generation](http://arxiv.org/abs/2510.07249v2)|**贡献点：**  <br/>1. **提出TalkCuts数据集**：首个大规模多镜头人类语音视频数据集，包含164k条视频片段（>500小时）和多样视角（特写/半身/全身），突破传统单镜头数据集的局限。  <br/>2. **多模态标注体系**：集成高质量文本描述、2D关键点、3D SMPL-X运动注释，覆盖10k+身份，支持跨模态学习与性能评估。  <br/>3. **LLM引导生成框架Orator**：创新性地将语言模型作为多模态导演，实现镜头转换、肢体动作、声调调节的协同生成。  <br/>4. **提升生成视频质量**：验证TalkCuts在姿态引导和音频驱动场景下显著增强生成视频的叙事连贯性与视觉吸引力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出TalkCuts数据集及Orator框架，首次实现多镜头语音视频的多模态生成与协同控制，显著提升生成内容的连贯性与视觉表现，为可控语音视频生成和多模态学习提供重要资源。|
|2510.06917v2|[SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](http://arxiv.org/abs/2510.06917v2)|**贡献点：**  <br/>1. **提出SHANKS框架**：首次设计支持语音语言模型（SLM）在用户输入过程中实时生成无言推理链的通用推理框架。  <br/>2. **实时交互机制**：通过固定时长语音分块处理，实现边听边推理，允许模型在用户发言未结束时进行干预和工具调用。  <br/>3. **实验验证效果**：在两个场景中显著提升性能：(1) 数学问题回答中错误打断准确率提高37.1%；(2) 工具对话中提前完成56.9%的工具调用。  <br/>4. **推动模型演进**：改变传统模型仅在用户回合结束后进行推理的范式，实现对话全程持续思考。  <br/><br/>**总结（100字以内）：**  <br/>SHANKS框架通过实时语音分块处理与无言推理，使SLM能在用户发言过程中动态干预并完成任务，显著提升数学问题回答的准确性及工具调用效率，推动语音交互向全程持续思考演进。|
|2510.05864v1|[Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input](http://arxiv.org/abs/2510.05864v1)|总结：  <br/>本研究系统分析了大语言模型在长上下文下对有害内容的敏感性，揭示了内容比例、位置、类型及上下文长度对模型表现的影响规律，为安全关键场景下的模型优化提供理论依据。<br/><br/>贡献点：  <br/>1. **首次系统研究**：系统探讨了LLMs在长上下文中对有毒、攻击性和仇恨言论的敏感性，填补了安全关键场景下的研究空白。  <br/>2. **多维度分析**：量化评估了有害内容的类型（显性/隐性）、位置（开头/中间/结尾）、比例（0.01-0.50）和上下文长度（600-6000 tokens）对模型性能的影响。  <br/>3. **关键发现**：提出性能随有害比例呈倒U型变化，召回率随上下文长度增加而下降，显性内容检测更可靠等规律性结论。  <br/>4. **应用指导意义**：揭示LLMs在安全关键场景中的能力边界与优化方向，强调了在长上下文处理中需平衡内容分布与检测效率的挑战。|
|2510.05150v2|[Chronological Thinking in Full-Duplex Spoken Dialogue Language Models](http://arxiv.org/abs/2510.05150v2)|**贡献点**：  <br/>1. 提出**Chronological Thinking**机制，解决全双工对话系统中听与说脱节的问题，模拟人类对话时的轻量思考行为。  <br/>2. 引入**严格因果性**原则：模型在监听过程中仅依赖历史音频更新推理，避免未来信息干扰。  <br/>3. 实现**无额外延迟**的推理：在用户停止说话后立即终止思考并生成响应，提升实时性。  <br/>4. 通过客观指标与人工评估验证有效性，证明响应质量显著提升。  <br/>5. 机制具备鲁棒性，能处理对话动态并保持全双工交互指标的竞争力。  <br/><br/>**总结**：  <br/>提出Chronological Thinking机制，通过严格因果推理和消除延迟提升全双工对话系统响应质量，并验证其在对话动态处理中的有效性。|
|2510.05150v1|[Chronological Thinking in Full-Duplex Spoken Dialogue Language Models](http://arxiv.org/abs/2510.05150v1)|**贡献点总结：**  <br/>1. 提出Chronological Thinking机制，解决全双工对话系统中监听阶段代理空闲的问题。  <br/>2. 实现严格因果推理（仅基于历史音频更新假设）与无额外延迟（监听后立即生成响应）。  <br/>3. 通过实验验证提升响应质量，兼容动态对话行为并取得竞争力的全双工交互效果。  <br/><br/>（注：全文共98字，符合要求。）|
|2510.04899v1|[Human Behavior Atlas: Benchmarking Unified Psychological and Social   Behavior Understanding](http://arxiv.org/abs/2510.04899v1)|总结：本研究提出Human Behavior Atlas统一基准数据集，整合多模态与多任务，优化多模态大模型训练效率与跨域泛化能力，验证三种模型在行为理解任务中的优越性及迁移潜力。<br/><br/>贡献点：<br/>1. 构建首个统一多模态行为理解基准Human Behavior Atlas，涵盖10万+跨文本、音频、视觉样本，覆盖情感、认知、病理与社交任务<br/>2. 实现多任务联合训练框架，显著降低数据冗余与标注成本，提升跨任务规模化训练效率<br/>3. 建立行为特征跨域泛化机制，增强模型在不同场景下的行为理解适应性<br/>4. 开发OmniSapiens系列模型（7B参数量三版本），验证其在多模态行为分析任务中超越现有模型的性能<br/>5. 证实预训练数据集对新型行为数据的迁移价值，行为描述符定向利用实现性能突破|
|2510.04641v2|[Evaluating LLMs for Demographic-Targeted Social Bias Detection: A   Comprehensive Benchmark Study](http://arxiv.org/abs/2510.04641v2)|**贡献点（分点列出）：**  <br/>1. 提出首个针对英文文本的综合评估框架，系统分析LLMs在检测人口统计学偏见中的能力。  <br/>2. 将偏见检测任务定义为多标签问题，构建基于人口统计学的分类体系以满足监管需求。  <br/>3. 总结并对比了不同模型（提示、上下文学习、微调）及规模在偏见检测中的表现。  <br/>4. 使用12个跨多领域和多人群的多样化数据集，全面验证方法的有效性与局限性。  <br/>5. 首次证明微调的小型模型在可扩展性检测中的潜力，为实际应用提供新思路。  <br/>6. 暴露当前研究在跨维度偏见和多群体交互偏见中的空白，强调构建更高效审计框架的必要性。  <br/><br/>**总结（100字内）：**  <br/>本研究提出综合评估框架，系统分析LLMs在检测人口统计学偏见中的表现，揭示当前方法局限，并论证微调小型模型的可行性，为实现可扩展、多维度的偏见审计提供方向。|
|2510.04641v1|[Evaluating LLMs for Demographic-Targeted Social Bias Detection: A   Comprehensive Benchmark Study](http://arxiv.org/abs/2510.04641v1)|**贡献点总结：**  <br/>1. 提出全面评估框架，系统分析LLMs检测英文文本中的demographic-targeted社会偏见能力。  <br/>2. 将偏见检测框架为多标签任务，引入基于群体的分类体系。  <br/>3. 跨尺度验证多种检测方法（提示、上下文学习、微调）的效能。  <br/>4. 通过12个多类型、多群体数据集证明小型微调模型的可扩展性潜力。  <br/>5. 揭示现有方法在跨群体偏见及多轴覆盖上的不足，强调需更有效的审计框架。  <br/><br/>**100字内总结：**  <br/>该研究构建了首个针对英文文本的LLMs偏见检测评估框架，通过多标签任务和多样数据集系统分析模型性能，指出小型微调模型具备可扩展检测潜力，但现有技术在跨群体偏见和多轴覆盖上存在显著局限，亟需更全面的审计方法改进。|
|2510.04637v1|[Social Agent: Mastering Dyadic Nonverbal Behavior Generation via   Conversational LLM Agents](http://arxiv.org/abs/2510.04637v1)|总结：  <br/>提出Social Agent框架，通过LLM驱动的代理系统与双人手势生成模型，实现自然同步的对话非语言行为合成，并建立动态反馈机制提升交互质量。<br/><br/>贡献点：  <br/>1. **提出新型交互框架**：Social Agent首次整合LLM与非语言行为生成，实现双人对话中语境适配的非语言行为合成。  <br/>2. **LLM驱动的代理系统**：通过LLM控制对话流程，自主决策双方互动行为，提升对话连贯性与自然度。  <br/>3. **双人手势生成模型**：基于自回归扩散模型，从语音信号中生成协调一致的手势动作，实现行为与运动的同步。  <br/>4. **动态反馈机制**：系统实时分析对方动作，推断意图并形成闭环反馈，支持更自然的响应式交互。  <br/>5. **实证验证有效性**：用户研究与定量评估证明模型显著提升双人互动质量，生成高度自然的非语言行为。|
|2510.04145v1|[Automating construction safety inspections using a multi-modal   vision-language RAG framework](http://arxiv.org/abs/2510.04145v1)|**总结（100字以内）:**  <br/>本研究提出多模态RAG框架SiteShield，结合视觉与音频输入实现施工安全报告自动化，解决传统方法低效及现有模型存在的模态限制、幻觉和响应不精准等问题，实验表明其在准确率和召回率上显著优于单模态模型。<br/><br/>---<br/><br/>**贡献点分点总结:**  <br/>1. **提出多模态RAG框架**  <br/>   - 开发SiteShield系统，整合视觉（如施工场景图像）与音频（如现场声音）输入，突破传统单模态模型的局限性。  <br/><br/>2. **解决关键技术挑战**  <br/>   - 针对现有方法存在的无关/不具体响应、模态输入受限、幻觉等问题，设计增强的模态融合机制与语义对齐策略。  <br/><br/>3. **实验证明有效性**  <br/>   - 在真实数据集上验证，SiteShield在F1分数（0.82）、Hamming损失（0.04）、精确率（0.76）和召回率（0.96）等指标上显著优于未使用RAG的单模态LLMs。  <br/><br/>4. **推动安全报告自动化**  <br/>   - 首次将LVLM与RAG结合应用于施工安全报告生成，为高效、精准的多模态信息检索与报告生成提供新范式。|
|2510.04016v1|[Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](http://arxiv.org/abs/2510.04016v1)|**贡献点总结（100字以内）：**  <br/>提出首个针对泰语音文字转换的系统性EOT检测研究，对比零样本/少样本提示与监督微调方法，利用语料库和语言特征构建二进制决策模型，建立泰语基准并验证小型模型在实时设备上的低延迟有效性。<br/><br/>**分点贡献：**  <br/>1. **首次系统性研究**：提出首个针对泰语音文字转换的结束点检测（EOT）方法，填补该领域的研究空白。  <br/>2. **方法对比**：对比零样本提示、少样本提示与监督微调在轻量模型中的性能，分析其准确率与延迟的权衡关系。  <br/>3. **语言特征利用**：结合泰国语料库（YODAS）和本土语言线索（如句末助词），构建基于词边界的二进制EOT决策框架。  <br/>4. **公开实现方案**：提供可直接使用的实现计划，推动泰国语音处理技术的实际应用。  <br/>5. **设备端适用性验证**：验证小规模模型在实时设备上的高效性，证明其适合部署于低资源环境下的语音交互系统。|
|2510.03965v1|[FinCall-Surprise: A Large Scale Multi-modal Benchmark for Earning   Surprise Prediction](http://arxiv.org/abs/2510.03965v1)|总结：<br/>本文提出首个大规模多模态金融会议数据集FinCall-Surprise，通过基准测试揭示LLMs在盈利预测中的关键局限，并建立新的研究基准。<br/><br/>贡献点：<br/>1. 构建首个开放获取的多模态金融会议数据集（含文本/音频/视觉模态），覆盖2688次2019-2021年财报电话会议<br/>2. 开发全面的基准评测体系，系统评估26个最新单模态和多模态LLMs在财务预测任务中的表现<br/>3. 识别LLMs在财务推理中的三个关键问题：类别不平衡导致的性能虚高、指令遵循能力缺陷、多模态信号利用不足<br/>4. 为未来研究提供具有挑战性的基准线，推动语音与金融文本联合建模技术的发展|
|2510.03829v1|[A4FN: an Agentic AI Architecture for Autonomous Flying Networks](http://arxiv.org/abs/2510.03829v1)|总结（100字以内）:  <br/>本文提出A4FN架构，通过生成式AI与LLM实现飞行网络的意图驱动自动化，支持实时动态网络控制与适应性重构，适用于灾难响应等高要求场景，并探讨多代理协调与Agentic AI集成的挑战。<br/><br/>贡献点分点列出:  <br/>1. 提出A4FN架构：首个集成生成式AI与LLM的Agentic AI系统，用于飞行网络（FNs）的意图驱动自动化控制。  <br/>2. 双代理协同设计：  <br/>   - **感知代理（PA）**：实现多模态输入（图像/音频/遥测）的语义解析，生成服务级别规范（SLSs）。  <br/>   - **决策-行动代理（DAA）**：基于意图推断动态重构网络并管理资源。  <br/>3. 关键特性融合：支持自主决策、目标导向推理及持续的感知-行动闭环，适应高动态环境。  <br/>4. 场景针对性优化：专为灾难响应等基础设施受限场景设计，强化自适应性和跨技术互操作性。  <br/>5. 指明研究方向：提出下一代飞行网络中多代理协调与Agentic AI集成的开放挑战与技术需求。|
|2510.03153v1|[Improving Cooperation in Collaborative Embodied AI](http://arxiv.org/abs/2510.03153v1)|总结：  <br/>本研究通过优化提示策略和整合语音功能，显著提升了多智能体系统的协作效率，提出改进的CoELA框架并验证了其在虚拟环境中的有效性，为AI协作交互提供了新方向。<br/><br/>贡献点：  <br/>1. **提出优化的提示策略**：系统评估多种提示方法，发现优化组合可提升协作系统性能（如Gemma3效率提升22%）。  <br/>2. **改进CoELA框架**：增强原有框架，整合LLM能力以支持多智能体通信、推理与任务协调。  <br/>3. **引入语音交互功能**：扩展研究范围，实现语音驱动的协作交互，提升用户界面的沉浸感与可用性。|
|2510.03093v1|[Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better   Scaling than CoT Prompting?](http://arxiv.org/abs/2510.03093v1)|总结（100字以内）：  <br/>本研究通过伪标签方法构建多语言S2TT数据集，系统对比了CoT与直接提示策略在不同数据规模下的效果，发现直接提示在数据量增加时表现更优，挑战了CoT在S2TT领域的主流地位。<br/><br/>贡献点：  <br/>1. **系统比较CoT与直接提示策略**：首次系统分析两种方法在S2TT任务中的性能差异，揭示其对数据量的依赖性。  <br/>2. **提出多语言伪标签数据生成方法**：创新性地将ASR语料翻译为六种欧洲语言，构建跨语言S2TT训练数据。  <br/>3. **验证数据规模对模型效果的影响**：通过不同数据量实验，明确数据量增长对两种策略性能的动态影响规律。  <br/>4. **得出直接提示更优的结论**：实验证明直接提示在数据量增加时更稳定，为S2TT模型设计提供了新方向。|
|2510.02425v1|[Words That Make Language Models Perceive](http://arxiv.org/abs/2510.02425v1)|总结：  <br/>本研究提出通过感官提示激活纯文本LLM的视觉/听觉表示，证明轻量级提示工程能提升其在多模态任务中的表现，为语音处理提供了新方法。<br/><br/>贡献点：  <br/>1. **提出感官提示机制**：首次将“视觉”“听觉”等感官指令作为显式提示，引导文本模型模拟多模态感知流程。  <br/>2. **验证表示对齐效果**：实验证明轻量级提示工程可使纯文本训练的LLM在视觉、音频任务中与专业编码器达成表征对齐。  <br/>3. **揭示隐含多模态规律**：发现文本模型虽未直接接触感官数据，但其内部表示已隐含多模态规律，通过提示可激活相关表征。  <br/>4. **拓展LLM应用场景**：为文本模型在语音任务中的潜力提供理论依据，推动跨模态任务的轻量化实现。|
|2510.02352v1|[Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and   Recommendations](http://arxiv.org/abs/2510.02352v1)|**总结**：  <br/>该论文首次系统性研究语音对话模型中的偏见，提出GUS和SNSR新指标，对比开源与闭源模型表现差异，并发布FairDialogue数据集与代码，助力构建公平可靠的音频交互系统。<br/><br/>**贡献点**：  <br/>1. **首次系统性研究**：系统评估了端到端语音对话模型（SDMs）中的偏见问题，填补了该领域研究空白。  <br/>2. **多轮对话影响分析**：揭示重复负面反馈在多轮对话中可能加剧偏见，影响决策与推荐任务的公平性。  <br/>3. **模型对比**：通过开源模型（Qwen2.5-Omni、GLM-4-Voice）与闭源API（GPT-4o Audio、Gemini-2.5-Flash）对比，发现闭源模型偏见较低，而开源模型对年龄、性别更敏感。  <br/>4. **新评估指标**：提出Group Unfairness Score (GUS)用于决策任务，Similarity-based Normalized Statistics Rate (SNSR)用于推荐任务，量化偏见特性。  <br/>5. **数据集与工具发布**：公开FairDialogue数据集与评估代码，为后续研究提供基准与实验资源。|
|2510.02206v1|[Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](http://arxiv.org/abs/2510.02206v1)|**贡献点：**  <br/>1. **提出Poolformer模型**：通过用循环层替代自注意力机制，并引入池化操作，解决长序列处理的效率瓶颈问题。  <br/>2. **创新结构设计**：采用递归定义的SkipBlocks，包含残差块、双向池化（下池化与上池化）及嵌套SkipBlock，优化信息传递路径。  <br/>3. **实验验证有效性**：通过大量实验证明池化操作可加速训练、提升感知指标（FID/IS）并抑制过拟合。  <br/>4. **明确层级功能分工**：揭示深层处理长程依赖、浅层捕捉短时特征的机制，增强模型可解释性。  <br/>5. **性能突破**：在原始音频数据上超越现有SOTA模型（SaShiMi、Mamba），展现优越性。  <br/>6. **多模态应用潜力**：拓展至文本、视觉及多模态场景，支持对密集表示（如图像/视频）的高效处理。  <br/><br/>**总结（100字以内）**：  <br/>Poolformer通过结合循环结构与池化操作，解决了Transformer长序列处理效率问题，提升训练速度与感知指标，同时通过层级分工优化依赖建模，超越现有SOTA模型，并拓展至多模态任务。|
|2510.02044v1|[Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming   Tool Usage](http://arxiv.org/abs/2510.02044v1)|贡献点：  <br/>1. 提出首个将工具调用集成到语音输入-语音输出对话系统的方法（Streaming RAG）；  <br/>2. 通过并行预测工具查询与用户语音，显著降低用户感知延迟（减少20%）；  <br/>3. 设计后训练流程，使模型在对话中动态判断调用工具时机并生成融合语音与文本结果的口语摘要；  <br/>4. 构建AudioCRAG数据集，作为语音对话系统的基准测试工具；  <br/>5. 实验证明方法在问答准确率（提升200%）和用户体验上均优于传统系统，且具备模态无关性，可扩展至文本输入场景。  <br/><br/>总结：  <br/>本论文提出Streaming RAG框架，首次将工具调用引入语音对话系统，通过并行处理降低延迟并提升问答准确率，构建专用基准AudioCRAG，为实时AI助手提供通用解决方案。|
|2510.01995v1|[LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and   Target](http://arxiv.org/abs/2510.01995v1)|总结：  <br/>该研究提出首个多任务孟加拉语仇恨言论数据集，评估了LLMs在低资源环境下的表现，强调文化语料预训练对检测系统的重要性，并公开数据与代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **构建首个多任务数据集**：推出BanglaMultiHate，首次涵盖仇恨言论类型、严重程度和目标等多维度信号，填补低资源语言领域空白。  <br/>2. **全面模型对比**：系统性比较传统基线、单语预训练模型及LLMs（零样本提示+LoRA微调）的性能，揭示不同方法在低资源场景下的优劣势。  <br/>3. **文化语料预训练的重要性**：实验表明，尽管LoRA微调模型表现优异，但结合文化与语言背景的预训练仍是提升检测鲁棒性的关键因素。  <br/>4. **开源与可复现性**：公开数据集及代码脚本，推动学术研究和实际应用的可验证性，为后续研究提供基础支持。|
|2510.01606v1|[Bridging Collaborative Filtering and Large Language Models with Dynamic   Alignment, Multimodal Fusion and Evidence-grounded Explanations](http://arxiv.org/abs/2510.01606v1)|总结（100字以内）:  <br/>本研究提出\model{}框架，通过在线适应机制、统一多模态表示和可验证的解释系统，解决传统推荐模型在动态用户偏好、多模态内容处理和可解释性方面的不足，实现高效且实用的推荐系统。<br/><br/>贡献点：  <br/>1. **在线适应机制**：引入轻量模块持续整合新用户交互数据，无需重新训练大模型，提升动态环境下的推荐时效性。  <br/>2. **统一多模态表示**：融合协同过滤信号与视觉/音频特征，支持跨模态内容处理及缺失模态的鲁棒性。  <br/>3. **可验证解释系统**：生成基于协同模式和物品属性的自然语言解释，增强推荐结果的信任度与用户可理解性。|
|2510.00499v2|[MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance](http://arxiv.org/abs/2510.00499v2)|总结（100字以内）：  <br/>MOSS-Speech是首个无需文本中间步骤的语音到语音大语言模型，通过模态分层架构和冻结预训练策略实现高效语音交互。在语音问答和端到端语音生成任务中达到SOTA，在保持文本能力的同时突破传统系统瓶颈，开创表达性与效率兼备的新范式。<br/><br/>贡献点：  <br/>1. **提出首个端到端语音到语音模型**：MOSS-Speech直接生成和理解语音，无需依赖文本中间表示，突破传统级联流水线限制。  <br/>2. **模态分层架构与冻结预训练策略**：结合多模态分层设计与冻结预训练机制，保留文本LLM的知识推理能力，同时增强语音生成能力。  <br/>3. **性能优势**：在语音问答任务中取得SOTA结果，语音到语音生成性能与现有文本引导系统相当，且保持文本任务的竞争力。  <br/>4. **新范式建立**：通过弥合文本引导与直接语音生成的差距，为高效、可表达的语音交互系统提供全新技术框架。|
|2509.26593v1|[Exploring Large Language Model as an Interactive Sports Coach: Lessons   from a Single-Subject Half Marathon Preparation](http://arxiv.org/abs/2509.26593v1)|**贡献点分点总结**  <br/>1. **LLM作为长期虚拟教练的实证验证**：首次通过为期两个月的单案例研究，展示LLM在半马训练中的实际应用效果，覆盖规划、解释和激励功能。  <br/>2. **性能提升量化分析**：提供具体数据证明训练效果，如从2 km/7分54秒/公里提升至21.1 km/6分30秒/公里，以及步频、心率耦合和效率指数的改善。  <br/>3. **系统局限性识别**：指出当前LLM教练系统的不足，包括缺乏实时传感器集成、仅文本反馈、用户驱动的激励机制及个性化与安全措施的缺失。  <br/>4. **下一代系统设计框架**：提出针对个性化、安全性和交互性的设计要求，如持久化运动员模型、多模态本地感知、多反馈方式（音频、触觉、视觉）、主动性激励支持及隐私保护技术。  <br/>5. **跨领域设计导向**：为LLM从“回顾性顾问”向“闭环教练伴侣”的演进提供理论依据和实践设计方向，推动语音技术与运动科学的结合。  <br/><br/>**总结**（100字以内）：  <br/>本研究首次验证LLM作为长期虚拟教练的可行性，展示训练成效并揭示系统局限，提出多模态感知、主动激励与隐私保护等设计需求，推动LLM从顾问角色向闭环教练伴侣的演进。|
|2509.26514v1|[BatonVoice: An Operationalist Framework for Enhancing Controllable   Speech Synthesis with Linguistic Intelligence from LLMs](http://arxiv.org/abs/2509.26514v1)|**贡献点总结：**  <br/>1. 提出基于"操作主义"的新型范式，解耦指令理解与语音生成流程。  <br/>2. 开发BatonVoice框架，利用LLM作为"指挥"生成文本化语音特征（如音高、能量）。  <br/>3. 构建专用TTS模型BatonTTS，实现高效可控语音合成。  <br/>4. 验证方法在情感语音合成中的优越性，支持零样本跨语言泛化能力。  <br/><br/>**100字内摘要：**  <br/>本研究提出BatonVoice框架，通过解耦LLM的指令理解和TTS的语音生成，实现可控情感语音合成。结合专用BatonTTS模型，显著提升合成质量并支持零样本跨语言泛化，有效释放LLMs的语义智能。|
|2509.26329v1|[TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics](http://arxiv.org/abs/2509.26329v1)|总结：  <br/>本文提出TAU基准，聚焦台湾文化特有的声标音频，通过人工与LLM结合构建数据集，揭示现有大模型在本地化任务上的局限性，推动更具文化包容性的语音评估体系。<br/><br/>贡献点：  <br/>1. **构建首个聚焦文化独特声标的基准**：创建TAU，系统收录台湾日常"声标"（soundmarks），填补了现有大模型评估中忽视文化特有音频的空白。  <br/>2. **创新数据构建方法**：采用"人工筛选+人类编辑+LLM辅助生成问题"三阶段流程，生成702个音频片段和1794个多项选择题，突破依赖转录文本的评估局限。  <br/>3. **实证揭示文化盲点**：通过对比实验发现，顶尖模型（如Gemini 2.5、Qwen2-Audio）在认知本地文化音频任务中显著弱于本地人类，凸显模型文化适应性的缺陷。  <br/>4. **提出本地化评估的必要性**：强调需建立区域化基准以推动公平的多模态评估，确保模型服务于非主流文化社区。|
|2509.25842v1|[HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided   Controllable Speech Synthesis](http://arxiv.org/abs/2509.25842v1)|**贡献点：**  <br/>1. **揭示风格嵌入分布特性**：通过t-SNE分析发现不同TTS系统的全局风格嵌入存在明显的层次聚类结构，音色属性优先聚类，再按风格特征细分。  <br/>2. **提出HiStyle框架**：设计两阶段分层预测模型，结合文本提示与对比学习，实现更精准的风格嵌入生成与文本-音频空间对齐。  <br/>3. **创新风格注释策略**：融合统计方法与人类听觉偏好，生成高准确性且感知一致的文本提示，提升风格控制效果。  <br/>4. **验证性能提升**：实验表明HiStyle在保持语音自然度与可理解性的同时，显著优于现有方法，实现更优的风格可控性。  <br/><br/>**总结：**  <br/>该研究通过层次化分析揭示风格嵌入分布规律，提出HiStyle框架与注释策略，显著提升可控语音合成的风格控制能力，同时保持高质量语音输出。|
|2509.25773v1|[V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs](http://arxiv.org/abs/2509.25773v1)|**贡献点总结:**  <br/>1. 提出v-HUB，首个以视觉为核心的视频幽默理解基准，包含默片及在线资源的简短视频，依赖纯粹视觉线索。  <br/>2. 构建开放问答任务，增强基准通用性，可集成到现有视频理解评估体系。  <br/>3. 评估多类MLLMs（含音频处理模型），揭示模型在视觉幽默理解中的局限性。  <br/>4. 证实音频对视频幽默理解的重要性，强调多模态整合的潜力。  <br/><br/>**总结（100字内）:**  <br/>本文提出视觉核心的幽默理解基准v-HUB，评估多种MLLMs在纯视觉场景下的表现，并验证音频信息对幽默理解的辅助作用，推动多模态模型在复杂视频任务中的改进。|
|2509.25247v1|[Protocode: Prototype-Driven Interpretability for Code Generation in LLMs](http://arxiv.org/abs/2509.25247v1)|总结：该论文提出通过自动采样In-Context Learning演示提升代码生成性能与可解释性，利用AST分析识别关键代码区域，实验证明高质量演示可优化pass@10指标，强调高效采样策略对LLM任务表现的重要性。<br/><br/>贡献点：<br/>1. 首次系统研究ICL演示采样策略对代码生成任务的影响，提出自动优化演示选择的方法<br/>2. 开发基于AST的代码分析框架，精准定位演示对生成结果的关键作用区域<br/>3. 实验证明高质量示范可显著提升模型性能（pass@10指标）与代码可解释性<br/>4. 揭示不良示范对模型的负面影响，建立示范质量与任务性能的量化关联<br/>5. 为LLM在代码生成等领域的安全可靠应用提供可解释性增强的解决方案|
|2509.25131v1|[MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech](http://arxiv.org/abs/2509.25131v1)|总结：  <br/>MGM-Omni提出一种统一多模态语音生成模型，通过双轨架构实现高效跨模态交互与低延迟流式生成，在数据效率和语音质量上超越现有开源模型。<br/><br/>贡献点：  <br/>1. **统一多模态框架**：首次构建涵盖多模态理解和可控语音生成的端到端Omni LLM。  <br/>2. **双轨架构设计**：采用"brain-mouth"结构，分离多模态推理与实时语音生成，提升效率与稳定性。  <br/>3. **高效语音生成机制**：通过分块并行解码方案缩小文本-语音token速率差距，支持流式零样本语音克隆。  <br/>4. **跨模态感知优化**：引入双音频编码器与统一训练策略，增强长音频理解能力并适应多样声学环境。  <br/>5. **数据效率突破**：在保持音色一致性（timbre preservation）和生成自然语音方面，实现比现有工作更高效的数据训练。|
|2509.25048v1|[Confidence-Guided Error Correction for Disordered Speech Recognition](http://arxiv.org/abs/2509.25048v1)|总结：  <br/>该研究提出基于置信度引导的提示方法，通过融入单词级不确定性估计提升大语言模型在失语症语音识别中的纠错能力，实验证明其在多个数据集上显著降低单词错误率。<br/><br/>贡献点：  <br/>1. 提出"confidence-informed prompting"方法，将ASR的置信度估计直接融入LLM训练，增强对失语症语音的纠错能力。  <br/>2. 实现跨说话人和数据集的鲁棒性提升，突破传统LLM后处理的局限性。  <br/>3. 对比分析纯转录微调与后处理置信度过滤两种策略，验证其有效性。  <br/>4. 在Speech Accessibility Project和TORGO数据集上分别实现10%和47%的WER降低，证明方法显著优于基线。  <br/>5. 首次系统性探索置信度感知微调在失语症语音识别中的应用价值。|
|2509.24322v1|[Multimodal Large Language Models Meet Multimodal Emotion Recognition and   Reasoning: A Survey](http://arxiv.org/abs/2509.24322v1)|总结（100字以内）:  <br/>本文首次系统综述了多模态大语言模型在情感识别与推理中的应用，涵盖模型架构、数据集及性能基准，分析关键挑战并提出未来方向，为研究者提供权威参考与实践指导。<br/><br/>**贡献点：**  <br/>1. **首个全面综述**：首次系统性调研多模态大语言模型（MLLMs）与情感识别/推理的交叉领域，填补该方向缺乏系统总结的空白。  <br/>2. **多维度覆盖**：全面梳理模型架构、数据集和性能评估基准，为研究者提供完整的技术发展脉络。  <br/>3. **挑战与方向分析**：明确当前技术的关键瓶颈（如跨模态对齐、上下文建模等），并提出未来研究重点。  <br/>4. **开源资源支持**：提供GitHub链接（Awesome-Emotion-Reasoning）汇总现有方法，便于研究复现与进一步探索。|
|2509.24310v1|[Code-switching Speech Recognition Under the Lens: Model- and   Data-Centric Perspectives](http://arxiv.org/abs/2509.24310v1)|总结：  <br/>该研究提出SECT提示策略与TTS数据增强方法，系统分析CS-ASR的模型与数据挑战，有效提升识别性能与语言质量，强调策略需与代码切换数据的语言特征精准匹配。<br/><br/>贡献点：  <br/>1. **系统分析框架**：首次从模型中心和数据中心双视角，全面解析代码切换语音识别（CS-ASR）的核心挑战（语言混淆、口音偏差）及现有方法的局限性。  <br/>2. **算法对比研究**：对比分析语言特定处理与多任务学习等前沿算法在不同语言特征数据集上的表现差异，揭示其有效性边界。  <br/>3. **TTS数据增强**：探索TTS作为代码切换数据生成工具的潜力，通过调整文本特征和口音特性，量化语言混淆与口音偏差对CS-ASR的影响。  <br/>4. **SECT策略创新**：提出基于简化等价约束理论（SECT）的提示方法，通过简化理论约束引导大语言模型生成符合语言规则的代码切换文本。  <br/>5. **性能验证**：实验证明SECT在提升ASR准确率及语言质量评估中优于现有方法，并通过TTS生成的语音-文本对有效验证其改进效果。  <br/>6. **跨领域协同**：强调模型策略与代码切换数据语言特征的精准对齐需求，推动语音识别与自然语言处理的跨学科方法融合。|
|2509.24187v1|[Reasoning Beyond Majority Vote: An Explainable SpeechLM Framework for   Speech Emotion Recognition](http://arxiv.org/abs/2509.24187v1)|总结：  <br/>该研究提出基于生成推理的可解释SER框架，通过融合教师LLM生成的解释和注释者感知评分，提升模型透明度与解释性，同时保持高性能。<br/><br/>贡献点：  <br/>1. **提出解释性框架**：将SER定义为生成性推理任务，通过转录文本生成情感标签和自然语言解释，增强模型决策透明度。  <br/>2. **引入教师LLM监督**：利用reasoning-capable教师LLM生成解释作为中间监督信号，结合多数投票标签进行联合训练。  <br/>3. **注释者感知评分机制**：设计互补性评估指标，同时考虑所有注释者标签的匹配情况，提升模型对主观标注的敏感性。  <br/>4. **实验验证有效性**：在MSP-Podcast数据集上，模型在保持预测质量的同时，生成的解释被人类评估者认为合理且有依据。|
|2509.23938v1|[Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust   Turn-Taking in Full-Duplex Spoken Dialogue Systems](http://arxiv.org/abs/2509.23938v1)|**贡献点总结：**  <br/>1. 提出开源模块化全双工对话轮次检测模型Easy Turn，融合声学与语言双模态信息。  <br/>2. 构建1,145小时专用训练数据集Easy Turn trainset，解决数据稀缺问题。  <br/>3. 模型实现四种对话状态（complete/incomplete/backchannel/wait）的精准预测。  <br/>4. 在开源测试集上超越现有模型（如TEN Turn Detection、Smart Turn V2）的性能表现。  <br/>5. 提供全流程开源实现，便于社区复用与扩展。  <br/><br/>**摘要精简总结（100字内）：**  <br/>本研究提出开源全双工对话轮次检测模型Easy Turn，融合声学与语言信息，预测四种对话状态，并发布专用数据集，显著提升检测准确率，推动语音交互技术发展。|
|2509.23755v1|[Understanding Textual Capability Degradation in Speech LLMs via   Parameter Importance Analysis](http://arxiv.org/abs/2509.23755v1)|总结：  <br/>该论文提出基于参数重要性估计的分析框架，发现语音微调导致文本参数分布偏移，并通过分层学习率和LoRA策略缓解这一问题，实验证明这些方法能同时保持文本能力并提升口语问答性能。<br/><br/>贡献点：  <br/>1. **揭示问题机制**：通过研究encoder-adaptor范式，首次分析语音整合导致LLMs文本能力下降的根本原因（参数重要性分布偏移）。  <br/>2. **提出分析框架**：构建参数重要性估计框架，量化语音微调对文本推理参数分配的影响，为问题诊断提供理论依据。  <br/>3. **设计缓解策略**：提出两种方法（分层学习率调度与LoRA）以保留原始参数分布，通过结构化调整优化模型性能。  <br/>4. **实验证明有效性**：验证所提策略在保持文本能力的同时提升口语问答效果，优于传统全量微调方法。  <br/>5. **理论解释有效性**：从LLMs文本知识的结构特性出发，系统阐明缓解策略在语音任务中的实际作用机制。|
|2509.23630v1|[Game-Oriented ASR Error Correction via RAG-Enhanced LLM](http://arxiv.org/abs/2509.23630v1)|总结：提出GO-AEC框架，整合LLM与RAG技术，通过数据增强和动态知识库显著提升游戏场景下ASR的准确率。<br/><br/>贡献点：<br/>1. 提出针对游戏语音通信的GO-AEC框架，集成大型语言模型、检索增强生成（RAG）和语音数据增强策略<br/>2. 创新性地设计三类核心模块：数据增强技术、基于N-best假设的纠错机制、动态游戏知识库<br/>3. 针对游戏场景实现语音识别技术突破，有效解决短语识别、快速语速、专业术语和环境噪声等挑战<br/>4. 实验验证在游戏语音场景中将字符错误率降低6.22%、句子错误率降低29.71%的显著性能提升|
|2509.22744v1|[Index-MSR: A high-efficiency multimodal fusion framework for speech   recognition](http://arxiv.org/abs/2509.22744v1)|总结（100字以内）：  <br/>该论文提出Index-MSR框架，通过多模态融合解码器（MFD）有效整合视频文本信息，显著提升语音识别准确率并降低替换错误，为需要严格音频-文本同步的应用（如音频翻译）提供高效解决方案。<br/><br/>贡献点：  <br/>1. **提出多模态融合解码器（MFD）**：核心创新点，首创将视频中的文本信息（如字幕、幻灯片）与语音识别结合的解码架构。  <br/>2. **解决领域术语和短语识别难题**：针对现有ASR在特定领域或短语识别中的性能下降问题，通过跨模态信息增强模型鲁棒性。  <br/>3. **实现替换错误显著降低**：实验数据表明，替代错误率减少20.5%，提升语音识别的准确性与可靠性。  <br/>4. **高效跨模态信息利用**：框架设计兼顾计算效率，有效挖掘视频文本线索对语音识别的辅助作用。  <br/>5. **验证实际应用价值**：通过多数据集测试证明方法有效性，适用于需要严格音频-文本同步的场景（如音频翻译）。|
|2509.22363v2|[Investigating Faithfulness in Large Audio Language Models](http://arxiv.org/abs/2509.22363v2)|**贡献点：**  <br/>1. **首次系统研究LALMs的CoT忠实性**：填补了语音领域对大规模音频-语言模型生成思维链（CoT）是否忠实的空白，强调其在安全敏感应用中的重要性。  <br/>2. **设计针对性干预方法**：提出改写、填充符注入、提前回答和引入错误等实验手段，用于评估CoT的可靠性与模型决策过程的一致性。  <br/>3. **验证LALMs的CoT表现**：通过实验发现，LALMs在处理语音推理任务时，生成的CoT普遍更忠实于其内部决策过程。  <br/>4. **揭示语音推理的独特挑战**：指出LALMs需先从音频中提取线索再进行推理，相较于文本模型更具复杂性。  <br/><br/>**总结（100字内）：**  <br/>本文首次系统评估LALMs的思维链忠实性，通过干预实验验证其可靠性，发现语音模型在推理中表现出更高的CoT忠实性，为安全应用提供了理论基础。|
|2509.22363v1|[Investigating Faithfulness in Large Audio Language Models](http://arxiv.org/abs/2509.22363v1)|**贡献点总结：**<br/>1. **填补研究空白**：首次系统性研究大型音频-语言模型（LALMs）的思维链（CoT）忠实性，揭示其在安全敏感应用中的关键意义。  <br/>2. **创新评估方法**：提出针对CoT的四种干预策略（改写、填充token、提前回答、引入错误），用于量化检验模型推理过程与输出的一致性。  <br/>3. **实验证实可信性**：在挑战性数据集SAKURA和MMAR上验证，发现LALMs生成的CoT普遍具有较高忠实性，为模型可解释性提供新证据。  <br/>4. **强调应用价值**：指出音频推理的复杂性（需先提取音频线索）对模型可信度的潜在影响，推动安全敏感场景下的模型优化方向。  <br/><br/>**总结（100字内）：**  <br/>本研究首次探讨LALMs的CoT忠实性，通过四种干预方法在SAKURA和MMAR数据集上验证，结果显示LALMs生成的CoT普遍忠实，为音频模型的可靠性与解释性提供了新视角。|
|2509.22287v1|[Leveraging Large Language Models for Robot-Assisted Learning of   Morphological Structures in Preschool Children with Language Vulnerabilities](http://arxiv.org/abs/2509.22287v1)|总结（100字以内）:  <br/>该研究开发基于LLM的机器人辅助语言学习应用TalBot，通过"Alias"游戏提升学龄前儿童语言能力，解决人类教员难以实时生成语法结构的问题。机器人可作为教学模型和导师，支持多语言教学，满足语言脆弱儿童沟通需求，推动LLM在语音康复中的创新应用。<br/><br/>贡献点:<br/>1. **创新性应用LLM**：首次将大型语言模型整合进机器人辅助语言学习系统，实现游戏对话、情感响应与回合控制的智能化管理。<br/>2. **形态结构教学突破**：开发机器人实时生成特定形态结构（如第三人称-s）的教学能力，解决人类教员在游戏化教学中的语法生成难题。<br/>3. **双角色教学模式**：构建机器人兼具"语言模型"与"教学导师"双重功能的框架，既指导儿童学习又为教育专业人员提供范例。<br/>4. **游戏化学习范式**：设计"Alias"类互动游戏机制，通过趣味性情境自然融入语言训练，提升儿童参与度与学习效果。<br/>5. **多语言扩展能力**：提出可跨语言实施的语法教学模型，为不同语言背景的语言脆弱儿童提供统一的干预方案。|
|2509.22243v1|[FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](http://arxiv.org/abs/2509.22243v1)|总结（100字以内）:  <br/>本文提出FLEXI基准，系统评估全双工LLM-人类对话性能，揭示开源与商业模型在紧急响应与延迟方面的差距，并建议基于next token-pair预测的改进方法。<br/><br/>贡献点:  <br/>1. **首创全双工交互基准**：提出FLEXI，首个针对全双工语音-语音LLM与人类对话的基准，涵盖模型中断机制。  <br/>2. **紧急场景建模**：明确纳入模型在紧急情况下的中断能力，填补现有评估体系的空白。  <br/>3. **多维度系统评估**：通过6大场景全面分析延迟、质量与对话效果，量化模型性能差异。  <br/>4. **揭示性能差距**：对比开源与商业模型，发现紧急意识、回合终止及延迟方面的显著差距。  <br/>5. **提出改进方向**：建议next token-pair预测作为实现无缝、类人对话的关键技术路径。|
|2509.22220v1|[StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient   SpeechLLMs](http://arxiv.org/abs/2509.22220v1)|总结（100字以内）：<br/>提出StableToken语音分词器，通过多分支架构与位级投票机制提升token序列稳定性，显著降低不同噪声条件下的单位编辑距离（UED），为下游SpeechLLMs提供更鲁棒的输入表示。<br/><br/>贡献点：<br/>1. 首次揭示语义语音分词器对语义无关听觉扰动的脆弱性，指出单路径量化架构与训练信号缺失中间token稳定性的两大缺陷<br/>2. 提出StableToken共识驱动架构，创新性采用多分支并行处理结合位级投票机制构建稳定token序列<br/>3. 在多种噪声条件下实现前所未有的token稳定性，将Unit Edit Distance（UED）降低至新水平<br/>4. 建立基础稳定性与下游模型性能的强关联，显著提升SpeechLLMs在各类任务中的鲁棒性|
|2509.21932v1|[SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech   Translation](http://arxiv.org/abs/2509.21932v1)|总结（100字以内）:  <br/>本文提出SimulSense框架，模仿人类译者通过持续感知意义单元触发写决策，解决SimulST系统对交错数据和高成本LLM推理的依赖，显著提升实时效率与质量-延迟平衡，决策速度较基线快9.6倍。<br/><br/>贡献点：  <br/>1. **提出SimulSense框架**：首次将SimulST决策机制设计为持续感知意义单元并动态触发写操作，更贴近人类译者实时处理方式。  <br/>2. **摆脱交错数据依赖**：无需专为多轮对话设计的交错训练数据，简化了数据准备流程。  <br/>3. **高效实时决策**：通过轻量化的模型结构降低计算成本，显著提升系统实时效率（决策速度提高9.6倍）。  <br/>4. **优化质量-延迟平衡**：实验验证方法在翻译质量与实时性之间取得优于现有系统的最佳权衡。|
|2509.21801v1|[Redefining Machine Simultaneous Interpretation: From Incremental   Translation to Human-Like Strategies](http://arxiv.org/abs/2509.21801v1)|总结：该研究通过扩展SiMT动作空间，结合动作感知提示和延迟感知TTS，提升了翻译质量与实时性，尤其在DROP与SENTENCE_CUT协同作用下取得最佳平衡。<br/><br/>贡献点：<br/>1. 提出四项自适应动作（SENTENCE_CUT/DROP/PARTIAL_SUMMARIZATION/PRONOMINALIZATION）以解决SiMT中实时性与质量的矛盾  <br/>2. 将动作空间集成到解码器-only的大语言模型框架中，实现动态翻译重构与简化  <br/>3. 构建动作感知提示机制生成训练参考，提升模型对复杂任务的适应能力  <br/>4. 开发延迟感知TTS流水线，提供真实场景下的翻译-语音转换延迟评估  <br/>5. 在ACL60/60基准测试中验证方法有效性，实现语义指标提升与延迟降低的双重突破  <br/>6. 证实DROP与SENTENCE_CUT组合在流畅性与实时性之间取得最佳平衡效果|
|2509.21155v2|[Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in   Language Models](http://arxiv.org/abs/2509.21155v2)|**总结（100字以内）**  <br/>该研究分析了语言模型中语法模板、领域和语义的关系，发现语法与领域间的虚假关联可能影响模型性能并绕过安全微调拒绝机制，提出需测试此类关联并增强训练数据语法多样性以避免问题。<br/><br/>**贡献点分点列出**  <br/>1. **表征关系**：首次系统分析了任务-指令对中语法模板、领域和语义的关联性，揭示语法模板在训练数据和模型输出中的普遍性。  <br/>2. **虚假关联识别**：发现模型可能因训练数据中语法与领域的共现而形成错误关联，导致语法覆盖提示语义的问题。  <br/>3. **性能验证**：通过合成数据集实验，证明语法-领域关联会导致OLMo-2模型在实体知识任务上的性能下降（均值0.51±0.06）。  <br/>4. **评估框架提出**：设计一种检测语法-领域虚假关联的框架，并验证其在FlanV2数据集和多模型（OLMo-2-7B、Llama-4-Maverick、GPT-4o）中的有效性。  <br/>5. **安全微调影响**：案例研究表明，语法-领域关联可能被恶意利用以绕过安全微调中的拒绝机制（如OLMo-2-7B Instruct和GPT-4o）。  <br/>6. **解决建议**：提出需在模型训练和评估中主动检测语法-领域关联，并通过增加域内语法多样性以减少其负面影响。|
|2509.21155v1|[Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in   Language Models](http://arxiv.org/abs/2509.21155v1)|**贡献点总结（分点）**  <br/>1. **提出语法模板与领域关联的表征方法**，揭示任务-指令对中语法、领域和语义的复杂关系。  <br/>2. **发现语法与领域间的虚假相关性**，表明模型可能在训练中错误关联领域与语法，从而覆盖提示语义。  <br/>3. **构建评估框架**，可检测模型是否存在语法-领域相关性，并验证其在多个模型（包括开放和闭源）中的普遍性。  <br/>4. **通过合成数据集量化影响**，证明语法-领域相关可显著降低OLMo-2模型在实体知识任务中的性能（均值下降0.51±0.06）。  <br/>5. **案例研究揭示安全隐患**，显示未预期的语法-领域相关可能被用于绕过安全微调中的拒绝机制。  <br/>6. **提出改进方向**：强调需主动测试语法-领域相关性，以及在训练数据中增强语法多样性以避免此类问题。  <br/><br/>**摘要总结（100字以内）**  <br/>研究发现语法与领域间的虚假相关可能影响LLM性能及安全，提出了检测该现象的评估框架，并呼吁在训练中确保语法多样性以防止错误关联。|
|2509.21125v1|[Acoustic-based Gender Differentiation in Speech-aware Language Models](http://arxiv.org/abs/2509.21125v1)|总结（100字以内）:  <br/>本文提出首个系统分析SpeechLMs性别差异的数据集，发现模型在性别刻板问题中偏向男性，而在需性别区分的情境中忽略性别信息，且该偏差源于Whisper编码器的声学标记生成，而非语音性别或中性选项，强调需改进性别信息处理技术。<br/><br/>贡献点:  <br/>1. **提出首个系统性数据集**：构建包含9,208个语音样本（性别独立/刻板/依赖分类）的基准，用于量化SpeechLMs中的性别差异化响应现象。  <br/>2. **揭示矛盾偏差模式**：发现模型在性别刻板问题中普遍偏向男性响应，但在需性别区分的场景下却忽略性别信息，与预期公平性原则矛盾。  <br/>3. **排除替代性解释**：验证偏差非由中性选项或语音性别感知导致，即使允许中性回答，模型仍倾向于忽略性别信息。  <br/>4. **证明性别中性化无效**：即使应用性别中性化处理方法，偏差模式仍存在，表明技术手段未根除问题。  <br/>5. **定位偏差根源**：通过对比实验发现，Whisper语音编码器生成的男性导向声学标记是导致偏差的核心因素。|
|2509.20419v1|[Wartime Media Dynamics in Emerging Democracies: Case Study of Pakistani   Media in May 2025 Indo-Pak Conflict](http://arxiv.org/abs/2509.20419v1)|**贡献点总结（按原文内容）:**<br/>1. **研究问题新颖性**：首次系统探讨区域冲突对新兴民主国家言论自由的压制效应，揭示冲突背景下媒体议程的偏移现象。<br/>2. **方法创新**：采用大型语言模型（LLM）分析2600篇新闻文本，通过大规模语料库挖掘冲突对媒体报道的结构性影响。<br/>3. **实证发现**：量化证明战争相关报道在巴基斯坦主流媒体中显著挤压了政治异见与批评的传播空间，为民主制度研究提供新证据。<br/>4. **政策启示**：强调在地缘政治动荡时期保障新闻自由的必要性，为国际社会理解冲突与言论自由的互动关系提供理论支持。<br/><br/>**说明**：原文实际属于媒体政治学研究领域，而非语音技术。若您需与语音相关的贡献点（如语音识别、语音合成、语音情感分析等），可提供更具体的文本内容以便精准分析。|
|2509.20410v4|[Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex   Speech Interaction](http://arxiv.org/abs/2509.20410v4)|**贡献点：**<br/>1. 提出Phoenix-VAD：首个基于大语言模型（LLM）的流式语义端点检测模块，解决现有模型全双工预测能力不足的问题；<br/>2. 方法创新：融合LLM语义理解能力与滑动窗口训练策略，实现可靠且实时的语义端点检测；<br/>3. 场景验证：通过语义完整与不完整的语音场景实验，验证模型的高性能与普适性；<br/>4. 模块化设计：支持全双工预测模块与对话模型的独立优化，提升系统灵活性与可靠性。<br/><br/>**总结（100字内）：**  <br/>Phoenix-VAD是首个基于LLM的流式语义端点检测模型，通过语义理解和滑动窗口训练实现高效检测，适用于复杂语音场景，并支持模块独立优化，推动下一代人机交互发展。|
|2509.20410v3|[Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex   Speech Interaction](http://arxiv.org/abs/2509.20410v3)|**贡献点：**  <br/>1. 提出Phoenix-VAD：首个基于大语言模型（LLM）的流式语义终点检测框架，实现全双工预测模块的即插即用性。  <br/>2. 创新滑动窗口策略：结合LLM语义理解能力与滑动窗口训练，提升检测可靠性并支持实时流式推理。  <br/>3. 通用性验证：在语义完整和不完整场景中均取得优越性能，验证模型在多样化的语音任务中的有效性。  <br/>4. 模块解耦设计：使语义终点检测模块可独立优化，增强系统灵活性与对话模型的协同效率。  <br/><br/>**总结：**  <br/>该研究提出了Phoenix-VAD，通过LLM与滑动窗口训练实现流式语义终点检测，解决了现有模型的不足，并支持模块独立优化，为下一代人机交互提供更强实时性和灵活性。|
|2509.20410v2|[Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex   Speech Interaction](http://arxiv.org/abs/2509.20410v2)|**贡献点：**  <br/>1. 提出Phoenix-VAD，首个基于LLM的流式语义端点检测模型，解决传统语音对话系统缺乏全双工预测模块的问题。  <br/>2. 融合LLM的语义理解能力与滑动窗口训练策略，实现可靠且高效的语义端点检测。  <br/>3. 通过实验验证了模型在语义完整和不完整场景下的优异性能（SOTA或竞争力）。  <br/>4. 设计支持全双工模块独立优化，提升对话模型的灵活性与可靠性。  <br/>5. 为下一代人机交互提供更无缝、实时的音频交互支持。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Phoenix-VAD，基于LLM实现流式语义端点检测，结合语义理解与滑动窗口策略，通过实验验证其性能，并支持模块独立优化，显著提升全双工交互能力，推动人机对话系统发展。|
|2509.20410v1|[Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex   Speech Interaction](http://arxiv.org/abs/2509.20410v1)|贡献点总结（100字以内）：<br/>提出Phoenix-VAD模型，实现基于LLM的流式语义端点检测，通过滑动窗口策略提升可靠性，支持全双工模块独立优化，增强智能人机交互的实时性与灵活性。<br/><br/>分点贡献：<br/>1. 提出Phoenix-VAD：首个基于大语言模型（LLM）的流式语义端点检测系统，填补语音对话模型中全双工预测模块的空白。<br/>2. 语义感知机制：整合LLM语义理解能力与滑动窗口训练策略，实现对连续语音流的高效语义边界判断。<br/>3. 场景验证：在完整/不完整语音场景中均展现优异性能，证明模型在复杂音频环境中的鲁棒性。<br/>4. 模块化设计：实现语义端点检测模块与对话模型的独立优化，提升系统整体的可靠性和部署灵活性。|
|2509.20321v1|[DRES: Benchmarking LLMs for Disfluency Removal](http://arxiv.org/abs/2509.20321v1)|总结：  <br/>该论文提出DRES基准测试，系统评估LLMs在语音中去除不流畅表达的表现，揭示关键发现并给出九条实用建议，为构建鲁棒的语音语言系统提供可复现、模型无关的基础。<br/><br/>贡献点：  <br/>1. **提出DRES基准**：首个基于人类标注的文本级语音不流畅去除可控评估框架，解决ASR错误和声学变化干扰的问题。  <br/>2. **系统性评估LLMs**：对比不同规模、架构和提示策略的模型，分析其在不流畅去除任务中的性能差异。  <br/>3. **发现关键规律**：(i) 简单分段显著提升性能；(ii) 推理导向模型易误删流畅内容；(iii) 微调虽提升指标但损害泛化能力。  <br/>4. **定义LLM特定错误模式**：识别模型在处理不流畅表达时的典型失效方式，为改进提供方向。  <br/>5. **提供九条部署建议**：针对实际应用提出可操作的优化策略，增强语音驱动系统的鲁棒性。|
|2509.20319v1|[Z-Scores: A Metric for Linguistically Assessing Disfluency Removal](http://arxiv.org/abs/2509.20319v1)|总结：  <br/>提出基于span级别的Z-Scores评估指标，结合语言学分类和确定性对齐技术，揭示模型在处理不同非流利性类型时的系统性缺陷，为模型优化提供针对性诊断工具。<br/><br/>贡献点：  <br/>1. **引入span-level评估指标**：Z-Scores通过区分不同非流利性类型（EDITED, INTJ, PRN）提供更细粒度的分析，突破传统词级指标（如F1）的局限。  <br/>2. **提出确定性对齐模块**：实现生成文本与非流利转录本的可靠映射，解决对齐不确定性问题，提升评估结果的准确性。  <br/>3. **提供类别特异性诊断**：识别模型在特定非流利性类型上的失败模式，指导设计针对性干预措施（如提示优化、数据增强）。  <br/>4. **案例验证有效性**：通过LLMs实验表明Z-Scores能发现隐藏于整体F1中的具体问题，直接辅助模型改进策略制定。|
|2509.20153v2|[Affective Computing and Emotional Data: Challenges and Implications in   Privacy Regulations, The AI Act, and Ethics in Large Language Models](http://arxiv.org/abs/2509.20153v2)|**贡献点：**  <br/>1. **跨学科整合**：结合计算机科学、心理学与神经科学，提出情感智能在AI系统中的理论框架。  <br/>2. **神经架构分析**：明确CNN与RNN在情感识别中的基础作用，解析其处理面部表情与序列数据（如语音/文本）的技术机制。  <br/>3. **数据区分与伦理探讨**：区分显式（知情同意）与隐式（被动收集）情感数据，提出法律处理、AI透明度及个体自主权的核心挑战。  <br/>4. **应用场景覆盖**：评估医疗、教育、客户服务等领域的实际影响，注重文化差异与情感识别系统的群体偏见问题。  <br/>5. **监管框架解读**：从GDPR与欧盟AI法案角度，强调情感数据作为敏感信息的法律地位，呼吁明确数据最小化、目的限制及知情同意机制。  <br/><br/>**总结（100字以内）：**  <br/>本文从跨学科视角出发，分析情感智能在AI系统中的实现与挑战，探讨情感数据分类、伦理问题及监管要求，强调技术与法律结合对保护用户情感自主权的重要性。|
|2509.20153v1|[Affective Computing and Emotional Data: Challenges and Implications in   Privacy Regulations, The AI Act, and Ethics in Large Language Models](http://arxiv.org/abs/2509.20153v1)|**贡献点**  <br/>1. 提出跨学科框架：融合计算机科学、心理学与神经科学，构建情感智能整合AI系统的理论基础。  <br/>2. 技术架构分析：系统研究CNN与RNN在语音/文本情感识别中的应用，强调结构化数据的处理方法。  <br/>3. 情感数据分类：区分显式（知情同意）与隐式（被动收集）情感数据，提出法律与伦理风险的探讨方向。  <br/>4. 跨文化与偏见问题：揭示情感识别系统在不同文化背景及群体间的差异与潜在偏见。  <br/>5. 监管视角创新：基于GDPR与欧盟AI法案，提出情感数据作为敏感信息的合规性保护机制设计。  <br/>6. 应用领域影响：评估情感AI在医疗、教育、客服等场景中的潜力与挑战，推动实际落地的可行性研究。  <br/><br/>**总结**（100字以内）：  <br/>本文构建跨学科情感AI框架，分析CNN/RNN技术在情绪识别中的应用，探讨显式/隐式情感数据的法律伦理问题，提出文化差异与偏见应对策略，并研究GDPR与欧盟AI法案下的合规保护机制，为情感计算与语音技术的伦理与监管提供理论支持。|
|2509.20086v1|[OLaPh: Optimal Language Phonemizer](http://arxiv.org/abs/2509.20086v1)|总结（100字以内）:  <br/>提出OLaPh框架，融合大词典、NLP技术与概率评分函数，显著提升文本转语音中对复杂词汇的处理能力，结合语言模型进一步增强泛化性，为语音领域提供开源资源。<br/><br/>贡献点分点列举:  <br/>1. **提出OLaPh框架**：整合大词典、多NLP技术、组合解析与概率评分函数，解决传统方法在处理专有名词、外来词等复杂词汇时的不足。  <br/>2. **概率评分函数创新**：通过动态权重分配优化拼写一致性，提升对复杂词汇的处理精度。  <br/>3. **语言模型增强性能**：利用OLaPh生成的数据训练大语言模型，进一步提高文本转语音的泛化能力和准确性。  <br/>4. **多语言验证**：在德语和英语数据集上验证框架有效性，尤其在挑战性数据集上表现优于现有方法。  <br/>5. **开源资源提供**：释放框架与生成数据，为语音领域研究提供可复用的工具和基础资源。|
|2509.20072v2|[From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint   Training](http://arxiv.org/abs/2509.20072v2)|总结（100字以内）:  <br/>提出Text-to-Talk（TtT）框架，统一音频-文本生成，结合自回归文本与非自回归音频扩散；设计模态感知注意力机制与训练策略；实现块级扩散并行合成，提升多模态对话系统的性能。  <br/><br/>贡献点：  <br/>1. **统一生成框架**：首次将自回归文本生成与非自回归音频扩散集成于单一Transformer，突破传统模态分离处理方式。  <br/>2. **模态感知注意力机制**：支持文本因果解码与音频双向建模，解决文本依赖目标-目标关系、音频依赖源-目标关系的建模差异。  <br/>3. **三重训练策略**：针对性减少训练-测试分布差异，提升模型泛化能力。  <br/>4. **块级扩散推理**：实现音频并行生成与可变长度输出处理，优化生成效率与灵活性。  <br/>5. **实验验证与开源**：在Audio-QA和ASR任务中验证有效性，并公开模型、数据与代码推动研究。|
|2509.20072v1|[From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint   Training](http://arxiv.org/abs/2509.20072v1)|总结：  <br/>提出TtT框架，通过自回归文本生成与非自回归音频扩散的结合，解决多模态模型中音频-文本依赖结构不对称问题，降低计算成本并统一建模。<br/><br/>贡献点：  <br/>1. **提出统一建模框架**：首次将自回归文本生成与非自回归音频扩散集成到单一Transformer架构中，适用于音图文交互场景。  <br/>2. **解决依赖结构差异**：针对性别处理文本（因果依赖）与音频（源文本驱动）的异构依赖关系，避免统一使用自回归生成的局限性。  <br/>3. **降低训练复杂度**：通过基于预训练LLM的初始化与单阶段训练设计，减少传统多阶段训练流程的计算开销。  <br/>4. **实现端到端语音生成**：构建语音输入-语音输出的对话系统模型，支持直接生成语音而非依赖多模态交叉处理。|
|2509.19965v1|[SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding](http://arxiv.org/abs/2509.19965v1)|**总结（100字以内）：**  <br/>提出SynchroRaMa框架，融合文本情感与音频情绪信号，引入LLM生成的场景描述，提升情感表达与动作同步，实验验证其在图像质量、表达保持和自然度上的优越性。<br/><br/>**贡献点分点列举：**  <br/>1. **多模态情感嵌入**：首次结合文本（通过情感分析）与音频（通过语音情绪识别及语音衍生的愉悦/唤醒特征），实现更丰富的情感表达。  <br/>2. **动态动作建模**：引入音频到动作（A2M）模块，生成与音频对齐的头部运动帧，提升自然头动与唇部同步效果。  <br/>3. **语义属性融合**：利用大语言模型（LLM）生成场景描述作为文本输入，捕捉动态动作及高阶语义属性，增强时间变化的表征能力。  <br/>4. **多模态条件增强**：通过视觉与文本信号联合条件约束，提升生成视频的时间一致性与视觉真实感。  <br/>5. **性能验证**：在基准数据集上取得图像质量、表情保持和动作真实感的显著提升，并通过用户研究验证其整体自然度和视频平滑度优势。|
|2509.19926v1|[MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection](http://arxiv.org/abs/2509.19926v1)|1. **提出两种新型Prompting方法**：开发了MMSE-Proxy Prompting（基于MMSE评分段确定性映射）和Reasoning-augmented Prompting（结合多模态LLM的推理输出），分别达成0.82准确率和0.86/0.83 AUC。  <br/>2. **首次将MMSE评分锚定到概率提取**：通过MMSE带定义概率锚定，实现更精确的评分关联，提升模型对阿尔茨海默病的诊断相关性。  <br/>3. **引入跨模态构建范式**：利用Cookie Theft图像与转录文本的多模态输入，增强模型可解释性，同时保持评估仅依赖转录文本。|
|2509.19902v2|[WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction](http://arxiv.org/abs/2509.19902v2)|**贡献点：**<br/>1. **全LLM架构**：集成成熟大模型架构、生态（如Hugging Face）和方法（如序列打包），提升语音处理的效率与灵活性。  <br/>2. **全栈功能**：支持语音识别、合成、理解、对话及多模态任务，具备模块化扩展能力以适配开源模型。  <br/>3. **易用性设计**：强调"Simple and Stupid"理念，降低使用门槛，适配广泛用户群体。  <br/>4. **多样化实验方案**：提供两种实验配置——基于开源模型/数据的可复现基础框架与大规模数据训练的高性能模型，满足不同应用需求。  <br/><br/>**总结：**  <br/>WEST是首个全LLM语音工具包，集成多任务处理能力，兼顾开源可复现性与高性能应用，推动语音领域的研究与实践。|
|2509.19902v1|[WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction](http://arxiv.org/abs/2509.19902v1)|**贡献点：**  <br/>1. **全LLM架构**：基于大语言模型，整合现有成熟架构、生态（如Hugging Face）与技术（如序列打包），提升语音处理效率与效果。  <br/>2. **全栈功能**：覆盖语音识别、合成、理解、对话及多模态能力，支持开源模型扩展，满足多样化任务需求。  <br/>3. **易用性设计**：强调简单直观的设计理念，降低使用门槛，目标实现广泛普及与用户友好性。  <br/>4. **双模式实验**：提供两种可复现方案：  <br/>   - **开源模式**：基于开源模型与数据，便于实验复现与验证。  <br/>   - **高性能模式**：依托海量数据训练，直接提供高精度模型应用。  <br/>5. **开源可用性**：公开发布工具包（GitHub），推动语音技术研究与实践的开放协作。  <br/><br/>**总结（100字以内）：**  <br/>WEST是一款基于大语言模型的语音工具包，通过全LLM架构、全栈功能与简单设计实现语音处理的高效与普及，提供开源复现与高性能预训练模型，推动语音技术的开放研究和应用。|
|2509.19858v1|[Benchmarking Gaslighting Attacks Against Speech Large Language Models](http://arxiv.org/abs/2509.19858v1)|**贡献点**：  <br/>1. **提出gaslighting攻击**：首次将"gaslighting"（操纵性误导）攻击概念引入语音领域，设计专门针对Speech LLMs的对抗性提示方法。  <br/>2. **构建五种操纵策略**：开发Anger、Cognitive Disruption、Sarcasm、Implicit、Professional Negation五种攻击策略，覆盖多样化任务场景。  <br/>3. **多维度评估框架**：框架同时检测模型性能退化（如准确率下降）与行为反应（如非自愿道歉、拒绝），全面诊断脆弱性维度。  <br/>4. **结合声学扰动实验**：通过声学扰动测试Speech LLMs在多模态场景下的鲁棒性，揭示语音与文本/视觉模态的交互风险。  <br/>5. **大规模实验证据**：基于10,000+跨5个数据集样本，量化分析5类Speech LLMs的脆弱性，发现平均准确率下降24.3%的显著结果。  <br/><br/>**总结**（100字内）：  <br/>本研究提出针对语音大模型的gaslighting攻击，设计五种策略并构建多维评估框架，结合声学扰动实验验证模型脆弱性，发现平均准确率下降24.3%，强调提高语音AI安全性的紧迫性。|
|2509.19852v1|[Eliminating stability hallucinations in llm-based tts models via   attention guidance](http://arxiv.org/abs/2509.19852v1)|总结：  <br/>本研究提出Optimal Alignment Score指标与预训练注意力引导的链式思维训练方法，有效缓解了LLM-based TTS中稳定性幻觉问题，提升了生成语音的连续性与稳定性。<br/><br/>贡献点：  <br/>1. **分析文本-语音对齐机制**：揭示LLM中文本与语音标记对齐的关键问题。  <br/>2. **提出OAS评估指标**：基于维特比算法设计，量化对齐质量并指导训练。  <br/>3. **整合OAS优化训练**：提升CosyVoice2的连续性与稳定性对齐能力。  <br/>4. **引入预训练注意力引导**：通过链式思维（CoT）方式减少合成语音的稳定性幻觉。  <br/>5. **实验验证有效性**：在Seed-TTS-Eval与CV3-Eval数据集上证明方法有效性且无负面影响。|
|2509.19817v2|[MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition](http://arxiv.org/abs/2509.19817v2)|**总结**：  <br/>提出首个针对多轮全双工中文医疗会话的公开数据集MMedFD，包含同步视角、角色标签与多模态标注，设计模型无关处理流程与医疗专用评估指标，推动临床ASR研究。<br/><br/>**贡献点**：  <br/>1. **首个中文医疗全双工ASR语料库**：MMedFD是首个针对多轮次全双工交互场景的中文医疗领域语音数据集，填补该领域的开放基准空白。  <br/>2. **多维度标注与高质量数据**：包含5,805个标注会话，提供同步用户与混合通道视角、RTTM/CTM时间戳、角色标签等详细信息，支持复杂语音任务分析。  <br/>3. **模型无关的流式处理流程**：提出适用于任何模型的流式分割、说话人归因和对话记忆框架，提升系统通用性与可复用性。  <br/>4. **医疗场景专用评估指标**：引入HC-WER（概念级准确率）与基于评分标准的LLM回复评估方法，更精准地衡量医疗对话ASR与生成模型的性能。  <br/>5. **公开资源与社区贡献**：数据集与相关工具完全公开，为医疗语音处理研究提供标准化基准和实验环境。|
|2509.19817v1|[MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition](http://arxiv.org/abs/2509.19817v1)|总结：  <br/>本文提出首个面向多轮全双工场景的中文医疗ASR数据集MMedFD，包含丰富标注和同步信息，并构建通用模型处理流程与评估体系，推动医疗流式语音识别技术发展。<br/><br/>贡献点：  <br/>1. **首个医疗全双工ASR数据集**：创建MMedFD，包含5,805个标注会话，支持多轮对话、全双工交互，提供同步用户与混合通道视角、RTTM/CTM时间戳、角色标签。  <br/>2. **通用流式处理框架**：提出模型无关的流式分割、说话人归属与对话记忆方法，适用于多种模型架构。  <br/>3. **专用模型优化**：基于角色拼接的音频数据微调Whisper-small，针对医疗场景的长上下文识别需求。  <br/>4. **医疗场景专用评估指标**：引入HC-WER衡量概念级准确性，结合LLM生成响应的评分标准与成对协议评估。  <br/>5. **开放共享资源**：公开数据集及工具，为研究提供可复现基准与实验环境。|
|2509.19631v1|[Advancing Speech Summarization in Multi-modal LLMs with Reinforcement   Learning](http://arxiv.org/abs/2509.19631v1)|**贡献点:**  <br/>1. 提出多阶段强化学习训练框架，提升开放源码MLLMs在语音摘要任务中的性能。  <br/>2. 实现无需中间文本转录的端到端语音摘要生成。  <br/>3. 支持可控文本风格和零样本泛化能力。  <br/>4. 在强基线和更大规模MLLMs上超越，显著缩小与文本基LLMs的性能差距。  <br/><br/>**总结:**  <br/>本研究通过多阶段强化学习框架，显著提升开放源码多模态大语言模型的语音摘要能力，实现端到端生成、风格控制及零样本泛化，突破开源模型性能瓶颈。|
|2509.19567v1|[Retrieval Augmented Generation based context discovery for ASR](http://arxiv.org/abs/2509.19567v1)|总结：  <br/>本文提出基于嵌入的检索增强生成方法，用于提升上下文感知ASR系统在罕见词识别中的转录准确率，对比评估两种LLM方案，实验证明其有效性并减少WER达17%。<br/><br/>贡献点：  <br/>1. 提出一种基于嵌入的高效检索增强生成（RAG）方法，用于自动上下文发现以提升ASR性能。  <br/>2. 设计两种基于大语言模型（LLM）的替代方案：提示生成上下文和后识别转录修正。  <br/>3. 在TED-LIUMv3、Earnings21和SPGISpeech三个数据集上验证方法有效性，对比无上下文方案的WER降低17%。  <br/>4. 通过Oracle上下文实验，证明所提方法相对于最优上下文的性能差距仅为24.1%的提升。|
|2509.19515v3|[A Longitudinal Randomized Control Study of Companion Chatbot Use:   Anthropomorphism and Its Mediating Role on Social Impacts](http://arxiv.org/abs/2509.19515v3)|**贡献点：**  <br/>1. **首个纵向实证研究**：通过21天的跟踪实验（N=183），系统验证了人机交互对人类社交关系的长期影响，填补了相关领域的研究空白。  <br/>2. **揭示中介机制**：发现“社交连接动机→拟人化倾向→社交影响”是影响人机交互效果的关键心理路径，为理解AI与人类社交行为的关系提供理论框架。  <br/>3. **量化社交影响差异**：证实社交连接欲望强的用户更易对AI产生拟人化认知，并认为与AI的互动对人际关系有更显著作用，揭示了个体差异对技术影响的调节作用。  <br/>4. **社会警示意义**：在社交需求上升的背景下，指出AI伴侣可能因强化拟人化而对人类关系产生潜在影响，引发对技术伦理与社会影响的反思。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过21天纵向实验，发现社交连接动机强的用户更易对AI拟人化，进而增强其对人际互动的影响。揭示了拟人化作为中介机制的作用，警示AI伴侣可能对人类社交关系产生潜在影响，为技术伦理与社会影响研究提供新视角。|
|2509.18798v1|[Group Relative Policy Optimization for Text-to-Speech with Large   Language Models](http://arxiv.org/abs/2509.18798v1)|**贡献点:**  <br/>1. **提出GRPO-based方法**：首次将GRPO（可能指某种优化算法）应用于LLM-based TTS模型，通过ASR模型获取奖励信号提升性能。  <br/>2. **无需专用奖励模型**：与传统RL方法不同，无需额外训练或维护奖励计算模型，简化流程。  <br/>3. **复合奖励函数设计**：结合CER和ASR的NLL，提供更准确的奖励信号以优化语音生成。  <br/>4. **实验验证有效性**：通过零样本TTS评估，证实方法显著提升合成语音的可懂度与自然度。  <br/>5. **消融研究支持**：分析表明两个奖励组件的整合对性能提升具有关键作用。  <br/><br/>**总结:**  <br/>该论文提出一种无需专用奖励模型的GRPO方法，结合ASR的CER与NLL设计复合奖励函数，显著提升LLM-based TTS的可懂度与自然度。|
|2509.18377v1|[Interactive Real-Time Speaker Diarization Correction with Human Feedback](http://arxiv.org/abs/2509.18377v1)|总结：提出LLM辅助的实时说话人分割校正系统，通过流式ASR、用户反馈与SWM技术结合，显著降低分割错误率并分析不同场景下的校正效果。<br/><br/>贡献点：<br/>1. 构建实时用户交互的说话人分割校正系统，实现流式ASR与分割结果的动态修正<br/>2. 开发Split-When-Merged（SWM）技术，有效识别并分割错误归并的多说话人段<br/>3. 引入在线说话人注册机制，通过用户修正数据提升未来分割准确性<br/>4. 在AMI数据集验证系统有效性，DER降低9.92%，说话人混淆错误减少44.23%<br/>5. 系统性分析不同交互模式（摘要/完整转录）及约束条件（注册限制/修正频率）对校正效果的影响|
|2509.18200v1|[Conversational Orientation Reasoning: Egocentric-to-Allocentric   Navigation with Multimodal Chain-of-Thought](http://arxiv.org/abs/2509.18200v1)|总结（100字以内）:  <br/>该研究提出CONVERSATIONAL ORIENTATION REASONING (COR)基准，开发多模态链式思维推理框架MCoT，在中文对话导航任务中实现高精度的自我中心到环境中心方向转换，并在资源受限场景和噪声条件下表现出强鲁棒性。<br/><br/>贡献点:  <br/>1. **提出首个中文对话导航基准**：构建COR（Conversational Orientation Reasoning）基准，专门针对真实环境中非英语及语音识别转录场景的对话导航任务。  <br/>2. **创新多模态链式推理框架（MCoT）**：设计结构化三步推理流程（提取空间关系→映射坐标到绝对方向→推断用户方向），融合语音转录文本与地标坐标信息。  <br/>3. **课程学习策略优化模型**：在资源受限的Taiwan-LLM-13B-v2.0-Chat模型上采用渐进式课程学习，提升其在复杂环境下的空间推理能力。  <br/>4. **高鲁棒性验证**：实验表明MCoT在ASR转录文本（98.1%准确率）及多语言切换、噪声干扰等场景下表现优异，优于单模态及非结构化基线。  <br/>5. **跨领域与多语言兼容性**：模型在领域迁移、语言变化及指代模糊等挑战中保持高准确率，证明其泛化能力与实际应用潜力。|
|2509.17965v1|[Benchmarking Humans and Machines on Complex Multilingual Speech   Understanding Tasks](http://arxiv.org/abs/2509.17965v1)|总结（100字以内）:  <br/>本研究提出系统范式，比较人机在多语言混合语音场景中的语音问答表现。发现人类母语注意力优于第二语言，机器在干净语音中表现优异，但难以在多说话人情况下聚焦。揭示了人机注意力机制的关键差异。<br/><br/>贡献点:  <br/>1. 提出首个系统性范式，用于同时研究人类与机器在多语言及混合通道语音环境下的问答任务。  <br/>2. 首次揭示人类在母语（L1）中相较第二语言（L2）具有显著更强的选择性注意力能力。  <br/>3. 通过对比实验发现，语音大模型在单说话人场景匹配或超越人类表现，但难以处理双说话人混合场景，凸显人机注意力机制的本质差异（人类依赖高效线索，机器采用并行提取）。|
|2509.17855v1|[Make Every Letter Count: Building Dialect Variation Dictionaries from   Monolingual Corpora](http://arxiv.org/abs/2509.17855v1)|**贡献点：**  <br/>1. **提出DiaLemma标注框架**：开发了一种新型标注方法，仅基于单语数据构建方言变异词典，解决方言标准化缺失的问题。  <br/>2. **构建大规模方言对照数据集**：利用DiaLemma框架生成包含100K人类标注的德语-巴伐利亚词对数据集，作为方言理解的基准数据。  <br/>3. **系统评估LLMs的方言处理能力**：对九种主流的大语言模型进行实验，分析其在方言词汇识别、翻译及词形变体判断任务中的表现差异。  <br/>4. **揭示方言识别的关键难点**：发现模型在区分直接翻译与词形变体方面存在显著困难，尤其在动词等词类上表现不佳。  <br/>5. **探索上下文对性能的影响**：证明上下文信息可提升翻译准确率，但可能干扰对方言变体的识别，揭示任务间的权衡关系。  <br/>6. **强调方言适配的重要需求**：指出现有LLMs在正字法方言处理中的局限性，呼吁加强方言适应性的研究与技术改进。  <br/><br/>**总结（100字内）：**  <br/>本研究提出DiaLemma框架并构建德语-巴伐利亚方言数据集，系统评估LLMs在方言理解中的表现，揭示其在区分翻译与变体上的局限性，强调需进一步优化模型以适应方言处理需求。|
|2509.17449v1|[SLAyiNG: Towards Queer Language Processing](http://arxiv.org/abs/2509.17449v1)|总结：  <br/>本研究构建了首个专注酷儿俚语的SLAyiNG数据集，涵盖多源真实语料，提出专家与社区协作的注释方法，并评估了模型在意义消歧任务的表现，为应对非法语料标注挑战提供新思路。<br/><br/>贡献点：  <br/>1. **首次构建专用数据集**：提出SLAyiNG，首个系统收集注释的酷儿俚语数据集，来源包括字幕、社交媒体和播客。  <br/>2. **多源真实语料整合**：覆盖多样化语境（影视对话、社交媒体、播客）以反映真实社会身份表达场景。  <br/>3. **注释流程标准化**：详细描述术语收集、定义匹配、语境示例抓取及持续迭代的注释方法。  <br/>4. **跨模态评估**：通过人机协作计算Krippendorff's alpha（平均0.746），验证数据质量并暴露模型对敏感语料的处理局限。  <br/>5. **推动伦理实践**：强调酷儿语言的敏感性，倡导专家与社区驱动的注释机制以提升数据可靠性。|
|2509.17022v1|[VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven   Module](http://arxiv.org/abs/2509.17022v1)|**总结（100字以内）:**  <br/>本文提出VAInpaint方法，结合视觉分割与语言模型多阶段处理，实现音视频协同修复，并通过定制数据集提升音频分离性能，实验结果达到现有基准水平。<br/><br/>---<br/><br/>**贡献点:**  <br/>1. **提出VAInpaint新框架**：首次整合视觉分割模型与大语言模型（LLM），通过多阶段协同处理实现视频和音频的同步修复。  <br/>2. **多尺度描述生成机制**：利用全局LLM分析场景，结合区域特定模型的局部描述，生成更精确的文本查询引导音频分离。  <br/>3. **定制化数据集优化**：构建包含分段乐器图像和VGGSound背景的专用数据集，针对性提升音频分离模型的泛化能力。  <br/>4. **效果验证**：在音视频修复任务中实现与当前主流方法相当的性能，验证了跨模态协同处理的有效性。|
|2509.16648v3|[FESTA: Functionally Equivalent Sampling for Trust Assessment of   Multimodal LLMs](http://arxiv.org/abs/2509.16648v3)|**贡献点**：<br/><br/>1. 提出FESTA方法，一种用于多模态大语言模型的信任评估输入采样技术。  <br/>2. 通过等效与互补采样生成不确定性度量，无需真实标签（无监督）。  <br/>3. 保持任务一致性，扩展输入空间以评估模型的稳定性和敏感性。  <br/>4. 在视觉与听觉推理任务中实现显著的不确定性估计提升（33.3%与29.6%）。  <br/>5. 代码开源，便于进一步研究与应用。  <br/><br/>**总结**：FESTA是一种无监督的多模态输入采样方法，用于提升MLLMs的信任评估与选择性预测性能，已在视觉和听觉任务中取得显著效果，并开源。|
|2509.16648v2|[FESTA: Functionally Equivalent Sampling for Trust Assessment of   Multimodal LLMs](http://arxiv.org/abs/2509.16648v2)|**贡献点：**  <br/>1. 提出FESTA方法：一种基于等价与互补输入采样的无监督不确定性评估技术，用于多模态大语言模型（MLLMs）的信任度量化。  <br/>2. 任务保留的采样策略：通过扩展输入空间，同时探测模型的**一致性**（等价样本）与**敏感性**（互补样本），提升预测可靠性。  <br/>3. 黑盒无监督设计：仅依赖模型的输入-输出接口，无需真实标签或人工标注，简化实现流程。  <br/>4. 多任务验证：在视觉与音频推理任务中，通过对比实验验证FESTA的有效性，显著提升选择性预测性能（AUROC提升33.3%和29.6%）。  <br/>5. 代码开源：提供可复现的实现，推动研究与实际应用。  <br/><br/>**总结（100字以内）：**  <br/>本文提出FESTA方法，通过等价与互补输入采样实现多模态模型的信任度评估，显著提升视觉/音频任务的选择性预测性能（AUROC提升33.3%/29.6%），且无需真实数据，代码已开源。|
|2509.16648v1|[FESTA: Functionally Equivalent Sampling for Trust Assessment of   Multimodal LLMs](http://arxiv.org/abs/2509.16648v1)|贡献点：<br/>1. 提出FESTA方法：首个针对多模态大语言模型的不确定性度量框架，通过等效采样与互补采样生成输入空间扩展<br/>2. 任务保留设计：创新性地保持任务相关性的同时，同时评估模型一致性（等效样本）和敏感性（互补样本）<br/>3. 无监督特性：仅需模型输入输出接口（黑盒），无需依赖ground truth标注数据<br/>4. 多模态验证：在视觉和音频推理任务中均实现显著性能提升（视觉LLMs提升33.3%，音频LLMs提升29.6%）<br/>5. 开源实现：提供可复现的代码框架，促进后续研究与应用<br/><br/>总结：本研究提出FESTA方法，通过等效与互补采样生成不确定性度量，实现无监督多模态模型信任评估，显著提升视觉和音频任务的预测选择性性能。|
|2509.16589v2|[Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A   Case Study with In-the-Wild Data](http://arxiv.org/abs/2509.16589v2)|**贡献点总结（100字以内）：**  <br/>本文提出CP-Bench基准，首次系统评估语音-LLM在情感和语调等非语言线索理解上的能力，构建两个结合语言与共情理解的问答数据集，分析模型表现差异及温度调优的影响，揭示现有评估的不足并为开发更情感智能的语音模型提供方向。<br/><br/>---<br/><br/>**分点贡献列表：**  <br/>1. **提出首个针对语音-LLM的上下文情感推理基准**（CP-Bench），弥补现有评估在非语言特征理解上的空白。  <br/>2. **构建两个结合语言内容与情感理解的QA数据集**，要求同时具备语言分析和共情能力。  <br/>3. **全面评估开源与闭源的SOTA语音-LLM**，分析其在不同问题类型中的表现差异。  <br/>4. **通过温度调优实验探究模型性能提升机制**，揭示调参对情感推理任务的关键影响。  <br/>5. **系统分析现存评估方法的局限性**，为未来开发更具上下文感知与情感智能的语音模型提供理论与实践指导。|
|2509.16589v1|[Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A   Case Study with In-the-Wild Data](http://arxiv.org/abs/2509.16589v1)|总结（100字以内）:  <br/>本研究提出CP-Bench基准，聚焦语音大模型在语境中理解情感与语调等非语言线索的能力，构建了需语言与共情理解的问答数据集，评估多类模型并分析温度调优效果，揭示当前评估体系缺陷，为提升语音模型的社会情感理解能力提供指导。<br/><br/>贡献点：  <br/>1. **提出新型基准**：CP-Bench专为评估语音大模型的语境下语用推理能力（如情感、语调理解）设计，填补现有评测在社交情感智能方向的空白。  <br/>2. **构建多模态数据集**：包含两个需同时整合语言内容与非语言线索（如情感、韵律）的问答数据集，强调共情理解与多模态交互。  <br/>3. **多模型对比分析**：系统评估开放源与闭源的SOTA语音大模型，覆盖不同问题类型，揭示模型在语用理解上的差异。  <br/>4. **温度调优研究**：针对Top 2模型进行温度调优实验，分析其对语用推理任务的调控作用，探讨模型响应的可解释性。  <br/>5. **启发模型优化**：通过基准测试结果指出现有评估的局限性，为开发更情境感知、情感智能的语音模型提供理论与实践依据。|
|2509.16496v1|[Synergies between Federated Foundation Models and Smart Power Grids](http://arxiv.org/abs/2509.16496v1)|总结：  <br/>本文首次将多模态多任务联邦基础模型应用于智能电网领域，提出双向视角框架，探讨模型在电网关键任务中的增强潜力及电网约束对模型设计的影响，推动跨学科融合研究。<br/><br/>贡献点：  <br/>1. **首次引入M3T FedFMs到电力系统领域**：提出将多模态多任务基础模型与联邦学习结合，探索其在智能电网中的应用潜力。  <br/>2. **双向视角分析**：从"智能电网赋能FedFMs"和"FedFMs赋能智能电网"两方面，系统研究模型与电网系统的相互作用。  <br/>3. **隐私保护与可扩展性**：强调FedFMs在分布式数据源上实现隐私安全建模的特性，适用于电网边缘异构数据处理。  <br/>4. **多任务场景验证**：验证模型在负荷预测、故障检测等电网关键任务中的有效性，展示其跨模态处理能力。  <br/>5. **电网约束建模**：分析电力系统在能源、通信、监管等维度的约束条件，指导FedFMs的架构设计与部署优化。|
|2509.16264v2|[Gender and Political Bias in Large Language Models: A Demonstration   Platform](http://arxiv.org/abs/2509.16264v2)|总结：ParlAI Vote是一个整合欧洲议会数据与模型分析的交互平台，揭示LLMs在政治预测中的系统性偏见，促进立法研究及公众参与。<br/><br/>贡献点：<br/>1. 构建多维度欧洲议会数据系统（辩论/演讲/投票结果+人口统计学信息）<br/>2. 提供LLMs投票预测与偏见分析交互实验环境<br/>3. 显式呈现模型预测与真实投票结果对比及分群误差分析<br/>4. 集成数据、模型与可视化分析的统一界面降低研究门槛<br/>5. 支持立法决策的学术研究、教育传播与公众监督<br/>6. 系统揭示当前LLMs在政治文本分析中的性能局限性|
|2509.16264v1|[Gender and Political Bias in Large Language Models: A Demonstration   Platform](http://arxiv.org/abs/2509.16264v1)|总结：  <br/>ParlAI Vote是一款整合欧洲议会辩论、演讲及投票数据的交互平台，揭示LLMs在政治分析中的系统性偏差，支持多场景应用与可视化分析，降低研究复现门槛。<br/><br/>贡献点：  <br/>1. **构建交互式政治分析平台**：首次整合欧洲议会的辩论、演讲与投票结果，提供统一的数据访问和可视化界面。  <br/>2. **丰富多维度数据支持**：包含性别、年龄、国家、政治派别等详细人口统计学数据，增强分析的全面性。  <br/>3. **量化LLMs的预测偏差与偏见**：通过对比真实投票结果与LLMs预测，揭示其在不同群体间的系统性性能差异。  <br/>4. **统一数据-模型-分析流程**：集成数据、前沿LLMs和可视化工具，简化研究复现、行为审计及反事实实验的流程。  <br/>5. **促进多方应用与透明度**：支持立法决策的研究、教育及公众参与，客观展现LLMs在政治分析中的优势与局限。|
|2509.16028v1|[Think, Verbalize, then Speak: Bridging Complex Thoughts and   Comprehensible Speech](http://arxiv.org/abs/2509.16028v1)|总结（100字以内）：  <br/>提出Think-Verbalize-Speak框架，解耦LLM推理与语音生成，引入ReVerT延迟优化算法，提升语音自然度与简洁性，同时保持推理性能，为语音对话系统提供高效解决方案。<br/><br/>贡献点：  <br/>1. **提出新型框架**：设计Think-Verbalize-Speak架构，分离LLM推理过程与语音生成，保留LLM完整推理能力。  <br/>2. **引入verbalizing机制**：通过将思维转化为自然语音文本，解决文本与语音传递的适配问题。  <br/>3. **开发ReVerT算法**：基于增量异步摘要的延迟高效verbalizer，优化生成效率与质量。  <br/>4. **实验证明有效性**：在多基准测试中验证方法对语音自然度、简洁性的提升，且推理性能损失可忽略。|
|2509.15476v1|[Evaluating Multimodal Large Language Models on Spoken Sarcasm   Understanding](http://arxiv.org/abs/2509.15476v1)|**贡献点：**  <br/>1. 系统性评估LLMs与多模态LLMs在英语（MUStARD++）和中文（MCSD 1.0）数据集上的讽刺检测性能，涵盖零样本、少样本及LoRA微调场景。  <br/>2. 提出协作门控融合模块，用于整合多模态特征表示，提升模型在跨模态任务中的表现。  <br/>3. 揭示音频单模态模型在讽刺检测中的最强性能，同时验证文本-语音和音频-视觉组合优于传统单模态及三模态模型。  <br/>4. 验证多模态LLMs（如Qwen-Omni）在跨语言、零样本及微调设置下的竞争力。  <br/>5. 强调多模态LLMs在跨语言音频-视觉-文本讽刺理解中的潜力，推动综合模态分析方法的探索。  <br/><br/>**总结：**  <br/>本文系统评估多模态大模型在中文和英文数据集的讽刺检测能力，提出协同融合机制，发现语音单模态最优，且多模态组合优于传统方法，验证了模型在跨语言任务中的潜力。|
|2509.15373v2|[Frustratingly Easy Data Augmentation for Low-Resource ASR](http://arxiv.org/abs/2509.15373v2)|**贡献点：**  <br/>1. 提出三种自包含的数据增强方法（基于词表替换、随机替换、LLM生成），通过生成文本并合成音频提升低资源ASR性能。  <br/>2. 在四种极低资源语言（Vatlongos、Nashta、Shinekhen Buryat、Kakabe）上验证方法有效性，实现显著性能提升（如Nashta的14.3%绝对WER下降）。  <br/>3. 验证方法对于高资源语言（如英语）同样适用，展示其跨语言推广能力与广泛适用性。  <br/><br/>**总结（100字以内）：**  <br/>该文提出三种文本生成驱动的自包含数据增强方法，通过合成音频提升低资源ASR性能，尤其在Nashta上实现14.3%的WER降低，并验证其适用于高资源语言，具有跨语言泛化能力。|
|2509.15362v2|[Speech Language Models for Under-Represented Languages: Insights from   Wolof](http://arxiv.org/abs/2509.15362v2)|总结：  <br/>提出首个沃洛夫语Speech LLM，通过大规模自发语音数据预训练提升ASR和翻译性能，并引入Chain-of-Thought机制增强多步推理能力，开源模型与代码促进研究。<br/><br/>贡献点：<br/>1. **构建首个沃洛夫语Speech LLM**：首次将语音编码器集成至沃洛夫语语言模型，实现语音翻译等跨模态任务的突破。  <br/>2. **优化预训练策略**：基于HuBERT的持续预训练方法在大规模自发语音数据上效果优于基线模型及非洲语言专用模型。  <br/>3. **引入Chain-of-Thought机制**：探索Speech LLM在语音转录/翻译前进行多步推理，提升复杂任务处理能力。  <br/>4. **开源贡献**：公开模型与训练代码，推动语言资源匮乏地区的语音技术发展。|
|2509.15362v1|[Speech Language Models for Under-Represented Languages: Insights from   Wolof](http://arxiv.org/abs/2509.15362v1)|**总结（100字以内）:**  <br/>该研究首次构建沃洛夫语Speech LLM，通过大规模自发语音数据预训练实现优于基线和非洲专用模型的ASR性能，并探索链式思维提升翻译效果，所有模型与代码将开源。<br/><br/>**贡献点:**  <br/>1. **数据构建与预训练策略**：提出针对沃洛夫语的大规模自发语音数据收集方案，证明持续预训练HuBERT在ASR任务中超越基线和非洲专用模型。  <br/>2. **首个沃洛夫语Speech LLM**：将语音编码器整合到沃洛夫语语言模型中，开发出首个支持该语言的Speech LLM，实现语音翻译等多任务能力。  <br/>3. **链式推理增强性能**：探索Speech LLM在转录或翻译前的多步骤链式思维训练，提升模型对复杂任务的处理能力。  <br/>4. **跨任务效果验证**：实验证明Speech LLM在语音识别和语音翻译任务中均表现优异，验证其综合性能优势。  <br/>5. **开源共享**：公开模型与代码，推动资源匮乏语言的语音模型研究与应用。|
|2509.15095v2|[Listening, Imagining & Refining: A Heuristic Optimized ASR Correction   Framework with LLMs](http://arxiv.org/abs/2509.15095v2)|总结：  <br/>本文提出LIR-ASR框架，结合LLM与人类听觉机制，通过“听-想-改”策略生成并修正语音变体，引入FSM优化和规则约束提升准确性，实验验证其在英汉ASR中显著降低CER/WER。<br/><br/>贡献点：  <br/>1. 提出LIR-ASR框架：基于LLM的启发式迭代修正方法，模拟人类听觉感知机制。  <br/>2. “听-想-改”策略：生成语音变体并结合上下文进行修正，提升转录质量。  <br/>3. FSM启发式优化：防止修正过程陷入局部最优，增强算法鲁棒性。  <br/>4. 规则约束机制：维护语义一致性，避免错误修正导致语义偏差。  <br/>5. 实验验证：在英汉ASR任务中实现平均1.5个百分点的CER/WER降低，证明有效性。|
|2509.15095v1|[Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction   Framework with LLMs](http://arxiv.org/abs/2509.15095v1)|总结：  <br/>提出LIR-ASR框架，结合LLM与人类听觉机制，通过"听-想-改"策略显著提升语音识别准确率。<br/><br/>贡献点：  <br/>1. **创新框架设计**：提出基于LLMs的迭代修正框架LIR-ASR，模仿人类听觉感知过程，实现更精准的语音转文字。  <br/>2. **"听-想-改"策略**：通过生成音素变体并上下文优化，逐层修正识别错误，提升语义一致性。  <br/>3. **有限状态机优化**：引入FSM防止修正过程陷入局部最优，提升算法收敛效率。  <br/>4. **规则约束机制**：通过规则保持语义 fidelity，避免语义偏差。  <br/>5. **跨语言有效性验证**：在英汉ASR任务中均实现平均CER/WER降低1.5个百分点的显著提升。|
|2509.15082v1|[From Who Said What to Who They Are: Modular Training-free Identity-Aware   LLM Refinement of Speaker Diarization](http://arxiv.org/abs/2509.15082v1)|贡献点总结：<br/>1. 提出首个无需额外训练的模块化说话人对齐系统，整合SD、ASR和LLM<br/>2. 创新性地使用结构化LLM提示，通过语义连续性优化低置信度说话人标签<br/>3. 实现说话人身份识别的端到端流程，突破SD的伪标签限制<br/>4. 在真实医疗场景数据集上验证，相较基线方法降低29.7%错误率<br/>5. 构建了可扩展的实用化技术管道，无需数据标注即可完成语音内容分析<br/><br/>（99字）  <br/>该研究提出无需训练的模块化框架，结合SD、ASR与LLM实现真实场景中说话人对齐、语音识别和身份识别，相较基线方法降低29.7%错误率，为实际应用提供完整解决方案。|
|2509.14930v1|[Cross-Modal Knowledge Distillation for Speech Large Language Models](http://arxiv.org/abs/2509.14930v1)|总结：  <br/>该论文首次系统评估语音大模型中的灾难性遗忘与模态不等价问题，提出跨模态知识蒸馏框架解决跨模态对齐与推理能力下降问题，并通过实验验证其有效性。<br/><br/>贡献点：  <br/>1. **首次系统评估**：提出首个针对语音大语言模型（Speech LLM）的灾难性遗忘（Catastrophic Forgetting）和模态不等价（Modality Inequivalence）问题的系统性分析。  <br/>2. **提出解决方案**：设计跨模态知识蒸馏框架（Cross-modal Knowledge Distillation），通过文本-文本和语音-文本双向通道迁移文本教师模型的知识至语音LLM。  <br/>3. **实验验证有效性**：在对话与音频理解任务中验证方法有效性，证明其可保留文本知识、提升跨模态对齐能力，并增强语音交互中的推理性能。|
|2509.14882v1|[Llama-Mimi: Speech Language Models with Interleaved Semantic and   Acoustic Tokens](http://arxiv.org/abs/2509.14882v1)|**贡献点：**<br/>1. 提出Llama-Mimi架构：首次联合建模语义与声学标记，采用统一tokenizer和单Transformer解码器处理交错序列。<br/>2. 实现SOTA性能：在声学一致性与说话人身份保持方面达到当前最优效果。<br/>3. 揭示量化器矛盾：分析发现量化器数量增加虽提升声学保真度，但会损害语言性能，凸显长期连贯性挑战。<br/>4. 创新评估方法：引入LLM-as-a-Judge框架，系统评估生成语音内容质量。<br/>5. 开源资源：公开模型、代码及语音样本，促进研究复现与应用。<br/><br/>**总结（100字内）：**  <br/>Llama-Mimi通过统一token化与Transformer解码器联合建模语义和声学信息，在声学一致性与说话人身份保持上达SOTA。研究揭示量化器数量与性能的权衡，并提出LLM-based评估方法，开源资源推动领域发展。|
|2509.14880v2|[From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition](http://arxiv.org/abs/2509.14880v2)|总结：  <br/>本研究系统评估LLM解码器对VSR的影响，发现其性能提升主要源于词汇处理而非语义理解，组合数据集可增强泛化能力，并确立了Llama-2-13B在无额外监督下的SOTA表现，强调需强化视觉编码器以推动模型发展。<br/><br/>贡献点：  <br/>1. **系统性对比实验设计**：通过冻结/选择性更新视觉编码器、扩展解码器规模、对比不同适应策略与架构，全面分析LLM解码器对VSR的贡献。  <br/>2. **跨数据集泛化验证**：在LRS2、LRS3及WildVSR上评估，揭示组合多数据集可显著提升模型的通用性与性能。  <br/>3. **语义分析区分贡献来源**：明确性能提升主要来自**词汇层面**的处理而非语义层面，推动对VSR机制的理解。  <br/>4. **SOTA模型表现**：基于组合数据集训练的Llama-2-13B模型在LRS3和WildVSR上分别达到24.7%和47.0%的WER，超越无额外监督的基线模型。  <br/>5. **关键结论与方向指引**：指出LLM解码器优化更多是提升**上下文推理能力**而非视觉特征，强调需进一步加强视觉编码器以实现突破。|
|2509.14880v1|[From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition](http://arxiv.org/abs/2509.14880v1)|总结（100字以内）:  <br/>本研究通过系统评估LLM解码器在VSR中的作用，发现其主要提升来源于词汇处理而非语义理解，强调需加强视觉编码器能力。所提模型在LRS3和WildVSR上取得SOTA性能，为自监督语音识别的优化提供新方向。<br/><br/>贡献点:  <br/>1. **系统评估LLM解码器的作用机制**：通过冻结/更新视觉编码器、扩展解码器规模、对比不同适应策略，明确LLM解码器的性能提升是否源于视觉理解或语言建模能力。  <br/>2. **揭示数据集组合的泛化优势**：实验显示跨数据集训练（LRS2 + LRS3）显著提升模型在LRS3和WildVSR的泛化能力，而单一数据集扩展效果有限。  <br/>3. **区分词汇与语义处理的影响**：语义分析表明，LLM解码器对VSR的提升主要来自词汇层面的建模，而非深层次语义理解。  <br/>4. **提供SOTA性能基准**：基于组合数据集训练的Llama-2-13B模型在LRS3（24.7% WER）和WildVSR（47.0% WER）中达到当前无额外监督模型的最优性能。  <br/>5. **指导未来模型改进方向**：研究指出LLM解码器优化更多在于上下文推理能力，而非视觉特征，进而强调视觉编码器的改进是推动VSR突破的关键。|
|2509.14627v1|[Towards Human-like Multimodal Conversational Agent by Generating   Engaging Speech](http://arxiv.org/abs/2509.14627v1)|**贡献点总结（100字以内）:**  <br/>提出多感官对话数据集与基于多模态LLM的语音生成模型，结合视觉和语音模态提升对话的自然性与吸引力，验证多模态融合的有效性，并开放源代码促进研究复现。<br/><br/>**分点贡献：**  <br/>1. **构建首个针对语音生成的多感官对话数据集**  <br/>   - 突破传统文本为中心的数据局限，提供语音与视觉多模态关联数据，支持生成自然语音响应。<br/><br/>2. **提出多模态LLM-based语音生成框架**  <br/>   - 首次结合文本响应与语音描述生成，融入语调、语速等非语言信息（paralinguistic），实现更丰富的语音表达。<br/><br/>3. **实验验证多模态融合对语音生成的效果**  <br/>   - 通过对比实验证明，整合视觉与音频模态显著提升生成语音的吸引力与互动性。<br/><br/>4. **开源实现推动领域研究**  <br/>   - 开放源代码（GitHub链接）促进技术复现与后续研究，加速语音对话系统的发展。|
|2509.14515v1|[From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken   Language Models](http://arxiv.org/abs/2509.14515v1)|**贡献点：**  <br/>1. **提出TFD语音通信的重要性**：强调TFD作为实现类人AI互动的关键技术，支持自然对话中同时听与说、回合切换、语音重叠和打断等特性。  <br/>2. **建立分类框架**：区分Engineered Synchronization（模块化架构）与Learned Synchronization（端到端架构），为FD-SLMs设计提供理论依据。  <br/>3. **统一评估体系**：整合碎片化评价方法，提出涵盖时间动态性、行为仲裁、语义连贯性和声学性能的综合性评估框架。  <br/>4. **识别核心挑战与路线图**：通过主流模型对比，指出同步数据稀缺、架构多样性及评估标准缺失等问题，并给出技术发展方向建议。  <br/><br/>**总结（100字以内）：**  <br/>该综述系统梳理了LLM时代FD-SLMs研究，构建同步方法分类与评估框架，揭示同步数据不足、架构差异和评估漏洞等挑战，为提升人机自然语音交互提供技术指引。|
|2509.14480v1|[Process-Supervised Reinforcement Learning for Interactive Multimodal   Tool-Use Agents](http://arxiv.org/abs/2509.14480v1)|总结（100字以内）:  <br/>提出TARL框架通过LLM作为回合级裁判解决长时任务信用分配问题，构建支持语音-文本交替的强化学习沙箱，结合数学推理课程提升探索，显著提升文本基准性能，并验证框架在多模态模型微调中的有效性，推动语音驱动交互代理的发展。<br/><br/>贡献点:<br/>1. **提出TARL策略**：通过LLM作为回合级裁判，解决多模态长时任务中信用分配问题。<br/>2. **构建语音-文本沙箱环境**：支持多模态上下文下的交替式强化学习训练。<br/>3. **混合任务训练课程**：整合数学推理问题，增强模型探索能力与任务完成效率。<br/>4. **文本基准性能提升**：在τ-bench上任务通过率优于强RL基线6%。<br/>5. **多模态模型适配性验证**：证明框架适用于微调多模态基础模型以实现工具使用能力，推动自然语音交互发展。|
|2509.14187v1|[Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual   Descriptions and LLMs](http://arxiv.org/abs/2509.14187v1)|总结：  <br/>本文提出TextPA，一种基于文本描述和LLM的零样本发音评估方法，提供评分解释并优化准确性，克服传统系统仅输出数值评分的局限，展现成本效率与领域外数据表现优势。<br/><br/>贡献点：  <br/>1. **零样本发音评估**：无需音频-评分对训练，直接通过文本描述进行发音分析。  <br/>2. **文本特征替代声学模型**：利用人类可读的文本表示替代传统声学特征，增强可解释性。  <br/>3. **评分解释机制**：LLM生成评分原因，帮助学习者理解错误，提升学习指导价值。  <br/>4. **音素序列匹配优化**：引入音素序列匹配方法精炼准确性评分。  <br/>5. **新方向探索**：首次将LLM的文本知识应用于发音评估，突破监督学习的局限。  <br/>6. **高效且泛化性强**：在成本和跨领域数据表现上优于传统模型，验证方法有效性。|
|2509.14128v2|[Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST](http://arxiv.org/abs/2509.14128v2)|总结：Canary-1B-v2是一款支持25种欧洲语言的高效多模态语音模型，通过FastConformer与Transformer架构结合、数据增强和两阶段训练提升性能，显著优于Whisper在英语文本识别中，同时推出更小的Parakeet-TDT-0.6B-v3模型。<br/><br/>贡献点：<br/>1. 提出Canary-1B-v2模型，融合FastConformer编码器和Transformer解码器，实现多语言ASR与AST的高效处理<br/>2. 构建包含1.7M小时数据的训练集（含Granary、NeMo ASR Set 3.0），通过添加非语音音频降低幻觉现象<br/>3. 设计两阶段预训练-微调流程与动态数据平衡策略，优化模型性能<br/>4. 引入nGPT编码器实验验证，证实不同架构在大规模数据下的不同优势<br/>5. 开发基于NeMo Forced Aligner与辅助CTC模型的分段时间戳技术<br/>6. 展示模型在英语ASR任务中超越Whisper-large-v3（10倍速度优势），且具备与大模型相当的多语言能力<br/>7. 发布轻量化模型Parakeet-TDT-0.6B-v3，实现相同25种语言支持下的参数量压缩（600M参数）|
|2509.14128v1|[Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST](http://arxiv.org/abs/2509.14128v1)|总结：  <br/>本研究提出Canary-1B-v2多语言ASR/AST模型，结合FastConformer与Transformer架构，通过非语音音频数据优化减少幻觉，并采用两阶段训练方法，实现在英语ASR上超越Whisper-large-v3，同时推出更轻量的Parakeet-TDT-0.6B-v3，兼顾性能与效率。<br/><br/>贡献点：  <br/>1. **模型架构创新**：采用FastConformer编码器与Transformer解码器组合，支持25种欧洲语言，兼顾速度与准确性。  <br/>2. **数据增强策略**：引入非语音音频数据训练，有效降低ASR和AST的幻觉问题。  <br/>3. **两阶段训练优化**：设计动态数据平衡机制的预训练与微调流程，提升模型泛化能力。  <br/>4. **时间戳生成技术**：集成NeMo Forced Aligner（NFA）与辅助CTC模型，实现可靠的分段时间戳输出。  <br/>5. **性能对比优势**：在英语ASR任务中超越Whisper-large-v3，速度提升10倍；多语言性能与大模型（如Seamless-M4T-v2-large）竞争。  <br/>6. **轻量级变体发布**：推出参数仅为600M的Parakeet-TDT-0.6B-v3，保持25语言支持的同时降低计算成本。|
|2509.13899v1|[AI as a teaching tool and learning partner](http://arxiv.org/abs/2509.13899v1)|总结（100字以内）:  <br/>本研究探讨LLM在教学中的应用，开发聊天机器人和播客工具，评估其对学生学习的影响，发现聊天机器人更受欢迎，播客接受度有限，为教育实践提供数据支持与改进建议。<br/><br/>贡献点:  <br/>1. **创新性工具整合**：首次将RAG技术与LLM结合，开发针对课程内容的智能聊天机器人，探索其在教学场景中的实际应用价值。  <br/>2. **多模态教学工具设计**：提出AI生成的音频播客作为补充教学资源，验证不同形式AI工具对学习效果的差异化影响。  <br/>3. **实证研究方法**：通过学期末学生调查，系统评估AI工具对学习态度和教学效果的影响，提供可量化的教学反馈数据。  <br/>4. **跨层次教学验证**：在本科与研究生两个不同的教学阶段同步测试工具效果，揭示AI干预的适应性与适用性差异。  <br/>5. **生物学科应用案例**：聚焦生物课程领域，为学科特定的AI教学工具开发与实施提供实证依据和实践参考。|
|2509.13785v1|[Summary on The Multilingual Conversational Speech Language Model   Challenge: Datasets, Tasks, Baselines, and Methods](http://arxiv.org/abs/2509.13785v1)|总结：  <br/>本论文总结了Interspeech2025多语言对话语音语言模型挑战，发布了1,604小时真实场景多语言对话语料库，提供基线系统，并分析了78支团队的参与成果，为构建高效多语言对话SLLM提供了宝贵见解。<br/><br/>贡献点：  <br/>1. **任务框架总结**：系统阐述MLC-SLM挑战的目标与任务设置，推动多语言对话语音LLM研究。  <br/>2. **大规模数据集发布**：提供约1,604小时的多语言对话语音数据，支持模型训练与评估。  <br/>3. **基线系统构建**：设计并发布参与者的基线系统，为后续研究提供基准。  <br/>4. **参与成果分析**：汇总78支国际团队的489个有效排名结果及14篇技术报告，展示研究进展。  <br/>5. **实践洞见提炼**：通过参与者经验总结，为多语言对话SLLM的开发提供方法论与技术参考。|
|2509.12647v1|[PAC: Pronunciation-Aware Contextualized Large Language Model-based   Automatic Speech Recognition](http://arxiv.org/abs/2509.12647v1)|总结：  <br/>本研究提出PAC框架，通过两阶段学习提升LLM-ASR系统的发音建模与同音字区分能力，显著降低词错误率，尤其在长尾词识别中表现突出。<br/><br/>贡献点：  <br/>1. **解决发音建模与同音区分挑战**：针对LLM-ASR系统在原始词和长尾词识别中的关键问题，提出发音感知的上下文建模方法。  <br/>2. **双阶段学习策略**：  <br/>   - 第一阶段：采用交织的音素-字符上下文建模，引入字符干扰项以强化音素线索利用。  <br/>   - 第二阶段：通过扰动标签抽样的发音区分强化学习，提升上下文同音字的区分能力。  <br/>3. **实验验证有效性**：在Librispeech和AISHELL-1数据集上，PAC实现30.2%和53.8%的绝对WER降低，长尾词偏见WER分别减少31.8%和60.5%。|
|2509.12591v1|[MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with   CLIP Models](http://arxiv.org/abs/2509.12591v1)|总结：  <br/>本文提出一种零样本音频字幕生成方法，结合预训练音频CLIP模型与LLM，通过结构化提示和优化token选择显著提升生成质量，验证了关键词选择对性能的关键影响。<br/><br/>贡献点：  <br/>1. **零样本框架**：提出无需大规模训练的AAC系统，利用预训练模型解决数据集不足问题。  <br/>2. **多模态融合**：结合音频CLIP模型提取听觉特征，并通过结构化提示引导LLM生成字幕。  <br/>3. **动态Token优化**：改进传统贪心解码，通过音频CLIP模型动态调整token选择以增强内容对齐。  <br/>4. **关键词影响验证**：实验表明关键词选择对性能有显著影响，单关键词提示效果最佳，无关键词列表时性能下降50%。  <br/>5. **性能提升**：使用MAGIC搜索方法，在WavCaps模型上实现NLG mean score 35%的提升（4.7→7.3）。|
|2509.11937v1|[MMORE: Massive Multimodal Open RAG & Extraction](http://arxiv.org/abs/2509.11937v1)|**贡献点：**  <br/>1. 提出MMORE，支持15种以上异构文档格式（含文本、表格、图像、音频等），统一处理格式方便下游应用。  <br/>2. 架构具备模块化和分布式特性，支持CPU/GPU并行化，实现可扩展的高效处理。  <br/>3. 在扫描PDF处理任务中，相比Docling提升40%准确性，且比单节点基线快3.8倍。  <br/>4. 集成混合稠密-稀疏检索技术，兼容交互式API和批量RAG端点。  <br/>5. 通过PubMedQA验证，MMORE增强的医学LLM随检索深度增加显著提升问答准确性。  <br/>6. 提供任务无关的多模态RAG系统基础，支持多样化真实数据部署，代码开源。  <br/><br/>**总结：**  <br/>MMORE是首个支持多模态数据的开源处理管道，集成高效检索与分布式架构，在扫描文档处理和医学问答等任务中均取得显著性能优势。|
|2509.11914v1|[EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](http://arxiv.org/abs/2509.11914v1)|**贡献点**  <br/>1. 提出EgoMem，首个针对全双工多模态实时模型的终身记忆代理。  <br/>2. 实现从原始音频视觉流中识别多用户，并提供个性化响应。  <br/>3. 通过长期记忆维护用户事实、偏好及社会关系信息。  <br/>4. 设计三个异步模块：用户检索（基于人脸/语音）、多模态对话生成、记忆管理（自动检测对话边界并更新记忆）。  <br/>5. 完全依赖原始音频视觉流，适用于实时、具身场景无需额外标注数据。  <br/>6. 实验验证检索与记忆管理模块准确率超95%，整合后对话事实一致性达87%，建立未来研究基准。  <br/><br/>**总结（100字内）**  <br/>本文提出EgoMem，首个全双工多模态实时模型的终身记忆代理，通过异步模块实现用户识别、个性化响应及记忆管理，实验表明其准确率超95%，整合后对话事实一致性达87%，为实时具身场景下的记忆系统提供新范式和基准。|
|2509.11868v1|[Growing Perspectives: Modelling Embodied Perspective Taking and Inner   Narrative Development Using Large Language Models](http://arxiv.org/abs/2509.11868v1)|**贡献点总结（100字以内）**  <br/>本研究提出PerspAct系统，结合ReAct范式与LLMs模拟视角转换的发展动态，基于Selman理论设计导演任务评估框架，揭示语言交流对内部表征的调节作用，并发现发展阶段与协作效果的关联，为建模跨模态认知和评估LLMs内部过程提供了新视角。<br/><br/>**分点贡献：**  <br/>1. **系统开发**：首次将ReAct范式与LLMs结合，构建PerspAct系统，模拟视角转换的发展阶段，嵌入Selman理论框架。  <br/>2. **评估框架创新**：设计扩展的导演任务，量化评估模型生成的内部叙事与发展阶段的一致性及其对协作表现的影响。  <br/>3. **认知动态发现**：揭示LLMs在交互过程中动态调整视角阶段的现象，证明语言交换对内部表征的持续优化作用。  <br/>4. **发展阶段差异**：实证表明高级阶段提升协作效率，但早期阶段在复杂任务中表现更不稳定，为认知发展建模提供依据。  <br/>5. **理论与实践意义**：强调隐式整合具身视角转换与语言模型的重要性，推动对LLMs内部语言过程的系统性评估研究。|
|2509.11127v1|[Joint Effects of Argumentation Theory, Audio Modality and Data   Enrichment on LLM-Based Fallacy Classification](http://arxiv.org/abs/2509.11127v1)|总结（100字以内）:  <br/>该研究探讨了上下文与情感元数据对LLM在政治辩论谬误分类任务中的影响，发现情感元数据易导致模型偏倚并损害逻辑推理，基本提示策略反更优，为多模态数据下的语言模型优化提供了新思路。<br/><br/>贡献点分点:  <br/>1. **提出理论框架**：首次引入Pragma-Dialectics和Periodic Table of Arguments两个理论指导的Chain-of-Thought框架，用于增强LLM在谬误分类中的推理能力。  <br/>2. **多模态实验设计**：系统评估了文本、上下文信息及音频情感元数据三种输入设置对模型性能的影响，探索了多模态数据对语言模型任务的具体作用机制。  <br/>3. **发现情感元数据负面影响**：实验证明情感元数据可能干扰模型逻辑推理，导致误标为"诉诸情感"谬误，揭示了多模态输入潜在的注意力分散问题。  <br/>4. **对比实验结论**：验证了基础提示策略在特定场景下优于复杂增强策略，为语音领域中语言模型任务优化提供了实证依据。  <br/>5. **政治辩论场景应用**：聚焦政治辩论数据，填补了语音语义分析与特定领域谬误分类结合的研究空白，强调了上下文与情感信息在复杂文本分析中的挑战。|
|2509.10748v1|[SCOPE: Speech-guided COllaborative PErception Framework for Surgical   Scene Segmentation](http://arxiv.org/abs/2509.10748v1)|总结：  <br/>提出语音引导的协作感知（SCOPE）框架，结合大语言模型与开放集视觉基础模型，实现手术场景中实时分割、标注与跟踪，解决传统方法依赖标注数据的局限性，展示人机协作在动态手术环境中的潜力。<br/><br/>贡献点：  <br/>1. **提出SCOPE框架**：首次将语音指导机制与视觉分割模型结合，构建支持手术场景实时分割（intraoperative video streams）的协同感知系统。  <br/>2. **引入协作感知代理**：通过整合LLM的推理能力与VFM的感知能力，生成高质量分割候选并利用自然语言反馈动态优化手术器械分割。  <br/>3. **动态标注机制**：利用手术器械作为交互指针，实现对其他术中元素（如解剖结构）的实时补充标注与跟踪。  <br/>4. **开放集与零样本能力**：在无预定义类别的场景中，通过语音与视觉协同实现灵活分割和泛化，突破传统监督模型的限制。  <br/>5. **多数据验证**：基于公开Cataract1k数据集和内部颅底数据集的评估，结合实时模拟实验，验证框架的动态适应性与实用性。|
|2509.10729v1|[Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](http://arxiv.org/abs/2509.10729v1)|**贡献点总结**<br/>1. **构建多场景活动识别数据集**：从Ego4D数据集中筛选出涵盖家庭活动、体育等多样化场景的子集，用于跨上下文活动分类研究。<br/>2. **提出LLM晚期融合方法**：首次将大语言模型应用于音频与运动时间序列的晚期融合，实现多模态活动分类。<br/>3. **实现零/单样本分类性能突破**：在无需任务特定训练的前提下，LLM融合模型在12类零样本和单样本分类中F1分数显著高于随机基线。<br/>4. **解决嵌入空间共享难题**：通过LLM融合，减少对对齐训练数据的依赖，适用于多模态时序应用中嵌入空间学习的挑战。<br/>5. **提升模型部署效率**：LLM融合无需额外内存和计算资源，支持轻量级部署针对特定应用场景的多模态模型。<br/><br/>**摘要总结**  <br/>该研究提出利用大语言模型进行音频与运动数据的晚期融合，构建了跨场景活动识别数据集，并在零/单样本分类中取得显著性能提升，同时解决了共享嵌入空间学习与部署效率的双重挑战。|
|2509.09921v2|[A Taxonomy of Response Strategies to Toxic Online Content: Evaluating   the Evidence](http://arxiv.org/abs/2509.09921v2)|总结：本研究提出在线话语参与（ODE）的分类体系，系统梳理25种应对有毒在线内容的策略，澄清概念分歧，为构建证据驱动的健康网络公共讨论提供框架。<br/><br/>贡献点：<br/>1. 提出首个系统化的ODE分类框架，整合25种在线回应策略<br/>2. 建立五类响应机制：缓和分散/参与视角/价值识别/受害者支持/信息构建<br/>3. 界定ODE研究中的核心目标差异，解决学术争议<br/>4. 开展各策略类别的文献证据系统性回顾与元分析<br/>5. 构建知识图谱，推动证据指导的在线言论干预实践|
|2509.09921v1|[A Taxonomy of Response Strategies to Toxic Online Content: Evaluating   the Evidence](http://arxiv.org/abs/2509.09921v1)|总结：  <br/>本文提出在线话语参与（ODE）分类框架，系统梳理25种应对网络有害言论策略，澄清概念并开展元分析，为构建建设性网络公共话语提供证据支持。<br/><br/>贡献点：  <br/>1. **构建ODE理论框架**：首次提出Online Discourse Engagement（在线话语参与）的分类体系，涵盖五类响应策略。  <br/>2. **系统化策略分类**：归纳25种具体策略（如幽默、共情、事实反驳等），按功能划分为五大类别，填补现有文献空白。  <br/>3. **实证证据整合**：通过系统文献综述与元分析，评估各策略对促进健康网络讨论的有效性，增强研究可信度。  <br/>4. **概念澄清与区分**：明确ODE目标与方法的区别，解决现有研究中目标模糊、术语混用的问题。  <br/>5. **推动实践应用**：为制定基于证据的应对网络有害内容措施提供理论指导与实证依据。|
|2509.09321v1|[Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain   Expansion, and Metric Optimization](http://arxiv.org/abs/2509.09321v1)|**贡献点分点总结：**  <br/>1. **自动化任务收集系统**：基于浏览器自动化与LLM，自动从Kaggle、AIcrowd等平台提取多类型（表格、文本、图像等）和多模态的ML挑战，实现任务多样性。  <br/>2. **基于排行榜的难度建模机制**：通过参与者数量和分数分布动态估计任务复杂度，提升难度评估的客观性和可扩展性。  <br/>3. **多维评估框架**：综合性能、格式合规性、约束遵守及任务泛化能力，全面验证LLM代理的能力。  <br/>4. **多规模基准子集设计**：构建Lite（18任务）、Medium和Full三个子集，覆盖不同难度和模态，适配多样化的评估场景。  <br/><br/>**总结（100字以内）：**  <br/>TAM Bench通过三个创新（自动化收集、难度建模、多维评估）构建了多模态、多难度的基准测试，同时推出Lite、Medium、Full三个子集以适应不同评估需求，全面衡量LLM代理在端到端ML任务中的能力。|
|2509.09198v2|[GmSLM : Generative Marmoset Spoken Language Modeling](http://arxiv.org/abs/2509.09198v2)|总结（100字以内）：  <br/>提出GmSLM模型，通过无监督数据评估Marmoset声通信，揭示其类似人类语言的特征，为神经科学与进化生物学提供跨学科研究框架。<br/><br/>**贡献点：**  <br/>1. **挑战传统观点**：通过Marmoset复杂声通信揭示非人灵长类发声行为并非完全先天，为语言起源研究提供新视角。  <br/>2. **构建研究桥梁**：连接Marmoset声通信与脑活动研究，弥补人类语言研究中对大脑难以直接探索的局限。  <br/>3. **提出专用模型**：设计生成式Marmoset语音建模框架GmSLM，针对性解决其发声数据特性与标准LLM方法不兼容的问题。  <br/>4. **创新评估方法**：开发基于无监督野外数据和弱标签对话数据的零样本评估指标，验证模型有效性并优于人类语音基线。  <br/>5. **高质量生成能力**：GmSLM生成的语音在声学特征上高度接近真实样本，并在下游任务中表现优异。  <br/>6. **跨学科应用价值**：模型可区分真实与人工对话，为探索声通信的神经基础及联结语音与脑活动的实践框架提供工具。|
|2509.08344v1|[Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model](http://arxiv.org/abs/2509.08344v1)|总结：  <br/>提出基于上下文学习的语音情感识别个性化方法，通过少量目标说话人的情感样本学习其特征，解决传统方法依赖完整情绪数据集的问题，并设计扩展的语音-语言模型进行元训练，实验验证其有效性。<br/><br/>贡献点：  <br/>1. **提出新型个性化方法**：首次将上下文学习（ICL）应用于语音情感识别（SER），通过条件化目标说话人的情感样本实现个性化适应。  <br/>2. **解决数据不足难题**：克服传统方法需收集目标说话人所有情绪标签语音的限制，仅需少量样本即可完成特征学习。  <br/>3. **构建扩展语音-语言模型**：基于大语言模型（LLM）设计专门的语音-语言模型，用于元训练以支持ICL驱动的SER个性化。  <br/>4. **实验验证有效性**：通过全新构建的SER数据集验证方法性能，证明其优于传统个性化技术。|
|2509.08031v2|[AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs](http://arxiv.org/abs/2509.08031v2)|总结（100字以内）：  <br/>AU-Harness 提出高效、全面的LALM评估框架，解决现有工具效率低、提示不一致和任务覆盖不足的问题，引入新评估类别并揭示模型在时间理解和复杂语音推理上的显著差距，推动系统化开发。<br/><br/>贡献点：  <br/>1. **高效评估框架**：通过优化批处理与并行执行，实现比现有工具快127%的处理速度，支持大规模模型评估。  <br/>2. **标准化与灵活性**：提供统一的提示协议和可配置接口，提升模型间比较的公平性与结果可复现性。  <br/>3. **新评估类别**：提出LLM-Adaptive Diarization（时序音频理解）和Spoken Language Reasoning（复杂语音推理任务）两大新型评估方向。  <br/>4. **全面实验验证**：对380+任务进行系统评估，揭示当前LALMs在时序理解与复杂推理任务中的性能短板。  <br/>5. **标准化缺失分析**：指出音频基准中指令模态标准化不足，导致复杂任务性能差异高达9.5分，引发对评估体系的反思。  <br/>6. **工具与洞察结合**：不仅提供实用评估工具，还通过结果分析为模型改进和领域发展提供关键方向指引。|
|2509.07274v1|[LLM Analysis of 150+ years of German Parliamentary Debates on Migration   Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](http://arxiv.org/abs/2509.07274v1)|总结（100字以内）:  <br/>该研究评估多LLM在标注德国议会政治话语中（反）团结类型的准确性，比较人类标注结果，分析模型规模与训练数据影响，并揭示战后至2015年后德国移民议题的团结趋势变化，强调LLM在政治文本分析中的应用价值。<br/><br/>贡献点：  <br/>1. **首次系统评估多LLM在标注政治话语（反）团结子类型中的表现**，涵盖德国议会历史与当代数据。  <br/>2. **建立大规模人类标注基准**（数千条标注，历时一年），提供可靠的对照数据。  <br/>3. **探究关键影响因素**：模型规模、提示策略差异、微调效果及历史/当代数据分布对标注结果的影响。  <br/>4. **识别系统性错误**（如偏见或分类偏差），提升模型在敏感政治话题中的可靠性。  <br/>5. **从社会科学研究视角解读标注结果**，揭示德国移民政策中的团结与反团结趋势演变。  <br/>6. **发现时间维度上的显著趋势**：战后时期存在高程度 migrant-directed solidarity，2015年后反-solidarity 趋势增强。  <br/>7. **推动跨学科应用**：验证LLM在政治文本分析中的潜力，为社会科学研究提供自动化工具支持。|
|2509.06382v1|[Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn   Multimodal LLM Conversation](http://arxiv.org/abs/2509.06382v1)|总结：  <br/>提出CAFA系统，通过多代理LLM架构实现动态环境下的个性化助听器实时调参，融合环境音频、听力图与用户反馈，结合轻量级YAMNet嵌入分类和伦理监管模块，显著提升对话效率。<br/><br/>贡献点：  <br/>1. **动态适配机制**：首次提出基于多代理架构的Context-Adaptive Fitting Advisor (CAFA)，突破传统静态助听器配置的局限性。  <br/>2. **多模态融合处理**：整合实时环境音频、听力图及用户反馈，构建多任务协同的交互系统。  <br/>3. **轻量级音频分类**：采用基于YAMNet嵌入的神经网络实现91.2%准确率的环境声分类（对话/噪声/静音）。  <br/>4. **模块化LLM流程**：设计包含上下文获取、子问题分类、策略生成及伦理监管的分层处理框架。  <br/>5. **实时调参验证**：通过实验验证实时音频分类对提升对话效率的关键作用，证明系统有效性。|
|2509.06221v1|[Beamforming-LLM: What, Where and When Did I Miss?](http://arxiv.org/abs/2509.06221v1)|**总结（100字以内）:**  <br/>提出Beamforming-LLM系统，融合麦克风阵列空间音频分离、RAG检索与轻量级LLM摘要，实现多说话者环境中语义级对话回忆，支持自然语言查询与时空对齐，为智能听觉记忆系统提供新方案。<br/><br/>**贡献点分点列出:**  <br/>1. **空间音频与语义检索结合**：首次将定向音频分离（beamforming）与检索增强生成（RAG）技术结合，解决多说话者环境下对话内容缺失问题。  <br/>2. **实时转录与向量化处理**：采用Whisper模型进行高精度转录，并通过sentence encoder将音频片段嵌入向量数据库，提升语义检索效率。  <br/>3. **轻量级摘要生成**：利用GPT-4o-mini等轻量模型实现非关注音频段的语义摘要，降低计算成本同时保持信息完整性。  <br/>4. **时空对齐与对比摘要**：通过时间对齐技术整合查询相关片段与未关注内容，生成对比性总结并支持时间戳音频播放，增强用户交互体验。  <br/>5. **多场景应用潜力**：为助听设备、会议纪要生成及空间计算等场景提供基础框架，推动智能听觉记忆技术的实际落地。|
|2509.06164v2|[Benchmarking Gender and Political Bias in Large Language Models](http://arxiv.org/abs/2509.06164v2)|总结：提出EuroParlVote数据集，评估LLMs在政治敏感场景下的性别分类与投票预测偏差，揭示模型对女性与极左/右立场的系统性偏见，并对比专有模型与开源模型的性能差异。<br/><br/>贡献点：<br/>1. 构建首个融合欧洲议会辩论视频与投票数据的基准EuroParlVote，包含性别、年龄、国籍、政党等多维元数据<br/>2. 首次系统评估LLMs在政治敏感场景下的两个关键任务：性别分类与投票预测<br/>3. 揭示LLMs存在系统性偏差：女性MEP误分类率高、对女性演讲者投票预测准确率下降<br/>4. 发现LLMs对中间派政治团体表现优异，而对极左/右团体预测能力显著不足<br/>5. 对比分析专有模型（如GPT-4o）与开源模型的性能差异，证实其在稳健性与公平性上的优势<br/>6. 公开发布数据集、代码及演示工具，推动NLP政治应用领域的公平性与问责研究|