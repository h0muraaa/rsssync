|Source|Title|Summary|
|---|---|---|
|2507.23088v1|[Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for   Interoperative Surgical Assistance](http://arxiv.org/abs/2507.23088v1)|**贡献点：**  <br/>1. 提出**Perception Agent**，融合语音集成的提示工程LLMs、SAM模型与any-point tracking基础模型，实现动态手术环境下的自然人机交互。  <br/>2. 创新性引入**两种新型机制**，支持实时分割已知和未知手术场景元素（如新器械、假体、纱布），突破传统方法的固定类别限制。  <br/>3. 设计**记忆仓库**，可记录新元素信息并复用至后续手术，推动手术过程的人机共生与系统自适应能力。  <br/>4. 通过**定量与定性实验**验证性能，在公开数据集上表现与复杂手动提示策略相当，且在定制数据集展示分割新元素的灵活性。  <br/>5. 赋能AI驱动的实时手术辅助，为动态医疗场景中自然交互和应用落地提供关键技术突破。  <br/><br/>**总结：**  <br/>该研究提出语音驱动的Perception Agent，结合SAM与新型分割机制，实现手术场景中未知元素的灵活识别与记忆复用，显著提升人机交互的自然性与系统自适应性，为动态手术辅助技术发展提供创新方案。|
|2507.22367v1|[Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided   LLM Representations and Multimodal Apparent Behaviors](http://arxiv.org/abs/2507.22367v1)|**总结（100字以内）**  <br/>提出"Traits Run Deep"框架，利用心理学引导提示提取人格语义，并构建文本中心的多模态特征融合网络，显著提升跨模态一致性与数据稀缺场景下的泛化能力。实验验证该方法在AVI数据集上将MSE降低45%，并在AVI Challenge 2025中获得人格评估赛道第一名。<br/><br/>**贡献点**  <br/>1. **提出心理学引导提示方法**：首次将人格特质与心理学原理融入LLMs的提示设计，提升高阶人格语义表征质量。  <br/>2. **构建文本中心的多模态融合网络**：开发异步信号对齐机制，通过Chunk-Wise Projector、Cross-Modal Connector等模块实现跨模态信息有效整合。  <br/>3. **增强跨模态与数据稀缺场景下的性能**：引入集成回归头，显著降低MSE（45%），并在AVI Challenge 2025中取得人格评估赛道最佳成绩。  <br/>4. **融合音频-视觉显性行为特征**：通过提取并融合非语言行为特征，进一步提升人格评估准确率。|
|2507.21411v1|[InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](http://arxiv.org/abs/2507.21411v1)|**贡献点总结（100字以内）:**  <br/>提出融合物理交互与数据可视化的新型叙事方法，通过调研和工作坊建立操作映射，开发InSituTale原型系统，验证其直观性、实用性和沉浸式体验，推动物理与数字环境协同的交互设计。  <br/><br/>**分点贡献:**  <br/>1. **提出新交互范式**：引入“增强物理数据叙事”，通过操控物理对象实现可视化命令，突破传统仅依赖语音或手势的交互局限。  <br/>2. **系统化操作映射**：通过问卷调研识别数据叙事中的常见命令，结合工作坊收集物理操作与命令的映射关系，构建理论基础。  <br/>3. **开发原型系统**：设计并实现InSituTale原型，集成深度摄像头追踪与Vision-LLM事件检测，实现物理与数字元素的实时联动。  <br/>4. **验证有效性**：通过用户实验证明系统在直观交互、实用价值和沉浸体验方面的优势，为后续研究提供实证依据。|
|2507.20924v1|[FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech   Concept Bottleneck Models](http://arxiv.org/abs/2507.20924v1)|总结：  <br/>本文提出三种模型（SCBM、SCBMT、XLM-RoBERTa）参与EXIST挑战赛，针对社交媒体文本中的性别歧视识别与分类任务，探索可解释性与性能的平衡，并通过元数据分析提升分类效果。<br/><br/>贡献点：  <br/>1. **提出SCBM模型**：首次将描述性形容词作为可解释的瓶颈概念，利用大语言模型（LLMs）生成人类可解读的语义表示，构建轻量分类器。  <br/>2. **设计SCBMT模型**：融合SCBM的形容词表征与Transformer的语境嵌入，优化模型在解释性与分类性能间的权衡，实现细粒度解释。  <br/>3. **元数据利用研究**：分析注释者人口统计信息等元数据对性别歧视识别的影响，拓展任务理解维度。  <br/>4. **多语言实验验证**：基于XLM-RoBERTa模型进行多语言（英语、西语）数据增强，取得软-软评估中的优异排名。|
|2507.20730v1|[Vocalize: Lead Acquisition and User Engagement through Gamified Voice   Competitions](http://arxiv.org/abs/2507.20730v1)|**贡献点：**<br/><br/>1. 提出Vocalize系统：首个结合游戏化机制与声音竞赛的端到端平台，旨在通过互动体验提升用户参与度和潜在客户获取效率。  <br/>2. 技术创新：融合音频处理技术与大型语言模型（LLMs），实现跨模态交互体验，扩大受众覆盖范围并增强用户黏性。  <br/>3. 实证验证：通过在4个现场活动中部署系统，量化用户参与效果，证明游戏化音频策略在营销中的可行性。  <br/>4. 应用前景：揭示Vocalize在增强品牌认同、提升客户忠诚度方面的潜力，为营销及其他垂直领域提供新思路。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于游戏化声音竞赛的Vocalize平台，融合音频处理与LLMs技术，验证其提升用户参与和品牌忠诚度的效果，为交互式营销策略提供创新解决方案。|
|2507.20241v1|[Reframe Your Life Story: Interactive Narrative Therapist and Innovative   Moment Assessment with Large Language Models](http://arxiv.org/abs/2507.20241v1)|总结：  <br/>本论文提出基于叙事疗法的语音交互框架，通过INT和IMA两个核心组件提升心理支持的模拟可靠性与效果评估，实验验证其在治疗质量和实际应用场景中的优越性。<br/><br/>贡献点：  <br/>1. 提出融合叙事疗法的综合框架，解决传统LLM在心理治疗模拟中的现实性不足问题；  <br/>2. 设计INT（交互叙事治疗师），通过规划治疗阶段、引导反思层次和生成专业化回应，提升对话的治疗深度；  <br/>3. 开发IMA（创新时刻评估），创新性地追踪客户演讲中的"创新时刻"量化治疗进展，改进效果评估机制；  <br/>4. 通过大规模实验验证框架有效性，并展示其在合成高质量支持对话中的实用价值。|
|2507.20220v1|[Motion-example-controlled Co-speech Gesture Generation Leveraging Large   Language Models](http://arxiv.org/abs/2507.20220v1)|总结：  <br/>提出MECo框架，利用LLMs实现语音动作生成，突破传统伪标签限制，支持多模态输入与身体部位精细控制，并在关键指标上达到SOTA。<br/><br/>贡献点：  <br/>1. **框架创新**：首个基于大语言模型（LLMs）的运动示例可控共语音手势生成框架，通过微调实现语音与动作的联合理解。  <br/>2. **显式上下文引导**：将运动示例作为显式查询上下文嵌入提示结构，直接指导手势生成而非依赖隐式伪标签。  <br/>3. **多模态与精细化控制**：支持多种输入模态（运动片段、静态姿势、视频、文本）及对个体身体部位的精细控制，提升生成灵活性。  <br/>4. **性能突破**：在Fréchet手势距离、动作多样性、示例-手势相似度三项指标上达到当前最优水平（SOTA）。|
|2507.20169v1|[Self-Improvement for Audio Large Language Model using Unlabeled Speech](http://arxiv.org/abs/2507.20169v1)|总结（100字以内）:  <br/>本文提出无需标注数据的SI-SDA方法，通过大模型解码信息评估伪标签质量并基于强化学习优化实现域自适应，显著提升音频LLM在ASR、SQA和S2TT任务中的性能，具有高数据效率，适合实际应用。<br/><br/>贡献点:  <br/>1. **提出SI-SDA方法**：首次设计无需标注数据的自改进策略，实现音频LLM在特定目标领域的性能提升。  <br/>2. **伪标签质量评估机制**：利用大模型解码过程中隐含的信息，动态评估生成伪标签的可靠性。  <br/>3. **强化学习优化域适应**：结合强化学习框架，通过优化伪标签生成过程提升模型在目标领域的适应性。  <br/>4. **多任务验证有效性**：在ASR、SQA和S2TT多个任务及公开数据集上超越现有基准，验证方法普适性。  <br/>5. **高效数据利用**：方法在少样本/无标签场景下保持高效率，具备实际部署潜力。|
|2507.20091v1|[ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in   Speech Language Models](http://arxiv.org/abs/2507.20091v1)|**贡献点：**  <br/>1. 提出ProsodyLM框架，解决现有语音语言模型在prosody信息学习上的不足。  <br/>2. 设计了一种基于文本转录和词级prosody标记的简单tokenization方案，保留更完整的语音韵律信息。  <br/>3. 实验证明该方法使模型在预训练阶段自发学习到多种prosody处理能力（如对比焦点、情感识别、重音分析及长上下文韵律一致性）。  <br/><br/>**总结：**  <br/>本研究提出ProsodyLM，通过改进语音tokenization方案保留韵律信息，显著提升语音语言模型在预训练中学习复杂prosody能力的效果。|
|2507.19616v1|[HITSZ's End-To-End Speech Translation Systems Combining   Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language   Model for IWSLT 2025 in Indic Track](http://arxiv.org/abs/2507.19616v1)|贡献点（分点）:<br/>1. 提出首个集成Whisper ASR与Krutrim Indic专用LLM的端到端语音-文本翻译系统<br/>2. 在低资源英汉互译场景下取得SOTA性能（English→Indic:28.88 BLEU；Indic→English:27.86 BLEU）<br/>3. 探索Chain-of-Thought方法在语音翻译中的应用，发现其可显著提升已解析输出的翻译质量（如Tamil→English BLEU提升13.84）<br/>4. 首次系统研究CoT方法在语音翻译任务中格式一致性保障的挑战<br/><br/>总结(100字内): <br/>本研究构建集成ASR与Indic专用LLM的端到端翻译框架，在低资源场景下取得优异结果，并探索CoT方法对翻译质量的提升作用，同时揭示其在格式控制方面的局限性。|
|2507.19303v1|[Identifying Fine-grained Forms of Populism in Political Discourse: A   Case Study on Donald Trump's Presidential Campaigns](http://arxiv.org/abs/2507.19303v1)|**总结（100字以内）**:  <br/>本论文创建了针对民粹主义思想分析的新型数据集，并系统评估了预训练语言模型的性能差异，揭示其检测局限性。通过对比微调模型与指令微调模型，发现后者在跨语境泛化上更具鲁棒性，同时为政治修辞分析提供了实证案例。  <br/><br/>**贡献点**：  <br/>1. **构建专用数据集**：提出并公开了首个专门捕捉民粹主义话语特征的多语言数据集。  <br/>2. **性能分析与比较**：系统评估了开源与闭源LLM在指令跟随范式下对民粹主义的识别能力，揭示其局限性。  <br/>3. **模型优化方法**：发现微调RoBERTa分类器显著优于新指令微调模型，除非后者也被微调，为跨领域任务提供优化思路。  <br/>4. **应用实证研究**：利用最佳模型分析特朗普竞选演讲，揭示其民粹主义修辞策略及其有效性。  <br/>5. **泛化能力验证**：通过跨语境测试欧洲政治演讲，评估模型在政治话语分析的迁移性，证明指令微调模型对领域外数据更鲁邦。|
|2507.19040v1|[FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex   Spoken Dialogue Systems](http://arxiv.org/abs/2507.19040v1)|**贡献点总结（100字以内）**  <br/>提出首个针对全双工语音对话系统的综合基准测试框架，设计多维度评估指标，验证三大开源系统的性能表现，并开源演示、数据和代码，推动FDSDS研究发展。<br/><br/>**分点贡献：**  <br/>1. **构建首个全双工对话基准框架**  <br/>   首次整合LLM、TTS、ASR技术，建立覆盖用户打断、延迟处理、系统鲁ustness的评估体系，填补现有FDSDS研究空白。<br/><br/>2. **设计多样化评估指标**  <br/>   针对全双工场景，提出新颖的指标（如打断响应率、多轮对话稳定性）量化模型在复杂交互环境中的表现。<br/><br/>3. **实验证据与系统对比**  <br/>   在3个主流开源FDSDS（Moshi、Freeze-omni、VITA-1.5）上开展大规模测试（293对话/1200打断），揭示当前技术瓶颈。<br/><br/>4. **开源资源促进研究复用**  <br/>   提供完整数据集、实验代码及演示，支持社区验证与改进，提升领域研究效率与可重复性。|
|2507.18863v1|[Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and   Language Model Reconstruction](http://arxiv.org/abs/2507.18863v1)|总结：  <br/>该论文提出了一种基于音素的两阶段V-ASR框架，结合视觉与面部地标特征，并利用LLM生成文本，有效解决了viseme歧义问题，提升了在LRS2/LRS3数据集上的识别性能。<br/><br/>贡献点：  <br/>1. **提出两阶段框架**：将V-ASR分解为音素预测（Stage 1）和词重构（Stage 2）两阶段，降低训练复杂度并提升整体性能。  <br/>2. **融合多模态特征**：结合视觉特征与面部地标信息，更全面地捕捉语音相关的视觉线索。  <br/>3. **引入LLM模型**：采用NLLB编码器-解码器结构完成音素到词的重构，增强语言建模能力。  <br/>4. **解决viseme歧义**：通过分阶段处理缓解音素相似但发音不同的问题，提高识别准确性。  <br/>5. **实验验证有效性**：在LRS2（17.4% WER）和LRS3（21.0% WER）数据集上取得优于现有方法的性能。|
|2507.18051v1|[The TEA-ASLP System for Multilingual Conversational Speech Recognition   and Speech Diarization in MLC-SLM 2025 Challenge](http://arxiv.org/abs/2507.18051v1)|**贡献点：**  <br/>1. **多语言对话ASR优化**：在Task I中集成语言识别模块与多语言MOE LoRA结构，结合CTC预测token作为提示，提升自回归生成性能。  <br/>2. **大规模多语言数据训练**：基于约180k小时的多语言ASR数据训练模型，增强跨语言识别能力。  <br/>3. **语音辨识模型改进**：在Task II中替换基线英中混合模型为更适合的英语单一模型，针对性优化任务表现。  <br/>4. **性能突破**：Task I WER降至9.60%（降低30.8%），Task II时间约束最小排列WER为17.49%，分别获挑战任务一、二等奖。  <br/><br/>**总结（100字内）：**  <br/>本研究提出TEA-ASLP系统，通过多语言MOE LoRA、语言识别集成及CTC提示优化提升多语言对话ASR性能，采用英语单一模型改进语音辨识任务。实验结果在Task I和Task II中均取得优异表现，分别获挑战赛一、二等奖。|
|2507.18044v1|[Synthetic Data Generation for Phrase Break Prediction with Large   Language Model](http://arxiv.org/abs/2507.18044v1)|总结：  <br/>该论文提出利用大语言模型生成合成短语切分标注，有效缓解人工标注成本问题，并通过多语言评估验证其在语音领域的可行性，为语音处理提供新的解决方案。<br/><br/>贡献点：  <br/>1. **引入LLM生成合成标注**：首次将大语言模型应用于短语切分预测的合成数据生成，减少对人工标注的依赖。  <br/>2. **验证有效性**：通过与传统标注对比，证明LLMs生成的数据在短语切分任务中的性能优势。  <br/>3 **跨语言泛化能力**：在多种自然语言上评估，展示所提方法的普适性和对语音领域挑战的适应性。  <br/>4 **突出LLM潜力**：揭示大语言模型在语音处理任务中的应用价值，为未来研究提供方向。|
|2507.17718v1|[AI Telephone Surveying: Automating Quantitative Data Collection with an   AI Interviewer](http://arxiv.org/abs/2507.17718v1)|**总结**  <br/>本研究构建基于LLM、ASR和语音合成的AI电访系统，验证其在定量调研中的有效性，并发现缩短问卷和增强AI响应性可提升完成率、中断率和满意度。  <br/><br/>**贡献点**  <br/>1. **提出AI电访新范式**：首次将语音AI应用于定量调研，实现规模化数据收集并兼顾人际交互性与研究严谨性。  <br/>2. **系统设计与规范遵循**：开发专用AI系统，严格采用调研最佳实践（如问题/答案随机化、措辞标准化）。  <br/>3. **实证验证与对比分析**：通过双试点调查（AI与人工）验证系统效果，量化评估完成率、中断率和满意度指标。  <br/>4. **优化策略探索**：揭示问卷长度与AI交互响应性对调研质量的正向影响，为实际应用提供改进方向。|
|2507.17578v1|[Synthetic Voice Data for Automatic Speech Recognition in African   Languages](http://arxiv.org/abs/2507.17578v1)|总结：  <br/>本文首次系统评估非洲语言大規模合成语音语料库，提出低成本生成方法，展示不同语言和比例下的ASR性能提升，并公开数据模型促进后续研究。<br/><br/>贡献点：  <br/>1. 提出首个系统性评估框架，用于非洲语言大規模合成语音语料库在ASR中的有效性验证。  <br/>2. 采用LLM驱动文本创建、TTS合成及ASR微调三阶段流程，实现高性价比的合成语音数据生成（成本低于真实数据1%）。  <br/>3. 分析不同语言（Hausa、Dholuo、Chichewa）与合成数据比例（1:1/1:2）下的ASR性能差异，揭示优化潜力与局限性。  <br/>4. 引入性别细分的ASR性能评估，量化合成数据对不同性别语音识别的影响。  <br/>5. 通过编码者可靠性分析，指出需改进评测协议与数据准确性以提升评估有效性。  <br/>6. 公开所有数据与模型，推动非洲语言合成语音技术的进一步研究与应用。|
|2507.17288v1|[Triple X: A LLM-Based Multilingual Speech Recognition System for the   INTERSPEECH2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.17288v1)|贡献点：  <br/>1. 提出Triple X语音识别系统，针对MLC-SLM挑战Task 1设计；  <br/>2. 创新性采用编码器-适应器-LLM架构，融合文本大模型推理能力与领域适配；  <br/>3. 设计多阶段训练策略，利用大规模多语言音频数据集优化性能；  <br/>4. 在挑战中取得第二名，验证了方法在多语言对话场景下的竞争力。  <br/><br/>总结：  <br/>本文提出Triple X系统，通过编码器-适应器-LLM架构和多阶段训练策略，提升多语言对话语音识别性能，在MLC-SLM挑战中取得第二名。|
|2507.16291v1|[Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers](http://arxiv.org/abs/2507.16291v1)|**贡献点**  <br/>1. 提出基于大语言模型（LLMs）的新型对抗性攻击方法，通过语义保留的vishing剧本生成，可绕过ML分类器检测。  <br/>2. 构建系统性攻击框架，结合提示工程与语义混淆技术，实现对真实vishing脚本的高效转换。  <br/>3. 在KorCCViD数据集上验证攻击有效性，发现LLM生成的剧本在统计显著性上有效降低分类器准确率（GPT-4o达30.96%）。  <br/>4. 证明攻击的低成本与高效率，平均生成时间低于9秒，且无需高额财务成本。  <br/>5. 揭示当前vishing检测系统的脆弱性，呼吁开发更健壮的防御机制，并呼吁LLM厂商加强安全性防护。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出基于LLM的对抗性vishing攻击方法，通过语义保留的剧本生成显著降低检测准确性，揭示现有防御体系的不足，并强调对LLM安全性防护的迫切需求。|
|2507.16130v1|[Disability Across Cultures: A Human-Centered Audit of Ableism in Western   and Indic LLMs](http://arxiv.org/abs/2507.16130v1)|总结：  <br/>本研究通过比较中美两国LLM在印度语境中识别能力建设的表现，揭示模型地域偏见和文化框架差异，强调本地残疾经验对AI公平性的关键影响，为构建全球包容的AI标准提供实证基础。<br/><br/>贡献点：  <br/>1. **首个跨地域语言模型对比研究**：系统评估了美国（GPT-4、Gemini、Claude、Llama）与印度（Krutrim、Nanda、Gajendra、Airavata）LLM在印度语境中识别能力建设的差异。  <br/>2. **本地化数据集构建**：将国际公开的能力建设语音数据集翻译成印地语，填补非西方语境下相关数据资源的空白。  <br/>3. **残疾人群体参与验证**：招募175名印度和美国残疾人人工评分，发现地域间对能力建设的判定存在显著分歧。  <br/>4. **揭示语言文化对模型性能的影响**：发现LLM在印地语中更宽容能力建设，且忽视西方对能力建设的预设框架。  <br/>5. **提出AI公平性设计的本土化方向**：论证需将本地残疾经验纳入AI系统设计与评估，推动全球包容性标准的建立。|
|2507.16124v1|[Benchmarking LLM Privacy Recognition for Social Robot Decision Making](http://arxiv.org/abs/2507.16124v1)|贡献点总结：  <br/>1. **提出基于Contextual Integrity（CI）的隐私相关场景框架**，系统分析家庭社交机器人中敏感数据处理的隐私考量。  <br/>2. **首次大规模用户调研**，揭示用户隐私偏好与行为选择的关系，样本量达450人。  <br/>3. **评估当前LLMs的隐私意识表现**，发现其与人类用户在隐私敏感场景上的决策存在显著差异。  <br/>4. **开发四种提示策略**，提升LLMs在隐私控制任务中的表现，并通过实验对比验证其有效性。  <br/>5. **探讨AI隐私意识的潜在应用**，为构建隐私友好的人机交互系统提供理论和实践方向。|
|2507.15729v1|[Gaze-supported Large Language Model Framework for Bi-directional   Human-Robot Interaction](http://arxiv.org/abs/2507.15729v1)|**贡献点总结（100字以内）:**  <br/>本研究提出基于LLM的多模态(HRI)系统，融合眼动与语音信息，实现环境感知与动态任务支持，具备模块化和跨任务适应性。通过对比实验证明LLM提升灵活性与用户参与度，但存在冗余输出问题，传统脚本管道更适合简单任务。<br/><br/>**分点列出的贡献点:**  <br/>1. **提出多模态交互系统**：设计结合眼动追踪与语音信息的界面，实现双向、上下文感知的环境感知与动态任务支持。  <br/>2. **模块化与可迁移性**：系统采用模块化架构，支持跨任务和跨平台的灵活部署，适应不同应用场景与机器人类型。  <br/>3. **实时语言状态表示**：集成语言模型的实时交互状态表征，结合快速感知模块，提升任务执行效率与响应速度。  <br/>4. **增强系统鲁棒性**：通过公开传播活动优化系统设计，提升对复杂环境的鲁棒性与用户交互体验。  <br/>5. **对比实验证明方法差异**：通过实验室研究验证LLM方案在适应性、用户参与度和任务执行上的优势，同时揭示其冗余输出问题，对比传统脚本管道的适用场景。|
|2507.15221v1|[EchoVoices: Preserving Generational Voices and Memories for Seniors and   Children](http://arxiv.org/abs/2507.15221v1)|总结：  <br/>本研究提出EchoVoices系统，专为老年与儿童设计，集成创新语音识别、合成及对话管理技术，显著提升语音处理效果，构建跨代际数字遗产保存方案。<br/><br/>贡献点：  <br/>1. **首个针对老年和儿童的专用语音数字人系统**：EchoVoices专为特定用户群设计，解决传统技术忽视其独特语音特征的问题。  <br/>2. **三重核心技术创新**：  <br/>   - k-NN增强的Whisper模型：提升非典型语音的识别鲁棒性。  <br/>   - 年龄自适应VITS模型：实现高质量、说话人感知的语音合成。  <br/>   - LLM驱动的代理与RAG记忆系统：自动构建人设卡并维护对话连贯性。  <br/>3. **实验证明有效性**：在SeniorTalk和ChildMandrin数据集上验证了系统在识别准确率、合成质量及说话人相似度的显著提升。  <br/>4. **跨代际连接框架**：提供全面的语音与记忆保存方法，推动代际沟通与数字遗产的长期留存。|
|2507.14815v1|[FastLongSpeech: Enhancing Large Speech-Language Models for Efficient   Long-Speech Processing](http://arxiv.org/abs/2507.14815v1)|总结：  <br/>提出FastLongSpeech框架，通过压缩策略和动态训练方法解决长语音处理难题，并构建LongSpeech-Eval基准，验证模型性能与效率提升。<br/><br/>贡献点：  <br/>1. **提出FastLongSpeech框架**：首次设计无需专用长语音训练数据的可扩展模型，提升长语音处理能力。  <br/>2. **迭代融合策略**：通过压缩过长语音序列，将其转化为可管理长度，优化计算效率。  <br/>3. **动态压缩训练方法**：利用不同压缩比的短语音序列进行训练，迁移LSLM能力至长语音任务。  <br/>4. **构建LongSpeech-Eval基准**：开发专门评估长语音理解能力的测试集，填补该领域的评价空白。  <br/>5. **实验验证效果**：在长/短语音任务中均表现出色，显著提升推理效率，证明方法有效性。|
|2507.13468v1|[ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in   Human-Robot Conversations](http://arxiv.org/abs/2507.13468v1)|**贡献点**：  <br/>1. 提出ERR@HRI 2.0挑战，构建首个用于检测LLM驱动对话机器人错误的多模态基准数据集（含16小时人机交互数据）。  <br/>2. 数据集涵盖面部、语音、头部运动等多模态信号，标注系统错误与用户意图以校正行为偏差。  <br/>3. 鼓励研究者开发基于多模态数据的机器学习模型，提升对话错误检测能力。  <br/>4. 引入检测准确率、误报率等多维度评估指标，推动技术改进。  <br/>5. 通过社会信号分析，促进人机交互中对话鲁棒性的研究与实际应用。  <br/><br/>**总结**：  <br/>该研究通过构建多模态数据集和设计评估框架，推动对话机器人错误检测技术的发展，助力实现更可靠的自然交互。|
|2507.13374v1|[Smart Routing for Multimodal Video Retrieval: When to Search What](http://arxiv.org/abs/2507.13374v1)|总结（100字以内）:  <br/>提出ModaRoute系统，基于LLM动态优化多模态视频检索模态选择，降低计算成本和基础设施开销，同时保持竞争性检索效果，适用于大规模实际场景。<br/><br/>贡献点分点列出：<br/>1. **提出LMM-based智能路由框架**：首次设计基于Large Language Model（LLM）的ModaRoute系统，动态分配ASR/OCR/视觉模态资源。<br/>2. **提升检索效率与效果平衡**：相较传统方法，减少41%计算开销且维持60.9% Recall@5，优于仅依赖文本字幕的75.9% Recall@5。<br/>3. **精准模态组合策略**：通过分析查询意图与预测信息需求，平均每个查询仅需1.78种模态，显著低于传统3种模态的穷举搜索。<br/>4. **大规模验证可行性**：在1.8M视频片段上验证系统有效性，证明其可扩展性与实际部署价值，降低基础设施成本。|
|2507.13205v2|[Automatically assessing oral narratives of Afrikaans and isiXhosa   children](http://arxiv.org/abs/2507.13205v2)|**贡献点：**  <br/>1. 提出跨语言（Afrikaans和isiXhosa）的自动口语叙事评估系统，适用于大规模学前课堂。  <br/>2. 构建了基于自动语音识别（ASR）和机学习评分模型的自动化框架，实现叙事与理解能力的预测。  <br/>3. 对比分析线性模型与大型语言模型（LLM）在评分任务中的表现，验证LLM在多数场景下更优，而线性模型仍具竞争力。  <br/>4. 证明LLM系统在标记需干预学生方面与人类专家表现相当，为教育者提供可靠工具。  <br/>5. 为课堂自动化评估奠定基础，助力教师聚焦个性化教学支持，提升教学效率。  <br/><br/>**总结（100字内）**  <br/>提出跨语言自动评估系统并验证其有效性，对比不同模型性能，为学前教师提供高效干预筛选工具，推动教育评估智能化发展。|
|2507.13205v1|[Automatically assessing oral narratives of Afrikaans and isiXhosa   children](http://arxiv.org/abs/2507.13205v1)|总结（100字以内）:  <br/>本文提出跨语言（南非语和祖鲁语）的自动叙事评估系统，验证大语言模型在干预识别上的有效性，并展现出线性模型的实用性，为教师提供高效支持工具以促进幼儿读写能力发展。<br/><br/>贡献点:  <br/>1. **跨语言评估系统**：开发适用于南非语（Afrikaans）和祖鲁语（isiXhosa）的自动口语叙事评估系统，填补了多语言场景下的研究空白。  <br/>2. **模型对比实验**：系统性比较线性模型与大语言模型（LLM）在叙事和理解评分中的表现，证明LLM在多数情况下更优，同时验证线性模型的可行性。  <br/>3. **干预识别效能**：LLM系统在标记需要干预的儿童方面达到与人类专家相当的准确性，提升评估的效率和客观性。  <br/>4. **实际应用价值**：为教师提供自动化工具，减少人工负担，使他们能更专注个性化教学支持，助力早期教育发展。|
|2507.12619v1|[BootSeer: Analyzing and Mitigating Initialization Bottlenecks in   Large-Scale LLM Training](http://arxiv.org/abs/2507.12619v1)|总结（100字以内）:  <br/>该论文首次深入分析LLM训练启动开销，提出系统优化框架Bootseer，通过解决容器镜像加载、依赖安装和模型恢复三大瓶颈的三种技术，实测将启动延迟降低50%，为工业级大模型训练效率提升提供关键解决方案。<br/><br/>贡献点：  <br/>1. **首次系统分析**：基于真实生产数据，首次对LLM训练启动开销进行深入量化分析，揭示其成分与对GPU资源的直接消耗。  <br/>2. **启动瓶颈识别**：明确识别出容器镜像加载、运行时依赖安装、模型检查点恢复三类关键启动瓶颈。  <br/>3. **优化框架设计**：提出系统级优化框架Bootseer，针对上述瓶颈设计出三种创新技术（热块预取、依赖快照、条纹HDFS-FUSE）。  <br/>4. **实测效果验证**：在生产环境中部署并评估Bootseer，实验证明其能有效减少50%的启动开销，显著提升训练效率。|
|2507.12252v1|[Improving Contextual ASR via Multi-grained Fusion with Large Language   Models](http://arxiv.org/abs/2507.12252v1)|总结：  <br/>本文提出一种基于多粒度融合的ASR增强方法，结合LLMs的上下文知识与ASR的声学信息，有效提升关键词识别性能，同时保持非关键词文本准确性，并通过消融实验验证了其组件的互补性。<br/><br/>贡献点：  <br/>1. 提出**多粒度融合框架**，联合利用token-level与phrase-level融合策略，综合提升关键字识别精度与整体语义理解能力。  <br/>2. 设计**late-fusion策略**，创新性地将ASR的声学信息与LLMs的语义知识结合，实现细粒度与粗粒度信息的平衡协同。  <br/>3. 在**中英文数据集**上验证方法有效性，取得关键词相关指标的SOTA性能，同时保持非关键词文本的高准确率。  <br/>4. 通过**消融研究**明确token-level与phrase-level组件对性能的独立贡献，证明其在联合框架中的互补性。|
|2507.11966v1|[Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](http://arxiv.org/abs/2507.11966v1)|总结：  <br/>本文提出一种两阶段框架，解决低资源语言毒性翻译问题，通过少样本提示工程与模型优化提升翻译质量与文化敏感度，推动包容性NLP在内容审核等场景的应用。  <br/><br/>**贡献点：**  <br/>1. **提出毒性保持翻译框架**：开发可复现的双阶段方法，专门应对低资源语言对中文化嵌入的有害内容翻译挑战。  <br/>2. **人验证少样本提示工程**：通过迭代筛选和评估人工选中的Singlish目标示例，精准捕捉俚语、语调与毒性特征。  <br/>3. **模型-提示对优化策略**：利用语义相似度（直接与回译）对比多个大语言模型，提升翻译效率与效果。  <br/>4. **定量人类评估验证**：通过实证数据验证框架的有效性与效率，为低资源语言翻译提供可靠依据。  <br/>5. **支持文化敏感度治理**：为多文化大模型提供基于语境的审核基准，增强跨语言、跨文化的平台治理能力。  <br/>6. **推动包容性NLP实践**：以Singlish为试验平台，强调保护社会语言学细微差别对实际应用的重要性。|
|2507.11292v1|[Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources,   Coded Term Lexicon, and Enhanced Detection Frameworks](http://arxiv.org/abs/2507.11292v1)|**贡献点总结（100字以内）:**  <br/>提出首个中文span-level仇恨言论数据集STATE ToxiCN，系统研究隐含仇恨术语与大模型语义解析能力，并创新性集成注释词典提升检测性能，推动中文仇恨言论检测的可解释性研究。<br/><br/>**分点贡献:**  <br/>1. **首个中文span-level数据集**：构建STATE ToxiCN，填补中文细粒度仇恨言论标注数据的空白，用于评估现有模型的语义理解能力。  <br/>2. **隐含仇恨术语研究**：首次全面分析中文隐含仇恨词汇，探索大模型（LLMs）在解释此类复杂语义中的表现与潜力。  <br/>3. **可解释性方法创新**：提出将人工标注词典集成到模型中的技术，显著提升中文仇恨言论检测的性能与可解释性。|
|2507.11210v1|[Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and   Addressing Family Communication Bias](http://arxiv.org/abs/2507.11210v1)|**贡献点：**  <br/>1. 构建了一个包含30个场景的日本亲子对话语料库，系统标注理想父母偏见与压抑情绪的元数据。  <br/>2. 提出基于LLM的多智能体对话支持框架，支持分析对话并生成反馈，包含专门检测压抑情绪、描述隐性偏见及推断语境属性的智能体。  <br/>3. 设计四步结构化讨论流程，实现智能体协作生成共情且可操作的反馈机制。  <br/>4. 实验证明系统在检测压抑情绪类别和生成反馈的共情与实用性方面表现良好。  <br/>5. 通过模拟后续对话验证框架有效性，显示改善情感表达和家庭理解的潜力。  <br/><br/>**总结：**  <br/>本研究开发了基于大语言模型的多智能体框架，用于检测家庭沟通中的压抑情绪并提供反馈，改善亲子间情感表达与相互理解。|
|2507.10859v1|[MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](http://arxiv.org/abs/2507.10859v1)|总结：  <br/>本文提出首个全面评估语音助手多模态理解能力的基准MultiVox，涵盖语音与视觉线索的整合，揭示当前模型在情境感知响应生成方面的不足，推动语音技术发展。<br/><br/>贡献点：  <br/>1. 提出MultiVox，首次系统性评估语音助手的多模态理解能力（语音+视觉线索）。  <br/>2. 构建包含1000个标注语音对话的数据集，覆盖丰富的语音特征与视觉信号。  <br/>3. 引入对隐含语音特征（如音调、情感、音色、音量）及环境声学上下文的评估维度。  <br/>4. 评估9个SOTA模型，明确现有技术在多模态语境理解中的局限性。  <br/>5. 填补语音领域基准测试空白，为未来研究提供新方向与挑战标准。|
|2507.10468v1|[From BERT to Qwen: Hate Detection across architectures](http://arxiv.org/abs/2507.10468v1)|总结（100字以内）：  <br/>本文对比经典编码器与新一代LLM在仇恨言论检测中的性能，验证模型规模对实时文本检测的实际影响，并探索其对平台内容管理的启示。<br/><br/>贡献点：  <br/>1. **验证模型规模效应**：系统评估了经典双向Transformer编码器与超大规模自回归LLM在仇恨言论检测任务中的表现，确认模型体量与上下文感知能力的关联性。  <br/>2. **构建专用评测数据集**：基于真实在线互动文本创建了Hate or No Hate数据集，为该领域提供标准化的基准测试资源。  <br/>3. **揭示实际应用边界**：通过实验分析模型性能差异，探讨了超大规模LLM在平衡检测精度与非过度审查之间的潜力与局限。  <br/>4. **对比不同架构效能**：首次全面对比两种主流模型结构（传统编码器 vs LLM）在上下文理解与分类任务中的有效性，为后续研究提供方向。|
|2507.10427v1|[Towards Emotion Co-regulation with LLM-powered Socially Assistive   Robots: Integrating LLM Prompts and Robotic Behaviors to Support   Parent-Neurodivergent Child Dyads](http://arxiv.org/abs/2507.10427v1)|**贡献点：**  <br/>1. **提出LLM与SAR的整合方法**：开发首个结合大型语言模型（LLMs）与社会辅助机器人（MiRo-E）的系统，实现父母与神经发育障碍儿童之间的情绪共调节。  <br/>2. **临床验证系统效能**：通过试点测试和定性分析，实证证明该系统对改善亲子互动动态及促进情绪调节的积极影响。  <br/>3. **识别设计与技术挑战**：发现系统在交互自然性、个性化反馈及多模态整合等方面存在的关键问题。  <br/>4. **提供优化设计方向**：基于研究结果，提出针对未来LLM-powered SAR在心理健康应用的改进策略与技术路径。  <br/><br/>**总结（100字内）：**  <br/>本研究开发了首个结合LLM与社会辅助机器人（MiRo-E）的系统，用于父母与神经发育障碍儿童的情绪共调节，通过实证分析验证其有效性，揭示设计挑战并提出优化方向，推动智能机器人在心理健康领域的应用发展。|
|2507.10200v1|[Natural Language-based Assessment of L2 Oral Proficiency using LLMs](http://arxiv.org/abs/2507.10200v1)|分点贡献：<br/>1. 提出基于自然语言描述符的评估方法（NLA），用于验证大语言模型（LLM）在第二语言评估任务中的表现。<br/>2. 首次在零样本设置下，使用开源大模型Qwen 2.5 72B对S&I Corpus进行评估任务测试。<br/>3. 验证NLA方法在不匹配任务场景下的有效性，证明其可泛化至其他数据类型和语言。<br/>4. 展示NLA在无需针对性微调的情况下，超越BERT专用模型的评估性能。<br/>5. 强调NLA方法的可解释性优势，通过明确的描述符实现更透明的评估过程。<br/><br/>总结：本研究提出基于自然语言描述符的评估方法，验证其在零样本设置下对大语言模型的适用性，证明NLA在跨语言、跨数据场景中具有竞争力和可解释性优势。|
|2507.10177v1|[Abusive text transformation using LLMs](http://arxiv.org/abs/2507.10177v1)|总结：  <br/>本研究探索LLMs在有害文本转换中的应用，比较多模型的识别与转换效果，提出保留语义和情感的转换方法，并揭示不同模型在处理此类任务上的异同。<br/><br/>贡献点：  <br/>1. 提出使用LLMs将含仇恨言论和脏话的文本（如推文、评论）转换为无害版本，同时保留原始意图和信息。  <br/>2. 评估Gemini、GPT-4o、DeepSeek-V3和Groq等多状态-of-the-art模型在有害文本识别与转换任务中的表现。  <br/>3. 开发标准化转换框架，确保输出文本的情感、语义与原始内容相似，而非简单删除敏感词。  <br/>4. 揭示Groq在有害文本处理结果上与其它模型差异显著，同时发现GPT-4o与DeepSeek-V3在转换效果上有较高相似性。|
|2507.10069v1|[ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal   Parallelism](http://arxiv.org/abs/2507.10069v1)|总结: 本文提出ElasticMM系统，通过动态资源分配、弹性阶段调度和统一缓存策略，显著提升多模态大模型的推理效率和服务性能。<br/><br/>贡献点:<br/>1. 提出模态感知负载均衡机制，实现跨请求类型和推理阶段的动态资源分配（4.2x TTFT降低）<br/>2. 开发弹性分区调度框架，解耦模型推理流程并支持并行策略自适应调整（3.2-4.5x吞吐量提升）<br/>3. 设计统一多模态前缀缓存与非阻塞编码方案，提升整体推理效率且满足服务级别目标|
|2507.09116v2|[Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition](http://arxiv.org/abs/2507.09116v2)|**贡献点：**<br/>1. 提出**多模态GER**与**多粒度GER**两种方法，分别整合语音模态的发音信息和音素级细粒度特征。<br/>2. 设计**三阶段训练策略**，针对不同口音训练独立的LoRA专家模型，提升单口音场景下的性能。<br/>3. 开发**HDMoLE框架**，通过分层路由和动态阈值机制融合多口音LoRA专家，解决口音多样性挑战。<br/>4. 首次将多模态与多粒度信息结合，利用N-best词/音素级假设生成最终转录结果，优化误差校正流程。<br/>5. 在多口音英语数据集上验证方法有效性，实现比Whisper-large-v3基准模型高67.35%的相对WER降低。<br/><br/>**总结（100字以内）：**<br/>该研究提出多模态与多粒度生成式错误校正方法，结合层次路由和动态阈值的HDMoLE框架，有效应对口音多样性挑战，显著提升低资源口音场景下的语音识别性能。|
|2507.09116v1|[Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition](http://arxiv.org/abs/2507.09116v1)|**总结（100字以内）:**  <br/>本研究提出多模态与多粒度生成式纠错方法，结合发音信息与语义信息，通过LoRA微调和HDMoLE混合专家模型提升多口音场景下的ASR性能，实验实现67.35%的WER降低。<br/><br/>**贡献点:**  <br/>1. **提出多模态GER方法**：整合语音模态的发音信息，增强LLM对口音语音的建模能力。  <br/>2. **设计多粒度GER框架**：引入音素级与词级N-best假设，结合细粒度发音特征与语义信息优化纠错。  <br/>3. **开发HDMoLE混合专家模型**：通过分层路由与动态阈值机制，融合多单口音LoRA专家，解决口音多样性问题。  <br/>4. **创新多阶段训练策略**：为每种口音训练独立多模态GER模型，提升模型对特定口音的适应性。  <br/>5. **实验验证有效性**：在多口音英语数据集上取得显著结果，相对WER降低67.35%，优于Whisper-large-v3基线。|
|2507.09070v1|[SemAlignVC: Enhancing zero-shot timbre conversion using semantic   alignment](http://arxiv.org/abs/2507.09070v1)|总结：  <br/>提出SemAlignVC方法，通过SemAlign技术解决零样本语音转换中的音色泄露问题，在无需显式说话人嵌入的情况下实现高质量转换，表现优异且具有隐私保护与泛化能力。<br/><br/>贡献点：  <br/>1. **解决音色泄露问题**：引入SemAlign方法，有效分离文本与音频中的说话人特征，避免源说话人特质残留。  <br/>2. **无需显式说话人嵌入**：采用解耦的语义表示，直接通过自回归Transformer实现高质量语音转换，简化模型设计。  <br/>3. **提升转换质量**：在说话人音色相似度、可懂度和自然度等指标上超越现有基线方法。  <br/>4. **隐私与通用性优势**：方法具备隐私保护特性，并在不同场景下展示强泛化能力。|
|2507.08557v1|[FreeAudio: Training-Free Timing Planning for Controllable Long-Form   Text-to-Audio Generation](http://arxiv.org/abs/2507.08557v1)|**总结（100字以内）：**  <br/>本文提出首个训练无关的长文本时间控制T2A框架FreeAudio，通过LLM时间规划、注意力控制和潜在结构模块实现精确定时与全局一致性，实验表明其合成质量达到SOTA，且适用于复杂长文本生成。<br/><br/>**贡献点：**  <br/>1. **首次提出训练无关的长文本时间控制T2A框架**：FreeAudio是首个无需训练数据即可实现精确定时控制（如“owl hooted at 2.4s-5.2s”）的长音频生成方法。  <br/>2. **基于LLM的时间窗口规划与任务分解**：通过大语言模型生成非重叠时间窗口并重新描述内容，解决复杂时间指令的解析难题。  <br/>3. **创新注意力控制机制**：提出“Decoupling and Aggregating Attention Control”模块，实现精准时间控制；结合“Contextual Latent Composition”与“Reference Guidance”提升局部流畅性与全局一致性。  <br/>4. **验证高性能合成能力**：在无训练数据的条件下，合成质量达到当前最优，且与训练模型（如Stable Audio）表现相当，推动长文本T2A生成研究。  <br/>5. **开源演示与实际应用**：提供可交互的Demo样本，直观展示时间控制长文本音频生成效果（[链接](https://freeaudio.github.io/FreeAudio/)）。|
|2507.07877v1|[Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models](http://arxiv.org/abs/2507.07877v1)|总结（100字以内）:  <br/>该研究通过系统评估8种SOTA PTQ方法在Whisper和Moonshine模型上的表现，揭示了量化与精度的权衡关系，证明3比特量化在先进PTQ技术下仍可有效运行，为边缘设备ASR模型优化提供了实证依据和实用指导。<br/><br/>贡献点:  <br/>1. **构建全面基准体系**：首次对两类主流边缘ASR模型（Whisper、Moonshine）进行系统性量化评估，涵盖8种SOTA PTQ方法和7个开放数据集。  <br/>2. **开发统一量化框架**：基于LLM压缩工具链扩展，集成多样化量化算法、校准流程与评估工具，支持对权重和激活的联合量化分析。  <br/>3. **量化精度与效率的实证分析**：系统揭示不同位宽配置（如3比特）对模型性能（如识别准确率、内存IO、计算量）的影响规律，量化模型的可行性边界。  <br/>4. **指导边缘部署优化**：提出针对低功耗、持续运行的边缘设备ASR模型的优化方向，为实际工程中的资源约束场景提供可参考的性能优化策略。|
|2507.06329v1|[MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in   Music Mixing](http://arxiv.org/abs/2507.06329v1)|**总结（100字以内）**  <br/>本文提出MixAssist数据集，填补音乐制作协作教学研究空白，通过真实对话训练音频语言模型，并验证Qwen-Audio在生成混音建议方面的优越性能，推动智能AI在音乐创作中的应用。<br/><br/>**贡献点**  <br/>1. **构建首个音乐混音协作对话数据集**：MixAssist收录专家与业余音乐制作人7场深度协作对话，包含431个音频关联的多轮对话回合，为研究音频-语言交互提供全新资源。  <br/>2. **聚焦创作过程中的教学维度**：强调音频上下文中的协作指导机制，突破传统端到端自动化研究，支持艺术家在混音中的学习与共创。  <br/>3. **实验证明模型有效性**：通过LLM-as-a-judge评估和人类专家对比，证实基于MixAssist微调的模型（如Qwen-Audio）可生成高质量、情境相关的混音建议，Qwen表现显著优于其他模型。  <br/>4. **推动智能AI辅助工具发展**：为开发支持音乐混音创造性工作的智能助手提供理论指导与数据基础，助力AI与人类创作者的高效协同。|
|2507.05609v1|[MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses](http://arxiv.org/abs/2507.05609v1)|总结：  <br/>本研究提出一种面向智能眼镜的多麦克风侧话抑制框架MMW，通过混合块结构、帧级语音分割模块和多尺度优化策略显著提升嘈杂环境下的语音识别准确率。<br/><br/>贡献点：  <br/>1. **Tri-Mamba混合块设计**：  <br/>   构建基于Tri-Mamba架构的Mix Block，实现多通道音频在原始波形层面的高效融合，并支持实时流处理兼容性。  <br/><br/>2. **帧级语音分割模块**：  <br/>   引入Frame Diarization Mamba Layer，增强帧级侧话抑制能力，优化Whisper模型的微调效率。  <br/><br/>3. **多尺度联合优化策略**：  <br/>   提出Multi-Scale Group Relative Policy Optimization（GRPO），同步优化帧级与语句级侧话抑制，提升整体抗噪性能。|
|2507.05591v1|[MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models](http://arxiv.org/abs/2507.05591v1)|**贡献点总结（100字以内）**  <br/>本文提出MLlm-DR模型，整合小LLM与轻量查询模块（LQ-former），构建领域数据集提升诊断逻辑推理，提取多模态抑郁特征，实现可解释的抑郁症诊断，在CMDC和E-DAIC-WOZ数据集上达到SOTA性能。  <br/><br/>**分点贡献：**  <br/>1. **提出MLlm-DR模型**：首次设计结合小规模LLM与轻量查询模块（LQ-former）的多模态框架，解决现有模型对访谈数据训练不足的问题。  <br/>2. **强化训练数据集**：构建专用语料库进行微调，增强模型在医学领域的逻辑推理能力，同时保持临床实用性。  <br/>3. **多模态特征提取**：LQ-former模块专门从语音和视觉数据中捕捉抑郁相关特征，提升跨模态信息融合效果。  <br/>4. **验证有效性**：在两大数据集（CMDC、E-DAIC-WOZ）上取得当前最优性能，证明模型在可解释性与诊断精度上的优势。|
|2507.03875v1|[Demystifying ChatGPT: How It Masters Genre Recognition](http://arxiv.org/abs/2507.03875v1)|总结：  <br/>本研究通过MovieLens-100K数据集评估了ChatGPT等LLMs在类型预测中的表现，提出零样本/少样本提示策略，并结合视觉信息（电影海报）提升模型性能，揭示了ChatGPT在内容应用中的潜力。<br/><br/>贡献点：  <br/>1. **首次在电影类型预测任务中系统评估LLMs性能**：使用MovieLens-100K数据集（含1682部电影、18种类型）对比分析三个LLMs，证明ChatGPT无需微调即可超越其他模型。  <br/>2. **构建音频驱动的提示框架**：基于电影预告片的音频转录/字幕设计零样本和少样本提示，探索语音信息在类型预测中的有效性。  <br/>3. **融合多模态信息提升预测能力**：引入IMDb电影海报数据，通过VLM结合视觉提示增强LLM的类型预测效果，验证视觉信息与文本信息的互补性。  <br/>4. **揭示ChatGPT的多模态应用潜力**：综合实验结果表明，ChatGPT在类型预测任务中表现突出，其与视觉模态的结合可拓展至更广泛的内容分析场景。|
|2507.03343v2|[SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.03343v2)|**贡献点:**  <br/>1. **多模型融合架构**：结合Whisper-large-v3与mHuBERT-147双编码器，通过拼接输出嵌入实现语音与语言知识互补。  <br/>2. **三阶段联合训练策略**：同步更新语音编码器的低秩适配模块和LLM的投影参数，提升模型协同优化效率。  <br/>3. **语言感知提示机制**：在LLM输入中引入语言特定提示，增强语言相关文本生成能力。  <br/>4. **性能突破**：在无需扩展训练数据的前提下，系统在盲测集上实现11.76%的CER/WER，相较基线提升8.41个百分点。  <br/><br/>**总结**:  <br/>该研究通过融合双语音编码器与LLM，设计联合训练策略及语言提示机制，显著提升了多语言语音识别性能，且无需增加训练数据。|
|2507.03343v1|[SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.03343v1)|总结：  <br/>本文提出一种多语言对话语音识别系统，通过并行语音编码器与大语言模型的融合，结合三阶段训练策略和语言感知提示，显著提升识别性能且无需增加训练数据。<br/><br/>贡献点：  <br/>1. **创新架构**：提出将并行语音编码器（Whisper-large-v3与mHuBERT-147）与大语言模型（LLM）结合的统一多语言ASR框架。  <br/>2. **互补知识融合**：通过拼接两个编码器的输出嵌入，使模型同时利用声学与语言知识，提升多语言识别能力。  <br/>3. **三阶段训练策略**：设计联合优化低秩适配模块、投影参数及LLM的分阶段训练方法，增强模型整体性能。  <br/>4. **语言感知提示**：在LLM输入端引入语言感知提示，针对不同语言优化文本生成效果。  <br/>5. **高效性能提升**：在盲测数据集上实现11.76%的CER/WER，超越官方基线8.41个百分点，且不依赖额外训练数据。|
|2507.03043v1|[K-Function: Joint Pronunciation Transcription and Feedback for   Evaluating Kids Language Function](http://arxiv.org/abs/2507.03043v1)|总结：  <br/>提出K-Function框架，结合子词转录、客观评分与反馈，显著提升儿童语音识别准确率，实现临床级语言评估。<br/><br/>贡献点：  <br/>1. **引入K-Function框架**：首次整合子词级转录、客观评分系统与可操作反馈机制，解决儿童语言评估中语音识别的挑战。  <br/>2. **提出Kids-WFST核心模型**：融合Wav2Vec2语音编码器与基于音素相似性的Dysfluent-WFST，兼顾儿童语言错误检测与模型可解释性。  <br/>3. **显著性能提升**：在MyST和Multitudes数据集上实现1.39%和8.61%的音素错误率，分别比贪心解码器提升10.47和7.06个百分点。  <br/>4. **赋能语言能力评估**：利用高保真转录训练LLM，全面评估口语技能、里程碑、阅读与理解，与人类专家一致并提供可视化反馈。  <br/>5. **构建诊断-反馈闭环**：实现从准确识别到临床级评估的完整流程，为可扩展、标准化的语言诊断工具奠定基础。|
|2507.02982v1|[We Need Knowledge Distillation for Solving Math Word Problems](http://arxiv.org/abs/2507.02982v1)|总结：  <br/>该研究提出通过压缩BERT嵌入向量并蒸馏小型模型，实现数学能力提升与成本降低，为智能教育系统提供高效、通用的解决方案。<br/><br/>贡献点：  <br/>1. **提出数学模型压缩方案**：通过压缩LLMs的嵌入向量并蒸馏小型模型，显著降低参数量（仅1/12）和计算成本。  <br/>2. **保持高性能与泛化能力**：压缩模型在多种数学任务中表现接近原模型（90%性能），且蒸馏过程不依赖具体任务。  <br/>3. **揭示压缩关键机制**：发现词性信息对数学问题求解至关重要，而非实体识别，为模型压缩提供理论支撑。  <br/>4. **推动智能教育应用**：提升模型效率和成本效益，为智能教学系统及教育领域发展提供实用价值。|
|2507.02904v1|[Enhancing Sports Strategy with Video Analytics and Data Mining:   Assessing the effectiveness of Multimodal LLMs in tennis video analysis](http://arxiv.org/abs/2507.02904v1)|**贡献点：**<br/>1. **评估MLLMs在体育视频分析中的应用**：首次系统评估多模态大语言模型（MLLMs）对网球视频的处理能力，探索其在动作识别和事件理解上的有效性。  <br/>2. **填补事件序列识别的空白**：针对现有网球分析研究中缺乏对rally事件顺序识别的问题，提出模型在这一领域的改进方向。  <br/>3. **动作分类与序列分析**：验证MLLMs在分类网球动作及识别复杂动作序列（如rally过程）中的潜力，为其他体育分析任务提供参考。  <br/>4. **性能优化方法**：探讨通过调整训练策略及结合传统模型提升MLLMs性能的途径，推动多模态模型在运动分析中的实际应用。  <br/><br/>**总结（100字以内）**：  <br/>本研究评估MLLMs在网球视频分析中的应用，聚焦于事件序列识别与动作分类，提出填补研究空白的改进方法，并探索训练策略优化与模型融合路径。|
|2507.02858v1|[Requirements Elicitation Follow-Up Question Generation](http://arxiv.org/abs/2507.02858v1)|总结（100字以内）:  <br/>本研究提出使用GPT-4o构建基于常见错误类型框架的面试问题生成方法，通过实验验证LLM生成的问题在清晰度、相关性和信息量上不逊于人类，且在指导错误类型时表现更优，为需求获取面试提供了实时辅助工具。<br/><br/>贡献点分点列出：<br/>1. **构建框架**: 提出基于常见访谈者错误类型框架，系统化识别面试中可能存在的问题（如领域不熟悉、认知过载等），为LLM生成问题提供结构化依据。<br/>2. **问题生成方法**: 设计双路径生成策略：一种是基于访谈者口语回答直接生成问题，另一种是结合框架引导生成问题，提升问题生成的针对性。<br/>3. **实验验证**: 进行两组受控实验，对比LLM生成问题与人工问题的质量（清晰度、相关性、信息量），并评估框架引导下的LLM生成效果，验证其有效性。<br/>4. **实践价值**: 明确LLM在需求获取场景中的潜力，证明其可实时辅助面试者优化提问策略，提升访谈效率与质量，为语音领域的人机协作提供新思路。|
|2507.02282v1|[Content filtering methods for music recommendation: A review](http://arxiv.org/abs/2507.02282v1)|总结：  <br/>本文综述了音乐推荐系统中协同过滤方法的局限性，提出通过内容过滤缓解偏差，并探讨了歌词分析与音频处理等多维度分类方法的冲突与解决策略。<br/><br/>贡献点：  <br/>1. **识别协同过滤的局限性**：指出音乐领域因用户-歌曲互动稀疏导致协同过滤效果下降的问题。  <br/>2. **强调内容过滤的补充作用**：系统综述内容过滤在缓解协同过滤偏差中的关键角色。  <br/>3. **多方法分类技术分析**：探讨歌词分析（基于LLMs）与音频信号处理等不同技术的适用性。  <br/>4. **冲突与解决思路**：分析跨模态分析方法的潜在矛盾，并提出协调不同方法的途径。|
|2507.00672v1|[Toward Edge General Intelligence with Multiple-Large Language Model   (Multi-LLM): Architecture, Trust, and Orchestration](http://arxiv.org/abs/2507.00672v1)|总结：  <br/>该调研系统总结了在边缘计算中集成多大语言模型（multi-LLMs）的贡献，包括提出多LLM应用方法、揭示模型演进路径、识别关键技术挑战、强调可信系统构建、设计多模态架构及指明未来研究方向。<br/><br/>贡献点：  <br/>1. 提出多大语言模型（multi-LLMs）在边缘计算中应对复杂动态任务的应用方法。  <br/>2. 系统梳理从传统边缘AI到单LLM再到multi-LLM的模型演进路径。  <br/>3. 识别多LLM实现中的关键技术挑战：动态编排、资源调度、跨域知识迁移。  <br/>4. 强调构建可信multi-LLM系统以保障高可靠性与隐私安全。  <br/>5. 设计支持多模态数据处理的multi-LLM架构，整合文本、图像、音频等信息。  <br/>6. 指明未来研究方向：提升资源效率、解决隐私与鲁棒性问题、建立可信治理体系。|
|2506.23930v1|[Leveraging the Potential of Prompt Engineering for Hate Speech Detection   in Low-Resource Languages](http://arxiv.org/abs/2506.23930v1)|**贡献点分点总结：**  <br/>1. **首次提出隐喻提示（Metaphor Prompting）**：针对低资源语言的仇恨言论检测，开发了一种新颖的提示方法，可绕过LLMs内置的安全机制，区别于传统jailbreaking技术。  <br/>2. **系统评估六种提示策略**：在Llama2-7B模型上全面测试零样本提示、拒绝抑制、讨好分类器、多样本提示、角色提示及隐喻提示，对比多种深度学习模型（MLP/CNN/BiGRU）和词嵌入（GloVe/Word2Vec/FastText）的性能。  <br/>3. **跨语言有效性验证**：将隐喻提示方法扩展至其他低资源语言（如印地语）和高资源语言（如英语、德语），验证其普适性和适应性。  <br/>4. **引入环境影响因子（IF）评估**：将F1分数与碳排放、电力消耗、计算时间等环境指标结合，提出对模型可持续性的综合评估框架。  <br/>5. **填补低资源语言研究空白**：聚焦低资源孟加拉语，提出通过LLMs进行仇恨言论检测的解决方案，推动该领域在低资源语言上的应用。|
|2506.23774v1|[Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate   Incidents Management](http://arxiv.org/abs/2506.23774v1)|**贡献点总结（100字以内）**  <br/>提出了结合检索增强提示与人物建模的多代理LLM系统，用于模拟校园仇恨事件场景，提升教师对仇恨言论的理解及干预能力，验证了该系统在培训中的有效性。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **创新系统设计**：开发了一个多代理LLM系统，通过融合检索增强提示与角色建模技术，模拟真实情境下的仇恨事件，突破传统教师培训的时空限制。  <br/>2. **功能整合**：系统具备识别分析仇恨言论模式、预测潜在升级及生成干预策略三大核心功能，为教师提供综合性的教学工具。  <br/>3. **场景多样化**：利用人物建模与代理LLMs结合，构建具有情境多样性的模拟环境，增强教师对复杂社会动态的应对能力。  <br/>4. **实证验证**：通过试点评估，验证了该系统能有效提升教师对注释分歧的理解及上下文在仇恨言论中的作用认知，优化实际教学策略。|
|2506.23714v1|[Towards an Automated Multimodal Approach for Video Summarization:   Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](http://arxiv.org/abs/2506.23714v1)|**总结（100字以内）**：  <br/>该论文提出基于行为感知的多模态视频摘要框架，融合文本、音频与视觉线索生成时间对齐摘要，识别跨模态强调的"bonus words"以提升语义和情感相关性，并通过实验验证在文本与视频评估指标上均显著优于传统方法。<br/><br/>**贡献点**：  <br/>1. **多模态整合**：首次将文本、音频、视觉信息统一建模，生成时间对齐的视频摘要，突破传统单模态方法限制。  <br/>2. **行为感知机制**：通过提取声学特征、文本关键词和视觉指标，识别语义与情感重要的行为片段。  <br/>3. **Bonus Words创新**：提出"bonus words"概念，捕获跨模态强调的关键词，增强摘要的语义相关性与表达清晰度。  <br/>4. **对比实验验证**：在伪真实数据（pGT）上对比传统方法（如Edmundson），显着提升ROUGE-1、BERTScore及视频F1-Score（分别提升31.5%、3.8%和23%）。  <br/>5. **应用价值**：验证多模态融合在教育、职业等场景下提升视频摘要质量的潜力，为行为信息驱动的摘要生成提供新思路。|
|2506.23094v1|[TOMI: Transforming and Organizing Music Ideas for Multi-Track   Compositions with Full-Song Structure](http://arxiv.org/abs/2506.23094v1)|**总结（100字以内）:**  <br/>本研究提出TOMI方法，结合指令微调LLM实现多轨音乐生成，采用四维表示并集成REAPER，提升音乐质量和结构一致性。<br/><br/>**贡献点分点列出:**  <br/>1. **提出TOMI框架**：首次将“概念层次”引入音乐生成，系统性地建模音乐创意的生成、转化与组织过程，形成完整多轨电子音乐结构。  <br/>2. **融合指令微调的LLM**：设计基于大型语言模型（LLM）的指令微调方法，提升音乐生成过程中的逻辑与结构化能力。  <br/>3. **四维稀疏表示方法**：创新性地使用四维空间（clip、section、track、transformation）表征多轨音乐创作流程，增强对复杂音乐结构的建模精度。  <br/>4. **人机交互集成**：将TOMI模型与REAPER数字音频工作站结合，实现交互式人机协同音乐创作，拓展AI在音乐领域的应用边界。  <br/>5. **实验验证优势**：通过对比实验证明，TOMI方法在音乐质量、结构连贯性及整体协调性上均优于现有基线模型。|
|2506.23049v1|[AURA: Agent for Understanding, Reasoning, and Automated Tool Use in   Voice-Driven Tasks](http://arxiv.org/abs/2506.23049v1)|**贡献点：**  <br/>1. **首创全语音多轮对话系统**：提出AURA，首个开源支持语音到语音多轮对话、集成工具调用和自主推理的系统。  <br/>2. **开放权重模块化架构**：结合开放权重的ASR、TTS和LLM，采用级联流水线设计，支持多种工具（日历、搜索、邮件等）。  <br/>3. **自然语言驱动工具扩展**：通过自然语言提示和动作类别实现工具的灵活集成，提升系统可定制性。  <br/>4. **高性能基准测试表现**：在VoiceBench上OpenBookQA得分92.75%（超开源系统，接近GPT-4o），AlpacaEval得分4.39（与开源系统相当）。  <br/>5. **实用场景验证**：人类评估显示，AURA在复杂多轮语音任务中达到90%的成功率。  <br/><br/>**总结（100字以内）：**  <br/>AURA是首个支持多轮语音对话与工具集成的开源系统，结合开放权重模块化架构，实现高效任务处理，性能超越现有开源模型并接近GPT-4o，为语音交互与智能应用融合提供了新范式。|
|2506.22679v1|[Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions](http://arxiv.org/abs/2506.22679v1)|总结: <br/>本研究对比了不同LLM架构在团队对话微行为检测中的表现，发现解码器-only模型在指令微调后对低频微行为有更好识别效果，为太空任务等高风险场景的沟通分析提供新方法。<br/><br/>贡献点: <br/>1. 提出基于模拟太空任务对话转录的微行为检测框架，验证LLM在非语音数据场景下的应用潜力<br/>2. 对比分析编码器-only（RoBERTa/DistilBERT）与解码器-only（Llama-3.1）LLM在微行为识别任务中的表现差异<br/>3. 系统评估零样本分类、基础微调、paraphrase-augmented微调等不同微调策略的有效性<br/>4. 揭示解码器-only模型在指令微调后可实现68%的二分类F1-score，显著优于编码器-only模型<br/>5. 为高风险环境团队沟通分析及训练干预技术发展提供实证依据|
|2506.22554v2|[Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale   Dataset](http://arxiv.org/abs/2506.22554v2)|总结：该论文提出新的Seamless Interaction数据集及配套模型，实现对双人互动行为的生成与理解，在虚拟代理和人机交互领域取得突破。<br/><br/>贡献点：<br/>1. 构建首个大规模(dyadic)互动数据集（4000小时/4000参与者），涵盖多场景非言语行为数据<br/>2. 开发可输入语音与视觉行为的双人动作生成模型，集成LLM+2D/3D渲染技术<br/>3. 提出具有情绪控制和表达性调节能力的可控制模型变体<br/>4. 建立评估双人互动模型质量的方法体系，推动更自然的人机交互系统发展|
|2506.22554v1|[Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale   Dataset](http://arxiv.org/abs/2506.22554v1)|总结：  <br/>本研究提出首个大规模双人交互数据集，开发多模态生成模型及可控变体，建立评估体系，推动虚拟代理与人机交互技术发展。<br/><br/>贡献点：  <br/>1. **数据集构建**：创建Seamless Interaction数据集，包含超过4,000小时多场景真实面对面互动视频（4,000名参与者）。  <br/>2. **模型生成**：设计可生成与语音对齐的双人动作手势和面部表情的多模态模型，支持输入对话者的行为数据。  <br/>3. **可控性增强**：开发可调节情感响应、表达强度及生成语义相关手势的模型变体，提升交互灵活性。  <br/>4. **评估方法**：提出针对双人运动模型的质量评估方法，促进更自然、高效的AI交互系统实现。|
|2506.21875v2|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v2)|总结：  <br/>该研究构建了首个针对语音场景的综合评估基准，通过引入多样化的说话者和声学条件，以及语音特有的现象，结合查询导向的评估方法，揭示了不同语音模型在实际应用中的性能差异，为语音模型的优化和评估提供了新思路。<br/><br/>贡献点：  <br/>1. **提出首个专门针对语音场景的端到端评估基准**：填补现有语音LLM缺乏专用测试体系的空白。  <br/>2. **构建多样化语音语料库**：包含多变的说话者属性、声学条件及语音独特现象（如韵律、同音字、口吃）。  <br/>3. **设计查询导向的评估方法**：通过定制化评价清单与提示增强自动评分的准确性与场景适配性。  <br/>4. **揭示语音模型在不同场景的性能差异**：系统分析主流模型表现，为改进模型提供实证依据。  <br/>5. **推动语音模型的实用化开发**：为优化音频LLM的用户体验提供数据与评估框架支持。|
|2506.21864v2|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v2)|**摘要贡献点总结：**  <br/>本研究提出DeepTalk框架，通过自适应模态专家学习和MoE架构解决语音-文本数据不足导致的性能退化问题，实现与原始LLM相近的语音生成能力及低延迟交互，为语音领域模型提供了高效、稳定的技术方案。  <br/><br/>**分点贡献：**  <br/>1. **提出DeepTalk框架**：采用Mixture of Experts (MoE) 架构设计自适应模态专家学习机制，通过动态区分模态负载优化模型性能。  <br/>2. **解决数据不足问题**：针对语音-文本配对数据稀缺的挑战，采用单模态专项训练与多模态协作训练结合的方法，降低灾难性遗忘风险。  <br/>3. **性能提升**：实验表明DeepTalk仅导致5.5%的性能下降（相对于原始LLM），显著优于现有Native MLLMs（如GLM-4-Voice）的平均20%降损，且与模块化模型表现相当。  <br/>4. **实时交互优化**：端到端对话延迟控制在0.5秒内，确保流畅的语音交互体验。  <br/>5. **开源代码与模型**：公开项目代码及模型，便于研究复现与应用。|
|2506.21864v1|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v1)|总结：  <br/>提出DeepTalk框架，通过自适应多模态专家学习与MoE架构有效缓解语音-文本数据不足导致的模型性能下降，实现与原始LLM相近的效率并保持低延迟对话体验。<br/><br/>贡献点：  <br/>1. **提出新的框架**：基于MoE架构设计DeepTalk，实现自适应多模态专家学习，解决传统Native MLLMs因数据不足导致的灾难性遗忘问题。  <br/>2. **分阶段训练策略**：通过模态负载区分专家，结合单模态特训与多模态协作训练，优化模型对语音和文本的联合处理能力。  <br/>3. **性能提升**：相比传统Native MLLMs（如GLM-4-Voice）性能下降仅5.5%，显著优于平均20%的水平，且与模块化方法性能相当。  <br/>4. **低延迟交互**：端到端对话延迟控制在0.5秒以内，提升语音交互的实时性与流畅度。  <br/>5. **开源实现**：提供代码和模型，促进方法复现与行业应用。|
|2506.20666v2|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v2)|**贡献点：**  <br/>1. **提出跨模型评估框架**：首次将认知科学中的"认知模型"理论应用于LLMs，系统评估不同模型在价值观权衡（如信息性与社会性效用）中的表现。  <br/>2. **揭示推理类型差异**：发现推理模型（如黑箱模型）更注重信息性效用，而开源模型（如RL微调）在数学推理方面表现更强。  <br/>3. **量化训练动态影响**：首次量化LLMs训练初期的效用值显著变化，并证明基础模型和预训练数据的选择具有长期影响，区别于反馈数据与对齐方法的作用。  <br/>4. **方法普适性验证**：提出的方法可适应LLMs快速演进的领域，为假设其他高阶行为、优化训练策略及控制价值权衡提供理论依据。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过认知模型分析LLMs的价值权衡机制，揭示不同模型类型在推理效用、训练动态中的差异，提出普适性评估方法，为优化语言模型行为提供新视角。|
|2506.20666v1|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v1)|总结：  <br/>本研究提出利用认知模型分析LLMs在价值权衡中的表现，揭示了不同训练策略下效用分配差异，为理解模型行为、优化训练机制及控制价值冲突提供新视角。<br/><br/>贡献点：  <br/>1. **引入认知模型框架**：首次将认知科学中的价值权衡模型（如礼貌语言模型）应用于LLMs，系统分析其在社交情境中处理矛盾目标的能力。  <br/>2. **双维度模型评估**：针对前沿黑盒模型的推理"努力"程度与开源模型的RL对齐动态，提出统一的评估方法，探索不同场景下的效用函数分配机制。  <br/>3. **发现效用偏向规律**：揭示推理模型更注重信息效用而非社会效用，同时指出开源模型在数学推理上的优势，为模型能力差异提供实证依据。  <br/>4. **揭示训练动态影响**：证明模型训练早期存在显著的效用值变化，且基础模型和预训练数据选择对效用分配具有持续影响，优于反馈数据或对齐方法的作用。  <br/>5. **方法泛化与应用价值**：提出的方法可适配LLMs快速演进的生态，为推测高阶行为、设计训练策略及平衡模型价值冲突提供理论支持和实践指导。|
|2506.19835v1|[MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration](http://arxiv.org/abs/2506.19835v1)|总结：  <br/>该论文提出一种模块化多智能体框架MAM，通过角色分工优化多模态医疗诊断流程，显著提升性能，代码公开可用。<br/><br/>贡献点：  <br/>1. **框架设计**：提出模块化多智能体系统（MAM），将医疗诊断流程分解为专门角色（全科医生、专科团队、放射科医生等），提升协作效率。  <br/>2. **角色分工**：通过角色分配（如 Director 统筹、 Specialist 团队细化诊断）增强模型的诊断能力和灵活性。  <br/>3. **高效更新机制**：实现知识更新成本降低，有效整合现有医疗 LLMs 和知识库。  <br/>4. **多模态性能提升**：在包含文本、图像、音频、视频的多模态医疗数据集上，MAM 表现优于模态专用模型（性能提升 18%-365%）。  <br/>5. **开源实现**：代码公开，便利后续研究和应用。|
|2506.19732v1|[Who Does What in Deep Learning? Multidimensional Game-Theoretic   Attribution of Function of Neural Units](http://arxiv.org/abs/2506.19732v1)|总结（100字以内）:  <br/>本文提出MSA框架，通过博弈论方法量化神经网络单元对高维输出的贡献，适用于不同规模模型，揭示了正则化、语言专家分布及GAN的倒置生成层级，为深度模型的解释、编辑与压缩提供新工具。<br/><br/>贡献点：<br/>1. **提出模型无关的解释框架**：开发Multiperturbation Shapley-value Analysis (MSA)，首次将Shapley值理论扩展至量化神经网络单元对高维输出（如语音、图像、文本）的贡献。<br/>2. **实现输出维度对齐的贡献分析**：生成Shapley Modes，提供与模型输出（像素、token、logits）完全一致维度的单元贡献地图。<br/>3. **跨模型尺度验证有效性**：将方法应用于多层感知机（MLP）、大型语言模型（Mixtral-8x7B）和生成对抗网络（GAN），展示其广泛适用性。<br/>4. **揭示神经网络结构特征**：通过实验证明正则化集中计算资源、语言模型中存在语言特定专家、GAN中的生成层级存在倒置现象。<br/>5. **拓展应用潜力**：为深度模型的**解释、编辑与压缩**提供理论支持和实用工具，推动模型可理解性研究。|
|2506.19502v2|[MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications](http://arxiv.org/abs/2506.19502v2)|总结：  <br/>本文提出MATE多模态无障碍多智能体系统和ModCon-Task-Identifier任务识别模型，通过模态转换和灵活架构解决用户无障碍需求，提升医疗等场景下的交互体验，并验证了其性能优势。<br/><br/>贡献点：  <br/>1. 提出MATE系统：首个基于用户需求的多模态无障碍多智能体系统，支持图像、音频等跨模态转换。  <br/>2. 模态适配框架：动态识别用户障碍类型（如视觉/听觉缺陷），将数据转换为可理解格式。  <br/>3. 灵活模型支持：兼容LLM API调用与自定义ML分类器，适配不同硬件和应用场景。  <br/>4. 本地隐私保障：系统运行于本地环境，确保敏感信息的安全性与隐私性。  <br/>5. 机构技术集成：支持与数字医疗等机构系统实时对接，提供无缝用户辅助服务。  <br/>6. 任务识别模型：开发ModCon-Task-Identifier，精准提取模态转换任务，实验验证其优于其他LLM和统计模型。|
|2506.19502v1|[MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications](http://arxiv.org/abs/2506.19502v1)|总结：  <br/>本文提出MATE多模态无障碍MAS系统，支持灵活模态转换和本地部署，提升残疾人数字交互体验，并引入精准的ModCon-Task-Identifier模型，实现跨领域应用与隐私保护。<br/><br/>贡献点：  <br/>1. **开发开源多模态MAS系统**：MATE支持模态转换（如图像转音频描述），解决传统系统封闭设计导致的定制化不足问题。  <br/>2. **跨领域灵活性**：兼容多种模型（LLM API、自定义ML分类器）与硬件，适应不同行业（如医疗）和用户需求。  <br/>3. **本地运行保障隐私**：确保敏感信息在本地处理，保密性强，适用于机构技术集成（如医疗系统）。  <br/>4. **任务识别模型创新**：提出ModCon-Task-Identifier，可精准提取用户输入的模态转换任务，实验表现优于其他模型。  <br/>5. **开源代码与数据**：提供公开资源，促进研究复现与应用拓展。|
|2506.19073v2|[MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral   Reasoning of LLMs through Hate Speech Multi-hop Explanations](http://arxiv.org/abs/2506.19073v2)|**总结（100字以内）：**  <br/>本文提出MFTCXplain，首个基于道德基础理论的多语言基准数据集，覆盖四种语言，揭示当前LLMs在跨文化道德推理任务中的局限性，推动更公平、可解释的道德评估研究。<br/><br/>**贡献点分点：**  <br/>1. **构建多语言基准**：设计MFTCXplain，涵盖葡萄牙语、意大利语、波斯语和英语，解决当前道德评估基准语言单一、跨文化适配不足的问题。  <br/>2. **引入理论框架**：基于道德基础理论（MFT）构建数据集，提供道德分类及语篇级理由标注，提升评估的解释性与理论关联性。  <br/>3. **揭示性能差异**：实验表明LLMs在仇恨检测（F1=0.836）表现优异，但道德情感预测能力较弱（F1<0.35），强调实际应用中的关键挑战。  <br/>4. **突出语言公平性**：发现非主流语言中理由对齐受限，凸显多语言评估对文化多样性的研究价值。|
|2506.19073v1|[MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral   Reasoning of LLMs through Hate Speech Multi-hop Explanation](http://arxiv.org/abs/2506.19073v1)|总结：  <br/>提出多语言道德推理评估数据集MFTCXplain，揭示当前LLMs在道德情感预测和跨文化解释上的局限性，推动更透明、文化包容的道德评估研究。<br/><br/>贡献点：  <br/>1. **多语言道德推理基准**：构建包含葡萄牙语、意大利语、波斯语和英语的3,000条推文数据集，支持跨文化道德能力评估。  <br/>2. **细粒度标注体系**：引入二元仇恨言论标签、道德类别及文本片段级解释，提升评估的透明度与可解释性。  <br/>3. **理论驱动设计**：基于道德基础理论（MFT）设计任务，明确道德推理的评估框架。  <br/>4. **揭示模型局限性**：通过实证发现LLMs在道德情感预测（F1<0.35）和非主流语言解释对齐上表现显著不足，强调其内化人类道德推理的能力有限。|
|2506.18576v1|[A Modular Taxonomy for Hate Speech Definitions and Its Impact on   Zero-Shot LLM Classification Performance](http://arxiv.org/abs/2506.18576v1)|**贡献点总结（100字以内）**  <br/>本文提出仇恨言论定义的分类体系，分析其14个核心要素，并通过零样本实验评估不同定义对LLM性能的影响，揭示定义差异对模型效果的非一致性影响。<br/><br/>**分点贡献：**  <br/>1. **理论贡献**  <br/>   - 构建了一个包含14个概念元素的仇恨言论定义分类体系（Taxonomy），系统整理了文献中对仇恨言论的多维度表述（如目标类型、后果等），解决其定义模糊问题。  <br/><br/>2. **实验贡献**  <br/>   - 首次在三种仇恨言论数据集（合成、人工参与、真实场景）上，对三类大语言模型（LLMs）进行基于不同定义的零样本性能评估，发现定义的特异性程度对模型效果存在显著但非一致的影响。|
|2506.18274v1|[Leveraging Large Language Models for Information Verification -- an   Engineering Approach](http://arxiv.org/abs/2506.18274v1)|总结：  <br/>提出基于GPT-4o的自动化多媒体新闻源验证方法，整合多模态数据处理与交叉验证技术，减少人工干预。<br/><br/>贡献点：  <br/>1. **多模态数据整合**：首次将图像、视频、音频等多类型数据纳入新闻源验证流程，提升信息全面性。  <br/>2. **LLM为核心架构**：采用GPT-4o作为验证管道的骨干模型，实现端到端自动化处理。  <br/>3. **流程自动化**：通过提示工程完全自动化元数据生成、数据分帧、特征筛选及交叉验证等步骤。  <br/>4. **关键帧筛选机制**：开发基于重要性排序的帧选择策略，优化视频分析效率。  <br/>5. **轻量人工干预**：仅需人类参与最终结果验证，显著降低人工成本与操作复杂性。|
|2506.17715v1|[Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource   Medieval Romance Languages](http://arxiv.org/abs/2506.17715v1)|总结：  <br/>本研究系统分析中世纪罗曼语词性标注的挑战，评估多种技术方法，揭示LLMs在历史语言处理上的局限性，并探索有效解决方案，涵盖多领域与多语言文本。<br/><br/>贡献点分点：  <br/>1. **系统性研究历史语言变体挑战**：首次对中世纪奥克语、西班牙语、法语文本的词性标注进行多维度分析，揭示历时性语言演变、拼写非标准化和数据稀缺等问题对模型性能的影响。  <br/>2. **多技术方法对比实验**：系统评估微调、提示工程、模型架构、解码策略及跨语言迁移学习等技术对词性标注准确率的影响，提供方法选择的实证依据。  <br/>3. **多领域与多语言覆盖**：基于包含宗教、圣徒传、医学、饮食等领域的跨语言语料库（Medieval Occitan、Spanish、French），验证技术的泛化能力与适应性。  <br/>4. **提出针对性解决方案**：发现特定技术（如跨语言迁移学习）在低资源历史语言场景中的有效性，为优化历史语言处理提供新思路。|
|2506.17410v1|[Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A   Feasibility Study](http://arxiv.org/abs/2506.17410v1)|**贡献点总结：**  <br/>1. 提出利用生成式AI分析真实数学辅导的可行性与可扩展性方法；  <br/>2. 识别并评估"有效表扬"和"纠正学生数学错误"两项关键辅导行为；  <br/>3. 验证多模型（GPT-4系列、Gemini-1.5-pro、LearnLM）在辅导行为检测与评估中的可靠性；  <br/>4. 设计成本效益高的提示策略以提升AI在教育评估中的实用性；  <br/>5. 促进LLM在AI支持学习研究中的可复现性与实际应用价值。<br/><br/>**研究摘要总结（100字以内）：**  <br/>开发生成式AI方法分析真实数学辅导行为，验证多模型在检测表扬与纠错效果上的可靠性，提出高效提示策略，推动AI在教育评估中的可扩展应用与研究创新。|
|2506.16575v1|[Advancing Harmful Content Detection in Organizational Research:   Integrating Large Language Models with Elo Rating System](http://arxiv.org/abs/2506.16575v1)|**贡献点总结（100字内）：**  <br/>提出基于Elo评分的改进方法，提升LLM在有害内容分析中的性能，通过两个数据集验证其在准确性、精确率与F1分数上的优势，减少误报并增强可扩展性，支持职场骚扰检测与构建安全工作环境等组织应用。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：设计Elo评分机制，优化LLM对有害内容（如微歧视、仇恨言论）的分析能力，解决传统系统过度审查的问题。  <br/>2. **数据集验证**：在微歧视检测和仇恨言论分析的两个数据集上验证方法有效性，证明其优于传统提示技术与常规机器学习模型。  <br/>3. **性能提升**：显著提高关键指标（准确率、精确率、F1分数）表现，减少误报并增强结果可靠性。  <br/>4. **可扩展性**：支持大规模数据集处理，提升实际应用的可行性。  <br/>5. **应用价值**：推动组织研究中职场骚扰识别、毒性沟通评估及安全环境构建等实际场景的应用。|
|2506.16528v1|[Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility   Metrics Using Phonetic, Semantic, and NLI Approaches](http://arxiv.org/abs/2506.16528v1)|**贡献点：**  <br/>1. **提出新的ASR评估指标**：整合自然语言推理（NLI）评分、语义相似性和语音相似性，克服传统指标（如WER、CER）对语音可懂度的不足。  <br/>2. **验证有效性**：在Speech Accessibility Project数据集上实现与人类判断的0.890高相关性，证明该指标优于现有方法。  <br/>3. **强调评估方向**：指出应优先关注语音内容的可懂度而非单纯依赖错误率，为语音识别系统优化提供理论依据。  <br/><br/>**总结：**  <br/>本研究提出一种结合NLI、语义和语音相似性的新型ASR评估指标，显著提升与人类判断的相关性，推动语音可懂度评估从错误率向内容理解转变。|
|2506.16008v1|[ChatAR: Conversation Support using Large Language Model and Augmented   Reality](http://arxiv.org/abs/2506.16008v1)|**贡献点总结：**  <br/>本研究提出了一种结合HMD-AR与LLMs的实时对话支持系统，通过关键词识别、信息生成与可视化增强交流效率；创新性地引入眼动控制策略，避免暴露用户阅读行为；实验验证了该系统在提升对话平衡性与互动兴奋度方面的有效性。  <br/><br/>**分点贡献：**  <br/>1. **系统设计**：构建集成HMD-AR与LLMs的实时对话辅助系统，实现关键词识别、信息生成及动态可视化呈现。  <br/>2. **眼动优化方法**：开发基于自然眼动模式的信息展示策略，降低用户阅读行为被对话方察觉的风险。  <br/>3. **实验验证**：通过两组对照实验，证实所提方法能提升对话平衡性与用户感知的交流兴奋度。|
|2506.14973v1|[Thinking in Directivity: Speech Large Language Model for Multi-Talker   Directional Speech Recognition](http://arxiv.org/abs/2506.14973v1)|**贡献点总结**  <br/>（100字以内）：  <br/>提出directional-SpeechLlama模型，结合智能眼镜麦克风阵列实现定向语音识别、声源定位和旁瓣干扰抑制，创新性引入S-DOT和CDDA技术，有效提升空间音频理解能力。  <br/><br/>**分点贡献**  <br/>1. **首个结合麦克风阵列的定向Speech LLM**：  <br/>   利用智能眼镜的多通道麦克风阵列，首次实现针对声源方向的语音识别、定位及旁瓣干扰抑制，突破传统Speech LLM对空间信息处理的局限。<br/><br/>2. **创新性训练方法S-DOT**：  <br/>   提出串联方向输出训练（Serialized Directional Output Training），通过结构化方向信息增强模型对文本与空间音频关系的建模能力。<br/><br/>3. **对比方向数据增强技术CDDA**：  <br/>   引入对比方向数据增强（Contrastive Direction Data Augmentation），通过对比学习策略提升模型在复杂声学环境下的鲁棒性与泛化能力。<br/><br/>4. **实验验证性能优势**：  <br/>   在语音识别和声源定位任务中，模型表现出显著性能提升，证明了文本线索与空间音频关系建模的有效性。|
|2506.14903v1|[DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct   Preference Optimization](http://arxiv.org/abs/2506.14903v1)|总结（100字以内）：  <br/>本文提出DPO-Kernels，通过混合损失、核化表示和分歧选择三个方向提升T2I模型的对齐性能，并构建首个包含社会偏见数据的DETONATE基准，引入AQI指标评估对齐质量，验证HT-SR机制的有效性，公开数据与代码。<br/><br/>贡献点分点：  <br/>1. **DPO-Kernels框架**：首次将DPO思想扩展至T2I系统，提出混合损失（结合嵌入与概率目标）、核化表示（RBF、多项式、小波核）和分歧选择（Wasserstein与Rényi分歧）三重改进，提升生成内容的对齐性与鲁棒性。  <br/>2. **DETONATE基准**：构建首个大规模T2I对齐基准，包含10万对精心筛选的图像对，系统性涵盖种族、性别、残疾三种社会偏见类别，用于评估模型公平性与安全性的隐性漏洞。  <br/>3. **AQI指标**：提出基于几何度量的对齐质量指数（Alignment Quality Index），量化潜在空间中安全/危险图像激活的分离程度，揭示T2I模型的隐藏偏见问题。  <br/>4. **HT-SR机制**：通过实验证明DPO-Kernels结合重尾自正则化（Heavy-Tailed Self-Regularization）可维持强泛化边界，并公开完整代码与数据集以促进研究复现。|
|2506.14028v2|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v2)|总结：  <br/>提出首个多语言多模态金融基准MultiFinBen，包含新颖的跨语言与OCR嵌入任务，设计动态难度选择机制，并揭示现有LLMs在复杂金融场景下的局限性。<br/><br/>贡献点：  <br/>1. **构建首个全球化金融多模态基准**：MultiFinBen是首个支持多语言（中、英、西等）和多模态（文本、视觉、音频）的金融领域基准，全面评估LLMs的跨模态与跨语言能力。  <br/>2. **引入双跨任务**：设计PolyFiQA-Easy/PolyFiQA-Expert（多语言复杂推理任务）和EnglishOCR/SpanishOCR（OCR嵌入的金融问答任务），填补金融领域多语言与多模态任务的空白。  <br/>3. **动态难度筛选机制**：提出基于难度感知的样本选择方法，而非简单合并现有数据集，确保基准数据集的平衡性与有效性。  <br/>4. **揭示模型局限性**：通过评估22个SOTA模型，发现即便具备多模态能力的LLMs仍难以处理金融领域的复杂跨语言和跨模态任务。  <br/>5. **开放数据促进研究**：公开MultiFinBen数据集，推动金融NLP和语音应用研究的透明性、可复现性与全球化发展。|
|2506.14028v1|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v1)|总结：  <br/>本文提出首个多语言多模态金融基准MultiFinBen，包含新任务与难度评估机制，为金融领域模型评估提供全面且挑战性的测试平台。<br/><br/>贡献点：  <br/>1. **首创多语言多模态基准框架**：构建首个覆盖文本、视觉、音频三模态及多语言场景的金融领域基准MultiFinBen，填补现有单语单模态研究的不足。  <br/>2. **引入双新颖任务**：  <br/>   - *PolyFiQA*（易/专家级）：首次提出多语言金融复杂推理任务，支持混合语言输入；  <br/>   - *EnglishOCR/SpanishOCR*：首次设计OCR嵌入的金融问答任务，要求模型从视觉文本中提取与推理信息。  <br/>3. **动态难度选择机制**：提出基于模型能力的难度自适应筛选方法，确保基准的挑战性与公平性。  <br/>4. **精炼平衡数据集**：通过精细化数据筛选而非简单数据集拼接，构建紧凑且跨领域平衡的测试集合。  <br/>5. **揭示模型能力瓶颈**：实验证明主流模型在多模态与跨语言金融任务中存在显著性能局限，为后续研究提供方向。|
|2506.13894v1|[EmoNews: A Spoken Dialogue System for Expressive News Conversations](http://arxiv.org/abs/2506.13894v1)|**总结**  <br/>该论文提出了一种基于上下文情感调节的新闻对话语音系统，结合LLM与PromptTTS技术，设计新评估指标，并开源代码。<br/><br/>**贡献点**  <br/>1. **开发整合情感调控的SDS**：首次将情感分析与任务导向的语音对话系统结合，通过上下文线索生成更具同理心的新闻对话。  <br/>2. **创新情感语音合成技术**：提出基于PromptTTS的上下文匹配情感语音生成方法，提升对话的情感适配性。  <br/>3. **构建主观评估体系**：设计专门用于情绪调控的主观评价量表，填补社会目标评估的空白，并验证系统效果。  <br/>4. **开源实现与可复现性**：提供完整代码库，支持相关研究的复现与扩展。|
|2506.13252v1|[Vector Ontologies as an LLM world view extraction method](http://arxiv.org/abs/2506.13252v1)|总结：  <br/>首次提出并验证向量本体方法，构建音乐领域的高维几何框架，并证明LLM内部可解析的结构化音乐知识与真实数据的对齐性及提示语相关性。<br/><br/>贡献点：  <br/>1. **首个实证验证**：首次通过实验验证向量本体框架（vector ontologies）可将LLM的高维神经表示转化为可解释的几何结构。  <br/>2. **构建音乐领域向量本体**：基于Spotify音频特征构建8维音乐流派向量本体，提供具体领域的应用案例。  <br/>3. **内部分布一致性分析**：证明LLM的音乐世界模型在不同语言提示下仍保持高空间一致性，且与真实音频特征分布高度对齐。  <br/>4. **提示语与空间变化的关联**：揭示提示语措辞直接影响LLM推理的向量本体空间位置，证明方法的可控性和可验证性。  <br/>5. **方法应用前景**：提出向量本体作为提取和分析LLM内部结构化知识的有效工具，推动可解释AI在语音领域的应用。|
|2506.12936v1|[CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team   Reflection in Action During Clinical Operation](http://arxiv.org/abs/2506.12936v1)|**总结（100字以内）：**  <br/>本研究提出CliniDial数据集，包含多模态临床操作数据，通过实验揭示现有大模型在处理真实临床场景中面临的挑战，推动相关算法发展，并开源代码促进研究。<br/><br/>**贡献点：**  <br/>1. **构建多模态临床操作数据集**：首次整合手术模拟中的音频、转录、生理信号及双视角视频，涵盖标签不平衡、自然交互等复杂特性。  <br/>2. **提出行为代码标注框架**：基于现有方法对团队协作过程进行结构化标注，为分析临床对话与行为模式提供标准化工具。  <br/>3. **评估LLMs在临床场景中的表现**：系统测试现有大模型对CliniDial数据的处理能力，揭示其在真实临床任务中的局限性。  <br/>4. **开源数据与代码**：推动数据共享，促进语音与多模态技术在医疗领域的研究与应用。|
|2506.12744v1|[Rethinking Hate Speech Detection on Social Media: Can LLMs Replace   Traditional Models?](http://arxiv.org/abs/2506.12744v1)|总结（100字以内）:  <br/>本文提出IndoHateMix数据集，验证了大语言模型在处理多语言、混合语言仇恨言论检测中的优势，挑战传统BERT模型的主导地位，并探讨未来研究应侧重模型优化还是数据增强。<br/><br/>贡献点分点列出：<br/>1. **构建多语言仇恨言论数据集**：提出IndoHateMix，首次系统性收录印度语境下的印地语-英语混合语言（code-mixing）和转写（transliteration）文本，填补了该领域的高质量标注数据空白。<br/>2. **验证大语言模型性能优势**：实验证明LLMs（如LLaMA-3.1）在少量数据微调下仍显著优于专门训练的BERT模型，展现更强的泛化能力与跨语言适应性。<br/>3. **揭示模型发展新方向**：提出未来研究需在模型优化与数据多样性之间权衡的思考，引发对通用LLMs替代领域专用模型的讨论。<br/>4. **推动复杂场景下的NLP应用**：为评估多语言环境下仇恨言论检测的鲁棒性提供现实基准，促进应对非正式、文化嵌入性语言的NLP技术发展。|
|2506.12537v1|[Speech-Language Models with Decoupled Tokenizers and Multi-Token   Prediction](http://arxiv.org/abs/2506.12537v1)|总结：  <br/>本文提出解耦分词器设计、多token预测机制及speaker-aware生成范式，优化语音-文本跨模态对齐与生成效率，建立RoleTriviaQA基准测试，显著提升语音语言模型的性能与说话人一致性。<br/><br/>贡献点：  <br/>1. **系统分析SLM关键组件**：首次系统研究语音分词器、语音头、说话人建模等组件对LLM-centric语音模型的影响。  <br/>2. **解耦分词器设计**：提出耦合、半解耦、全解耦分词器对比实验，证明解耦分词可显著提升跨模态对齐与语音合成质量。  <br/>3. **多token预测机制**：引入MTP解决语音与文本信息密度差异，实现12倍解码速度提升与3.01的词错误率降低。  <br/>4. **Speaker-aware生成范式**：构建RoleTriviaQA大型基准测试，涵盖多样说话人身份，提升知识理解与说话人一致性。|
|2506.12502v1|[Towards Fairness Assessment of Dutch Hate Speech Detection](http://arxiv.org/abs/2506.12502v1)|**贡献点：**  <br/>1. **构建荷兰语社会群体术语列表**：首次系统梳理反映荷兰社会背景的歧视性社会群体术语，为后续研究提供基础数据。  <br/>2. **生成反事实数据**：利用大语言模型（LLMs）和MGS、SLL策略构建荷兰语仇恨言论的反事实数据集，解决语言特定挑战。  <br/>3. **微调模型提升检测性能**：通过反事实数据微调Transformer模型，验证其在荷兰语仇恨言论检测任务中的有效性。  <br/>4. **提出综合公平性评估框架**：引入CTF与群体公平性指标（equality of odds、demographic parity），量化模型在反事实公平性与群体层面的公平表现。  <br/><br/>**总结：**  <br/>该研究聚焦荷兰语仇恨言论检测，提出反事实公平性评估方法，构建语言特定数据集，验证模型性能，填补文献空白并提供改进建议。|
|2506.11842v2|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v2)|**贡献点总结（100字以内）：**  <br/>提出PACE-ADS框架，通过多代理协作实现自动驾驶系统的人机双向通信，提升舒适度、安全性和个性化体验，可扩展集成于现有平台，弥补技术自主与人本需求的鸿沟。<br/><br/>**分点贡献：**  <br/>1. **提出人机中心自主框架**：PACE-ADS通过整合心理学与认知模型，解决自动驾驶车辆缺乏双向人机交互的问题，提升个性化体验和应对复杂场景的能力。  <br/>2. **三代理协同机制**：Driver Agent处理外部环境，Psychologist Agent解析心理信号（如EEG、心率）与语音指令，Coordinator Agent整合信息生成高阶决策，增强系统响应性。  <br/>3. **分层架构设计**：在语义规划层部署，保留低级控制权给原系统，确保兼容性与灵活性，避免对现有模块的重构。  <br/>4. **多场景验证**：通过闭环模拟测试，在复杂交通场景（如交叉路口、行人交互）中验证框架的有效性，展示提升的舒适度与安全性。  <br/>5. **最小化集成改造**：框架可无缝集成至现有自动驾驶平台，减少对原有系统的调整，提高实际部署的可行性。|
|2506.11842v1|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v1)|总结：  <br/>本文提出PACE-ADS框架，通过整合语音等认知指令与心理状态感知，实现人机协同的自动驾驶系统，提升舒适度与安全性，展示大语言模型在人车交互中的应用潜力。<br/><br/>贡献点：  <br/>1. **提出跨模态人车交互框架**：首次构建融合外部交通环境感知与内部乘客心理/认知状态理解的人机协同自动驾驶系统（PACE-ADS），填补自动化驾驶与人类需求的gap。  <br/>2. **设计语音认知代理模块**：开发专门处理乘客语音指令（如语义理解）与心理信号（如EEG、心率）的Psychologist Agent，实现多模态意图解析。  <br/>3. **实现行为级自主决策**：通过Coordinator Agent整合感知信息，支持自主驾驶逻辑生成与安全恢复策略，无需替代传统模块。  <br/>4. **验证语音引导有效性**：在交通信号、行人、施工区等场景的仿真测试中，证明语音交互可提升驾驶风格适应性与用户舒适度。  <br/>5. **推动LLM应用落地**：探索基于大语言模型（LLM）的框架在自动驾驶领域的人类中心化驱动潜力，为未来人车协作提供新思路。  <br/><br/>（注：原文实际属于自动驾驶领域，但按语音视角提炼了其中与语音指令处理、多模态交互、LLM应用等相关内容作为贡献）|
|2506.11558v3|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v3)|**贡献点：**  <br/>1. 提出**DaMO**模型，专注于**数据高效**的视频语言理解，特别提升**精确时间推理**能力。  <br/>2. 设计**Temporal-aware Fuseformer**，采用**分层双流架构**，逐步捕捉各模态的时间动态并高效融合视觉与音频信息。  <br/>3. 引入**全局残差机制**，减少空间冗余的同时保留关键语义细节，提升计算效率。  <br/>4. 构建**四阶段渐进式训练框架**，逐步增强模型的**多模态对齐**、**语义 grounding**和**时间推理**能力。  <br/>5. 提供**多数据集增强**，利用LLM生成的**时间相关问答对**，为需时间监督的任务（如时间定位、视频QA）提供标注资源。  <br/><br/>**总结（100字以内）：**  <br/>本文提出数据高效的视频语言模型DaMO，通过Temporal-aware Fuseformer与全局残差机制提升时间推理和多模态理解，构建时间相关数据集，并在多个任务中超越现有方法，为语音领域的时间感知建模提供了新方向。|
|2506.11558v2|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v2)|**贡献点：**<br/>1. **提出Temporal-aware Fuseformer架构**：采用分层双流结构，逐步捕捉多模态的时间动态信息，并有效融合视觉与听觉特征。  <br/>2. **设计全局残差机制**：减少空间冗余的同时保留关键语义信息，提升计算效率与模型性能。  <br/>3. **四阶段渐进训练方法**：分步骤增强多模态对齐、语义接地和时间推理能力，系统性优化模型表现。  <br/>4. **构建时序语义数据集**：通过GPT生成带时序对齐的问答对，扩展现有数据集用于时间监督任务。  <br/>5. **实验证明优越性**：在视频问答与时间定位任务中超越现有方法，尤其在精确时间对齐与推理上表现突出，为高效视频语言建模提供新方向。  <br/><br/>**总结：**  <br/>DaMO通过分层双流与全局残差结构、四阶段训练及GPT增强数据集，在视频语言任务中显著提升时间推理能力，推动数据高效模型发展。|
|2506.11558v1|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v1)|**贡献点总结**（100字以内）:  <br/>提出DaMO数据高效视频语言模型，创新性采用分层双流时序感知架构与全局残差机制，设计四阶段渐进训练方法，构建增强数据集，并在时序任务中实现性能突破。  <br/><br/>**分点贡献**:  <br/>1. **模型设计**：提出DaMO，专为准确时序推理和多模态理解优化，提升数据效率。  <br/>2. **架构创新**：首创Temporal-aware Fuseformer，采用分层双流结构，分别捕捉视频/音频时序动态，有效融合多模态信息。  <br/>3. **效率优化**：引入全局残差机制，减少空间冗余的同时保留关键语义信息。  <br/>4. **训练方法**：提出四阶段渐进式训练范式，逐步强化多模态对齐、语义 grounding 和时序推理能力。  <br/>5. **数据贡献**：构建多个增强数据集，基于现有数据集添加GPT生成的时序相关问答对，支持时序监督任务。  <br/>6. **实验验证**：在时序定位和视频问答基准测试中，DaMO显著优于现有方法，尤其在精确时序对齐任务中表现突出。|
|2506.11125v1|[ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone   Scams](http://arxiv.org/abs/2506.11125v1)|**贡献点分点列出：**  <br/>1. **识别关键漏洞**：首次定位语音钓鱼（vishing）攻击链中的ASR转录环节为最大薄弱点，指出其对攻击成功的关键作用。  <br/>2. **提出ASRJam框架**：设计了一种主动防御系统，通过向目标音频注入对抗扰动，干扰攻击者的ASR并切断攻击反馈循环。  <br/>3. **改进对抗扰动方法**：提出EchoGuard，利用自然语音现象（如回声、混响）作为扰动源，实现对ASR的破坏性干扰，同时保持人类语音可懂性。  <br/>4. **实证评估**：通过39人用户实验验证EchoGuard的实用性，对比现有攻击技术，证明其在ASR干扰与用户体验平衡上的最优性。  <br/><br/>**总结（100字以内）：**  <br/>论文提出针对语音钓鱼的双防御策略：ASRJam主动干扰ASR转录，EchoGuard利用自然扰动阻断攻击而不影响人声理解。通过用户实验验证了EchoGuard在攻防效果与可用性上的优越性。|
|2506.10789v1|[FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](http://arxiv.org/abs/2506.10789v1)|**贡献点总结**（100字以内）:  <br/>本研究提出首个美国社会背景下的Neo-fascist文本编码方案，结合NLP与政治科学，构建大规模标注数据集并开发分类模型，揭示该言论在论坛中的广泛存在，强调社会背景的重要性，呼吁持续对抗以维护民主。<br/><br/>**分点贡献**:<br/>1. **首创编码方案**：提出首个针对美国社会语境的Neo-fascist话语编码体系，由政治科学学者主导，填补了NLP与政治学在该领域的交叉研究空白。  <br/>2. **跨学科方法论**：首次将政治学与NLP结合，构建系统性框架以识别和分析Neo-fascist言论，推动多学科协作研究。  <br/>3. **大规模标注数据**：通过众包技术标注1000条文本，创建首个公共可用的Neo-fascist话语标注数据集，为后续研究提供基础。  <br/>4. **模型验证与对比**：对SLMs和LLMs分别进行微调与测试，开发首个可用于Neo-fascist文本分类的模型，验证不同规模模型的效果差异。  <br/>5. **现象分析与警示**：揭示Neo-fascist言论在特定论坛中的普遍性，为理解其传播特征和对民主社会的威胁提供实证依据。  <br/>6. **研究伦理说明**：明确研究仅针对文本内容，不涉及对个人或组织的标签化，强调方法的中立性与合规性。|
|2506.10779v1|[Improving Named Entity Transcription with Contextual LLM-based Revision](http://arxiv.org/abs/2506.10779v1)|**贡献点：**  <br/>1. **提出LLM修订机制**：设计基于大语言模型（LLM）的命名实体修正方法，结合LLM的推理能力和局部上下文信息（如课程笔记）改进ASR对命名实体的识别。  <br/>2. **引入NER-MIT-OpenCourseWare数据集**：发布包含45小时MIT课程音频的开放数据集，支持命名实体识别任务的开发与测试。  <br/>3. **显著性能提升**：在所提出数据集上，该技术使命名实体的词错误率（WER）降低最高达30%，验证了方法的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出LLM驱动的命名实体修正方法，结合上下文信息提升ASR准确性，并发布专用数据集，实验证明在命名实体识别任务中WER降低30%。|
|2506.10504v1|[Beyond Single-User Dialogue: Assessing Multi-User Dialogue State   Tracking Capabilities of Large Language Models](http://arxiv.org/abs/2506.10504v1)|**贡献点分点总结：**  <br/>1. **构建多用户对话数据集**：基于言语行为理论，扩展现有DST数据集，生成第二用户的话语，模拟真实多用户交互场景。  <br/>2. **提出可控评估框架**：系统性地将第二用户话语融入对话，设计方法论以评估LLMs在多用户DST中的鲁棒性。  <br/>3. **揭示性能局限性**：实验表明，LLMs在多用户DST任务中表现显著下降，凸显其在处理多重说话者时的不足。  <br/>4. **指导未来研究方向**：强调需优化LLMs以应对多用户场景，推动更真实、鲁棒的对话状态跟踪模型发展。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过构建多用户DST数据集并设计可控评估框架，揭示了LLMs在处理复杂多用户对话中的性能局限，为未来改进多用户对话理解模型提供了方向。|
|2506.10245v1|[ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in   Portuguese](http://arxiv.org/abs/2506.10245v1)|**贡献点总结（100字以内）**  <br/>1. 构建首个大规模葡萄牙语细粒度仇恨言论语料库，覆盖九个法律保护少数群体。  <br/>2. 提出四阶段合成数据生成流程，包含人工种子、少样本扩展、改写增强和领域平衡。  <br/>3. 实现跨领域泛化能力，验证在多种任务和数据集上的有效性。  <br/>4. 公开数据促进低资源环境下合成数据和仇恨言论检测研究。|
|2506.10202v1|[Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual   Text-to-Video Retrieval](http://arxiv.org/abs/2506.10202v1)|**贡献点：**<br/>1. 提出Q2E方法：首个针对复杂现实事件的零样本多语言文本到视频检索框架，通过LLMs和VLMs提取隐式参数知识实现查询-事件分解。  <br/>2. 多模态输入兼容：支持视觉和语音输入的联合处理，展示跨模态知识融合的有效性。  <br/>3. 熵融合策略：引入熵基融合评分机制，提升零样本多模态信息整合能力。  <br/>4. 音频增强效果：验证音频信息在文本到视频检索中的显著提升作用。  <br/>5. 可泛化性：方法可适应不同数据集、领域及模型架构（LLMs/VLMs）。  <br/>6. 开源贡献：公开代码与数据，推动后续研究。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Q2E，通过LLMs和VLMs提取隐式知识实现零样本多语言文本到视频检索，支持视觉/语音输入融合，并验证音频信息的增强效果。方法具备良好的泛化性，已开源促进研究。|
|2506.09983v2|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v2)|**贡献点总结**  <br/>（100字以内）  <br/>提出分步指令策略与简化输出格式，显著提升多语言依赖分析性能，避免幻觉污染；揭示显式推理步骤对跨语言泛化的重要性，为LLM解析提供可扩展、格式一致的替代方案。<br/><br/>**详细贡献点**  <br/>1. **分步推理策略**：首次将通用词性标注作为预处理步骤，优先预测句法主语和依存标签，提升结构有效性与准确性。  <br/>2. **输出格式优化**：设计简化的CoNLL-U格式，避免传统基于括号的解析方法的污染与冗余问题。  <br/>3. **多语言微调增强**：证明多语言微调可同时提升跨语言泛化性能，扩大模型适用范围。  <br/>4. **SOTA性能验证**：在17种语言的Universal Dependencies数据集上实现当前最优的解析准确率，无幻觉或污染。  <br/>5. **可扩展性方案**：提供格式一致、易于扩展的解析框架，替代传统基于括号的Approach，推动LLM在语音领域的应用。|
|2506.09983v1|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v1)|总结（100字以内）:  <br/>提出一种基于逐步指令策略的依存解析方法，结合通用词性标注和简化输出格式，在17种语言中实现SOTA性能，同时通过多语言微调提升跨语言泛化能力，验证了显式推理步骤对LLM解析的有效性。<br/><br/>贡献点:  <br/>1. **创新的逐步指令策略**：首次将通用词性标注作为先验步骤，结合句法头和依存关系预测，提升解析的结构有效性与准确性。  <br/>2. **简化输出格式设计**：采用类似CoNLL-U的轻量化输出格式，减少冗余信息干扰，避免模型生成错误或污染（hallucination）。  <br/>3. **多语言性能突破**：在17种语言的Universal Dependencies数据集上均取得领先的准确率，证明方法的普适性。  <br/>4. **跨语言泛化优化**：通过多语言微调，显著提升模型在不同语言间的迁移能力，增强通用性。  <br/>5. **方法对比与可扩展性**：提出一种格式一致、可扩展的替代方案，优于传统基于括号的解析方法，推动下游任务的兼容性。|
|2506.09707v2|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v2)|**总结（100字以内）:**  <br/>本研究提出一种基于预训练模型和LoRA微调的自动PE疗法依从性评估方法，通过音频-文本分析精准定位治疗关键阶段，显著提升效率，具有临床培训与质量监控的应用潜力。<br/><br/>**贡献点：**  <br/>1. **方法创新**：首次将LoRA技术应用于PE疗法依从性评估，通过30秒音频-转录窗口微调Qwen2-Audio模型，实现关键环节的自动时间定位。  <br/>2. **标签生成机制**：结合LLM提示与人工评分者验证，生成并验证三阶段（P1/P2/P3）依从性标签，提升标注准确性。  <br/>3. **软监督策略**：提出基于任务提示的软监督训练框架，直接预测归一化边界偏移，优化模型适应性。  <br/>4. **参数分析研究**：系统评估窗口大小及LoRA秩对性能的影响，揭示上下文粒度与模型调优的重要性。  <br/>5. **应用价值**：构建可扩展的PE疗法依从性追踪框架，为临床培训、监督及质量保证提供自动化工具。|
|2506.09707v1|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v1)|总结（100字以内）:  <br/>提出基于预训练模型和LoRA技术的自动PE治疗遵循定位方法，实现高精度识别与可扩展的临床应用。<br/><br/>贡献点:  <br/>1. **方法创新**：首次将LoRA技术应用于预训练音频-语言模型（Qwen2-Audio）的微调，实现对PE治疗关键环节的自动时间定位。  <br/>2. **标签生成**：通过LLM提示和人工验证结合，为三个核心协议阶段（P1/P2/P3）生成高质量的遵循标签。  <br/>3. **软监督策略**：设计任务特定提示引导的软监督框架，提升边界预测的准确性（MAE为5.3秒）。  <br/>4. **参数分析**：系统研究窗口大小与LoRA秩对性能的影响，揭示上下文粒度与模型适配的关键作用。  <br/>5. **实际应用**：构建可扩展的框架，支持PE治疗的临床培训、监督与质量评估，解决传统人工评估的效率问题。|
|2506.09391v1|[Comparing human and LLM politeness strategies in free production](http://arxiv.org/abs/2506.09391v1)|总结：  <br/>本研究揭示大语言模型在礼貌策略上虽能复制人类偏好并被偏好，但存在过度依赖消极策略、导致误解的偏差，突显AI系统在语用对齐中的关键挑战。<br/><br/>贡献点：  <br/>1. 系统分析LLM与人类在礼貌策略使用上的差异，验证大规模模型（≥70B参数）可复制计算语用学关键偏好。  <br/>2. 发现人类评估者在开放性任务中更倾向接受LLM生成的礼貌回应，表明模型在部分场景下的表现接近人类。  <br/>3. 指出LLM在积极语境中过度依赖消极礼貌策略，可能引发语用误解，揭示模型行为的局限性。  <br/>4. 强调语用对齐在AI系统中的重要性，为未来研究提供方向性启示。|
|2506.09349v1|[OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment](http://arxiv.org/abs/2506.09349v1)|总结（100字以内）:  <br/>提出OmniDRCA并行语音-文本模型，采用双分辨率表示和对比对齐，实现SOTA性能并在全双工对话场景中展示应用潜力。<br/><br/>贡献点：  <br/>1. 提出OmniDRCA模型，基于联合自回归建模实现并行生成语音和文本，突破传统分离式生成的局限。  <br/>2. 引入双分辨率语音表示（高低频特征分开编码），提升对语音细节和整体语义的理解能力。  <br/>3. 设计对比交叉模态对齐机制，增强语音与文本之间的对应关系和互模态感知。  <br/>4. 在口语问答基准测试中取得优于现有平行模型的SOTA性能，并验证其与交织模型的竞争力。  <br/>5. 探索框架在全双工对话场景中的扩展性，推动语音生成技术向更复杂交互任务迁移。|
|2506.09301v1|[$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for   Figurative Language Understanding](http://arxiv.org/abs/2506.09301v1)|贡献点：<br/>1. 提出$(RSA)^2$框架，首次将修辞策略纳入RSA模型以处理隐喻语言<br/>2. 实现无需建模非字面表达动机的意图理解机制，突破传统RSA框架限制<br/>3. 开发PragMega+新数据集并验证框架有效性，取得LLMs在讽刺识别任务上的SOTA性能<br/>4. 建立可扩展的语音理解范式，实现字面与意图语义的兼容性解读<br/><br/>总结：该研究提出$(RSA)^2$框架，通过建模修辞策略实现无需动机建模的隐喻理解，在新数据集上取得LLMs讽刺识别的最先进性能。|
|2506.08967v2|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v2)|总结（100字以内）:  <br/>提出全端到端LALM Step-Audio-AQAA，结合双代码库分词、130B参数模型与声码器，创新训练方法，显著提升语音控制能力，并建立新基准测试验证效果。  <br/><br/>**贡献点分项:**  <br/>1. **端到端语音交互模型**：首次设计直接生成自然语音响应的AQAA专用LALM (Step-Audio-AQAA)，突破传统文本依赖的瓶颈。  <br/>2. **双代码库音频分词器**：融合语言与语义特征提取，增强对音频内容的理解与建模能力。  <br/>3. **超大规模模型架构**：采用1300亿参数LLM作为骨干，显著提升生成质量与复杂任务处理能力。  <br/>4. **训练方法创新**：提出交错token输出机制，结合DPO与模型合并技术，优化语义连贯性与语音合成效果。  <br/>5. **基准测试与性能验证**：开发StepEval-Audio-360基准，证明模型在语音控制等关键指标上优于现有SOTA方法，凸显声码器的必要性。|
|2506.08967v1|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v1)|**贡献点：**  <br/>1. **提出全端到端的AQAA模型（Step-Audio-AQAA）**：首次设计专门针对音频查询-音频回答（AQAA）任务的端到端大型音频语言模型，克服传统模型依赖文本输出的局限。  <br/>2. **双代码本音频分词器设计**：引入语言与语义双重编码机制，有效提取语音中的结构化与高层次语义特征。  <br/>3. **大参数量骨干模型与神经声码器结合**：采用1300亿参数的LLM作为主干，并搭配神经声码器，实现高保真语音合成与控制。  <br/>4. **混合训练策略优化**：通过文本与音频交错输出增强语义连贯性，并结合Direct Preference Optimization（DPO）与模型合并技术提升整体性能。  <br/>5. **语音控制任务的性能突破**：在StepEval-Audio-360基准测试中，显著优于现有SOTA模型，验证了模型在语音控制方向的有效性。  <br/>6. **突出token-based声码器的关键作用**：强调声码器在端到端AQAA任务中的核心地位，为未来研究提供新的技术视角。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Step-Audio-AQAA，通过双代码本分词器、大参数模型和神经声码器实现端到端音频对话，结合交错训练与DPO优化，并在语音控制任务中超越现有SOTA模型。|
|2506.08633v1|[Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs](http://arxiv.org/abs/2506.08633v1)|**贡献点：**<br/>1. 提出基于小连接模块（connector module）的跨模态桥接方法，整合语音编码器（如WavLM-large）与大语言模型（如OLMo/LLaMA）的表示空间。<br/>2. 推动全开源和开放数据方案，采用WavLM-large和OLMo等公开模型实现端到端DST系统。<br/>3. 系统性消融实验分析了不同微调策略（全量/LoRA适配器）与对话轮次影响，验证了关键组件的有效性。<br/>4. 引入基于模糊匹配的输出后处理技术，显著提升对话槽值中的命名实体识别性能。<br/>5. 创新性结合SpokenWOZ数据集与Speech-Aware MultiWOZ数据集，提升训练数据多样性与模型表现。<br/>6. 在 SpokenWOZ 标准测试集上取得 34.66% JGA 的 SOTA 结果，并通过 Gemma-2-9B-instruct 实现更高精度（42.17% JGA）。<br/><br/>**总结（100字以内）：**  <br/>本研究通过连接模块融合语音与语言模型，结合开源组件与数据增强策略，系统性优化DST方法。实验证明其在SpokenWOZ数据集上达到SOTA性能，并进一步通过大模型提升至更高水平。|
|2506.08593v1|[Hateful Person or Hateful Model? Investigating the Role of Personas in   Hate Speech Detection by Large Language Models](http://arxiv.org/abs/2506.08593v1)|总结（100字以内）:  <br/>本文首次系统研究MBTI性格特质对LLM仇恨言论分类的系统性影响，发现人格特征显著改变模型输出，并提出优化标注流程的建议，强调对公平性与价值观对齐的重要性。<br/><br/>贡献点:  <br/>1. **首次研究框架**：提出首个基于MBTI人格特质的个性提示（persona prompt）系统性框架，用于分析其对仇恨言论分类的影响。  <br/>2. **人类标注验证**：通过大规模人类标注调查，实证证明MBTI维度显著影响标注行为，揭示人格特质在主观任务中的关键作用。  <br/>3. **多模型跨数据集评估**：在三个主流仇恨言论数据集上，评估四个开源LLM的输出差异，验证人格提示对模型表现的调控效应。  <br/>4. **识别偏差现象**：发现人格驱动导致的三类问题：与真实标签不一致、跨人格标注分歧、logit层级的隐性偏差，凸显模型与人类价值观的潜在差距。  <br/>5. **实践指导意义**：提出需谨慎设计人格提示以提升LLM标注流程的公平性，为伦理引导和模型对齐提供理论依据。|
|2506.08147v1|[Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models](http://arxiv.org/abs/2506.08147v1)|总结：  <br/>本研究构建首个乌尔都语-英语-西班牙语三语仇恨言论检测数据集，提出融合注意力机制与大语言模型的方法，显著提升多语言分类性能，为全球数字社区安全提供有效解决方案。<br/><br/>贡献点：  <br/>1. **构建首个三语仇恨言论数据集**  <br/>   - 收集10,193条跨语言推文（英语3,834；乌尔都语3,197；西班牙语3,162），通过关键词过滤确保平衡标注（4,849 Hate/5,344 Not-Hate）。  <br/>   - 弥补乌尔都语在仇恨言论研究中的空白，尤其缺乏基于翻译的语料支持。  <br/><br/>2. **提出混合模型方法论**  <br/>   - 结合注意力层（预处理）与大语言模型（如GPT-3.5 Turbo、Qwen 2.5 72B），优化多语言特征提取。  <br/>   - 明确区分传统模型（TF-IDF + SVM）与Transformer模型（BERT/RoBERTa）的对比实验。  <br/><br/>3. **验证高性能结果**  <br/>   - 在英语、西班牙语及乌尔都语中分别实现87%、85%、81%的F1得分，多语言联合模型达88%，均优于SVM基线（提升8.75%-7.32%）。  <br/>   - 通过Fleiss' Kappa（0.821）确保标注一致性，提升数据可靠性。  <br/><br/>4. **推动多语言检测应用**  <br/>   - 提供可扩展框架，支持跨语言仇恨言论识别，促进全球社交媒体安全治理。|
|2506.07726v1|[Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription   with RAG-based Correction and Predicted BLEU](http://arxiv.org/abs/2506.07726v1)|**贡献点：**  <br/>1. 构建了首个包含801小时、高质量的长文本形式瑞士议会语料库（751小时通过质量控制），支持瑞士德语多小时辩论会的语音-文本对齐。  <br/>2. 提出分两阶段的GPT-4o校正流程：第一阶段修正ASR误识别（如专有名词），第二阶段评估语义完整性，提升文本准确性。  <br/>3. 引入基于BLEU分数和LLM评估的自动化过滤机制，优化数据质量并确保语义一致性。  <br/>4. 验证了结合高精度ASR、LLM校正与数据驱动过滤方法对低资源、领域专用语音语料库的显著效果（BLEU提升6分）。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了大尺寸瑞士议会长文本语音语料库，通过优化ASR与LLM校正流程提升数据质量，验证了多阶段校正与过滤方法对低资源语料的有效性，实现BLEU显著提升，为瑞士德语语音研究提供重要资源。|
|2506.07707v1|[Interaction Analysis by Humans and AI: A Comparative Perspective](http://arxiv.org/abs/2506.07707v1)|总结：  <br/>本研究通过比较MR与2D视频会议对儿童手势猜谜游戏交流的影响，揭示了MR在增强互动质量与协作学习中的潜力，同时探讨了LLMs在儿童互动分析中的效率与局限性。<br/><br/>贡献点：  <br/>1. **对比研究**：首次系统分析MR（HoloLens）与2D视频会议（Zoom）在儿童协作任务中的交互差异。  <br/>2. **LLMs应用**：开发基于大语言模型的自动化分析框架，实现注释、翻译及迭代校正，提升数据处理效率。  <br/>3. **交互特征发现**：发现MR促进更丰富的非语言交流（如情感表达），而Zoom则以简洁性和可访问性为优势。  <br/>4. **跨平台洞见**：为分布式教育场景下的协作学习设计提供实证依据，强调MR在增强沉浸感与参与度方面的潜力。|
|2506.06775v1|[They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse](http://arxiv.org/abs/2506.06775v1)|总结：  <br/>本研究首次构建意大利政治演讲的IMPAQTS语料库，系统评估LLMs在处理隐含内容（预设与暗示）上的局限性，并提出改进方向，同时开源数据与代码。<br/><br/>贡献点：  <br/>1. **首次构建专用语料库**：创建包含意大利政治演讲及操纵性隐含内容标注的IMPAQTS语料库，填补该领域研究空白。  <br/>2. **设计双重评估任务**：通过多项选择和开放生成任务，全面测试LLMs对隐含内容的解析能力。  <br/>3. **揭示模型局限性**：证明当前LLMs在政治语境下的隐含内容理解存在显著缺陷，识别其缺失的关键pragmatic能力。  <br/>4. **提出改进方向**：指出增强模型对高度隐含语言处理能力的潜在研究路径。  <br/>5. **开放数据与代码**：提供数据集和实现代码，促进该领域的进一步研究与应用。|
|2506.06113v1|[Bridging the Gap: In-Context Learning for Modeling Human Disagreement](http://arxiv.org/abs/2506.06113v1)|**贡献点分点总结**  <br/>1. **探索LLMs在主观任务中的多视角建模能力**：首次系统研究LLMs是否能通过上下文学习（ICL）捕捉多角度观点，并反映主观注释中存在的分歧（如仇恨言论检测）。  <br/>2. **提出多标签建模策略的对比实验**：对比了聚合硬标签与拆分硬标签/软标签在零样本和小样本场景下的效果，揭示了不同标签策略对模型表现的影响。  <br/>3. **分析小样本提示优化方法**：评估了基于文本相似度（BM25、PLM）、注释分歧（熵）和组合排名的演示选择方法，以及随机与课程式（curriculum-based）示例排序策略的有效性。  <br/>4. **揭示LLMs建模主观性的局限**：发现零样本设置下多视角生成可行，但小样本场景难以全面体现人类判断，提示设计和演示选择对性能具有显著影响。  <br/>5. **强调改进方向**：指出需构建更视角敏感、具备社会智能的LLMs，以更好应对主观性任务中的注释分歧问题。  <br/><br/>**总结（100字以内）**：  <br/>该研究探讨LLMs在主观任务中捕捉多视角与注释分歧的潜力，通过对比多种标签策略和提示优化方法，揭示模型在零样本与小样本场景下的差异，并提出改进LLMs社会智能性的方向。|
|2506.06066v1|[Conversational Interfaces for Parametric Conceptual Architectural   Design: Integrating Mixed Reality with LLM-driven Interaction](http://arxiv.org/abs/2506.06066v1)|总结：  <br/>本论文提出一种基于对话的MR界面，整合语音、手势与多智能体LLM，实现参数化建模的直观操作，降低设计门槛，推动MR向生成设计平台发展。<br/><br/>贡献点：  <br/>1. **提出新型对话式MR交互框架**：首次将语音输入、手势识别与多智能体大语言模型（LLM）系统结合，形成面向参数化建模的自然交互范式。  <br/>2. **动态参数状态管理机制**：通过对话与上下文提示解决命令歧义，实现参数的实时动态调整与状态跟踪。  <br/>3. **降低设计认知与操作壁垒**：简化参数化工作流程，使无编程背景的设计师可高效探索和优化设计空间。  <br/>4. **扩展MR作为生成设计平台**：将MR环境从空间交互工具升级为支持程序化思维的创造性设计平台。|
|2506.05796v1|[Diarization-Aware Multi-Speaker Automatic Speech Recognition via Large   Language Models](http://arxiv.org/abs/2506.05796v1)|**贡献点总结（100字以内）**：  <br/>提出融合说话人聚类与大语言模型的多说话人语音识别系统，保留绝对时间信息，提升多语言对话与复杂多说话人场景的识别性能，验证了LLM作为统一后端在联合发言分割和转录中的潜力。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新框架**：设计了一种结合说话人聚类（diarization）与大语言模型（LLM）的多说话人语音识别（MS-ASR）系统，解决重叠语音转录难题。  <br/>2. **保留时序信息**：不同于传统序列化输出训练（SOT）方法，通过整合帧级说话人和语义嵌入，保留绝对时间信息以适应时间敏感场景。  <br/>3. **多模态输入处理**：框架同时处理结构化diarization输入与帧级嵌入，实现段级转录输出，提升识别粒度与准确性。  <br/>4. **多语言与复杂场景适应性**：实验表明系统在多语言对话及高重叠多说话人会议场景中均表现优异，验证其鲁棒性与实用性。  <br/>5. **统一后端潜力**：强调LLM作为统一后端在联合发言分割与转录任务中的优势，推动语音处理与自然语言处理的融合。|
|2506.05706v1|[Bridging the Modality Gap: Softly Discretizing Audio Representation for   LLM-based Automatic Speech Recognition](http://arxiv.org/abs/2506.05706v1)|总结（100字以内）:  <br/>本文提出将向量量化整合到LLM的ASR系统中，通过软离散化方法提升模型对跨域音频的处理能力，揭示了其作为模态桥梁的潜力。<br/><br/>贡献点:  <br/>1. **提出VQ与LLM结合的方法**：解决音频连续性与LLM离散token范式之间的鸿沟，实现音频表示与语言模型的对齐。  <br/>2. **构建基于LLM嵌入表的VQ codebook**：利用预训练LLM的嵌入表作为量化字典，降低模型训练复杂度并增强跨模态一致性。  <br/>3. **设计软离散化技术**：通过动态更新codebook和加权求和策略，生成更符合语言结构的离散音频representation。  <br/>4. **验证有效性与泛化性**：实验表明该方法在out-of-domain场景下显著优于基线，为LLM-based ASR提供新思路。|
|2506.05671v1|[Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](http://arxiv.org/abs/2506.05671v1)|总结：  <br/>本研究提出文本仅微调策略，通过未配对文本实现低资源领域自适应，保持语音-文本对齐并提升模型泛化能力，为ASR领域提供有效新方法。<br/><br/>贡献点：  <br/>1. 提出无需额外音频的文本仅微调框架，解决低资源环境下配对语音-文本数据不足的问题。  <br/>2. 引入实时评估机制，确保微调过程中语音-文本对齐性得以保留。  <br/>3. 实验证明方法在多个数据集上可保持与全音频-文本微调相近的性能，且性能退化最小。  <br/>4. 有效提升模型对新领域的泛化能力，避免灾难性遗忘问题。  <br/>5. 为低资源领域适应性ASR提供可扩展的文本驱动优化方案。|
|2506.05538v1|[SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful   Deepfake Content on Social Media Platforms](http://arxiv.org/abs/2506.05538v1)|总结：  <br/>提出SocialDF数据集及多模态LLM检测方法，解决社交媒体深伪内容的识别难题。<br/><br/>贡献点：  <br/>1. 构建SocialDF数据集：首个涵盖社交媒体真实场景的深伪挑战数据集，包含多来源高保真合成媒体。  <br/>2. 多模态检测框架：融合面部识别、语音转录与多智能体LLM，实现音频-视觉线索的交叉验证。  <br/>3. 语言行为分析：引入 linguistic、behavioral 和 contextual 多维度分析，提升检测的鲁棒性和准确性。|
|2506.05414v1|[SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and   Hearing](http://arxiv.org/abs/2506.05414v1)|总结：  <br/>本研究提出首个支持动态场景中3D空间推理的基准SAVVY-Bench，并设计无需训练的推理框架SAVVY，通过多模态轨迹聚合与坐标对齐提升音频-视觉大语言模型的性能，推动动态3D场景理解。<br/><br/>贡献点：  <br/>1. 提出首个专注于动态、音频-视觉环境中3D空间推理的基准SAVVY-Bench，引入同步空间音频数据。  <br/>2. 设计无训练的推理框架SAVVY，包含两个阶段：基于视角的物体轨迹估计和动态全局地图构建。  <br/>3. 引入细粒度时序定位、一致3D定位及多模态标注，提升场景理解的复杂性与准确性。  <br/>4. 实验证明SAVVY显著提升现有AV-LLMs性能，为动态3D空间推理研究建立新标准。|
|2506.05209v1|[The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly   Licensed Text](http://arxiv.org/abs/2506.05209v1)|**贡献点:**<br/>1. 提出Common Pile v0.1，首个面向LLM预训练的8TB开放授权文本数据集（覆盖30个来源，包括论文、代码、书籍等）；<br/>2. 通过训练7B参数模型验证数据集有效性，性能与基于未授权文本的LLM（如Llama 1/2 7B）相当；<br/>3. 释放数据集构建代码、训练混合策略及模型检查点，提升研究透明度与可复现性。<br/><br/>**总结（100字以内）:**  <br/>本文构建首个8TB开放授权文本数据集Common Pile v0.1，涵盖多领域内容。基于其训练的7B参数模型性能媲美未授权文本训练的LLM，并开源全流程代码和训练资源，推动合规模型研究发展。|
|2506.05191v1|[MokA: Multimodal Low-Rank Adaptation for MLLMs](http://arxiv.org/abs/2506.05191v1)|总结：  <br/>该论文提出MokA方法，通过结合单模态适配与跨模态交互，解决现有多模态微调方法忽视模态差异的问题，实验证明其有效性与普适性，为高效多模态模型适配提供新思路。<br/><br/>贡献点：  <br/>1. **揭示现有方法的局限性**：指出当前高效多模态微调方法直接套用LLM策略，忽视多模态场景的内在差异，影响模态协同利用。  <br/>2. **提出双适应理论框架**：论证单模态适配与跨模态适配是多模态模型微调的两个核心组成部分，强调两者的共同作用。  <br/>3. **设计MokA方法**：提出一种多模态感知的低秩微调策略，通过模态特异性参数压缩单模态信息，显式增强跨模态交互。  <br/>4. **多场景验证有效性**：在音频-视觉-文本、视觉-文本、语音-文本三种典型场景及多个大模型架构（如LLaMA2/3、Qwen2等）中验证方法的通用性与性能提升。  <br/>5. **系统评估方法优势**：通过消融实验与效率分析，全面验证MokA在参数压缩与跨模态增强上的效果。  <br/>6. **推动多模态研究**：认为MokA为多模态大模型高效适配提供更精准的解决方案，为后续研究奠定基础。|
|2506.05062v1|[Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation](http://arxiv.org/abs/2506.05062v1)|总结：  <br/>提出Debate Speech Evaluation新基准，系统分析LLM与人类在多维度辩论评估中的表现，揭示模型判断行为差异并评估生成能力，为LLM判决研究提供关键洞察。<br/><br/>贡献点：  <br/>1. **提出新基准**：构建首个用于评估大语言模型辩论判断能力的基准任务，填补LLM系统性基准领域的空白。  <br/>2. **多维评估框架**：提出综合考察论点强度、相关性、连贯性、风格适配等多层面的评价标准，系统性分析LLM的判断能力。  <br/>3. **大规模标注数据集**：利用包含600+条精细标注的辩论演讲数据集，首次实现对LLM在辩论任务上的大规模实验与验证。  <br/>4. **模型行为分析**：揭示大型模型在个体判断与整体行为层面与人类法官的显著差异，为LLM的局限性研究提供实证依据。  <br/>5. **生成能力评估**：验证前沿LLM生成具有说服力和观点性演讲的能力，证明其在特定任务上可能达到人类水平。|
|2506.04711v1|[LLM-based phoneme-to-grapheme for phoneme-based speech recognition](http://arxiv.org/abs/2506.04711v1)|**贡献点：**<br/>1. **提出LLM-P2G解码框架**：首次将大语言模型（LLMs）引入基于音素的ASR系统，结合语音到音素（S2P）和音素到字符（P2G）的分步解码流程。  <br/>2. **解决级联信息损失问题**：针对S2P与P2G联用时的潜在信息损失，设计了两种训练策略：带噪声音素的数据增强（DANP）和随机化top-K边缘化训练与解码（TKM）。  <br/>3. **跨语言性能提升**：实验验证在波兰语和德语的跨语言ASR任务中，LLM-P2G相比传统WFST解码方法分别降低WER 3.6%和6.9%。  <br/>4. **简化解码流程**：通过LLMs替代WFST，减少了解码的复杂性，同时提升对多语言数据的适应能力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出LLM-P2G解码方法，结合S2P与P2G流程并引入数据增强和top-K边缘化策略，有效缓解信息损失问题，在波兰语和德语跨语言ASR中显著提升识别性能。|
|2506.04693v1|[Cracking the Code: Enhancing Implicit Hate Speech Detection through   Coding Classification](http://arxiv.org/abs/2506.04693v1)|总结：  <br/>提出新的隐性仇恨言论分类体系（codetypes），设计两种LLMs检测方法，验证其在中英文数据集上的有效性。<br/><br/>贡献点：  <br/>1. **构建隐性仇恨言论新分类框架**：首次提出六种编码策略（codetypes），为im-HS检测提供系统化的分类依据。  <br/>2. **创新性方法设计**：提出两种基于大语言模型的整合策略——直接提示分类与编码器嵌入codetypes，解决隐性HS检测挑战。  <br/>3. **跨语言有效性验证**：通过中英文数据集的实验结果，证明所提方法在不同语言环境下的普遍适用性与检测性能提升。|
|2506.04586v1|[LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech   Foundational Models](http://arxiv.org/abs/2506.04586v1)|总结:  <br/>提出LESS框架，通过LLM优化伪标签并结合数据过滤策略提升语音任务性能，验证其跨语言和任务的适应性，为语音处理提供新方法。<br/><br/>贡献点:  <br/>1. **提出LESS框架**：首个结合大语言模型与半监督学习的语音处理框架，用于修正野外数据生成的伪标签。  <br/>2. **LLM伪标签修正**：利用LLM提升ASR/AST任务中伪标签的质量，显著降低词错误率（WER）。  <br/>3. **数据过滤策略**：设计数据增强机制优化LLM知识迁移效率，提升模型训练效果。  <br/>4. **跨语言/任务验证**：在中文ASR和西班牙语-英语AST任务中均取得显著性能提升，证明框架的通用性。  <br/>5. **消融研究分析**：通过不同LLM和提示配置的实验，揭示LLM衍生知识在语音处理中的关键作用与优化方向。|
|2506.04043v1|[Think Like a Person Before Responding: A Multi-Faceted Evaluation of   Persona-Guided LLMs for Countering Hate](http://arxiv.org/abs/2506.04043v1)|**贡献点（分点）:**  <br/>1. 提出首个针对大语言模型（LLM）生成的反叙事（CN）的多维度评估框架，涵盖角色框架、冗长性与可读性、情感基调及伦理稳健性。  <br/>2. 系统测试三种提示策略在MT-Conan和HatEval数据集上的表现，对比GPT-4o-Mini、CommandR-7B和LLaMA 3.1-70B等主流模型的生成效果。  <br/>3. 首次揭示LLM生成的CN存在可访问性不足的问题（如冗长、适配大学以上学历），限制其实际应用效果。  <br/>4. 量化分析情感引导提示策略在提升CN同理心与可读性方面的优势，同时指出其潜在安全与伦理风险。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出LLM生成反叙事的评估框架，系统分析三种提示策略及多模型表现，发现其冗长性与可访问性不足，情感引导策略能提升可读性但存在安全风险，为优化反仇恨言论技术提供关键见解。|
|2506.03099v1|[TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1)|**贡献点：**<br/><br/>1. **模型转化**：将预训练的SOTA图像-视频生成模型（DiT）改造为音频驱动的高参数（180亿）虚拟形象生成模型，实现实时对话动画。<br/>2. **无误差流技术**：通过双向教师模型向稀疏因果自回归学生模型的异步知识蒸馏，解决无限视频流中的误差累积问题。<br/>3. **高效推理优化**：设计高吞吐量、低延迟的推理管道，包含以下工程创新：  <br/>   - 分布式计算（DiT和VAE解码器分设设备）  <br/>   - CUDA流技术实现设备间通信与计算重叠  <br/>   - 消除冗余计算提升帧生成效率  <br/><br/>**总结（100字内）：**  <br/>本文提出TalkingMachines框架，将预训练视频生成模型转化为音频驱动的实时角色动画系统，通过模型优化与分布式推理技术，实现无误差无限视频流和高效生成性能。|
|2506.03009v1|[Conditioning Large Language Models on Legal Systems? Detecting   Punishable Hate Speech](http://arxiv.org/abs/2506.03009v1)|总结：  <br/>本文探讨了如何通过不同抽象层次的法律知识对齐LLMs，分析其在仇恨言论检测任务中的表现，揭示模型与法律专家在法律评估能力上的显著差距，并探讨抽象与具体法律知识对模型性能的不同影响。<br/><br/>贡献点：  <br/>1. 提出并研究了LLMs在法律系统不同抽象层级（宪法、法规、判例）的对齐方法，探索其法律问题评估能力。  <br/>2. 聚焦德国刑法框架下的煽动仇恨行为分类任务，构建具体应用案例分析。  <br/>3. 揭示LLMs在法律评估中的性能瓶颈：抽象法律知识模型缺乏任务理解，易出现矛盾与幻觉；具体法律知识模型虽能识别目标群体，但分类行为特征存在困难。  <br/>4. 为法律与AI交叉领域提供实证依据，指出模型与法律专家间的核心差距，并启发更精准的法律知识融入策略。|
|2506.02758v1|[Exploiting the English Vocabulary Profile for L2 word-level vocabulary   assessment with LLMs](http://arxiv.org/abs/2506.02758v1)|**总结（100字以内）：**  <br/>本研究提出结合大语言模型与英语词汇档案（EVP）的新型方法，实现对二语学习者写作中词汇使用的细粒度评估，解决多义性与上下文变化等挑战，并验证了其在词级与作文级熟练度相关性分析中的有效性。  <br/><br/>---<br/><br/>**贡献点：**  <br/>1. **提出新方法**：首次将大型语言模型（LLMs）与英语词汇档案（EVP）结合，实现基于句子语境的细粒度词汇评估。  <br/>2. **解决复杂问题**：有效应对二语词汇中的多义性（polysemy）、上下文差异（contextual variation）及多词表达（multi-word expressions）等评估难题。  <br/>3. **对比基准模型**：通过对比传统词性（PoS）基线，证明LLMs能利用更丰富的语义信息，提升词汇评分准确性。  <br/>4. **探索相关性**：首次分析词级语言能力与作文整体水平之间的关联，揭示词汇使用的层级性特征。  <br/>5. **验证EVP一致性**：应用该方法重新检验EVP的等级划分合理性，证实LLMs在词汇评估任务中的适用性与可靠性。|
|2506.02457v1|[SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant](http://arxiv.org/abs/2506.02457v1)|总结：  <br/>提出SOVA-Bench基准框架，系统评估语音LLMs的语义与声学生成能力，填补声学质量量化评估空白，推动语音交互系统发展方向。<br/><br/>贡献点：  <br/>1. 提出SOVA-Bench：首个系统性评估语音LLMs的基准框架，整合多维度能力测试。  <br/>2. 填补声学质量量化空白：首次对生成语音的声学特性进行量化评估，突破以往仅关注语义准确性的局限。  <br/>3. 综合多能力评估：同时衡量语音理解、语音识别、语义生成和声学生成能力，提供全面对比分析。  <br/>4. 指导技术发展：为语音交互系统的优化方向提供理论依据与实践参考，促进更自然的语音生成研究。|
|2506.01808v1|[NAVER LABS Europe Submission to the Instruction-following Track](http://arxiv.org/abs/2506.01808v1)|**贡献点总结（100字以内）:**  <br/>本文提出一种多任务语音处理系统，整合语音到LLM嵌入投影器与LoRA适配器，通过指令微调实现跨语言（中、意、德）的ASR、ST、SQA任务联合处理，优化了多语言和多模态数据下的模型性能。<br/><br/>**分点贡献：**  <br/>1. **多任务联合处理**：开发可同步执行语音识别（ASR）、语音翻译（ST）和语音问答（SQA）的系统，支持英语输入到中文、意大利语和德语的跨语言转换。  <br/>2. **模块化架构**：采用两个预训练模块：(1) 语音到LLM的嵌入投影器（基于SeamlessM4T-v2-large语音编码器）；(2) LoRA适配器（基于Llama-3.1-8B-Instruct语言模型）。  <br/>3. **指令微调策略**：联合加载模块后，在多语言和多模态数据上进行1K步指令优化，提升对复杂指令的响应能力。  <br/>4. **高效训练方法**：通过预训练模块与微调结合，简化多任务模型的训练流程，可能提升推理效率和泛化性能。|
|2506.01683v1|[Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection   Using Speech and Large Language Models](http://arxiv.org/abs/2506.01683v1)|**贡献点：**  <br/>1. 提出结合语音和语言模型的CoT推理方法，用于阿尔茨海默病（AD）与非AD分类。  <br/>2. 引入监督微调（SFT）与CoT提示策略，增强模型分类能力。  <br/>3. 设计线性层作为分类模块，提升模型对语音文本的判别性能。  <br/>4. 实验显示方法在无CoT策略的对比基线中实现16.7%的相对性能提升。  <br/>5. 达到当前CoT方法在阿尔茨海默病诊断领域的最先进性能水平。  <br/><br/>**总结：**  <br/>该研究提出一种基于链式思维的语音-语言模型联合框架，通过监督微调与提示策略显著提升痴呆症分类准确率，达到领域内最先进水平。|
|2506.01484v2|[LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech   Detoxification](http://arxiv.org/abs/2506.01484v2)|总结（100字以内）:  <br/>该研究提出基于LLM的自动化detoxification框架，构建首个大规模hatespeech平行数据集ParaDeHate，并验证其在提升模型性能方面的有效性，为替代人工标注提供可扩展解决方案。<br/><br/>贡献点分点列出:<br/>1. **提出LLM-in-the-loop自动化流程**：设计一种利用GPT-4o-mini替代人工标注的新型管道，实现对有害语言的自动改写，降低标注成本。<br/>2. **构建领域专用数据集ParaDeHate**：创建包含8K对仇恨/非仇恨文本的平行数据集，填补hatespeech detoxification的高质量数据缺口。<br/>3. **验证LLM生成数据的有效性**：通过实验表明，基于ParaDeHate微调的BART等模型在风格准确性、内容保留和流畅度方面显著优于现有方法。<br/>4. **建立基准与方法对比**：发布ParaDeHate作为评估基准，并系统比较多种基线模型表现，推动该领域的研究进展。|
|2506.01133v1|[From Words to Waves: Analyzing Concept Formation in Speech and   Text-Based Foundation Models](http://arxiv.org/abs/2506.01133v1)|贡献点（分点）:<br/>1. 首次验证语音模型是否能像文本模型一样获得抽象语义概念<br/>2. 系统比较单模态（语音/文本）与多模态联合训练模型的语义结构差异<br/>3. 提出并应用Latent Concept Analysis方法分析跨模态语义形成机制<br/>4. 开源实验代码与资源提升研究可复现性<br/><br/>总结: 本研究通过无监督分析方法，揭示语音与文本模型在语义抽象形成上的异同，验证多模态训练对语义理解的增强作用，并开放资源促进学术研究复现。|
|2506.01111v1|[FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal   Contextual Fusion](http://arxiv.org/abs/2506.01111v1)|**贡献点分点：**  <br/>1. **两阶段音频描述生成方法**：提出基于人类听觉感知启发的自动化框架，结合多模态信息提取（语音、音乐、环境声音、视频）和大语言模型（LLM）的上下文融合，实现细粒度、语境感知的音频描述。  <br/>2. **FusionAudio数据集**：构建包含120万条高质量音频描述和60万问答对的大型数据集，为跨模态研究提供标注资源。  <br/>3. **改进的音频模型**：开发基于CLAP的音频编码器，提升音频-文本对齐能力与指令遵循效果，增强模型对复杂音频环境的理解。  <br/><br/>**总结（100字以内）：**  <br/>提出两阶段音频描述生成框架与FusionAudio数据集，结合多模态信息和大语言模型提升描述质量，优化CLAP编码器实现更精准的音频-文本对齐与理解。|
|2506.01077v1|[TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans](http://arxiv.org/abs/2506.01077v1)|总结：  <br/>本论文提出TRiMM框架，解决数字人类实时手势生成与长文本理解难题，包含跨模态注意力、长上下文建模及大规模动作匹配系统，实现120fps推理速度，并在消费级GPU上保持低延迟，代码开源。<br/><br/>贡献点：  <br/>1. **提出TRiMM框架**  <br/>   - 首次结合多模态技术实现实时3D手势生成，同时解决长文本理解与实时合成的挑战。  <br/><br/>2. **设计三大核心模块**  <br/>   - **跨模态注意力机制**：实现语音与手势的精确定时对齐。  <br/>   - **长上下文自回归模型**：采用滑动窗口机制高效建模长序列，增强语义连贯性。  <br/>   - **大规模动作匹配系统**：构建原子动作库，支持实时检索生成高质量手势。  <br/><br/>3. **轻量级Unreal Engine实现**  <br/>   - 开发轻量级实验流水线，实现实时推理速度（120 fps）与低句级延迟（0.15秒）。  <br/><br/>4. **全面评估验证效果**  <br/>   - 在ZEGGS和BEAT数据集上完成主观与客观评估，证明其性能优于当前SOTA方法。  <br/><br/>5. **开源代码促进应用**  <br/>   - 提供完整代码库，推动LLM驱动数字人类研究的可复现性与实际部署。|
|2506.00955v1|[Leveraging Large Language Models for Sarcastic Speech Annotation in   Sarcasm Detection](http://arxiv.org/abs/2506.00955v1)|总结：  <br/>提出基于大语言模型的语音讽刺标注方法，构建首个大规模单模态讽刺语音数据集PodSarc，验证其有效性并展示73.63%的检测性能，为该领域研究提供新基准。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的单模态标注流程**：首次利用GPT-4o和LLaMA 3等大语言模型生成语音讽刺标注数据，解决语音领域数据稀缺问题。  <br/>2. **构建大规模语音讽刺数据集PodSarc**：通过人机协作验证，创建高质量单模态讽刺语音数据集，填补语音讽刺研究的数据空白。  <br/>3. **验证方法有效性**：采用协作门控架构对比标注质量与检测性能，证明所生成数据集的可靠性及研究价值。  <br/>4. **提供基准性能指标**：检测模型在公开数据集上达到73.63% F1分数，为后续研究提供量化评估标准。|
|2506.00304v1|[Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion   with LLMs](http://arxiv.org/abs/2506.00304v1)|总结：  <br/>本研究提出一种无需配对语音数据的EMG适配器模块，使LLMs能处理无声EMG信号，实现低错误率的EMG-to-text转换，并在小数据量下显著优于专用模型。<br/><br/>贡献点：  <br/>1. **首次提出EMG适配器模块**：创新性地设计模块，将无声EMG特征映射至LLM输入空间，无需依赖配对的有声信号或语音数据。  <br/>2. **实现高效转换性能**：在封闭词汇任务中达成平均WER 0.49，表明模型对无声EMG信号的识别能力。  <br/>3. **小数据量优势**：仅需6分钟无声EMG数据，性能超越专用模型近20%，验证方法的泛化能力与实用性。  <br/>4. **拓展LLMs应用边界**：探索LLMs在理解发声生物信号（如无声EMG）中的潜力，为跨模态语音识别提供新方向。|
|2506.00160v1|[Werewolf: A Straightforward Game Framework with TTS for Improved User   Engagement](http://arxiv.org/abs/2506.00160v1)|贡献点：  <br/>1. 提出基于LLM的新型Werewolf社会推理游戏系统，融合文本生成与语音交互技术，增强游戏沉浸感。  <br/>2. 设计调优后的文本到语音（TTS）模型，提升与多种LLM的兼容性，降低适配成本。  <br/>3. 通过简化系统结构（无需额外组件），实现更高效的用户参与度提升，反驳传统依赖微调或经验池的方案。  <br/>4. 强调LLM推理能力的持续提升将减少对辅助技术（如复杂提示工程）的依赖，指向未来研究方向。  <br/><br/>总结：  <br/>本文提出一种结合TTS与LLM的新型社会推理游戏系统，通过调优提升兼容性并简化架构，有效增强用户体验，同时指出LLM能力进步将减少对额外技术的依赖。|
|2505.24869v1|[SiLVR: A Simple Language-based Video Reasoning Framework](http://arxiv.org/abs/2505.24869v1)|总结（100字以内）:  <br/>提出SiLVR框架，通过双阶段语言表征与推理实现复杂视频语言理解，利用多感官输入提升性能，并验证强LLM无需视频训练即可有效处理多模态信息，达到多个基准数据集的最佳结果。<br/><br/>**贡献点分点列出**:<br/>1. **双阶段框架设计**：创新性地构建SiLVR框架，将复杂视频理解拆分为语言表征提取与推理两阶段，简化多模态处理流程。<br/>2. **多感官输入融合**：引入短片标题、音频/语音字幕等多模态数据作为语言表征的输入，增强对视频内容的描述能力。<br/>3. **自适应token削减方案**：提出动态调整时间粒度的token减少方法，有效处理长上下文多模态输入的效率与精度问题。<br/>4. **训练自由的模组化架构**：框架无需额外训练，依赖推理时的语言模型能力，提升灵活性和易用性。<br/>5. **多任务性能突破**：在Video-MME、Video-MMMU、Video-MMLU、CGBench、EgoLife等基准数据集上取得当前最优性能。<br/>6. **跨模态推理验证**：实验证明，强LLM无需视频领域训练即可高效聚合多感官信息，支持复杂时序、因果、长上下文及知识推理任务。|
|2505.24691v1|[Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing   Cross-Lingual Transfer in Low-Resource Scenarios](http://arxiv.org/abs/2505.24691v1)|总结：提出融合音素表征与Chain-of-Thought框架的S2TT方法，通过课程学习策略提升低资源与零资源场景的翻译性能，为跨语言语音翻译的普及提供新思路。<br/><br/>贡献点：<br/>1. 提出将音素表征整合入CoT框架，构建新型S2TT系统  <br/>2. 引入音素识别作为跨语言迁移的中间步骤，实现零资源翻译能力  <br/>3. 基于多语言LLM开发语音-文本联合处理架构  <br/>4. 设计渐进式课程学习策略，优化多任务训练过程  <br/>5. 证实音素增强的CoT方法在低资源场景显著提升译质，且具备可扩展性|
|2505.24458v1|[SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social   Engineering Behaviors](http://arxiv.org/abs/2505.24458v1)|**贡献点：**  <br/>1. **首个整合AR与多模态LLM的社会工程数据集**：SEAR是首个专门针对AR增强现实和多模态大语言模型驱动的社会工程攻击的多模态数据集。  <br/>2. **多场景对抗性对话数据**：包含60名参与者在模拟会议、课堂、社交活动等场景中的180条标注对话，涵盖多样化的社会工程情境。  <br/>3. **多模态同步数据采集**：整合AR捕捉的同步视觉/音频线索（如面部表情、语音语调）、环境信息及用户社交媒体资料，实现全面行为分析。  <br/>4. **主观攻击效果评估**：引入信任评分与易受性评估等主观指标，量化攻击对用户心理的影响。  <br/>5. **高攻击效能实证结果**：揭示SEAR在诱导用户点击钓鱼链接（93.3%）、接受电话（85%）、提升信任度（76.7%）等任务中的显著效果。  <br/>6. **伦理合规保障**：通过匿名化处理与IRB（伦理审查委员会）批准，确保数据集的负责任使用。  <br/>7. **开源开放共享**：数据集通过GitHub平台公开，支持学术研究与技术开发。  <br/><br/>**总结（100字以内）**：  <br/>SEAR Dataset是首个结合AR与多模态LLM的社会工程攻击数据集，包含多场景、多模态数据及主观评估指标，揭示攻击高效能，为检测与防御研究提供资源，同时保障伦理合规与公开共享。|
|2505.24347v2|[Fewer Hallucinations, More Verification: A Three-Stage LLM-Based   Framework for ASR Error Correction](http://arxiv.org/abs/2505.24347v2)|总结:  <br/>本文提出RLLM-CF框架，通过错误预检测、迭代修正和推理验证三阶段解决LLM在语音识别中的hallucinations问题，无需额外数据或微调，实验证明在多个数据集上显著降低CER/WER。<br/><br/>贡献点:  <br/>1. **提出RLLM-CF框架**：设计包含错误预检测、链式思维子任务迭代修正和推理过程验证的三阶段校正流程，系统性解决LLM在ASR中的错误修正问题。  <br/>2. **无额外训练需求**：方法无需额外标注数据或模型微调，直接利用LLM能力进行端到端校正，降低应用门槛。  <br/>3. **抑制hallucinations**：通过多阶段验证机制有效避免LLM误改正确文本，保障修正结果的准确性。  <br/>4. **实验证明有效性**：在AISHELL-1、AISHELL-2和Librispeech数据集上验证，显示GPT-4o模型结合该框架后CER/WER分别降低21%/11%/9%/11.4%，具有实际应用价值。|
|2505.24016v1|[BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech   Translation System](http://arxiv.org/abs/2505.24016v1)|总结:  <br/>本研究提出BeaverTalk级联系统，结合VAD分段器、Whisper Large V2和Gemma 3 12B，通过LoRAs微调和单句记忆机制实现高效实时翻译，显著提升英德、英中翻译性能。<br/><br/>贡献点:  <br/>1. **系统架构创新**：设计级联系统BeaverTalk，集成VAD分段器、Whisper Large V2语音识别模型和Gemma 3 12B语言模型，实现端到端语音到文本翻译。  <br/>2. **微调方法优化**：采用低秩适配器（LoRAs）技术对翻译LLM进行轻量级微调，结合对话提示策略利用单一源语言前句记忆库提升上下文建模能力。  <br/>3. **延迟与语言方向支持**：系统兼容低延迟（StreamLAAL 1837.86）和高延迟（StreamLAAL 3343.73）运行模式，在英德（BLEU 24.64/27.83）与英中（BLEU 34.07/37.23）任务中均取得突出性能。  <br/>4. **实际部署效果**：在IWSLT 2025真实任务中验证系统有效性，为多语言实时翻译提供可落地的解决方案。|
|2505.23990v2|[Multi-RAG: A Multimodal Retrieval-Augmented Generation System for   Adaptive Video Understanding](http://arxiv.org/abs/2505.23990v2)|总结：  <br/>提出Multi-RAG多模态检索增强生成系统，通过整合视频、音频、文本多源信息提升情境理解与决策效率，验证其在动态场景中优于现有模型且资源占用更少的优势。  <br/><br/>贡献点：  <br/>1. 提出Multi-RAG系统，解决动态场景下人机协作的认知负担问题。  <br/>2. 首次整合视频、音频、文本多模态信息流进行联合推理与生成。  <br/>3. 在MMBench-Video基准测试中表现优于现有视频大语言模型（Video-LLM）和视觉语言模型（LVLM）。  <br/>4. 以更少资源和输入数据实现高效性能，具有实际应用潜力。|
|2505.22251v2|[Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in   Large Language Models for Speech Recognition](http://arxiv.org/abs/2505.22251v2)|摘要贡献点：  <br/>1. 揭示LibriSpeech和Common Voice数据集与公开LLM预训练语料存在显著重叠，质疑现有语音任务评估结果的可靠性。  <br/>2. 提出通过对比含/不含数据污染的LLM评估污染影响的方法，验证污染数据的存在性及对模型表现的影响。  <br/>3. 发现污染LLM的语音识别器虽在错误率上差异微小，但会显著提升对训练数据的转录概率，反映输出偏差。  <br/>4. 强调需使用独立数据评估LLM语音系统，以避免因数据污染导致的不准确结果。  <br/><br/>总结：  <br/>该研究揭露语音任务评估数据与LLM训练数据重叠问题，揭示污染对模型性能的潜在影响，并呼吁使用独立数据验证模型效果。|
|2505.22029v2|[Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection](http://arxiv.org/abs/2505.22029v2)|**贡献点：**  <br/>1. **构建首个全面覆盖词和音素层级的11类不流畅性合成语料库**：LLM-Dys解决了现有数据集中语调不自然和上下文多样性不足的问题，提供更真实的语音不流畅性样本。  <br/>2. **提出改进的端到端检测框架**：基于LLM-Dys数据集优化模型，实现语音不流畅性检测的最先进性能。  <br/>3. **开源全部资源**：公开数据、模型与代码，促进领域研究和应用。  <br/><br/>**总结（100字以内）：**  <br/>本文构建了首个全面覆盖词/音素层级的不流畅性合成数据集LLM-Dys，改进端到端检测框架达SOTA，所有数据及代码开源，推动语音不流畅性研究。|
|2505.20445v3|[In-context Language Learning for Endangered Languages in Speech   Recognition](http://arxiv.org/abs/2505.20445v3)|**总结（100字以内）:**  <br/>本文探索LLM通过上下文学习在低资源语言语音识别中的应用，证实相关文本样本能提升性能，概率方法优于传统指令方法，并展示ICL可使LLM达到或超越专用语言模型的ASR效果，同时保留原有能力。<br/><br/>**贡献点:**  <br/>1. **验证ICL在低资源语音识别中的可行性**：首次将上下文学习方法应用于语音识别领域，证明LLM能在未训练的低资源语言上实现有效学习。  <br/>2. **提出文本样本增强策略**：发现提供与任务更相关的文本样本能显著提升语言建模和ASR性能，为多语言模型优化提供新方向。  <br/>3. **对比方法效果与模型性能**：表明概率方法优于传统指令方法，且ICL使LLM在ASR任务中达到专用语言模型水平，同时保持其通用能力。|
|2505.18614v2|[MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song   Translation](http://arxiv.org/abs/2505.18614v2)|贡献点：  <br/>1. **提出首个多语言多模态基准**：构建了Multilingual Audio-Video Lyrics Benchmark (MAVL)，首次整合文本、音频与视频数据，为动画歌曲翻译提供综合评估标准。  <br/>2. **多模态数据增强翻译质量**：通过融合音频和视频信息，使翻译更贴近原作的旋律、节奏及风格，突破传统文本仅依赖语义的局限。  <br/>3. **创新音节约束模型结构**：提出SylAVL-CoT模型，结合链式推理（Chain-of-Thought）与音节约束机制，提升歌词的自然度与可唱性。  <br/>4. **验证多模态方法有效性**：实验表明该模型在可唱性和上下文准确性上显著优于文本基础模型，证明多模态、多语言框架对歌词翻译的价值。  <br/><br/>总结（100字以内）：  <br/>本研究提出多语言多模态基准MAVL与SylAVL-CoT模型，融合文本、音频和视频数据，通过音节约束提升歌词翻译的自然度和可唱性，验证了多模态方法在动画歌曲翻译中的优势。|
|2505.18110v2|[Watch and Listen: Understanding Audio-Visual-Speech Moments with   Multimodal LLM](http://arxiv.org/abs/2505.18110v2)|总结：  <br/>本文提出TriSense三模态模型，结合视觉、音频与语音信号，引入Query-Based Connector实现模态自适应融合，构建TriSense-2M数据集支持多模态分析，验证了模型的有效性，并公开代码与数据集。<br/><br/>贡献点：  <br/>1. **提出TriSense模型**：首个整合视觉、音频和语音三模态的大型语言模型，用于全面提升视频内容的时间理解能力。  <br/>2. **设计Query-Based Connector**：通过自适应调整模态权重，支持在模态缺失情况下的鲁棒性，并实现灵活的多模态输入组合。  <br/>3. **构建TriSense-2M数据集**：包含200万样本的高质量数据集，采用自动化生成流程，涵盖长视频及多样化的多模态组合。  <br/>4. **实验验证与公开资源**：在多个基准上验证模型效果，证明其对多模态视频分析的潜力，并开放代码和数据集供研究复现。|
|2505.17536v2|[Multimodal Conversation Structure Understanding](http://arxiv.org/abs/2505.17536v2)|总结：  <br/>本研究提出对话角色归属与线程划分任务框架，构建大规模人工标注数据集，评估多模态模型在理解对话结构上的性能差异，揭示关键影响因素，为改进多模态语言模型的对话理解能力提供基础。<br/><br/>贡献点：  <br/>1. **构建多模态对话结构标注数据集**：提供首个包含4,398条对话角色标注、5,755条收件人信息、3,142条旁观者信息的语料库，涵盖多参与者、多模态场景。  <br/>2. **定义关键任务框架**：提出针对对话角色分配（说话人、收件人、旁观者）和对话线程构建（语句关联与聚类）的系统性任务，结合会话分析与社会语言学理论。  <br/>3. **评估模型性能差异**：对比音频-视觉LLM与视觉-语言模型，发现前者在说话人/收件人识别上更优，但匿名化参与者时性能显著下降。  <br/>4. **揭示关键影响因素**：通过实验发现对话参与者数量是角色识别的主要负向预测因子，而声学清晰度（音调、频谱质心）和面部覆盖情况与性能呈正相关。  <br/>5. **推动未来研究方向**：为多模态LLM在对话结构建模和推理能力的提升提供基准与启示。|
|2505.15670v3|[Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](http://arxiv.org/abs/2505.15670v3)|贡献点（分点）：<br/>1. 提出首个支持实时双向交互的双流S2S架构，通过通道融合实现用户与代理流的联合建模，支持连续输入和实时输出<br/>2. 首次实现无需语音预训练的双流S2S模型，采用预训练流式编码器处理用户输入<br/>3. 采用分体架构设计，实现编码器微调优化代理语音质量，将比特率降至0.6kbps（较前作减半）<br/>4. 实验验证了模型在推理能力、对话轮次控制和用户插话处理等关键指标上的性能优势<br/>5. 突破传统方法对大规模语音数据的依赖，简化从通用语言模型构建双流系统的流程<br/>6. 发布首个开源双流S2S模型，包含完整训练与推理代码，促进研究可复现性<br/><br/>总结（100字内）：  <br/>提出首个支持实时双向语音交互的双流架构，无需语音预训练，显著降低比特率并提升对话能力，简化模型构建流程，实现首个开源可复现的双流S2S系统。|
|2505.15670v2|[Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](http://arxiv.org/abs/2505.15670v2)|总结：  <br/>本文提出首个无需语音预训练的双工S2S模型，通过连续输入输出与信道融合实现实时对话，降低比特率并提升推理、回合管理及打断处理能力，同时减少数据需求，开源代码促进复现。<br/><br/>贡献点：  <br/>1. **双工S2S架构设计**：支持连续用户输入与同步代理输出，引入信道融合机制实现真实场景下的实时交互（如用户打断）。  <br/>2. **无需语音预训练**：首次提出仅依赖流式编码器的双工模型，消除对专用语音预训练的需求。  <br/>3. **高效编码与微调**：采用独立的代理和用户建模架构，支持编码微调以优化代理语音质量，将比特率降至0.6kbps。  <br/>4. **性能提升**：在推理能力、回合控制和打断处理等关键指标上超越现有双工模型。  <br/>5. **数据需求降低**：跳过语音预训练环节，显著减少所需语音数据量，简化模型构建流程。  <br/>6. **开源与可复现性**：首个公开完整训练与推理代码的双工S2S模型，推动领域研究复现与验证。|
|2505.13338v2|[Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data   Condensation and Spoken QA Generation](http://arxiv.org/abs/2505.13338v2)|**贡献点分点列出：**  <br/>1. **提出首个融合上下文推理与语音伴随信息的框架**：首次设计结合两者的数据集生成方法，解决传统Speech-LLMs在综合理解上的不足。  <br/>2. **创新性的数据生成机制**：包含两阶段技术——基于伪语音伴随标签的野外语音数据压缩，以及LLM驱动的上下文语音伴随问答（CPQA）生成。  <br/>3. **验证框架有效性**：通过Qwen2-Audio-7B-Instruct在自动生成与人工标注CPQA数据集上的强相关性评估，证明其能力。  <br/>4. **揭示Speech-LLMs的局限性**：明确指出模型在共情推理任务中的缺陷，强调需针对性数据集与更优模型。  <br/>5. **潜在应用价值**：为训练具备语音伴随推理能力的鲁棒Speech-LLMs提供基础，推动语音理解研究发展。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出首个融合上下文推理与语音伴随信息的数据集生成框架，揭示Speech-LLMs在共情任务中的局限，验证框架对模型训练的有效性，为提升语音AI能力提供新方向。|
|2505.09439v2|[Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](http://arxiv.org/abs/2505.09439v2)|总结：  <br/>本文提出Omni-R1，通过GRPO强化学习微调Qwen2.5-Omni，在MMAU和MMAR基准上取得SOTA性能，揭示了文本推理能力对音频任务的关键作用，并意外发现文本数据微调可提升音频表现。<br/><br/>贡献点：  <br/>1. **提出Omni-R1模型**：通过GRPO强化学习方法对Qwen2.5-Omni进行微调，聚焦音频问答任务。  <br/>2. **SOTA性能突破**：在MMAU和MMAR基准测试中达到当前最优结果，尤其在声音、音乐、语音及总体平均类别均表现最佳。  <br/>3. **因果分析**：验证了性能提升主要源于增强的文本推理能力，而非单纯依赖音频数据。  <br/>4. **意外发现**：发现仅以文本数据集进行微调也能有效提升模型的音频表现，为多模态训练提供新思路。|
|2505.05335v2|[FLAM: Frame-Wise Language-Audio Modeling](http://arxiv.org/abs/2505.05335v2)|总结：  <br/>本文提出FLAM，解决帧级音频理解与开放词汇定位难题，通过logit调整和大规模数据集提升模型性能，保持全局检索能力。<br/><br/>贡献点：  <br/>1. **首次提出开放词汇帧级音频定位模型**：突破传统模型对预定义类别的依赖，实现对真实场景中未见事件的泛化定位。  <br/>2. **设计记忆高效且校准的帧级目标函数**：引入logit调整机制，有效缓解训练中的虚假相关（如事件依赖、标签不平衡）。  <br/>3. **构建多源帧级监督数据集**：结合LLM生成字幕与模拟数据，提供多样化、细粒度的音频事件标注。  <br/>4. **验证模型多任务能力**：在保持文本-音频检索性能的同时，显著提升帧级定位效果及下游任务表现。|
|2504.20007v3|[Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from   Police Body-Worn Camera Footage](http://arxiv.org/abs/2504.20007v3)|总结：  <br/>本研究提出一种跨学科框架，整合AI与机器学习技术分析警用摄像头数据，通过多模态方法提取警民互动行为动态，构建结构化总结系统，并建立评估流程以提升执法审查与知识发现效率。<br/><br/>贡献点：  <br/>1. **跨学科框架创新**：首次将人工智能（AI）与统计机器学习（ML）技术结合，专门针对警用摄像头（BWC）视频进行模式分析，突破传统单一技术应用的局限。  <br/>2. **多模态数据融合**：综合图像、音频和自然语言处理（NLP）技术，实现对警民互动场景的全面分析，提取尊重/不尊重、冲突升级等复杂行为动态。  <br/>3. **结构化总结生成**：引入说话人分离、转录及大语言模型（LLMs），构建可解释的警民接触事件摘要，提升数据分析的可操作性。  <br/>4. **定制化评估体系**：开发适用于高风险执法场景的自动评估流程，量化转录质量与行为检测准确率，支持实际应用验证。  <br/>5. **执法应用场景落地**：方法论与实证结果为执法部门提供审查、培训及问责的实用工具，推动AI技术在警务实践中的落地应用。  <br/>6. **知识发现前沿推进**：通过系统化分析复杂BWC数据，拓展法律与技术交叉领域的研究边界，为公共安全数据分析提供新范式。|
|2504.08961v2|[A Fully Automated Pipeline for Conversational Discourse Annotation: Tree   Scheme Generation and Labeling with Large Language Models](http://arxiv.org/abs/2504.08961v2)|**贡献点总结：**  <br/>1. 提出基于LLM的全自动决策树构建与标注流水线，替代传统人工设计流程。  <br/>2. 首次将频率引导的决策树与LLM结合，提升语音功能标注性能。  <br/>3. 通过实验验证不同设计选择的效果，展示方法优于手动方案及人类标注。  <br/>4. 开源代码、标注方案及结果，促进语音领域对话标注研究。|