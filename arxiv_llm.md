|Source|Title|Summary|
|---|---|---|
|2507.11292v1|[Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources,   Coded Term Lexicon, and Enhanced Detection Frameworks](http://arxiv.org/abs/2507.11292v1)|总结：  <br/>本研究提出首个span-level中文仇恨言论数据集，系统分析编码仇恨词汇并集成注释词典，显著提升检测性能与模型可解释性，为中文仇恨言论检测研究提供关键资源与方向。<br/><br/>贡献点：  <br/>1. **首创数据集**：构建Span-level Target-Aware Toxicity Extraction（STATE ToxiCN）中文仇恨言论数据集，填补span-level细粒度标注的空白，评估现有模型的语义理解能力。  <br/>2. **编码仇恨分析**：首次开展中文隐性仇恨词汇的全面研究，探索大语言模型（LLMs）对仇恨语义的识别与解释机制。  <br/>3. **可解释性方法**：提出将标注词典融合至模型的创新方法，通过增强语义关联性，显著提升仇恨言论检测的性能与可解释性。|
|2507.11210v1|[Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and   Addressing Family Communication Bias](http://arxiv.org/abs/2507.11210v1)|**贡献点：**  <br/>1. 提出"理想父母偏见"（ideal parent bias）与"压制情感"（suppressed emotion）理论框架，揭示家庭沟通中被传统指标忽视的心理动态机制。  <br/>2. 构建首个针对家庭互动分析的日本亲子对话语料库（30个场景），标注理想父母偏见和压制情感的元数据。  <br/>3. 设计基于角色扮演的LLM多智能体对话支持系统，集成专门化模块（情感检测、隐含偏见解析、上下文属性推断）和协作机制（元智能体报告+五位专家智能体反馈生成）。  <br/>4. 通过实验证明系统在情感识别准确率、反馈共情性与实用性方面表现突出，且模拟对话验证了其对改善家庭情感表达和理解的潜在价值。  <br/><br/>**总结（100字内）：**  <br/>本研究提出家庭沟通中的理想父母偏见理论，构建日语亲子对话语料库，设计LLM多智能体系统检测压制情感并生成反馈，验证了其提升家庭情感交流效果的可行性。|
|2507.10859v1|[MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](http://arxiv.org/abs/2507.10859v1)|**贡献点**  <br/>1. 提出MultiVox，首个专注于评估语音助手整合多模态（语音与视觉）线索能力的综合基准测试。  <br/>2. 构建包含1000个人工标注的语音对话数据集，涵盖丰富的语调特征（如音调、情感、音色、音量）和环境声音背景等视觉信号。  <br/>3. 通过系统评估9个最先进的模型，揭示当前模型在情境感知响应和隐含语音特征理解上存在显著不足，同时对比人类表现基准。  <br/><br/>**总结**  <br/>提出MultiVox，首个全面评估语音助手多模态理解能力的基准，发现模型在情境响应和语音特征处理上表现不佳，人类仍具优势。|
|2507.10468v1|[From BERT to Qwen: Hate Detection across architectures](http://arxiv.org/abs/2507.10468v1)|**贡献点：**  <br/>1. **对比研究模型家族**：系统评估经典双向Transformer编码器与新一代超大自回归LLMs在仇恨言论检测中的性能差异。  <br/>2. **构建专用评测数据集**：基于在线互动文本建立“Hate or No Hate”数据集，提供更贴近实际场景的基准测试。  <br/>3. **验证规模与性能关系**：通过实验证明超大模型是否在实际应用中优于传统方法，澄清其在上下文感知上的潜力与局限。  <br/>4. **提出实践指导方向**：揭示在线平台在平衡仇恨言论检测准确性与避免误删合法内容之间的核心挑战，为技术优化提供参考。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过对比经典模型与LLMs在Hate or No Hate数据集上的表现，验证超大模型的实际效用，为在线平台提供更精准的仇恨言论识别策略。|
|2507.10427v1|[Towards Emotion Co-regulation with LLM-powered Socially Assistive   Robots: Integrating LLM Prompts and Robotic Behaviors to Support   Parent-Neurodivergent Child Dyads](http://arxiv.org/abs/2507.10427v1)|总结：  <br/>本研究开发了基于LLM的社交机器人系统，探索其在父母与神经多样性儿童情绪共调节中的应用，通过试点测试验证了其有效性并提出了改进设计的方向。<br/><br/>贡献点：  <br/>1. **提出LLM与SAR的结合方法**：首次将大型语言模型与社交机器人集成，构建LLM驱动的社交机器人系统以支持情绪共调节。  <br/>2. **开发语音交互模块**：基于MiRo-E平台设计并部署语音通信模块，实现父母与儿童之间的动态情绪调节互动。  <br/>3. **实证研究验证效果**：通过两组家长-儿童对的试点测试，证明MiRo-E在提升互动质量与情绪调节中的潜力。  <br/>4. **识别技术挑战**：揭示当前系统在设计和技术实现中的关键问题，为后续优化提供依据。  <br/>5. **提出设计建议**：基于研究发现，总结设计启示以推动LLM赋能的社交机器人在心理健康领域的应用发展。|
|2507.10200v1|[Natural Language-based Assessment of L2 Oral Proficiency using LLMs](http://arxiv.org/abs/2507.10200v1)|总结：  <br/>该研究验证了基于自然语言的评估方法在零样本语音任务中有效性，相比BERT模型表现更优，并展示其在任务不匹配场景、跨语言数据泛化和可解释性方面的优势。<br/><br/>贡献点：  <br/>1. **验证NLA方法的有效性**：证明自然语言描述符可有效指导大语言模型（如Qwen 2.5 72B）进行语音任务评估。  <br/>2. **零样本评估性能**：在无需额外训练的情况下，NLA方法达到与微调专业语音LLM相当的评估效果。  <br/>3. **超越BERT基模型**：在特定任务中，NLA方法优于专门训练的BERT模型。  <br/>4. **任务不匹配适应性**：在任务设定不匹配场景下，NLA方法表现突出。  <br/>5. **跨语言与数据类型通用性**：方法可扩展至其他语言和数据类型。  <br/>6. **可解释性提升**：基于清晰可解释的描述符，增强评估结果的透明度和可理解性。|
|2507.10177v1|[Abusive text transformation using LLMs](http://arxiv.org/abs/2507.10177v1)|**贡献点总结（分点）**  <br/>1. **提出基于LLMs的攻击性文本转换方法**：首次探索利用大语言模型将包含仇恨言论和脏话的社交媒体文本（如推文、评论）转化为非攻击性版本，同时保留原始意图。  <br/>2. **对比多模型在攻击性文本识别中的表现**：系统评估Gemini、GPT-4o、DeekSeek和Groq等先进LLMs在识别攻击性内容的准确性与有效性。  <br/>3. **验证转换后文本的语义与情感保持性**：通过情感和语义分析，验证转换后的文本是否在去除攻击性内容的同时维持原意和情感倾向。  <br/>4. **揭示模型差异与相似性**：发现Groq在转换结果上与其他模型存在显著差异，而GPT-4o和DeekSeek在性能上表现出相似性。  <br/><br/>**摘要总结（100字以内）**  <br/>本研究探索使用LLMs将攻击性文本转化为非攻击性版本，并保留意图。对比多模型的识别与转换效果，发现Groq与他人差异显著，GPT-4o与DeekSeek性能相近。|
|2507.10069v1|[ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal   Parallelism](http://arxiv.org/abs/2507.10069v1)|总结（100字以内）:  <br/>提出Elastic Multimodal Parallelism（EMP）服务范式，设计模态感知负载均衡、弹性分区调度和多模态缓存机制，显著提升MLLMs的推理效率和吞吐量，降低TTFT延迟并满足SLO。<br/><br/>贡献点分点:  <br/>1. **提出EMP服务范式**：解决多模态大语言模型因资源异构性导致的高延迟与低利用率问题，实现动态适应不同请求类型的弹性并行策略。  <br/>2. **模态感知负载均衡**：将请求划分为独立模态组，通过动态资源分配提升系统整体效率。  <br/>3. **弹性分区调度**：解耦推理阶段，支持并行调整与自适应扩展，优化资源利用。  <br/>4. **多模态统一缓存机制**：引入非阻塞编码和统一前缀缓存，减少重复计算，提升推理效率。  <br/>5. **实验验证效果**：在真实数据集上展现优越性能，TTFT降低4.2倍，吞吐量提升3.2-4.5倍，同时满足服务等级目标（SLO）。|
|2507.09116v2|[Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition](http://arxiv.org/abs/2507.09116v2)|**贡献点总结（分点）：**  <br/>1. **提出双路径GER框架**：结合多模态GER（整合语音发音信息）与多粒度GER（引入音素级细节），强化LLM对口音语音的处理能力。  <br/>2. **设计HDMoLE混合模型**：通过分层路由和动态阈值机制，有效融合多口音LoRA专家，解决口音多样性挑战。  <br/>3. **验证性能提升**：在多口音英语数据集上实现人均相对WER降低67.35%，显著优于Whisper-large-v3基线模型。  <br/><br/>**总结（100字以内）：**  <br/>本文提出多模态与多粒度生成式错误纠正方法，结合HDMoLE模型，显著提升多口音语音识别的转录准确度，实现67.35%的WER降低。|
|2507.09116v1|[Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition](http://arxiv.org/abs/2507.09116v1)|**总结（100字以内）:**  <br/>本研究提出多模态与多粒度生成式纠错方法，结合发音信息与语义信息，通过LoRA微调和HDMoLE混合专家模型提升多口音场景下的ASR性能，实验实现67.35%的WER降低。<br/><br/>**贡献点:**  <br/>1. **提出多模态GER方法**：整合语音模态的发音信息，增强LLM对口音语音的建模能力。  <br/>2. **设计多粒度GER框架**：引入音素级与词级N-best假设，结合细粒度发音特征与语义信息优化纠错。  <br/>3. **开发HDMoLE混合专家模型**：通过分层路由与动态阈值机制，融合多单口音LoRA专家，解决口音多样性问题。  <br/>4. **创新多阶段训练策略**：为每种口音训练独立多模态GER模型，提升模型对特定口音的适应性。  <br/>5. **实验验证有效性**：在多口音英语数据集上取得显著结果，相对WER降低67.35%，优于Whisper-large-v3基线。|
|2507.09070v1|[SemAlignVC: Enhancing zero-shot timbre conversion using semantic   alignment](http://arxiv.org/abs/2507.09070v1)|**贡献点总结：**  <br/>1. 提出SemAlignVC架构，通过SemAlign方法对齐文本与音频表示，实现说话人无关语义编码。  <br/>2. 首次在神经编码器与LLM驱动的VC中有效解决音色泄漏问题。  <br/>3. 利用解耦的自回归Transformer，无需显式说话人嵌入即可生成高保真语音。  <br/>4. 实验验证在语义相似度、可懂度、自然度及隐私保护方面优于现有基线方法。  <br/><br/>**摘要100字内总结：**  <br/>本文提出SemAlignVC，通过语义对齐方法解决零样本语音转换中的音色泄漏问题，无需说话人嵌入即可实现高保真转换，显著提升语音质量与隐私安全性。|
|2507.08557v1|[FreeAudio: Training-Free Timing Planning for Controllable Long-Form   Text-to-Audio Generation](http://arxiv.org/abs/2507.08557v1)|总结：  <br/>提出训练无监督的FreeAudio框架，首次实现长文本时间控制语音生成，结合解耦注意力、上下文潜变量组合和参考引导技术，显著提升合成质量并具备与训练方法相当的长文本生成能力。<br/><br/>贡献点：  <br/>1. **首创训练无关框架**：FreeAudio是首个无需训练数据即可实现时间控制的长文本语音生成方法，突破传统数据依赖限制。  <br/>2. **多阶段时间控制机制**：通过LLM规划非重叠时间窗口，引入解耦聚合注意力控制（精确时间对齐）、上下文潜变量组合（局部平滑）和参考引导（全局一致性）三重策略。  <br/>3. **长文本生成能力**：支持复杂多事件时间序列生成（如同时生成"2.4s-5.2s"和"0s-24s"的音频内容），并实验证明其性能可与训练模型（如Stable Audio）媲美。  <br/>4. **无训练方法的SOTA表现**：在不依赖训练数据的场景下，合成质量达到当前最优，且与训练模型在长文本生成任务中表现相当。|
|2507.07877v1|[Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models](http://arxiv.org/abs/2507.07877v1)|总结：  <br/>该研究系统评估了8种SOTA PTQ方法对Whisper和Moonshine模型的量化效果，构建统一框架并分析不同位宽配置对效率与准确性的权衡，为边缘设备部署ASR模型提供关键优化指导。<br/><br/>贡献点：  <br/>1. **全面基准测试**：对两类主流边缘ASR模型（Whisper和Moonshine）进行跨七大数据集的系统性量化性能评估。  <br/>2. **统一框架开发**：基于LLM压缩工具包扩展，集成多量化算法、统一校准与评估流程及分析工具。  <br/>3. **量化配置分析**：探讨不同位宽（包括3比特）和量化方法对模型权重及激活的综合影响。  <br/>4. **效率-精度平衡**：揭示3比特量化在先进PTQ技术下仍可实现高容量模型的性能保持，突破传统认知。  <br/>5. **实用优化指导**：为低功耗、持续运行的边缘设备部署ASR模型提供数据驱动的优化策略和实验依据。|
|2507.06329v1|[MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in   Music Mixing](http://arxiv.org/abs/2507.06329v1)|贡献点总结：<br/>1. 提出MixAssist数据集：首个聚焦音乐混音协作对话的音频-语言数据集，涵盖专家与业余制作人的多轮互动<br/>2. 数据集特性：包含431个场景化对话片段，来自7场深度混音协作会（12位制作人参与）<br/>3. 应用场景：用于训练AI助理理解音乐制作的协作与教学需求<br/>4. 评估成果：验证Qwen-Audio等模型在混音建议生成任务中的有效性，Qwen表现最优<br/><br/>（99字）|
|2507.05609v1|[MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses](http://arxiv.org/abs/2507.05609v1)|**贡献点：**<br/>1. 提出基于Tri-Mamba架构的Mix Block，实现多通道音频在原始波形层面的高效融合并兼容流处理。<br/>2. 设计Frame Diarization Mamba Layer，提升帧级侧边语音抑制能力，优化Whisper模型的微调效率。<br/>3. 引入Multi-Scale Group Relative Policy Optimization (GRPO)策略，联合优化帧级与语句级侧边语音抑制性能。<br/><br/>**总结（100字以内）：**  <br/>本研究提出MMW框架的三大创新，显著提升智能眼镜在嘈杂环境中的侧边语音抑制效果，降低WER 4.95%，为实际应用中的多麦克风语音识别提供了更可靠的解决方案。|
|2507.05591v1|[MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models](http://arxiv.org/abs/2507.05591v1)|总结：  <br/>本研究提出了一种结合轻量模型与特征提取模块的多模态大语言模型MLlm-DR，解决LLM在面试数据上的训练不足问题，并在两个基准数据集上取得最优效果，推动抑郁症诊断的可解释性与临床适用性。<br/><br/>贡献点：  <br/>1. **提出多模态大语言模型MLlm-DR**：首次将小规模LLM与轻量级查询模块（LQ-former）结合，实现对面试视频多模态信息（语音、视觉）的集成分析，支持可解释的抑郁症诊断。  <br/>2. **构建专用训练数据集**：设计并建立用于微调的鲁棒数据集，增强模型在领域任务中的逻辑推理能力，弥补现有LLM缺乏面试数据训练的缺陷。  <br/>3. **引入LQ-former模块**：开发专门提取抑郁症相关语音与视觉特征的轻量模块，提升模型对多模态信息的处理能力，实现更全面的诊断。  <br/>4. **实验验证有效性**：在CMDC和E-DAIC-WOZ两个面试基准数据集上取得SOTA性能，验证了模型的诊断准确性和临床价值。|
|2507.03875v1|[Demystifying ChatGPT: How It Masters Genre Recognition](http://arxiv.org/abs/2507.03875v1)|**总结（100字以内）**  <br/>本研究评估了ChatGPT等LLM在电影体裁预测中的表现，发现微调后ChatGPT在zero-shot和few-shot设置下最优，并通过结合视觉信息（电影海报）进一步提升预测性能，凸显其在内容应用中的潜力。<br/><br/>**贡献点分点列出：**  <br/>1. **首次系统评估LLM在电影体裁预测中的表现**  <br/>   - 使用MovieLens-100K数据集（含1682部电影、18种体裁）对比三种LLM的预测能力，揭示ChatGPT在未微调时的优越性。  <br/><br/>2. **提出zero-shot和few-shot提示框架**  <br/>   - 基于电影预告片的音频转录与字幕设计提示语，探索LLM在跨模态（语音）任务中的泛化能力。  <br/><br/>3. **结合视觉语言模型（VLM）增强预测效果**  <br/>   - 引入IMDb电影海报的视觉信息，通过VLM与LLM的协同提升体裁预测精度，验证多模态信息对语音文本分析的价值。  <br/><br/>4. **突出ChatGPT在内容应用中的潜力**  <br/>   - 证明ChatGPT在整合视觉信息后仍能保持优势，为内容生成与推荐系统提供新的技术路径。|
|2507.03343v2|[SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.03343v2)|**总结（100字以内）:**  <br/>本文提出SHNU-mASR系统，融合平行语音编码器与大语言模型，通过双编码器输出嵌入、三层训练策略及语言感知提示提升多语言语音识别性能，盲测集CER/WER达11.76%，优于官方基线8.41个百分点，且无需增加训练数据。  <br/><br/>**贡献点分点总结:**  <br/>1. **架构创新**：提出将平行语音编码器（Whisper-large-v3 + mHuBERT-147）与大语言模型（LLM）结合的统一多语言ASR框架。  <br/>2. **双编码器协同**：利用两个预训练语音编码器的输出嵌入拼接，实现声学与语言知识的互补融合。  <br/>3. **三阶段训练策略**：联合优化语音编码器的低秩适配模块和投影参数，提升模型整体性能。  <br/>4. **语言感知提示**：在LLM输入端引入语言相关提示，增强语言特定的文本生成能力。  <br/>5. **高效性能提升**：在无需扩展训练数据的情况下，盲测集CER/WER优于官方基线8.41个百分点。|
|2507.03343v1|[SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge](http://arxiv.org/abs/2507.03343v1)|总结：  <br/>本文提出一种多语言对话语音识别系统，通过并行语音编码器与大语言模型的融合，结合三阶段训练策略和语言感知提示，显著提升识别性能且无需增加训练数据。<br/><br/>贡献点：  <br/>1. **创新架构**：提出将并行语音编码器（Whisper-large-v3与mHuBERT-147）与大语言模型（LLM）结合的统一多语言ASR框架。  <br/>2. **互补知识融合**：通过拼接两个编码器的输出嵌入，使模型同时利用声学与语言知识，提升多语言识别能力。  <br/>3. **三阶段训练策略**：设计联合优化低秩适配模块、投影参数及LLM的分阶段训练方法，增强模型整体性能。  <br/>4. **语言感知提示**：在LLM输入端引入语言感知提示，针对不同语言优化文本生成效果。  <br/>5. **高效性能提升**：在盲测数据集上实现11.76%的CER/WER，超越官方基线8.41个百分点，且不依赖额外训练数据。|
|2507.03043v1|[K-Function: Joint Pronunciation Transcription and Feedback for   Evaluating Kids Language Function](http://arxiv.org/abs/2507.03043v1)|**贡献点**  <br/>1. **提出K-Function框架**：整合了准确的子词转录、客观评分和可操作反馈，解决儿童语言评估中语音识别的挑战。  <br/>2. **设计Kids-WFST模型**：结合Wav2Vec2音素编码器与Dysfluent-WFST，专门捕捉儿童语言错误，并保持模型可解释性。  <br/>3. **提升识别性能**：在MyST和Multitudes数据集上分别实现1.39%和8.61%的音素错误率，显著优于贪心解码器（提升10.47和7.06个百分点）。  <br/>4. **构建LLM评估系统**：基于高保真转录结果，开发语言模型评估儿童语言能力、里程碑、阅读及理解，与人类监考员一致，并提供可视化和定向建议。  <br/>5. **实现诊断-反馈闭环**：首次将精确的音素识别与多维度评估结合，为可扩展、临床适用的语言评估提供完整解决方案。  <br/><br/>**总结**  <br/>本文提出K-Function框架，结合子词转录与可解释模型，显著提升儿童语音识别准确率，并开发LLM用于多维度语言能力评估，推动临床诊断应用。|
|2507.02982v1|[We Need Knowledge Distillation for Solving Math Word Problems](http://arxiv.org/abs/2507.02982v1)|总结：  <br/>该研究提出通过压缩BERT嵌入向量并蒸馏小型模型，实现数学能力提升与成本降低，为智能教育系统提供高效、通用的解决方案。<br/><br/>贡献点：  <br/>1. **提出数学模型压缩方案**：通过压缩LLMs的嵌入向量并蒸馏小型模型，显著降低参数量（仅1/12）和计算成本。  <br/>2. **保持高性能与泛化能力**：压缩模型在多种数学任务中表现接近原模型（90%性能），且蒸馏过程不依赖具体任务。  <br/>3. **揭示压缩关键机制**：发现词性信息对数学问题求解至关重要，而非实体识别，为模型压缩提供理论支撑。  <br/>4. **推动智能教育应用**：提升模型效率和成本效益，为智能教学系统及教育领域发展提供实用价值。|
|2507.02904v1|[Enhancing Sports Strategy with Video Analytics and Data Mining:   Assessing the effectiveness of Multimodal LLMs in tennis video analysis](http://arxiv.org/abs/2507.02904v1)|**贡献点：**<br/>1. **评估MLLMs在体育视频分析中的应用**：首次系统评估多模态大语言模型（MLLMs）对网球视频的处理能力，探索其在动作识别和事件理解上的有效性。  <br/>2. **填补事件序列识别的空白**：针对现有网球分析研究中缺乏对rally事件顺序识别的问题，提出模型在这一领域的改进方向。  <br/>3. **动作分类与序列分析**：验证MLLMs在分类网球动作及识别复杂动作序列（如rally过程）中的潜力，为其他体育分析任务提供参考。  <br/>4. **性能优化方法**：探讨通过调整训练策略及结合传统模型提升MLLMs性能的途径，推动多模态模型在运动分析中的实际应用。  <br/><br/>**总结（100字以内）**：  <br/>本研究评估MLLMs在网球视频分析中的应用，聚焦于事件序列识别与动作分类，提出填补研究空白的改进方法，并探索训练策略优化与模型融合路径。|
|2507.02858v1|[Requirements Elicitation Follow-Up Question Generation](http://arxiv.org/abs/2507.02858v1)|总结（100字以内）:  <br/>本文提出基于常见面试错误类型框架的LLM辅助访谈方法，开发了根据语音生成问题的技术，并通过实验验证LLM生成的问题在质量上不逊于人工，证明其能有效提升需求获取访谈的效率和准确性。<br/><br/>贡献点分点列出:  <br/>1. **构建框架**：提出基于常见面试者错误类型（如领域不熟悉、认知负荷等）的LLM辅助生成问题框架，系统化识别访谈难点。  <br/>2. **语音分析方法**：设计基于被访者语音内容提问生成的策略，实现动态适配需求获取场景。  <br/>3. **实验验证**：开展两组对照实验（无引导与有引导），对比LLM生成与人工编写问题在清晰度、相关性和信息量上的表现。  <br/>4. **性能证明**：发现LLM生成问题在未引导情况下与人工相当，且在引导下优于人工，证实LLM的可行性。  <br/>5. **应用价值**：指出LLM可实时辅助面试者提升需求获取质量，为语音交互与人工智能结合提供新思路。|
|2507.02530v1|[Open-Source System for Multilingual Translation and Cloned Speech   Synthesis](http://arxiv.org/abs/2507.02530v1)|总结：  <br/>系统集成了Whisper和VAD，结合LLMs实现多语言翻译与语音再生，支持本地API部署，适用于Zoom、公共广播等场景，提升通信包容性与自然性。<br/><br/>贡献点：  <br/>1. **系统架构创新**：构建多语言翻译与语音再生一体化开源系统，解决跨语言沟通及无障碍场景的复杂需求。  <br/>2. **端到端流程优化**：整合语音识别（Whisper）、说话间隔检测（VAD）与语言模型（LLMs），实现从语音到文本再到多语言输出的高效处理。  <br/>3. **双阶段LLM协作**：通过第一阶段句子分割与第二阶段翻译，提升翻译输出的连贯性与准确性。  <br/>4. **语音克隆技术**：采用支持语音克隆的文本到语音（TTS）模块，保留原说话人语音特征以增强再生语句的自然性。  <br/>5. **灵活部署方案**：支持本地运行与API调用，降低部署成本，适配不同应用场景（如实时会议、广播、个人设备）。  <br/>6. **性能分析与验证**：提供延迟、词准确率的系统性能评估，证明其在实际多语言环境中的有效性与可用性。  <br/>7. **开源社区推动**：开放源代码，促进技术共享与创新，提升多语言通信技术的普惠性。|
|2507.02282v1|[Content filtering methods for music recommendation: A review](http://arxiv.org/abs/2507.02282v1)|总结：  <br/>该论文探讨音乐推荐系统中内容过滤方法，重点分析LLMs歌词分析与音频信号处理技术的协作应用，解决协同过滤的偏见问题和跨方法冲突。<br/><br/>贡献点：  <br/>1. **系统分析内容过滤的作用**：明确指出内容过滤在缓解协同过滤因数据稀疏性导致的偏见中的关键价值。  <br/>2. **提出多模态分析方法**：综合歌词分析（利用大语言模型）与音频信号处理技术，为音乐分类提供多元化依据。  <br/>3. **解决方法冲突问题**：识别不同分析方法间的潜在矛盾，并提出解决此类不一致性的研究方向。|
|2507.00672v1|[Toward Edge General Intelligence with Multiple-Large Language Model   (Multi-LLM): Architecture, Trust, and Orchestration](http://arxiv.org/abs/2507.00672v1)|总结：  <br/>该调研系统总结了在边缘计算中集成多大语言模型（multi-LLMs）的贡献，包括提出多LLM应用方法、揭示模型演进路径、识别关键技术挑战、强调可信系统构建、设计多模态架构及指明未来研究方向。<br/><br/>贡献点：  <br/>1. 提出多大语言模型（multi-LLMs）在边缘计算中应对复杂动态任务的应用方法。  <br/>2. 系统梳理从传统边缘AI到单LLM再到multi-LLM的模型演进路径。  <br/>3. 识别多LLM实现中的关键技术挑战：动态编排、资源调度、跨域知识迁移。  <br/>4. 强调构建可信multi-LLM系统以保障高可靠性与隐私安全。  <br/>5. 设计支持多模态数据处理的multi-LLM架构，整合文本、图像、音频等信息。  <br/>6. 指明未来研究方向：提升资源效率、解决隐私与鲁棒性问题、建立可信治理体系。|
|2506.23930v1|[Leveraging the Potential of Prompt Engineering for Hate Speech Detection   in Low-Resource Languages](http://arxiv.org/abs/2506.23930v1)|**贡献点分点总结：**  <br/>1. **首次提出隐喻提示（Metaphor Prompting）**：针对低资源语言的仇恨言论检测，开发了一种新颖的提示方法，可绕过LLMs内置的安全机制，区别于传统jailbreaking技术。  <br/>2. **系统评估六种提示策略**：在Llama2-7B模型上全面测试零样本提示、拒绝抑制、讨好分类器、多样本提示、角色提示及隐喻提示，对比多种深度学习模型（MLP/CNN/BiGRU）和词嵌入（GloVe/Word2Vec/FastText）的性能。  <br/>3. **跨语言有效性验证**：将隐喻提示方法扩展至其他低资源语言（如印地语）和高资源语言（如英语、德语），验证其普适性和适应性。  <br/>4. **引入环境影响因子（IF）评估**：将F1分数与碳排放、电力消耗、计算时间等环境指标结合，提出对模型可持续性的综合评估框架。  <br/>5. **填补低资源语言研究空白**：聚焦低资源孟加拉语，提出通过LLMs进行仇恨言论检测的解决方案，推动该领域在低资源语言上的应用。|
|2506.23774v1|[Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate   Incidents Management](http://arxiv.org/abs/2506.23774v1)|**贡献点总结（100字以内）**  <br/>提出了结合检索增强提示与人物建模的多代理LLM系统，用于模拟校园仇恨事件场景，提升教师对仇恨言论的理解及干预能力，验证了该系统在培训中的有效性。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **创新系统设计**：开发了一个多代理LLM系统，通过融合检索增强提示与角色建模技术，模拟真实情境下的仇恨事件，突破传统教师培训的时空限制。  <br/>2. **功能整合**：系统具备识别分析仇恨言论模式、预测潜在升级及生成干预策略三大核心功能，为教师提供综合性的教学工具。  <br/>3. **场景多样化**：利用人物建模与代理LLMs结合，构建具有情境多样性的模拟环境，增强教师对复杂社会动态的应对能力。  <br/>4. **实证验证**：通过试点评估，验证了该系统能有效提升教师对注释分歧的理解及上下文在仇恨言论中的作用认知，优化实际教学策略。|
|2506.23714v1|[Towards an Automated Multimodal Approach for Video Summarization:   Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](http://arxiv.org/abs/2506.23714v1)|**总结（100字以内）**：  <br/>该论文提出基于行为感知的多模态视频摘要框架，融合文本、音频与视觉线索生成时间对齐摘要，识别跨模态强调的"bonus words"以提升语义和情感相关性，并通过实验验证在文本与视频评估指标上均显著优于传统方法。<br/><br/>**贡献点**：  <br/>1. **多模态整合**：首次将文本、音频、视觉信息统一建模，生成时间对齐的视频摘要，突破传统单模态方法限制。  <br/>2. **行为感知机制**：通过提取声学特征、文本关键词和视觉指标，识别语义与情感重要的行为片段。  <br/>3. **Bonus Words创新**：提出"bonus words"概念，捕获跨模态强调的关键词，增强摘要的语义相关性与表达清晰度。  <br/>4. **对比实验验证**：在伪真实数据（pGT）上对比传统方法（如Edmundson），显着提升ROUGE-1、BERTScore及视频F1-Score（分别提升31.5%、3.8%和23%）。  <br/>5. **应用价值**：验证多模态融合在教育、职业等场景下提升视频摘要质量的潜力，为行为信息驱动的摘要生成提供新思路。|
|2506.23094v1|[TOMI: Transforming and Organizing Music Ideas for Multi-Track   Compositions with Full-Song Structure](http://arxiv.org/abs/2506.23094v1)|**总结（100字以内）:**  <br/>本研究提出TOMI方法，结合指令微调LLM实现多轨音乐生成，采用四维表示并集成REAPER，提升音乐质量和结构一致性。<br/><br/>**贡献点分点列出:**  <br/>1. **提出TOMI框架**：首次将“概念层次”引入音乐生成，系统性地建模音乐创意的生成、转化与组织过程，形成完整多轨电子音乐结构。  <br/>2. **融合指令微调的LLM**：设计基于大型语言模型（LLM）的指令微调方法，提升音乐生成过程中的逻辑与结构化能力。  <br/>3. **四维稀疏表示方法**：创新性地使用四维空间（clip、section、track、transformation）表征多轨音乐创作流程，增强对复杂音乐结构的建模精度。  <br/>4. **人机交互集成**：将TOMI模型与REAPER数字音频工作站结合，实现交互式人机协同音乐创作，拓展AI在音乐领域的应用边界。  <br/>5. **实验验证优势**：通过对比实验证明，TOMI方法在音乐质量、结构连贯性及整体协调性上均优于现有基线模型。|
|2506.23049v1|[AURA: Agent for Understanding, Reasoning, and Automated Tool Use in   Voice-Driven Tasks](http://arxiv.org/abs/2506.23049v1)|**贡献点：**  <br/>1. **首创全语音多轮对话系统**：提出AURA，首个开源支持语音到语音多轮对话、集成工具调用和自主推理的系统。  <br/>2. **开放权重模块化架构**：结合开放权重的ASR、TTS和LLM，采用级联流水线设计，支持多种工具（日历、搜索、邮件等）。  <br/>3. **自然语言驱动工具扩展**：通过自然语言提示和动作类别实现工具的灵活集成，提升系统可定制性。  <br/>4. **高性能基准测试表现**：在VoiceBench上OpenBookQA得分92.75%（超开源系统，接近GPT-4o），AlpacaEval得分4.39（与开源系统相当）。  <br/>5. **实用场景验证**：人类评估显示，AURA在复杂多轮语音任务中达到90%的成功率。  <br/><br/>**总结（100字以内）：**  <br/>AURA是首个支持多轮语音对话与工具集成的开源系统，结合开放权重模块化架构，实现高效任务处理，性能超越现有开源模型并接近GPT-4o，为语音交互与智能应用融合提供了新范式。|
|2506.22679v1|[Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions](http://arxiv.org/abs/2506.22679v1)|总结: <br/>本研究对比了不同LLM架构在团队对话微行为检测中的表现，发现解码器-only模型在指令微调后对低频微行为有更好识别效果，为太空任务等高风险场景的沟通分析提供新方法。<br/><br/>贡献点: <br/>1. 提出基于模拟太空任务对话转录的微行为检测框架，验证LLM在非语音数据场景下的应用潜力<br/>2. 对比分析编码器-only（RoBERTa/DistilBERT）与解码器-only（Llama-3.1）LLM在微行为识别任务中的表现差异<br/>3. 系统评估零样本分类、基础微调、paraphrase-augmented微调等不同微调策略的有效性<br/>4. 揭示解码器-only模型在指令微调后可实现68%的二分类F1-score，显著优于编码器-only模型<br/>5. 为高风险环境团队沟通分析及训练干预技术发展提供实证依据|
|2506.22554v2|[Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale   Dataset](http://arxiv.org/abs/2506.22554v2)|总结：该论文提出新的Seamless Interaction数据集及配套模型，实现对双人互动行为的生成与理解，在虚拟代理和人机交互领域取得突破。<br/><br/>贡献点：<br/>1. 构建首个大规模(dyadic)互动数据集（4000小时/4000参与者），涵盖多场景非言语行为数据<br/>2. 开发可输入语音与视觉行为的双人动作生成模型，集成LLM+2D/3D渲染技术<br/>3. 提出具有情绪控制和表达性调节能力的可控制模型变体<br/>4. 建立评估双人互动模型质量的方法体系，推动更自然的人机交互系统发展|
|2506.22554v1|[Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale   Dataset](http://arxiv.org/abs/2506.22554v1)|总结：  <br/>本研究提出首个大规模双人交互数据集，开发多模态生成模型及可控变体，建立评估体系，推动虚拟代理与人机交互技术发展。<br/><br/>贡献点：  <br/>1. **数据集构建**：创建Seamless Interaction数据集，包含超过4,000小时多场景真实面对面互动视频（4,000名参与者）。  <br/>2. **模型生成**：设计可生成与语音对齐的双人动作手势和面部表情的多模态模型，支持输入对话者的行为数据。  <br/>3. **可控性增强**：开发可调节情感响应、表达强度及生成语义相关手势的模型变体，提升交互灵活性。  <br/>4. **评估方法**：提出针对双人运动模型的质量评估方法，促进更自然、高效的AI交互系统实现。|
|2506.21864v2|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v2)|总结：  <br/>提出DeepTalk框架，通过自适应模态专家学习和MoE架构解决多模态语言模型的性能退化问题，保持较高准确性和低延迟，推动语音交互应用。<br/><br/>贡献点：  <br/>1. **提出DeepTalk框架**：基于MoE架构设计新型适应性模态专家学习机制，有效缓解Native MLLMs的灾难性遗忘与性能下降问题。  <br/>2. **双阶段训练策略**：结合自适应模态专家区分、单模态专项训练与多模态协同训练，优化模型对语音和文本的处理能力。  <br/>3. **性能提升**：相比传统Native MLLMs（如GLM-4-Voice）仅下降5.5%（优于平均20%），性能接近模块化MLLMs，且保留丰富的语用特征（情感、语调）。  <br/>4. **低延迟交互**：端到端对话延迟控制在0.5秒以内，显著提升语音对话的流畅性和实时性体验。  <br/>5. **开源实现**：公开完整代码与模型，便于后续研究与应用验证。|
|2506.21864v1|[DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE](http://arxiv.org/abs/2506.21864v1)|总结：  <br/>提出DeepTalk框架，通过自适应多模态专家学习与MoE架构有效缓解语音-文本数据不足导致的模型性能下降，实现与原始LLM相近的效率并保持低延迟对话体验。<br/><br/>贡献点：  <br/>1. **提出新的框架**：基于MoE架构设计DeepTalk，实现自适应多模态专家学习，解决传统Native MLLMs因数据不足导致的灾难性遗忘问题。  <br/>2. **分阶段训练策略**：通过模态负载区分专家，结合单模态特训与多模态协作训练，优化模型对语音和文本的联合处理能力。  <br/>3. **性能提升**：相比传统Native MLLMs（如GLM-4-Voice）性能下降仅5.5%，显著优于平均20%的水平，且与模块化方法性能相当。  <br/>4. **低延迟交互**：端到端对话延迟控制在0.5秒以内，提升语音交互的实时性与流畅度。  <br/>5. **开源实现**：提供代码和模型，促进方法复现与行业应用。|
|2506.20666v2|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v2)|**贡献点总结（100字以内）:**  <br/>本文提出认知模型框架，分析LLM在社交情境中的价值权衡，发现推理模型更侧重信息效用，开源模型数学推理更强，并揭示训练早期效用值显著变化，强调基础模型和预训练数据的长期影响，为优化训练策略和控制价值冲突提供新视角。<br/><br/>**分点贡献:**  <br/>1. **提出认知模型分析框架**：应用人类礼貌语言的“认知模型”理论，系统解读LLM中价值权衡的决策机制。  <br/>2. **评估两种关键模型场景**：（1）前沿黑盒模型的推理“努力”程度；（2）开源模型的RL后训练动态。  <br/>3. **揭示效用偏向性差异**：推理模型更注重信息效用而非社会效用，开源模型在数学推理能力上表现突出。  <br/>4. **发现训练早期动态影响**：模型训练初期效用值发生显著变化，且基础模型选择和预训练数据对后续表现有持久影响。  <br/>5. **推动方法论与应用价值**：方法适用于LLM快速演进趋势，为构建高阶行为假设、优化训练策略及调控价值权衡提供理论支持。|
|2506.20666v1|[Inside you are many wolves: Using cognitive models to interpret value   trade-offs in LLMs](http://arxiv.org/abs/2506.20666v1)|总结：  <br/>本研究提出利用认知模型分析LLMs在价值权衡中的表现，揭示了不同训练策略下效用分配差异，为理解模型行为、优化训练机制及控制价值冲突提供新视角。<br/><br/>贡献点：  <br/>1. **引入认知模型框架**：首次将认知科学中的价值权衡模型（如礼貌语言模型）应用于LLMs，系统分析其在社交情境中处理矛盾目标的能力。  <br/>2. **双维度模型评估**：针对前沿黑盒模型的推理"努力"程度与开源模型的RL对齐动态，提出统一的评估方法，探索不同场景下的效用函数分配机制。  <br/>3. **发现效用偏向规律**：揭示推理模型更注重信息效用而非社会效用，同时指出开源模型在数学推理上的优势，为模型能力差异提供实证依据。  <br/>4. **揭示训练动态影响**：证明模型训练早期存在显著的效用值变化，且基础模型和预训练数据选择对效用分配具有持续影响，优于反馈数据或对齐方法的作用。  <br/>5. **方法泛化与应用价值**：提出的方法可适配LLMs快速演进的生态，为推测高阶行为、设计训练策略及平衡模型价值冲突提供理论支持和实践指导。|
|2506.19835v1|[MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration](http://arxiv.org/abs/2506.19835v1)|总结：  <br/>该论文提出一种模块化多智能体框架MAM，通过角色分工优化多模态医疗诊断流程，显著提升性能，代码公开可用。<br/><br/>贡献点：  <br/>1. **框架设计**：提出模块化多智能体系统（MAM），将医疗诊断流程分解为专门角色（全科医生、专科团队、放射科医生等），提升协作效率。  <br/>2. **角色分工**：通过角色分配（如 Director 统筹、 Specialist 团队细化诊断）增强模型的诊断能力和灵活性。  <br/>3. **高效更新机制**：实现知识更新成本降低，有效整合现有医疗 LLMs 和知识库。  <br/>4. **多模态性能提升**：在包含文本、图像、音频、视频的多模态医疗数据集上，MAM 表现优于模态专用模型（性能提升 18%-365%）。  <br/>5. **开源实现**：代码公开，便利后续研究和应用。|
|2506.19732v1|[Who Does What in Deep Learning? Multidimensional Game-Theoretic   Attribution of Function of Neural Units](http://arxiv.org/abs/2506.19732v1)|总结（100字以内）:  <br/>本文提出MSA框架，通过博弈论方法量化神经网络单元对高维输出的贡献，适用于不同规模模型，揭示了正则化、语言专家分布及GAN的倒置生成层级，为深度模型的解释、编辑与压缩提供新工具。<br/><br/>贡献点：<br/>1. **提出模型无关的解释框架**：开发Multiperturbation Shapley-value Analysis (MSA)，首次将Shapley值理论扩展至量化神经网络单元对高维输出（如语音、图像、文本）的贡献。<br/>2. **实现输出维度对齐的贡献分析**：生成Shapley Modes，提供与模型输出（像素、token、logits）完全一致维度的单元贡献地图。<br/>3. **跨模型尺度验证有效性**：将方法应用于多层感知机（MLP）、大型语言模型（Mixtral-8x7B）和生成对抗网络（GAN），展示其广泛适用性。<br/>4. **揭示神经网络结构特征**：通过实验证明正则化集中计算资源、语言模型中存在语言特定专家、GAN中的生成层级存在倒置现象。<br/>5. **拓展应用潜力**：为深度模型的**解释、编辑与压缩**提供理论支持和实用工具，推动模型可理解性研究。|
|2506.19502v2|[MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications](http://arxiv.org/abs/2506.19502v2)|总结：  <br/>该论文提出MATE多模态无障碍多智能体系统，支持模态转换、多模型兼容、本地运行保障隐私，以及与机构技术集成的实时辅助功能，显著提升残障人士数字交互体验。<br/><br/>贡献点：  <br/>1. **提出MATE系统**：首个多模态无障碍多智能体系统（MAS），通过用户需求驱动的模态转换（如图像转音频描述）解决数据可理解性问题。  <br/>2. **多领域适用性**：可广泛应用于医疗、教育等场景，适应不同用户群体和行业需求。  <br/>3. **模块化架构**：支持LLM API调用、自定义ML分类器等多种模型，增强灵活性与硬件兼容性。  <br/>4. **本地运行机制**：确保敏感信息隐私与安全，适用于需离线操作的场景。  <br/>5. **任务识别模型**：开发ModCon-Task-Identifier，精准提取用户输入中的模态转换任务，实验验证性能优于其他模型。  <br/>6. **开源共享**：提供代码和数据，促进技术复用与研究透明。|
|2506.19502v1|[MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications](http://arxiv.org/abs/2506.19502v1)|总结：  <br/>本文提出MATE多模态无障碍MAS系统，支持灵活模态转换和本地部署，提升残疾人数字交互体验，并引入精准的ModCon-Task-Identifier模型，实现跨领域应用与隐私保护。<br/><br/>贡献点：  <br/>1. **开发开源多模态MAS系统**：MATE支持模态转换（如图像转音频描述），解决传统系统封闭设计导致的定制化不足问题。  <br/>2. **跨领域灵活性**：兼容多种模型（LLM API、自定义ML分类器）与硬件，适应不同行业（如医疗）和用户需求。  <br/>3. **本地运行保障隐私**：确保敏感信息在本地处理，保密性强，适用于机构技术集成（如医疗系统）。  <br/>4. **任务识别模型创新**：提出ModCon-Task-Identifier，可精准提取用户输入的模态转换任务，实验表现优于其他模型。  <br/>5. **开源代码与数据**：提供公开资源，促进研究复现与应用拓展。|
|2506.19073v1|[MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral   Reasoning of LLMs through Hate Speech Multi-hop Explanation](http://arxiv.org/abs/2506.19073v1)|总结：  <br/>提出多语言道德推理评估数据集MFTCXplain，揭示当前LLMs在道德情感预测和跨文化解释上的局限性，推动更透明、文化包容的道德评估研究。<br/><br/>贡献点：  <br/>1. **多语言道德推理基准**：构建包含葡萄牙语、意大利语、波斯语和英语的3,000条推文数据集，支持跨文化道德能力评估。  <br/>2. **细粒度标注体系**：引入二元仇恨言论标签、道德类别及文本片段级解释，提升评估的透明度与可解释性。  <br/>3. **理论驱动设计**：基于道德基础理论（MFT）设计任务，明确道德推理的评估框架。  <br/>4. **揭示模型局限性**：通过实证发现LLMs在道德情感预测（F1<0.35）和非主流语言解释对齐上表现显著不足，强调其内化人类道德推理的能力有限。|
|2506.18576v1|[A Modular Taxonomy for Hate Speech Definitions and Its Impact on   Zero-Shot LLM Classification Performance](http://arxiv.org/abs/2506.18576v1)|**贡献点总结（100字以内）**  <br/>本文提出仇恨言论定义的分类体系，分析其14个核心要素，并通过零样本实验评估不同定义对LLM性能的影响，揭示定义差异对模型效果的非一致性影响。<br/><br/>**分点贡献：**  <br/>1. **理论贡献**  <br/>   - 构建了一个包含14个概念元素的仇恨言论定义分类体系（Taxonomy），系统整理了文献中对仇恨言论的多维度表述（如目标类型、后果等），解决其定义模糊问题。  <br/><br/>2. **实验贡献**  <br/>   - 首次在三种仇恨言论数据集（合成、人工参与、真实场景）上，对三类大语言模型（LLMs）进行基于不同定义的零样本性能评估，发现定义的特异性程度对模型效果存在显著但非一致的影响。|
|2506.18274v1|[Leveraging Large Language Models for Information Verification -- an   Engineering Approach](http://arxiv.org/abs/2506.18274v1)|总结：  <br/>提出基于GPT-4o的自动化多媒体新闻源验证方法，整合多模态数据处理与交叉验证技术，减少人工干预。<br/><br/>贡献点：  <br/>1. **多模态数据整合**：首次将图像、视频、音频等多类型数据纳入新闻源验证流程，提升信息全面性。  <br/>2. **LLM为核心架构**：采用GPT-4o作为验证管道的骨干模型，实现端到端自动化处理。  <br/>3. **流程自动化**：通过提示工程完全自动化元数据生成、数据分帧、特征筛选及交叉验证等步骤。  <br/>4. **关键帧筛选机制**：开发基于重要性排序的帧选择策略，优化视频分析效率。  <br/>5. **轻量人工干预**：仅需人类参与最终结果验证，显著降低人工成本与操作复杂性。|
|2506.17715v1|[Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource   Medieval Romance Languages](http://arxiv.org/abs/2506.17715v1)|总结：  <br/>本研究系统分析中世纪罗曼语词性标注的挑战，评估多种技术方法，揭示LLMs在历史语言处理上的局限性，并探索有效解决方案，涵盖多领域与多语言文本。<br/><br/>贡献点分点：  <br/>1. **系统性研究历史语言变体挑战**：首次对中世纪奥克语、西班牙语、法语文本的词性标注进行多维度分析，揭示历时性语言演变、拼写非标准化和数据稀缺等问题对模型性能的影响。  <br/>2. **多技术方法对比实验**：系统评估微调、提示工程、模型架构、解码策略及跨语言迁移学习等技术对词性标注准确率的影响，提供方法选择的实证依据。  <br/>3. **多领域与多语言覆盖**：基于包含宗教、圣徒传、医学、饮食等领域的跨语言语料库（Medieval Occitan、Spanish、French），验证技术的泛化能力与适应性。  <br/>4. **提出针对性解决方案**：发现特定技术（如跨语言迁移学习）在低资源历史语言场景中的有效性，为优化历史语言处理提供新思路。|
|2506.17410v1|[Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A   Feasibility Study](http://arxiv.org/abs/2506.17410v1)|**贡献点总结：**  <br/>1. 提出利用生成式AI分析真实数学辅导的可行性与可扩展性方法；  <br/>2. 识别并评估"有效表扬"和"纠正学生数学错误"两项关键辅导行为；  <br/>3. 验证多模型（GPT-4系列、Gemini-1.5-pro、LearnLM）在辅导行为检测与评估中的可靠性；  <br/>4. 设计成本效益高的提示策略以提升AI在教育评估中的实用性；  <br/>5. 促进LLM在AI支持学习研究中的可复现性与实际应用价值。<br/><br/>**研究摘要总结（100字以内）：**  <br/>开发生成式AI方法分析真实数学辅导行为，验证多模型在检测表扬与纠错效果上的可靠性，提出高效提示策略，推动AI在教育评估中的可扩展应用与研究创新。|
|2506.16575v1|[Advancing Harmful Content Detection in Organizational Research:   Integrating Large Language Models with Elo Rating System](http://arxiv.org/abs/2506.16575v1)|**贡献点总结（100字内）：**  <br/>提出基于Elo评分的改进方法，提升LLM在有害内容分析中的性能，通过两个数据集验证其在准确性、精确率与F1分数上的优势，减少误报并增强可扩展性，支持职场骚扰检测与构建安全工作环境等组织应用。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：设计Elo评分机制，优化LLM对有害内容（如微歧视、仇恨言论）的分析能力，解决传统系统过度审查的问题。  <br/>2. **数据集验证**：在微歧视检测和仇恨言论分析的两个数据集上验证方法有效性，证明其优于传统提示技术与常规机器学习模型。  <br/>3. **性能提升**：显著提高关键指标（准确率、精确率、F1分数）表现，减少误报并增强结果可靠性。  <br/>4. **可扩展性**：支持大规模数据集处理，提升实际应用的可行性。  <br/>5. **应用价值**：推动组织研究中职场骚扰识别、毒性沟通评估及安全环境构建等实际场景的应用。|
|2506.16528v1|[Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility   Metrics Using Phonetic, Semantic, and NLI Approaches](http://arxiv.org/abs/2506.16528v1)|**贡献点：**  <br/>1. **提出新的ASR评估指标**：整合自然语言推理（NLI）评分、语义相似性和语音相似性，克服传统指标（如WER、CER）对语音可懂度的不足。  <br/>2. **验证有效性**：在Speech Accessibility Project数据集上实现与人类判断的0.890高相关性，证明该指标优于现有方法。  <br/>3. **强调评估方向**：指出应优先关注语音内容的可懂度而非单纯依赖错误率，为语音识别系统优化提供理论依据。  <br/><br/>**总结：**  <br/>本研究提出一种结合NLI、语义和语音相似性的新型ASR评估指标，显著提升与人类判断的相关性，推动语音可懂度评估从错误率向内容理解转变。|
|2506.16008v1|[ChatAR: Conversation Support using Large Language Model and Augmented   Reality](http://arxiv.org/abs/2506.16008v1)|**贡献点总结：**  <br/>本研究提出了一种结合HMD-AR与LLMs的实时对话支持系统，通过关键词识别、信息生成与可视化增强交流效率；创新性地引入眼动控制策略，避免暴露用户阅读行为；实验验证了该系统在提升对话平衡性与互动兴奋度方面的有效性。  <br/><br/>**分点贡献：**  <br/>1. **系统设计**：构建集成HMD-AR与LLMs的实时对话辅助系统，实现关键词识别、信息生成及动态可视化呈现。  <br/>2. **眼动优化方法**：开发基于自然眼动模式的信息展示策略，降低用户阅读行为被对话方察觉的风险。  <br/>3. **实验验证**：通过两组对照实验，证实所提方法能提升对话平衡性与用户感知的交流兴奋度。|
|2506.14973v1|[Thinking in Directivity: Speech Large Language Model for Multi-Talker   Directional Speech Recognition](http://arxiv.org/abs/2506.14973v1)|**贡献点总结**  <br/>（100字以内）：  <br/>提出directional-SpeechLlama模型，结合智能眼镜麦克风阵列实现定向语音识别、声源定位和旁瓣干扰抑制，创新性引入S-DOT和CDDA技术，有效提升空间音频理解能力。  <br/><br/>**分点贡献**  <br/>1. **首个结合麦克风阵列的定向Speech LLM**：  <br/>   利用智能眼镜的多通道麦克风阵列，首次实现针对声源方向的语音识别、定位及旁瓣干扰抑制，突破传统Speech LLM对空间信息处理的局限。<br/><br/>2. **创新性训练方法S-DOT**：  <br/>   提出串联方向输出训练（Serialized Directional Output Training），通过结构化方向信息增强模型对文本与空间音频关系的建模能力。<br/><br/>3. **对比方向数据增强技术CDDA**：  <br/>   引入对比方向数据增强（Contrastive Direction Data Augmentation），通过对比学习策略提升模型在复杂声学环境下的鲁棒性与泛化能力。<br/><br/>4. **实验验证性能优势**：  <br/>   在语音识别和声源定位任务中，模型表现出显著性能提升，证明了文本线索与空间音频关系建模的有效性。|
|2506.14903v1|[DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct   Preference Optimization](http://arxiv.org/abs/2506.14903v1)|总结（100字以内）：  <br/>本文提出DPO-Kernels，通过混合损失、核化表示和分歧选择三个方向提升T2I模型的对齐性能，并构建首个包含社会偏见数据的DETONATE基准，引入AQI指标评估对齐质量，验证HT-SR机制的有效性，公开数据与代码。<br/><br/>贡献点分点：  <br/>1. **DPO-Kernels框架**：首次将DPO思想扩展至T2I系统，提出混合损失（结合嵌入与概率目标）、核化表示（RBF、多项式、小波核）和分歧选择（Wasserstein与Rényi分歧）三重改进，提升生成内容的对齐性与鲁棒性。  <br/>2. **DETONATE基准**：构建首个大规模T2I对齐基准，包含10万对精心筛选的图像对，系统性涵盖种族、性别、残疾三种社会偏见类别，用于评估模型公平性与安全性的隐性漏洞。  <br/>3. **AQI指标**：提出基于几何度量的对齐质量指数（Alignment Quality Index），量化潜在空间中安全/危险图像激活的分离程度，揭示T2I模型的隐藏偏见问题。  <br/>4. **HT-SR机制**：通过实验证明DPO-Kernels结合重尾自正则化（Heavy-Tailed Self-Regularization）可维持强泛化边界，并公开完整代码与数据集以促进研究复现。|
|2506.14028v2|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v2)|总结：  <br/>提出首个多语言多模态金融基准MultiFinBen，包含新颖的跨语言与OCR嵌入任务，设计动态难度选择机制，并揭示现有LLMs在复杂金融场景下的局限性。<br/><br/>贡献点：  <br/>1. **构建首个全球化金融多模态基准**：MultiFinBen是首个支持多语言（中、英、西等）和多模态（文本、视觉、音频）的金融领域基准，全面评估LLMs的跨模态与跨语言能力。  <br/>2. **引入双跨任务**：设计PolyFiQA-Easy/PolyFiQA-Expert（多语言复杂推理任务）和EnglishOCR/SpanishOCR（OCR嵌入的金融问答任务），填补金融领域多语言与多模态任务的空白。  <br/>3. **动态难度筛选机制**：提出基于难度感知的样本选择方法，而非简单合并现有数据集，确保基准数据集的平衡性与有效性。  <br/>4. **揭示模型局限性**：通过评估22个SOTA模型，发现即便具备多模态能力的LLMs仍难以处理金融领域的复杂跨语言和跨模态任务。  <br/>5. **开放数据促进研究**：公开MultiFinBen数据集，推动金融NLP和语音应用研究的透明性、可复现性与全球化发展。|
|2506.14028v1|[MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark   for Financial LLM Evaluation](http://arxiv.org/abs/2506.14028v1)|总结：  <br/>本文提出首个多语言多模态金融基准MultiFinBen，包含新任务与难度评估机制，为金融领域模型评估提供全面且挑战性的测试平台。<br/><br/>贡献点：  <br/>1. **首创多语言多模态基准框架**：构建首个覆盖文本、视觉、音频三模态及多语言场景的金融领域基准MultiFinBen，填补现有单语单模态研究的不足。  <br/>2. **引入双新颖任务**：  <br/>   - *PolyFiQA*（易/专家级）：首次提出多语言金融复杂推理任务，支持混合语言输入；  <br/>   - *EnglishOCR/SpanishOCR*：首次设计OCR嵌入的金融问答任务，要求模型从视觉文本中提取与推理信息。  <br/>3. **动态难度选择机制**：提出基于模型能力的难度自适应筛选方法，确保基准的挑战性与公平性。  <br/>4. **精炼平衡数据集**：通过精细化数据筛选而非简单数据集拼接，构建紧凑且跨领域平衡的测试集合。  <br/>5. **揭示模型能力瓶颈**：实验证明主流模型在多模态与跨语言金融任务中存在显著性能局限，为后续研究提供方向。|
|2506.13894v1|[EmoNews: A Spoken Dialogue System for Expressive News Conversations](http://arxiv.org/abs/2506.13894v1)|**总结**  <br/>该论文提出了一种基于上下文情感调节的新闻对话语音系统，结合LLM与PromptTTS技术，设计新评估指标，并开源代码。<br/><br/>**贡献点**  <br/>1. **开发整合情感调控的SDS**：首次将情感分析与任务导向的语音对话系统结合，通过上下文线索生成更具同理心的新闻对话。  <br/>2. **创新情感语音合成技术**：提出基于PromptTTS的上下文匹配情感语音生成方法，提升对话的情感适配性。  <br/>3. **构建主观评估体系**：设计专门用于情绪调控的主观评价量表，填补社会目标评估的空白，并验证系统效果。  <br/>4. **开源实现与可复现性**：提供完整代码库，支持相关研究的复现与扩展。|
|2506.13252v1|[Vector Ontologies as an LLM world view extraction method](http://arxiv.org/abs/2506.13252v1)|总结：  <br/>首次提出并验证向量本体方法，构建音乐领域的高维几何框架，并证明LLM内部可解析的结构化音乐知识与真实数据的对齐性及提示语相关性。<br/><br/>贡献点：  <br/>1. **首个实证验证**：首次通过实验验证向量本体框架（vector ontologies）可将LLM的高维神经表示转化为可解释的几何结构。  <br/>2. **构建音乐领域向量本体**：基于Spotify音频特征构建8维音乐流派向量本体，提供具体领域的应用案例。  <br/>3. **内部分布一致性分析**：证明LLM的音乐世界模型在不同语言提示下仍保持高空间一致性，且与真实音频特征分布高度对齐。  <br/>4. **提示语与空间变化的关联**：揭示提示语措辞直接影响LLM推理的向量本体空间位置，证明方法的可控性和可验证性。  <br/>5. **方法应用前景**：提出向量本体作为提取和分析LLM内部结构化知识的有效工具，推动可解释AI在语音领域的应用。|
|2506.12936v1|[CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team   Reflection in Action During Clinical Operation](http://arxiv.org/abs/2506.12936v1)|**总结（100字以内）：**  <br/>本研究提出CliniDial数据集，包含多模态临床操作数据，通过实验揭示现有大模型在处理真实临床场景中面临的挑战，推动相关算法发展，并开源代码促进研究。<br/><br/>**贡献点：**  <br/>1. **构建多模态临床操作数据集**：首次整合手术模拟中的音频、转录、生理信号及双视角视频，涵盖标签不平衡、自然交互等复杂特性。  <br/>2. **提出行为代码标注框架**：基于现有方法对团队协作过程进行结构化标注，为分析临床对话与行为模式提供标准化工具。  <br/>3. **评估LLMs在临床场景中的表现**：系统测试现有大模型对CliniDial数据的处理能力，揭示其在真实临床任务中的局限性。  <br/>4. **开源数据与代码**：推动数据共享，促进语音与多模态技术在医疗领域的研究与应用。|
|2506.12744v1|[Rethinking Hate Speech Detection on Social Media: Can LLMs Replace   Traditional Models?](http://arxiv.org/abs/2506.12744v1)|总结（100字以内）:  <br/>本文提出IndoHateMix数据集，验证了大语言模型在处理多语言、混合语言仇恨言论检测中的优势，挑战传统BERT模型的主导地位，并探讨未来研究应侧重模型优化还是数据增强。<br/><br/>贡献点分点列出：<br/>1. **构建多语言仇恨言论数据集**：提出IndoHateMix，首次系统性收录印度语境下的印地语-英语混合语言（code-mixing）和转写（transliteration）文本，填补了该领域的高质量标注数据空白。<br/>2. **验证大语言模型性能优势**：实验证明LLMs（如LLaMA-3.1）在少量数据微调下仍显著优于专门训练的BERT模型，展现更强的泛化能力与跨语言适应性。<br/>3. **揭示模型发展新方向**：提出未来研究需在模型优化与数据多样性之间权衡的思考，引发对通用LLMs替代领域专用模型的讨论。<br/>4. **推动复杂场景下的NLP应用**：为评估多语言环境下仇恨言论检测的鲁棒性提供现实基准，促进应对非正式、文化嵌入性语言的NLP技术发展。|
|2506.12537v1|[Speech-Language Models with Decoupled Tokenizers and Multi-Token   Prediction](http://arxiv.org/abs/2506.12537v1)|总结：  <br/>本文提出解耦分词器设计、多token预测机制及speaker-aware生成范式，优化语音-文本跨模态对齐与生成效率，建立RoleTriviaQA基准测试，显著提升语音语言模型的性能与说话人一致性。<br/><br/>贡献点：  <br/>1. **系统分析SLM关键组件**：首次系统研究语音分词器、语音头、说话人建模等组件对LLM-centric语音模型的影响。  <br/>2. **解耦分词器设计**：提出耦合、半解耦、全解耦分词器对比实验，证明解耦分词可显著提升跨模态对齐与语音合成质量。  <br/>3. **多token预测机制**：引入MTP解决语音与文本信息密度差异，实现12倍解码速度提升与3.01的词错误率降低。  <br/>4. **Speaker-aware生成范式**：构建RoleTriviaQA大型基准测试，涵盖多样说话人身份，提升知识理解与说话人一致性。|
|2506.12502v1|[Towards Fairness Assessment of Dutch Hate Speech Detection](http://arxiv.org/abs/2506.12502v1)|**贡献点：**  <br/>1. **构建荷兰语社会群体术语列表**：首次系统梳理反映荷兰社会背景的歧视性社会群体术语，为后续研究提供基础数据。  <br/>2. **生成反事实数据**：利用大语言模型（LLMs）和MGS、SLL策略构建荷兰语仇恨言论的反事实数据集，解决语言特定挑战。  <br/>3. **微调模型提升检测性能**：通过反事实数据微调Transformer模型，验证其在荷兰语仇恨言论检测任务中的有效性。  <br/>4. **提出综合公平性评估框架**：引入CTF与群体公平性指标（equality of odds、demographic parity），量化模型在反事实公平性与群体层面的公平表现。  <br/><br/>**总结：**  <br/>该研究聚焦荷兰语仇恨言论检测，提出反事实公平性评估方法，构建语言特定数据集，验证模型性能，填补文献空白并提供改进建议。|
|2506.11842v2|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v2)|**贡献点总结（100字以内）：**  <br/>提出PACE-ADS框架，通过多代理协作实现自动驾驶系统的人机双向通信，提升舒适度、安全性和个性化体验，可扩展集成于现有平台，弥补技术自主与人本需求的鸿沟。<br/><br/>**分点贡献：**  <br/>1. **提出人机中心自主框架**：PACE-ADS通过整合心理学与认知模型，解决自动驾驶车辆缺乏双向人机交互的问题，提升个性化体验和应对复杂场景的能力。  <br/>2. **三代理协同机制**：Driver Agent处理外部环境，Psychologist Agent解析心理信号（如EEG、心率）与语音指令，Coordinator Agent整合信息生成高阶决策，增强系统响应性。  <br/>3. **分层架构设计**：在语义规划层部署，保留低级控制权给原系统，确保兼容性与灵活性，避免对现有模块的重构。  <br/>4. **多场景验证**：通过闭环模拟测试，在复杂交通场景（如交叉路口、行人交互）中验证框架的有效性，展示提升的舒适度与安全性。  <br/>5. **最小化集成改造**：框架可无缝集成至现有自动驾驶平台，减少对原有系统的调整，提高实际部署的可行性。|
|2506.11842v1|[Your Ride, Your Rules: Psychology and Cognition Enabled Automated   Driving Systems](http://arxiv.org/abs/2506.11842v1)|总结：  <br/>本文提出PACE-ADS框架，通过整合语音等认知指令与心理状态感知，实现人机协同的自动驾驶系统，提升舒适度与安全性，展示大语言模型在人车交互中的应用潜力。<br/><br/>贡献点：  <br/>1. **提出跨模态人车交互框架**：首次构建融合外部交通环境感知与内部乘客心理/认知状态理解的人机协同自动驾驶系统（PACE-ADS），填补自动化驾驶与人类需求的gap。  <br/>2. **设计语音认知代理模块**：开发专门处理乘客语音指令（如语义理解）与心理信号（如EEG、心率）的Psychologist Agent，实现多模态意图解析。  <br/>3. **实现行为级自主决策**：通过Coordinator Agent整合感知信息，支持自主驾驶逻辑生成与安全恢复策略，无需替代传统模块。  <br/>4. **验证语音引导有效性**：在交通信号、行人、施工区等场景的仿真测试中，证明语音交互可提升驾驶风格适应性与用户舒适度。  <br/>5. **推动LLM应用落地**：探索基于大语言模型（LLM）的框架在自动驾驶领域的人类中心化驱动潜力，为未来人车协作提供新思路。  <br/><br/>（注：原文实际属于自动驾驶领域，但按语音视角提炼了其中与语音指令处理、多模态交互、LLM应用等相关内容作为贡献）|
|2506.11558v2|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v2)|**贡献点：**<br/>1. **提出Temporal-aware Fuseformer架构**：采用分层双流结构，逐步捕捉多模态的时间动态信息，并有效融合视觉与听觉特征。  <br/>2. **设计全局残差机制**：减少空间冗余的同时保留关键语义信息，提升计算效率与模型性能。  <br/>3. **四阶段渐进训练方法**：分步骤增强多模态对齐、语义接地和时间推理能力，系统性优化模型表现。  <br/>4. **构建时序语义数据集**：通过GPT生成带时序对齐的问答对，扩展现有数据集用于时间监督任务。  <br/>5. **实验证明优越性**：在视频问答与时间定位任务中超越现有方法，尤其在精确时间对齐与推理上表现突出，为高效视频语言建模提供新方向。  <br/><br/>**总结：**  <br/>DaMO通过分层双流与全局残差结构、四阶段训练及GPT增强数据集，在视频语言任务中显著提升时间推理能力，推动数据高效模型发展。|
|2506.11558v1|[DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning   with Video LLMs](http://arxiv.org/abs/2506.11558v1)|**贡献点总结**（100字以内）:  <br/>提出DaMO数据高效视频语言模型，创新性采用分层双流时序感知架构与全局残差机制，设计四阶段渐进训练方法，构建增强数据集，并在时序任务中实现性能突破。  <br/><br/>**分点贡献**:  <br/>1. **模型设计**：提出DaMO，专为准确时序推理和多模态理解优化，提升数据效率。  <br/>2. **架构创新**：首创Temporal-aware Fuseformer，采用分层双流结构，分别捕捉视频/音频时序动态，有效融合多模态信息。  <br/>3. **效率优化**：引入全局残差机制，减少空间冗余的同时保留关键语义信息。  <br/>4. **训练方法**：提出四阶段渐进式训练范式，逐步强化多模态对齐、语义 grounding 和时序推理能力。  <br/>5. **数据贡献**：构建多个增强数据集，基于现有数据集添加GPT生成的时序相关问答对，支持时序监督任务。  <br/>6. **实验验证**：在时序定位和视频问答基准测试中，DaMO显著优于现有方法，尤其在精确时序对齐任务中表现突出。|
|2506.11125v1|[ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone   Scams](http://arxiv.org/abs/2506.11125v1)|**贡献点分点列出：**  <br/>1. **识别关键漏洞**：首次定位语音钓鱼（vishing）攻击链中的ASR转录环节为最大薄弱点，指出其对攻击成功的关键作用。  <br/>2. **提出ASRJam框架**：设计了一种主动防御系统，通过向目标音频注入对抗扰动，干扰攻击者的ASR并切断攻击反馈循环。  <br/>3. **改进对抗扰动方法**：提出EchoGuard，利用自然语音现象（如回声、混响）作为扰动源，实现对ASR的破坏性干扰，同时保持人类语音可懂性。  <br/>4. **实证评估**：通过39人用户实验验证EchoGuard的实用性，对比现有攻击技术，证明其在ASR干扰与用户体验平衡上的最优性。  <br/><br/>**总结（100字以内）：**  <br/>论文提出针对语音钓鱼的双防御策略：ASRJam主动干扰ASR转录，EchoGuard利用自然扰动阻断攻击而不影响人声理解。通过用户实验验证了EchoGuard在攻防效果与可用性上的优越性。|
|2506.10789v1|[FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](http://arxiv.org/abs/2506.10789v1)|**贡献点总结**（100字以内）:  <br/>本研究提出首个美国社会背景下的Neo-fascist文本编码方案，结合NLP与政治科学，构建大规模标注数据集并开发分类模型，揭示该言论在论坛中的广泛存在，强调社会背景的重要性，呼吁持续对抗以维护民主。<br/><br/>**分点贡献**:<br/>1. **首创编码方案**：提出首个针对美国社会语境的Neo-fascist话语编码体系，由政治科学学者主导，填补了NLP与政治学在该领域的交叉研究空白。  <br/>2. **跨学科方法论**：首次将政治学与NLP结合，构建系统性框架以识别和分析Neo-fascist言论，推动多学科协作研究。  <br/>3. **大规模标注数据**：通过众包技术标注1000条文本，创建首个公共可用的Neo-fascist话语标注数据集，为后续研究提供基础。  <br/>4. **模型验证与对比**：对SLMs和LLMs分别进行微调与测试，开发首个可用于Neo-fascist文本分类的模型，验证不同规模模型的效果差异。  <br/>5. **现象分析与警示**：揭示Neo-fascist言论在特定论坛中的普遍性，为理解其传播特征和对民主社会的威胁提供实证依据。  <br/>6. **研究伦理说明**：明确研究仅针对文本内容，不涉及对个人或组织的标签化，强调方法的中立性与合规性。|
|2506.10779v1|[Improving Named Entity Transcription with Contextual LLM-based Revision](http://arxiv.org/abs/2506.10779v1)|**贡献点：**  <br/>1. **提出LLM修订机制**：设计基于大语言模型（LLM）的命名实体修正方法，结合LLM的推理能力和局部上下文信息（如课程笔记）改进ASR对命名实体的识别。  <br/>2. **引入NER-MIT-OpenCourseWare数据集**：发布包含45小时MIT课程音频的开放数据集，支持命名实体识别任务的开发与测试。  <br/>3. **显著性能提升**：在所提出数据集上，该技术使命名实体的词错误率（WER）降低最高达30%，验证了方法的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出LLM驱动的命名实体修正方法，结合上下文信息提升ASR准确性，并发布专用数据集，实验证明在命名实体识别任务中WER降低30%。|
|2506.10504v1|[Beyond Single-User Dialogue: Assessing Multi-User Dialogue State   Tracking Capabilities of Large Language Models](http://arxiv.org/abs/2506.10504v1)|**贡献点分点总结：**  <br/>1. **构建多用户对话数据集**：基于言语行为理论，扩展现有DST数据集，生成第二用户的话语，模拟真实多用户交互场景。  <br/>2. **提出可控评估框架**：系统性地将第二用户话语融入对话，设计方法论以评估LLMs在多用户DST中的鲁棒性。  <br/>3. **揭示性能局限性**：实验表明，LLMs在多用户DST任务中表现显著下降，凸显其在处理多重说话者时的不足。  <br/>4. **指导未来研究方向**：强调需优化LLMs以应对多用户场景，推动更真实、鲁棒的对话状态跟踪模型发展。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过构建多用户DST数据集并设计可控评估框架，揭示了LLMs在处理复杂多用户对话中的性能局限，为未来改进多用户对话理解模型提供了方向。|
|2506.10245v1|[ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in   Portuguese](http://arxiv.org/abs/2506.10245v1)|**贡献点总结（100字以内）**  <br/>1. 构建首个大规模葡萄牙语细粒度仇恨言论语料库，覆盖九个法律保护少数群体。  <br/>2. 提出四阶段合成数据生成流程，包含人工种子、少样本扩展、改写增强和领域平衡。  <br/>3. 实现跨领域泛化能力，验证在多种任务和数据集上的有效性。  <br/>4. 公开数据促进低资源环境下合成数据和仇恨言论检测研究。|
|2506.10202v1|[Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual   Text-to-Video Retrieval](http://arxiv.org/abs/2506.10202v1)|**贡献点：**<br/>1. 提出Q2E方法：首个针对复杂现实事件的零样本多语言文本到视频检索框架，通过LLMs和VLMs提取隐式参数知识实现查询-事件分解。  <br/>2. 多模态输入兼容：支持视觉和语音输入的联合处理，展示跨模态知识融合的有效性。  <br/>3. 熵融合策略：引入熵基融合评分机制，提升零样本多模态信息整合能力。  <br/>4. 音频增强效果：验证音频信息在文本到视频检索中的显著提升作用。  <br/>5. 可泛化性：方法可适应不同数据集、领域及模型架构（LLMs/VLMs）。  <br/>6. 开源贡献：公开代码与数据，推动后续研究。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Q2E，通过LLMs和VLMs提取隐式知识实现零样本多语言文本到视频检索，支持视觉/语音输入融合，并验证音频信息的增强效果。方法具备良好的泛化性，已开源促进研究。|
|2506.09983v2|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v2)|**贡献点总结**  <br/>（100字以内）  <br/>提出分步指令策略与简化输出格式，显著提升多语言依赖分析性能，避免幻觉污染；揭示显式推理步骤对跨语言泛化的重要性，为LLM解析提供可扩展、格式一致的替代方案。<br/><br/>**详细贡献点**  <br/>1. **分步推理策略**：首次将通用词性标注作为预处理步骤，优先预测句法主语和依存标签，提升结构有效性与准确性。  <br/>2. **输出格式优化**：设计简化的CoNLL-U格式，避免传统基于括号的解析方法的污染与冗余问题。  <br/>3. **多语言微调增强**：证明多语言微调可同时提升跨语言泛化性能，扩大模型适用范围。  <br/>4. **SOTA性能验证**：在17种语言的Universal Dependencies数据集上实现当前最优的解析准确率，无幻觉或污染。  <br/>5. **可扩展性方案**：提供格式一致、易于扩展的解析框架，替代传统基于括号的Approach，推动LLM在语音领域的应用。|
|2506.09983v1|[Step-by-step Instructions and a Simple Tabular Output Format Improve the   Dependency Parsing Accuracy of LLMs](http://arxiv.org/abs/2506.09983v1)|总结（100字以内）:  <br/>提出一种基于逐步指令策略的依存解析方法，结合通用词性标注和简化输出格式，在17种语言中实现SOTA性能，同时通过多语言微调提升跨语言泛化能力，验证了显式推理步骤对LLM解析的有效性。<br/><br/>贡献点:  <br/>1. **创新的逐步指令策略**：首次将通用词性标注作为先验步骤，结合句法头和依存关系预测，提升解析的结构有效性与准确性。  <br/>2. **简化输出格式设计**：采用类似CoNLL-U的轻量化输出格式，减少冗余信息干扰，避免模型生成错误或污染（hallucination）。  <br/>3. **多语言性能突破**：在17种语言的Universal Dependencies数据集上均取得领先的准确率，证明方法的普适性。  <br/>4. **跨语言泛化优化**：通过多语言微调，显著提升模型在不同语言间的迁移能力，增强通用性。  <br/>5. **方法对比与可扩展性**：提出一种格式一致、可扩展的替代方案，优于传统基于括号的解析方法，推动下游任务的兼容性。|
|2506.09707v2|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v2)|**总结（100字以内）:**  <br/>本研究提出一种基于预训练模型和LoRA微调的自动PE疗法依从性评估方法，通过音频-文本分析精准定位治疗关键阶段，显著提升效率，具有临床培训与质量监控的应用潜力。<br/><br/>**贡献点：**  <br/>1. **方法创新**：首次将LoRA技术应用于PE疗法依从性评估，通过30秒音频-转录窗口微调Qwen2-Audio模型，实现关键环节的自动时间定位。  <br/>2. **标签生成机制**：结合LLM提示与人工评分者验证，生成并验证三阶段（P1/P2/P3）依从性标签，提升标注准确性。  <br/>3. **软监督策略**：提出基于任务提示的软监督训练框架，直接预测归一化边界偏移，优化模型适应性。  <br/>4. **参数分析研究**：系统评估窗口大小及LoRA秩对性能的影响，揭示上下文粒度与模型调优的重要性。  <br/>5. **应用价值**：构建可扩展的PE疗法依从性追踪框架，为临床培训、监督及质量保证提供自动化工具。|
|2506.09707v1|[Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal   Localization of Prolonged Exposure Therapy Elements](http://arxiv.org/abs/2506.09707v1)|总结（100字以内）:  <br/>提出基于预训练模型和LoRA技术的自动PE治疗遵循定位方法，实现高精度识别与可扩展的临床应用。<br/><br/>贡献点:  <br/>1. **方法创新**：首次将LoRA技术应用于预训练音频-语言模型（Qwen2-Audio）的微调，实现对PE治疗关键环节的自动时间定位。  <br/>2. **标签生成**：通过LLM提示和人工验证结合，为三个核心协议阶段（P1/P2/P3）生成高质量的遵循标签。  <br/>3. **软监督策略**：设计任务特定提示引导的软监督框架，提升边界预测的准确性（MAE为5.3秒）。  <br/>4. **参数分析**：系统研究窗口大小与LoRA秩对性能的影响，揭示上下文粒度与模型适配的关键作用。  <br/>5. **实际应用**：构建可扩展的框架，支持PE治疗的临床培训、监督与质量评估，解决传统人工评估的效率问题。|
|2506.09391v1|[Comparing human and LLM politeness strategies in free production](http://arxiv.org/abs/2506.09391v1)|总结：  <br/>本研究揭示大语言模型在礼貌策略上虽能复制人类偏好并被偏好，但存在过度依赖消极策略、导致误解的偏差，突显AI系统在语用对齐中的关键挑战。<br/><br/>贡献点：  <br/>1. 系统分析LLM与人类在礼貌策略使用上的差异，验证大规模模型（≥70B参数）可复制计算语用学关键偏好。  <br/>2. 发现人类评估者在开放性任务中更倾向接受LLM生成的礼貌回应，表明模型在部分场景下的表现接近人类。  <br/>3. 指出LLM在积极语境中过度依赖消极礼貌策略，可能引发语用误解，揭示模型行为的局限性。  <br/>4. 强调语用对齐在AI系统中的重要性，为未来研究提供方向性启示。|
|2506.09349v1|[OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment](http://arxiv.org/abs/2506.09349v1)|总结（100字以内）:  <br/>提出OmniDRCA并行语音-文本模型，采用双分辨率表示和对比对齐，实现SOTA性能并在全双工对话场景中展示应用潜力。<br/><br/>贡献点：  <br/>1. 提出OmniDRCA模型，基于联合自回归建模实现并行生成语音和文本，突破传统分离式生成的局限。  <br/>2. 引入双分辨率语音表示（高低频特征分开编码），提升对语音细节和整体语义的理解能力。  <br/>3. 设计对比交叉模态对齐机制，增强语音与文本之间的对应关系和互模态感知。  <br/>4. 在口语问答基准测试中取得优于现有平行模型的SOTA性能，并验证其与交织模型的竞争力。  <br/>5. 探索框架在全双工对话场景中的扩展性，推动语音生成技术向更复杂交互任务迁移。|
|2506.09301v1|[$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for   Figurative Language Understanding](http://arxiv.org/abs/2506.09301v1)|贡献点：<br/>1. 提出$(RSA)^2$框架，首次将修辞策略纳入RSA模型以处理隐喻语言<br/>2. 实现无需建模非字面表达动机的意图理解机制，突破传统RSA框架限制<br/>3. 开发PragMega+新数据集并验证框架有效性，取得LLMs在讽刺识别任务上的SOTA性能<br/>4. 建立可扩展的语音理解范式，实现字面与意图语义的兼容性解读<br/><br/>总结：该研究提出$(RSA)^2$框架，通过建模修辞策略实现无需动机建模的隐喻理解，在新数据集上取得LLMs讽刺识别的最先进性能。|
|2506.08967v2|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v2)|总结（100字以内）:  <br/>提出全端到端LALM Step-Audio-AQAA，结合双代码库分词、130B参数模型与声码器，创新训练方法，显著提升语音控制能力，并建立新基准测试验证效果。  <br/><br/>**贡献点分项:**  <br/>1. **端到端语音交互模型**：首次设计直接生成自然语音响应的AQAA专用LALM (Step-Audio-AQAA)，突破传统文本依赖的瓶颈。  <br/>2. **双代码库音频分词器**：融合语言与语义特征提取，增强对音频内容的理解与建模能力。  <br/>3. **超大规模模型架构**：采用1300亿参数LLM作为骨干，显著提升生成质量与复杂任务处理能力。  <br/>4. **训练方法创新**：提出交错token输出机制，结合DPO与模型合并技术，优化语义连贯性与语音合成效果。  <br/>5. **基准测试与性能验证**：开发StepEval-Audio-360基准，证明模型在语音控制等关键指标上优于现有SOTA方法，凸显声码器的必要性。|
|2506.08967v1|[Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language   Model](http://arxiv.org/abs/2506.08967v1)|**贡献点：**  <br/>1. **提出全端到端的AQAA模型（Step-Audio-AQAA）**：首次设计专门针对音频查询-音频回答（AQAA）任务的端到端大型音频语言模型，克服传统模型依赖文本输出的局限。  <br/>2. **双代码本音频分词器设计**：引入语言与语义双重编码机制，有效提取语音中的结构化与高层次语义特征。  <br/>3. **大参数量骨干模型与神经声码器结合**：采用1300亿参数的LLM作为主干，并搭配神经声码器，实现高保真语音合成与控制。  <br/>4. **混合训练策略优化**：通过文本与音频交错输出增强语义连贯性，并结合Direct Preference Optimization（DPO）与模型合并技术提升整体性能。  <br/>5. **语音控制任务的性能突破**：在StepEval-Audio-360基准测试中，显著优于现有SOTA模型，验证了模型在语音控制方向的有效性。  <br/>6. **突出token-based声码器的关键作用**：强调声码器在端到端AQAA任务中的核心地位，为未来研究提供新的技术视角。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Step-Audio-AQAA，通过双代码本分词器、大参数模型和神经声码器实现端到端音频对话，结合交错训练与DPO优化，并在语音控制任务中超越现有SOTA模型。|
|2506.08633v1|[Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs](http://arxiv.org/abs/2506.08633v1)|**贡献点：**<br/>1. 提出基于小连接模块（connector module）的跨模态桥接方法，整合语音编码器（如WavLM-large）与大语言模型（如OLMo/LLaMA）的表示空间。<br/>2. 推动全开源和开放数据方案，采用WavLM-large和OLMo等公开模型实现端到端DST系统。<br/>3. 系统性消融实验分析了不同微调策略（全量/LoRA适配器）与对话轮次影响，验证了关键组件的有效性。<br/>4. 引入基于模糊匹配的输出后处理技术，显著提升对话槽值中的命名实体识别性能。<br/>5. 创新性结合SpokenWOZ数据集与Speech-Aware MultiWOZ数据集，提升训练数据多样性与模型表现。<br/>6. 在 SpokenWOZ 标准测试集上取得 34.66% JGA 的 SOTA 结果，并通过 Gemma-2-9B-instruct 实现更高精度（42.17% JGA）。<br/><br/>**总结（100字以内）：**  <br/>本研究通过连接模块融合语音与语言模型，结合开源组件与数据增强策略，系统性优化DST方法。实验证明其在SpokenWOZ数据集上达到SOTA性能，并进一步通过大模型提升至更高水平。|
|2506.08593v1|[Hateful Person or Hateful Model? Investigating the Role of Personas in   Hate Speech Detection by Large Language Models](http://arxiv.org/abs/2506.08593v1)|总结（100字以内）:  <br/>本文首次系统研究MBTI性格特质对LLM仇恨言论分类的系统性影响，发现人格特征显著改变模型输出，并提出优化标注流程的建议，强调对公平性与价值观对齐的重要性。<br/><br/>贡献点:  <br/>1. **首次研究框架**：提出首个基于MBTI人格特质的个性提示（persona prompt）系统性框架，用于分析其对仇恨言论分类的影响。  <br/>2. **人类标注验证**：通过大规模人类标注调查，实证证明MBTI维度显著影响标注行为，揭示人格特质在主观任务中的关键作用。  <br/>3. **多模型跨数据集评估**：在三个主流仇恨言论数据集上，评估四个开源LLM的输出差异，验证人格提示对模型表现的调控效应。  <br/>4. **识别偏差现象**：发现人格驱动导致的三类问题：与真实标签不一致、跨人格标注分歧、logit层级的隐性偏差，凸显模型与人类价值观的潜在差距。  <br/>5. **实践指导意义**：提出需谨慎设计人格提示以提升LLM标注流程的公平性，为伦理引导和模型对齐提供理论依据。|
|2506.08147v1|[Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models](http://arxiv.org/abs/2506.08147v1)|总结：  <br/>本研究构建首个乌尔都语-英语-西班牙语三语仇恨言论检测数据集，提出融合注意力机制与大语言模型的方法，显著提升多语言分类性能，为全球数字社区安全提供有效解决方案。<br/><br/>贡献点：  <br/>1. **构建首个三语仇恨言论数据集**  <br/>   - 收集10,193条跨语言推文（英语3,834；乌尔都语3,197；西班牙语3,162），通过关键词过滤确保平衡标注（4,849 Hate/5,344 Not-Hate）。  <br/>   - 弥补乌尔都语在仇恨言论研究中的空白，尤其缺乏基于翻译的语料支持。  <br/><br/>2. **提出混合模型方法论**  <br/>   - 结合注意力层（预处理）与大语言模型（如GPT-3.5 Turbo、Qwen 2.5 72B），优化多语言特征提取。  <br/>   - 明确区分传统模型（TF-IDF + SVM）与Transformer模型（BERT/RoBERTa）的对比实验。  <br/><br/>3. **验证高性能结果**  <br/>   - 在英语、西班牙语及乌尔都语中分别实现87%、85%、81%的F1得分，多语言联合模型达88%，均优于SVM基线（提升8.75%-7.32%）。  <br/>   - 通过Fleiss' Kappa（0.821）确保标注一致性，提升数据可靠性。  <br/><br/>4. **推动多语言检测应用**  <br/>   - 提供可扩展框架，支持跨语言仇恨言论识别，促进全球社交媒体安全治理。|
|2506.07726v1|[Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription   with RAG-based Correction and Predicted BLEU](http://arxiv.org/abs/2506.07726v1)|**贡献点：**  <br/>1. 构建了首个包含801小时、高质量的长文本形式瑞士议会语料库（751小时通过质量控制），支持瑞士德语多小时辩论会的语音-文本对齐。  <br/>2. 提出分两阶段的GPT-4o校正流程：第一阶段修正ASR误识别（如专有名词），第二阶段评估语义完整性，提升文本准确性。  <br/>3. 引入基于BLEU分数和LLM评估的自动化过滤机制，优化数据质量并确保语义一致性。  <br/>4. 验证了结合高精度ASR、LLM校正与数据驱动过滤方法对低资源、领域专用语音语料库的显著效果（BLEU提升6分）。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了大尺寸瑞士议会长文本语音语料库，通过优化ASR与LLM校正流程提升数据质量，验证了多阶段校正与过滤方法对低资源语料的有效性，实现BLEU显著提升，为瑞士德语语音研究提供重要资源。|
|2506.07707v1|[Interaction Analysis by Humans and AI: A Comparative Perspective](http://arxiv.org/abs/2506.07707v1)|总结：  <br/>本研究通过比较MR与2D视频会议对儿童手势猜谜游戏交流的影响，揭示了MR在增强互动质量与协作学习中的潜力，同时探讨了LLMs在儿童互动分析中的效率与局限性。<br/><br/>贡献点：  <br/>1. **对比研究**：首次系统分析MR（HoloLens）与2D视频会议（Zoom）在儿童协作任务中的交互差异。  <br/>2. **LLMs应用**：开发基于大语言模型的自动化分析框架，实现注释、翻译及迭代校正，提升数据处理效率。  <br/>3. **交互特征发现**：发现MR促进更丰富的非语言交流（如情感表达），而Zoom则以简洁性和可访问性为优势。  <br/>4. **跨平台洞见**：为分布式教育场景下的协作学习设计提供实证依据，强调MR在增强沉浸感与参与度方面的潜力。|
|2506.06775v1|[They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse](http://arxiv.org/abs/2506.06775v1)|总结：  <br/>本研究首次构建意大利政治演讲的IMPAQTS语料库，系统评估LLMs在处理隐含内容（预设与暗示）上的局限性，并提出改进方向，同时开源数据与代码。<br/><br/>贡献点：  <br/>1. **首次构建专用语料库**：创建包含意大利政治演讲及操纵性隐含内容标注的IMPAQTS语料库，填补该领域研究空白。  <br/>2. **设计双重评估任务**：通过多项选择和开放生成任务，全面测试LLMs对隐含内容的解析能力。  <br/>3. **揭示模型局限性**：证明当前LLMs在政治语境下的隐含内容理解存在显著缺陷，识别其缺失的关键pragmatic能力。  <br/>4. **提出改进方向**：指出增强模型对高度隐含语言处理能力的潜在研究路径。  <br/>5. **开放数据与代码**：提供数据集和实现代码，促进该领域的进一步研究与应用。|
|2506.06113v1|[Bridging the Gap: In-Context Learning for Modeling Human Disagreement](http://arxiv.org/abs/2506.06113v1)|**贡献点分点总结**  <br/>1. **探索LLMs在主观任务中的多视角建模能力**：首次系统研究LLMs是否能通过上下文学习（ICL）捕捉多角度观点，并反映主观注释中存在的分歧（如仇恨言论检测）。  <br/>2. **提出多标签建模策略的对比实验**：对比了聚合硬标签与拆分硬标签/软标签在零样本和小样本场景下的效果，揭示了不同标签策略对模型表现的影响。  <br/>3. **分析小样本提示优化方法**：评估了基于文本相似度（BM25、PLM）、注释分歧（熵）和组合排名的演示选择方法，以及随机与课程式（curriculum-based）示例排序策略的有效性。  <br/>4. **揭示LLMs建模主观性的局限**：发现零样本设置下多视角生成可行，但小样本场景难以全面体现人类判断，提示设计和演示选择对性能具有显著影响。  <br/>5. **强调改进方向**：指出需构建更视角敏感、具备社会智能的LLMs，以更好应对主观性任务中的注释分歧问题。  <br/><br/>**总结（100字以内）**：  <br/>该研究探讨LLMs在主观任务中捕捉多视角与注释分歧的潜力，通过对比多种标签策略和提示优化方法，揭示模型在零样本与小样本场景下的差异，并提出改进LLMs社会智能性的方向。|
|2506.06066v1|[Conversational Interfaces for Parametric Conceptual Architectural   Design: Integrating Mixed Reality with LLM-driven Interaction](http://arxiv.org/abs/2506.06066v1)|总结：  <br/>本论文提出一种基于对话的MR界面，整合语音、手势与多智能体LLM，实现参数化建模的直观操作，降低设计门槛，推动MR向生成设计平台发展。<br/><br/>贡献点：  <br/>1. **提出新型对话式MR交互框架**：首次将语音输入、手势识别与多智能体大语言模型（LLM）系统结合，形成面向参数化建模的自然交互范式。  <br/>2. **动态参数状态管理机制**：通过对话与上下文提示解决命令歧义，实现参数的实时动态调整与状态跟踪。  <br/>3. **降低设计认知与操作壁垒**：简化参数化工作流程，使无编程背景的设计师可高效探索和优化设计空间。  <br/>4. **扩展MR作为生成设计平台**：将MR环境从空间交互工具升级为支持程序化思维的创造性设计平台。|
|2506.05796v1|[Diarization-Aware Multi-Speaker Automatic Speech Recognition via Large   Language Models](http://arxiv.org/abs/2506.05796v1)|**贡献点总结（100字以内）**：  <br/>提出融合说话人聚类与大语言模型的多说话人语音识别系统，保留绝对时间信息，提升多语言对话与复杂多说话人场景的识别性能，验证了LLM作为统一后端在联合发言分割和转录中的潜力。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新框架**：设计了一种结合说话人聚类（diarization）与大语言模型（LLM）的多说话人语音识别（MS-ASR）系统，解决重叠语音转录难题。  <br/>2. **保留时序信息**：不同于传统序列化输出训练（SOT）方法，通过整合帧级说话人和语义嵌入，保留绝对时间信息以适应时间敏感场景。  <br/>3. **多模态输入处理**：框架同时处理结构化diarization输入与帧级嵌入，实现段级转录输出，提升识别粒度与准确性。  <br/>4. **多语言与复杂场景适应性**：实验表明系统在多语言对话及高重叠多说话人会议场景中均表现优异，验证其鲁棒性与实用性。  <br/>5. **统一后端潜力**：强调LLM作为统一后端在联合发言分割与转录任务中的优势，推动语音处理与自然语言处理的融合。|
|2506.05706v1|[Bridging the Modality Gap: Softly Discretizing Audio Representation for   LLM-based Automatic Speech Recognition](http://arxiv.org/abs/2506.05706v1)|总结（100字以内）:  <br/>本文提出将向量量化整合到LLM的ASR系统中，通过软离散化方法提升模型对跨域音频的处理能力，揭示了其作为模态桥梁的潜力。<br/><br/>贡献点:  <br/>1. **提出VQ与LLM结合的方法**：解决音频连续性与LLM离散token范式之间的鸿沟，实现音频表示与语言模型的对齐。  <br/>2. **构建基于LLM嵌入表的VQ codebook**：利用预训练LLM的嵌入表作为量化字典，降低模型训练复杂度并增强跨模态一致性。  <br/>3. **设计软离散化技术**：通过动态更新codebook和加权求和策略，生成更符合语言结构的离散音频representation。  <br/>4. **验证有效性与泛化性**：实验表明该方法在out-of-domain场景下显著优于基线，为LLM-based ASR提供新思路。|
|2506.05671v1|[Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](http://arxiv.org/abs/2506.05671v1)|总结：  <br/>本研究提出文本仅微调策略，通过未配对文本实现低资源领域自适应，保持语音-文本对齐并提升模型泛化能力，为ASR领域提供有效新方法。<br/><br/>贡献点：  <br/>1. 提出无需额外音频的文本仅微调框架，解决低资源环境下配对语音-文本数据不足的问题。  <br/>2. 引入实时评估机制，确保微调过程中语音-文本对齐性得以保留。  <br/>3. 实验证明方法在多个数据集上可保持与全音频-文本微调相近的性能，且性能退化最小。  <br/>4. 有效提升模型对新领域的泛化能力，避免灾难性遗忘问题。  <br/>5. 为低资源领域适应性ASR提供可扩展的文本驱动优化方案。|
|2506.05538v1|[SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful   Deepfake Content on Social Media Platforms](http://arxiv.org/abs/2506.05538v1)|总结：  <br/>提出SocialDF数据集及多模态LLM检测方法，解决社交媒体深伪内容的识别难题。<br/><br/>贡献点：  <br/>1. 构建SocialDF数据集：首个涵盖社交媒体真实场景的深伪挑战数据集，包含多来源高保真合成媒体。  <br/>2. 多模态检测框架：融合面部识别、语音转录与多智能体LLM，实现音频-视觉线索的交叉验证。  <br/>3. 语言行为分析：引入 linguistic、behavioral 和 contextual 多维度分析，提升检测的鲁棒性和准确性。|
|2506.05414v1|[SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and   Hearing](http://arxiv.org/abs/2506.05414v1)|总结：  <br/>本研究提出首个支持动态场景中3D空间推理的基准SAVVY-Bench，并设计无需训练的推理框架SAVVY，通过多模态轨迹聚合与坐标对齐提升音频-视觉大语言模型的性能，推动动态3D场景理解。<br/><br/>贡献点：  <br/>1. 提出首个专注于动态、音频-视觉环境中3D空间推理的基准SAVVY-Bench，引入同步空间音频数据。  <br/>2. 设计无训练的推理框架SAVVY，包含两个阶段：基于视角的物体轨迹估计和动态全局地图构建。  <br/>3. 引入细粒度时序定位、一致3D定位及多模态标注，提升场景理解的复杂性与准确性。  <br/>4. 实验证明SAVVY显著提升现有AV-LLMs性能，为动态3D空间推理研究建立新标准。|
|2506.05209v1|[The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly   Licensed Text](http://arxiv.org/abs/2506.05209v1)|**贡献点:**<br/>1. 提出Common Pile v0.1，首个面向LLM预训练的8TB开放授权文本数据集（覆盖30个来源，包括论文、代码、书籍等）；<br/>2. 通过训练7B参数模型验证数据集有效性，性能与基于未授权文本的LLM（如Llama 1/2 7B）相当；<br/>3. 释放数据集构建代码、训练混合策略及模型检查点，提升研究透明度与可复现性。<br/><br/>**总结（100字以内）:**  <br/>本文构建首个8TB开放授权文本数据集Common Pile v0.1，涵盖多领域内容。基于其训练的7B参数模型性能媲美未授权文本训练的LLM，并开源全流程代码和训练资源，推动合规模型研究发展。|
|2506.05191v1|[MokA: Multimodal Low-Rank Adaptation for MLLMs](http://arxiv.org/abs/2506.05191v1)|总结：  <br/>该论文提出MokA方法，通过结合单模态适配与跨模态交互，解决现有多模态微调方法忽视模态差异的问题，实验证明其有效性与普适性，为高效多模态模型适配提供新思路。<br/><br/>贡献点：  <br/>1. **揭示现有方法的局限性**：指出当前高效多模态微调方法直接套用LLM策略，忽视多模态场景的内在差异，影响模态协同利用。  <br/>2. **提出双适应理论框架**：论证单模态适配与跨模态适配是多模态模型微调的两个核心组成部分，强调两者的共同作用。  <br/>3. **设计MokA方法**：提出一种多模态感知的低秩微调策略，通过模态特异性参数压缩单模态信息，显式增强跨模态交互。  <br/>4. **多场景验证有效性**：在音频-视觉-文本、视觉-文本、语音-文本三种典型场景及多个大模型架构（如LLaMA2/3、Qwen2等）中验证方法的通用性与性能提升。  <br/>5. **系统评估方法优势**：通过消融实验与效率分析，全面验证MokA在参数压缩与跨模态增强上的效果。  <br/>6. **推动多模态研究**：认为MokA为多模态大模型高效适配提供更精准的解决方案，为后续研究奠定基础。|
|2506.05062v1|[Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation](http://arxiv.org/abs/2506.05062v1)|总结：  <br/>提出Debate Speech Evaluation新基准，系统分析LLM与人类在多维度辩论评估中的表现，揭示模型判断行为差异并评估生成能力，为LLM判决研究提供关键洞察。<br/><br/>贡献点：  <br/>1. **提出新基准**：构建首个用于评估大语言模型辩论判断能力的基准任务，填补LLM系统性基准领域的空白。  <br/>2. **多维评估框架**：提出综合考察论点强度、相关性、连贯性、风格适配等多层面的评价标准，系统性分析LLM的判断能力。  <br/>3. **大规模标注数据集**：利用包含600+条精细标注的辩论演讲数据集，首次实现对LLM在辩论任务上的大规模实验与验证。  <br/>4. **模型行为分析**：揭示大型模型在个体判断与整体行为层面与人类法官的显著差异，为LLM的局限性研究提供实证依据。  <br/>5. **生成能力评估**：验证前沿LLM生成具有说服力和观点性演讲的能力，证明其在特定任务上可能达到人类水平。|
|2506.04711v1|[LLM-based phoneme-to-grapheme for phoneme-based speech recognition](http://arxiv.org/abs/2506.04711v1)|**贡献点：**<br/>1. **提出LLM-P2G解码框架**：首次将大语言模型（LLMs）引入基于音素的ASR系统，结合语音到音素（S2P）和音素到字符（P2G）的分步解码流程。  <br/>2. **解决级联信息损失问题**：针对S2P与P2G联用时的潜在信息损失，设计了两种训练策略：带噪声音素的数据增强（DANP）和随机化top-K边缘化训练与解码（TKM）。  <br/>3. **跨语言性能提升**：实验验证在波兰语和德语的跨语言ASR任务中，LLM-P2G相比传统WFST解码方法分别降低WER 3.6%和6.9%。  <br/>4. **简化解码流程**：通过LLMs替代WFST，减少了解码的复杂性，同时提升对多语言数据的适应能力。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出LLM-P2G解码方法，结合S2P与P2G流程并引入数据增强和top-K边缘化策略，有效缓解信息损失问题，在波兰语和德语跨语言ASR中显著提升识别性能。|
|2506.04693v1|[Cracking the Code: Enhancing Implicit Hate Speech Detection through   Coding Classification](http://arxiv.org/abs/2506.04693v1)|总结：  <br/>提出新的隐性仇恨言论分类体系（codetypes），设计两种LLMs检测方法，验证其在中英文数据集上的有效性。<br/><br/>贡献点：  <br/>1. **构建隐性仇恨言论新分类框架**：首次提出六种编码策略（codetypes），为im-HS检测提供系统化的分类依据。  <br/>2. **创新性方法设计**：提出两种基于大语言模型的整合策略——直接提示分类与编码器嵌入codetypes，解决隐性HS检测挑战。  <br/>3. **跨语言有效性验证**：通过中英文数据集的实验结果，证明所提方法在不同语言环境下的普遍适用性与检测性能提升。|
|2506.04586v1|[LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech   Foundational Models](http://arxiv.org/abs/2506.04586v1)|总结:  <br/>提出LESS框架，通过LLM优化伪标签并结合数据过滤策略提升语音任务性能，验证其跨语言和任务的适应性，为语音处理提供新方法。<br/><br/>贡献点:  <br/>1. **提出LESS框架**：首个结合大语言模型与半监督学习的语音处理框架，用于修正野外数据生成的伪标签。  <br/>2. **LLM伪标签修正**：利用LLM提升ASR/AST任务中伪标签的质量，显著降低词错误率（WER）。  <br/>3. **数据过滤策略**：设计数据增强机制优化LLM知识迁移效率，提升模型训练效果。  <br/>4. **跨语言/任务验证**：在中文ASR和西班牙语-英语AST任务中均取得显著性能提升，证明框架的通用性。  <br/>5. **消融研究分析**：通过不同LLM和提示配置的实验，揭示LLM衍生知识在语音处理中的关键作用与优化方向。|
|2506.04043v1|[Think Like a Person Before Responding: A Multi-Faceted Evaluation of   Persona-Guided LLMs for Countering Hate](http://arxiv.org/abs/2506.04043v1)|**贡献点（分点）:**  <br/>1. 提出首个针对大语言模型（LLM）生成的反叙事（CN）的多维度评估框架，涵盖角色框架、冗长性与可读性、情感基调及伦理稳健性。  <br/>2. 系统测试三种提示策略在MT-Conan和HatEval数据集上的表现，对比GPT-4o-Mini、CommandR-7B和LLaMA 3.1-70B等主流模型的生成效果。  <br/>3. 首次揭示LLM生成的CN存在可访问性不足的问题（如冗长、适配大学以上学历），限制其实际应用效果。  <br/>4. 量化分析情感引导提示策略在提升CN同理心与可读性方面的优势，同时指出其潜在安全与伦理风险。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出LLM生成反叙事的评估框架，系统分析三种提示策略及多模型表现，发现其冗长性与可访问性不足，情感引导策略能提升可读性但存在安全风险，为优化反仇恨言论技术提供关键见解。|
|2506.03099v1|[TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1)|**贡献点：**<br/><br/>1. **模型转化**：将预训练的SOTA图像-视频生成模型（DiT）改造为音频驱动的高参数（180亿）虚拟形象生成模型，实现实时对话动画。<br/>2. **无误差流技术**：通过双向教师模型向稀疏因果自回归学生模型的异步知识蒸馏，解决无限视频流中的误差累积问题。<br/>3. **高效推理优化**：设计高吞吐量、低延迟的推理管道，包含以下工程创新：  <br/>   - 分布式计算（DiT和VAE解码器分设设备）  <br/>   - CUDA流技术实现设备间通信与计算重叠  <br/>   - 消除冗余计算提升帧生成效率  <br/><br/>**总结（100字内）：**  <br/>本文提出TalkingMachines框架，将预训练视频生成模型转化为音频驱动的实时角色动画系统，通过模型优化与分布式推理技术，实现无误差无限视频流和高效生成性能。|
|2506.03009v1|[Conditioning Large Language Models on Legal Systems? Detecting   Punishable Hate Speech](http://arxiv.org/abs/2506.03009v1)|总结：  <br/>本文探讨了如何通过不同抽象层次的法律知识对齐LLMs，分析其在仇恨言论检测任务中的表现，揭示模型与法律专家在法律评估能力上的显著差距，并探讨抽象与具体法律知识对模型性能的不同影响。<br/><br/>贡献点：  <br/>1. 提出并研究了LLMs在法律系统不同抽象层级（宪法、法规、判例）的对齐方法，探索其法律问题评估能力。  <br/>2. 聚焦德国刑法框架下的煽动仇恨行为分类任务，构建具体应用案例分析。  <br/>3. 揭示LLMs在法律评估中的性能瓶颈：抽象法律知识模型缺乏任务理解，易出现矛盾与幻觉；具体法律知识模型虽能识别目标群体，但分类行为特征存在困难。  <br/>4. 为法律与AI交叉领域提供实证依据，指出模型与法律专家间的核心差距，并启发更精准的法律知识融入策略。|
|2506.02758v1|[Exploiting the English Vocabulary Profile for L2 word-level vocabulary   assessment with LLMs](http://arxiv.org/abs/2506.02758v1)|**总结（100字以内）：**  <br/>本研究提出结合大语言模型与英语词汇档案（EVP）的新型方法，实现对二语学习者写作中词汇使用的细粒度评估，解决多义性与上下文变化等挑战，并验证了其在词级与作文级熟练度相关性分析中的有效性。  <br/><br/>---<br/><br/>**贡献点：**  <br/>1. **提出新方法**：首次将大型语言模型（LLMs）与英语词汇档案（EVP）结合，实现基于句子语境的细粒度词汇评估。  <br/>2. **解决复杂问题**：有效应对二语词汇中的多义性（polysemy）、上下文差异（contextual variation）及多词表达（multi-word expressions）等评估难题。  <br/>3. **对比基准模型**：通过对比传统词性（PoS）基线，证明LLMs能利用更丰富的语义信息，提升词汇评分准确性。  <br/>4. **探索相关性**：首次分析词级语言能力与作文整体水平之间的关联，揭示词汇使用的层级性特征。  <br/>5. **验证EVP一致性**：应用该方法重新检验EVP的等级划分合理性，证实LLMs在词汇评估任务中的适用性与可靠性。|
|2506.02457v1|[SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant](http://arxiv.org/abs/2506.02457v1)|总结：  <br/>提出SOVA-Bench基准框架，系统评估语音LLMs的语义与声学生成能力，填补声学质量量化评估空白，推动语音交互系统发展方向。<br/><br/>贡献点：  <br/>1. 提出SOVA-Bench：首个系统性评估语音LLMs的基准框架，整合多维度能力测试。  <br/>2. 填补声学质量量化空白：首次对生成语音的声学特性进行量化评估，突破以往仅关注语义准确性的局限。  <br/>3. 综合多能力评估：同时衡量语音理解、语音识别、语义生成和声学生成能力，提供全面对比分析。  <br/>4. 指导技术发展：为语音交互系统的优化方向提供理论依据与实践参考，促进更自然的语音生成研究。|
|2506.01808v1|[NAVER LABS Europe Submission to the Instruction-following Track](http://arxiv.org/abs/2506.01808v1)|**贡献点总结（100字以内）:**  <br/>本文提出一种多任务语音处理系统，整合语音到LLM嵌入投影器与LoRA适配器，通过指令微调实现跨语言（中、意、德）的ASR、ST、SQA任务联合处理，优化了多语言和多模态数据下的模型性能。<br/><br/>**分点贡献：**  <br/>1. **多任务联合处理**：开发可同步执行语音识别（ASR）、语音翻译（ST）和语音问答（SQA）的系统，支持英语输入到中文、意大利语和德语的跨语言转换。  <br/>2. **模块化架构**：采用两个预训练模块：(1) 语音到LLM的嵌入投影器（基于SeamlessM4T-v2-large语音编码器）；(2) LoRA适配器（基于Llama-3.1-8B-Instruct语言模型）。  <br/>3. **指令微调策略**：联合加载模块后，在多语言和多模态数据上进行1K步指令优化，提升对复杂指令的响应能力。  <br/>4. **高效训练方法**：通过预训练模块与微调结合，简化多任务模型的训练流程，可能提升推理效率和泛化性能。|
|2506.01683v1|[Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection   Using Speech and Large Language Models](http://arxiv.org/abs/2506.01683v1)|**贡献点：**  <br/>1. 提出结合语音和语言模型的CoT推理方法，用于阿尔茨海默病（AD）与非AD分类。  <br/>2. 引入监督微调（SFT）与CoT提示策略，增强模型分类能力。  <br/>3. 设计线性层作为分类模块，提升模型对语音文本的判别性能。  <br/>4. 实验显示方法在无CoT策略的对比基线中实现16.7%的相对性能提升。  <br/>5. 达到当前CoT方法在阿尔茨海默病诊断领域的最先进性能水平。  <br/><br/>**总结：**  <br/>该研究提出一种基于链式思维的语音-语言模型联合框架，通过监督微调与提示策略显著提升痴呆症分类准确率，达到领域内最先进水平。|
|2506.01484v2|[LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech   Detoxification](http://arxiv.org/abs/2506.01484v2)|总结（100字以内）:  <br/>该研究提出基于LLM的自动化detoxification框架，构建首个大规模hatespeech平行数据集ParaDeHate，并验证其在提升模型性能方面的有效性，为替代人工标注提供可扩展解决方案。<br/><br/>贡献点分点列出:<br/>1. **提出LLM-in-the-loop自动化流程**：设计一种利用GPT-4o-mini替代人工标注的新型管道，实现对有害语言的自动改写，降低标注成本。<br/>2. **构建领域专用数据集ParaDeHate**：创建包含8K对仇恨/非仇恨文本的平行数据集，填补hatespeech detoxification的高质量数据缺口。<br/>3. **验证LLM生成数据的有效性**：通过实验表明，基于ParaDeHate微调的BART等模型在风格准确性、内容保留和流畅度方面显著优于现有方法。<br/>4. **建立基准与方法对比**：发布ParaDeHate作为评估基准，并系统比较多种基线模型表现，推动该领域的研究进展。|
|2506.01133v1|[From Words to Waves: Analyzing Concept Formation in Speech and   Text-Based Foundation Models](http://arxiv.org/abs/2506.01133v1)|贡献点（分点）:<br/>1. 首次验证语音模型是否能像文本模型一样获得抽象语义概念<br/>2. 系统比较单模态（语音/文本）与多模态联合训练模型的语义结构差异<br/>3. 提出并应用Latent Concept Analysis方法分析跨模态语义形成机制<br/>4. 开源实验代码与资源提升研究可复现性<br/><br/>总结: 本研究通过无监督分析方法，揭示语音与文本模型在语义抽象形成上的异同，验证多模态训练对语义理解的增强作用，并开放资源促进学术研究复现。|
|2506.01111v1|[FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal   Contextual Fusion](http://arxiv.org/abs/2506.01111v1)|**贡献点分点：**  <br/>1. **两阶段音频描述生成方法**：提出基于人类听觉感知启发的自动化框架，结合多模态信息提取（语音、音乐、环境声音、视频）和大语言模型（LLM）的上下文融合，实现细粒度、语境感知的音频描述。  <br/>2. **FusionAudio数据集**：构建包含120万条高质量音频描述和60万问答对的大型数据集，为跨模态研究提供标注资源。  <br/>3. **改进的音频模型**：开发基于CLAP的音频编码器，提升音频-文本对齐能力与指令遵循效果，增强模型对复杂音频环境的理解。  <br/><br/>**总结（100字以内）：**  <br/>提出两阶段音频描述生成框架与FusionAudio数据集，结合多模态信息和大语言模型提升描述质量，优化CLAP编码器实现更精准的音频-文本对齐与理解。|
|2506.01077v1|[TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans](http://arxiv.org/abs/2506.01077v1)|总结：  <br/>本论文提出TRiMM框架，解决数字人类实时手势生成与长文本理解难题，包含跨模态注意力、长上下文建模及大规模动作匹配系统，实现120fps推理速度，并在消费级GPU上保持低延迟，代码开源。<br/><br/>贡献点：  <br/>1. **提出TRiMM框架**  <br/>   - 首次结合多模态技术实现实时3D手势生成，同时解决长文本理解与实时合成的挑战。  <br/><br/>2. **设计三大核心模块**  <br/>   - **跨模态注意力机制**：实现语音与手势的精确定时对齐。  <br/>   - **长上下文自回归模型**：采用滑动窗口机制高效建模长序列，增强语义连贯性。  <br/>   - **大规模动作匹配系统**：构建原子动作库，支持实时检索生成高质量手势。  <br/><br/>3. **轻量级Unreal Engine实现**  <br/>   - 开发轻量级实验流水线，实现实时推理速度（120 fps）与低句级延迟（0.15秒）。  <br/><br/>4. **全面评估验证效果**  <br/>   - 在ZEGGS和BEAT数据集上完成主观与客观评估，证明其性能优于当前SOTA方法。  <br/><br/>5. **开源代码促进应用**  <br/>   - 提供完整代码库，推动LLM驱动数字人类研究的可复现性与实际部署。|
|2506.00955v1|[Leveraging Large Language Models for Sarcastic Speech Annotation in   Sarcasm Detection](http://arxiv.org/abs/2506.00955v1)|总结：  <br/>提出基于大语言模型的语音讽刺标注方法，构建首个大规模单模态讽刺语音数据集PodSarc，验证其有效性并展示73.63%的检测性能，为该领域研究提供新基准。<br/><br/>贡献点：  <br/>1. **提出LLM驱动的单模态标注流程**：首次利用GPT-4o和LLaMA 3等大语言模型生成语音讽刺标注数据，解决语音领域数据稀缺问题。  <br/>2. **构建大规模语音讽刺数据集PodSarc**：通过人机协作验证，创建高质量单模态讽刺语音数据集，填补语音讽刺研究的数据空白。  <br/>3. **验证方法有效性**：采用协作门控架构对比标注质量与检测性能，证明所生成数据集的可靠性及研究价值。  <br/>4. **提供基准性能指标**：检测模型在公开数据集上达到73.63% F1分数，为后续研究提供量化评估标准。|
|2506.00304v1|[Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion   with LLMs](http://arxiv.org/abs/2506.00304v1)|总结：  <br/>本研究提出一种无需配对语音数据的EMG适配器模块，使LLMs能处理无声EMG信号，实现低错误率的EMG-to-text转换，并在小数据量下显著优于专用模型。<br/><br/>贡献点：  <br/>1. **首次提出EMG适配器模块**：创新性地设计模块，将无声EMG特征映射至LLM输入空间，无需依赖配对的有声信号或语音数据。  <br/>2. **实现高效转换性能**：在封闭词汇任务中达成平均WER 0.49，表明模型对无声EMG信号的识别能力。  <br/>3. **小数据量优势**：仅需6分钟无声EMG数据，性能超越专用模型近20%，验证方法的泛化能力与实用性。  <br/>4. **拓展LLMs应用边界**：探索LLMs在理解发声生物信号（如无声EMG）中的潜力，为跨模态语音识别提供新方向。|
|2506.00160v1|[Werewolf: A Straightforward Game Framework with TTS for Improved User   Engagement](http://arxiv.org/abs/2506.00160v1)|贡献点：  <br/>1. 提出基于LLM的新型Werewolf社会推理游戏系统，融合文本生成与语音交互技术，增强游戏沉浸感。  <br/>2. 设计调优后的文本到语音（TTS）模型，提升与多种LLM的兼容性，降低适配成本。  <br/>3. 通过简化系统结构（无需额外组件），实现更高效的用户参与度提升，反驳传统依赖微调或经验池的方案。  <br/>4. 强调LLM推理能力的持续提升将减少对辅助技术（如复杂提示工程）的依赖，指向未来研究方向。  <br/><br/>总结：  <br/>本文提出一种结合TTS与LLM的新型社会推理游戏系统，通过调优提升兼容性并简化架构，有效增强用户体验，同时指出LLM能力进步将减少对额外技术的依赖。|
|2505.24869v1|[SiLVR: A Simple Language-based Video Reasoning Framework](http://arxiv.org/abs/2505.24869v1)|总结（100字以内）:  <br/>提出SiLVR框架，通过双阶段语言表征与推理实现复杂视频语言理解，利用多感官输入提升性能，并验证强LLM无需视频训练即可有效处理多模态信息，达到多个基准数据集的最佳结果。<br/><br/>**贡献点分点列出**:<br/>1. **双阶段框架设计**：创新性地构建SiLVR框架，将复杂视频理解拆分为语言表征提取与推理两阶段，简化多模态处理流程。<br/>2. **多感官输入融合**：引入短片标题、音频/语音字幕等多模态数据作为语言表征的输入，增强对视频内容的描述能力。<br/>3. **自适应token削减方案**：提出动态调整时间粒度的token减少方法，有效处理长上下文多模态输入的效率与精度问题。<br/>4. **训练自由的模组化架构**：框架无需额外训练，依赖推理时的语言模型能力，提升灵活性和易用性。<br/>5. **多任务性能突破**：在Video-MME、Video-MMMU、Video-MMLU、CGBench、EgoLife等基准数据集上取得当前最优性能。<br/>6. **跨模态推理验证**：实验证明，强LLM无需视频领域训练即可高效聚合多感官信息，支持复杂时序、因果、长上下文及知识推理任务。|
|2505.24691v1|[Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing   Cross-Lingual Transfer in Low-Resource Scenarios](http://arxiv.org/abs/2505.24691v1)|总结：提出融合音素表征与Chain-of-Thought框架的S2TT方法，通过课程学习策略提升低资源与零资源场景的翻译性能，为跨语言语音翻译的普及提供新思路。<br/><br/>贡献点：<br/>1. 提出将音素表征整合入CoT框架，构建新型S2TT系统  <br/>2. 引入音素识别作为跨语言迁移的中间步骤，实现零资源翻译能力  <br/>3. 基于多语言LLM开发语音-文本联合处理架构  <br/>4. 设计渐进式课程学习策略，优化多任务训练过程  <br/>5. 证实音素增强的CoT方法在低资源场景显著提升译质，且具备可扩展性|
|2505.24458v1|[SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social   Engineering Behaviors](http://arxiv.org/abs/2505.24458v1)|**贡献点：**  <br/>1. **首个整合AR与多模态LLM的社会工程数据集**：SEAR是首个专门针对AR增强现实和多模态大语言模型驱动的社会工程攻击的多模态数据集。  <br/>2. **多场景对抗性对话数据**：包含60名参与者在模拟会议、课堂、社交活动等场景中的180条标注对话，涵盖多样化的社会工程情境。  <br/>3. **多模态同步数据采集**：整合AR捕捉的同步视觉/音频线索（如面部表情、语音语调）、环境信息及用户社交媒体资料，实现全面行为分析。  <br/>4. **主观攻击效果评估**：引入信任评分与易受性评估等主观指标，量化攻击对用户心理的影响。  <br/>5. **高攻击效能实证结果**：揭示SEAR在诱导用户点击钓鱼链接（93.3%）、接受电话（85%）、提升信任度（76.7%）等任务中的显著效果。  <br/>6. **伦理合规保障**：通过匿名化处理与IRB（伦理审查委员会）批准，确保数据集的负责任使用。  <br/>7. **开源开放共享**：数据集通过GitHub平台公开，支持学术研究与技术开发。  <br/><br/>**总结（100字以内）**：  <br/>SEAR Dataset是首个结合AR与多模态LLM的社会工程攻击数据集，包含多场景、多模态数据及主观评估指标，揭示攻击高效能，为检测与防御研究提供资源，同时保障伦理合规与公开共享。|
|2505.24347v2|[Fewer Hallucinations, More Verification: A Three-Stage LLM-Based   Framework for ASR Error Correction](http://arxiv.org/abs/2505.24347v2)|总结:  <br/>本文提出RLLM-CF框架，通过错误预检测、迭代修正和推理验证三阶段解决LLM在语音识别中的hallucinations问题，无需额外数据或微调，实验证明在多个数据集上显著降低CER/WER。<br/><br/>贡献点:  <br/>1. **提出RLLM-CF框架**：设计包含错误预检测、链式思维子任务迭代修正和推理过程验证的三阶段校正流程，系统性解决LLM在ASR中的错误修正问题。  <br/>2. **无额外训练需求**：方法无需额外标注数据或模型微调，直接利用LLM能力进行端到端校正，降低应用门槛。  <br/>3. **抑制hallucinations**：通过多阶段验证机制有效避免LLM误改正确文本，保障修正结果的准确性。  <br/>4. **实验证明有效性**：在AISHELL-1、AISHELL-2和Librispeech数据集上验证，显示GPT-4o模型结合该框架后CER/WER分别降低21%/11%/9%/11.4%，具有实际应用价值。|
|2505.24016v1|[BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech   Translation System](http://arxiv.org/abs/2505.24016v1)|总结:  <br/>本研究提出BeaverTalk级联系统，结合VAD分段器、Whisper Large V2和Gemma 3 12B，通过LoRAs微调和单句记忆机制实现高效实时翻译，显著提升英德、英中翻译性能。<br/><br/>贡献点:  <br/>1. **系统架构创新**：设计级联系统BeaverTalk，集成VAD分段器、Whisper Large V2语音识别模型和Gemma 3 12B语言模型，实现端到端语音到文本翻译。  <br/>2. **微调方法优化**：采用低秩适配器（LoRAs）技术对翻译LLM进行轻量级微调，结合对话提示策略利用单一源语言前句记忆库提升上下文建模能力。  <br/>3. **延迟与语言方向支持**：系统兼容低延迟（StreamLAAL 1837.86）和高延迟（StreamLAAL 3343.73）运行模式，在英德（BLEU 24.64/27.83）与英中（BLEU 34.07/37.23）任务中均取得突出性能。  <br/>4. **实际部署效果**：在IWSLT 2025真实任务中验证系统有效性，为多语言实时翻译提供可落地的解决方案。|
|2505.23990v2|[Multi-RAG: A Multimodal Retrieval-Augmented Generation System for   Adaptive Video Understanding](http://arxiv.org/abs/2505.23990v2)|总结：  <br/>提出Multi-RAG多模态检索增强生成系统，通过整合视频、音频、文本多源信息提升情境理解与决策效率，验证其在动态场景中优于现有模型且资源占用更少的优势。  <br/><br/>贡献点：  <br/>1. 提出Multi-RAG系统，解决动态场景下人机协作的认知负担问题。  <br/>2. 首次整合视频、音频、文本多模态信息流进行联合推理与生成。  <br/>3. 在MMBench-Video基准测试中表现优于现有视频大语言模型（Video-LLM）和视觉语言模型（LVLM）。  <br/>4. 以更少资源和输入数据实现高效性能，具有实际应用潜力。|
|2505.22251v2|[Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in   Large Language Models for Speech Recognition](http://arxiv.org/abs/2505.22251v2)|摘要贡献点：  <br/>1. 揭示LibriSpeech和Common Voice数据集与公开LLM预训练语料存在显著重叠，质疑现有语音任务评估结果的可靠性。  <br/>2. 提出通过对比含/不含数据污染的LLM评估污染影响的方法，验证污染数据的存在性及对模型表现的影响。  <br/>3. 发现污染LLM的语音识别器虽在错误率上差异微小，但会显著提升对训练数据的转录概率，反映输出偏差。  <br/>4. 强调需使用独立数据评估LLM语音系统，以避免因数据污染导致的不准确结果。  <br/><br/>总结：  <br/>该研究揭露语音任务评估数据与LLM训练数据重叠问题，揭示污染对模型性能的潜在影响，并呼吁使用独立数据验证模型效果。|
|2505.22029v2|[Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection](http://arxiv.org/abs/2505.22029v2)|**贡献点：**  <br/>1. **构建首个全面覆盖词和音素层级的11类不流畅性合成语料库**：LLM-Dys解决了现有数据集中语调不自然和上下文多样性不足的问题，提供更真实的语音不流畅性样本。  <br/>2. **提出改进的端到端检测框架**：基于LLM-Dys数据集优化模型，实现语音不流畅性检测的最先进性能。  <br/>3. **开源全部资源**：公开数据、模型与代码，促进领域研究和应用。  <br/><br/>**总结（100字以内）：**  <br/>本文构建了首个全面覆盖词/音素层级的不流畅性合成数据集LLM-Dys，改进端到端检测框架达SOTA，所有数据及代码开源，推动语音不流畅性研究。|
|2505.20445v3|[In-context Language Learning for Endangered Languages in Speech   Recognition](http://arxiv.org/abs/2505.20445v3)|**总结（100字以内）:**  <br/>本文探索LLM通过上下文学习在低资源语言语音识别中的应用，证实相关文本样本能提升性能，概率方法优于传统指令方法，并展示ICL可使LLM达到或超越专用语言模型的ASR效果，同时保留原有能力。<br/><br/>**贡献点:**  <br/>1. **验证ICL在低资源语音识别中的可行性**：首次将上下文学习方法应用于语音识别领域，证明LLM能在未训练的低资源语言上实现有效学习。  <br/>2. **提出文本样本增强策略**：发现提供与任务更相关的文本样本能显著提升语言建模和ASR性能，为多语言模型优化提供新方向。  <br/>3. **对比方法效果与模型性能**：表明概率方法优于传统指令方法，且ICL使LLM在ASR任务中达到专用语言模型水平，同时保持其通用能力。|
|2505.18614v2|[MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song   Translation](http://arxiv.org/abs/2505.18614v2)|贡献点：  <br/>1. **提出首个多语言多模态基准**：构建了Multilingual Audio-Video Lyrics Benchmark (MAVL)，首次整合文本、音频与视频数据，为动画歌曲翻译提供综合评估标准。  <br/>2. **多模态数据增强翻译质量**：通过融合音频和视频信息，使翻译更贴近原作的旋律、节奏及风格，突破传统文本仅依赖语义的局限。  <br/>3. **创新音节约束模型结构**：提出SylAVL-CoT模型，结合链式推理（Chain-of-Thought）与音节约束机制，提升歌词的自然度与可唱性。  <br/>4. **验证多模态方法有效性**：实验表明该模型在可唱性和上下文准确性上显著优于文本基础模型，证明多模态、多语言框架对歌词翻译的价值。  <br/><br/>总结（100字以内）：  <br/>本研究提出多语言多模态基准MAVL与SylAVL-CoT模型，融合文本、音频和视频数据，通过音节约束提升歌词翻译的自然度和可唱性，验证了多模态方法在动画歌曲翻译中的优势。|
|2505.18110v2|[Watch and Listen: Understanding Audio-Visual-Speech Moments with   Multimodal LLM](http://arxiv.org/abs/2505.18110v2)|总结：  <br/>本文提出TriSense三模态模型，结合视觉、音频与语音信号，引入Query-Based Connector实现模态自适应融合，构建TriSense-2M数据集支持多模态分析，验证了模型的有效性，并公开代码与数据集。<br/><br/>贡献点：  <br/>1. **提出TriSense模型**：首个整合视觉、音频和语音三模态的大型语言模型，用于全面提升视频内容的时间理解能力。  <br/>2. **设计Query-Based Connector**：通过自适应调整模态权重，支持在模态缺失情况下的鲁棒性，并实现灵活的多模态输入组合。  <br/>3. **构建TriSense-2M数据集**：包含200万样本的高质量数据集，采用自动化生成流程，涵盖长视频及多样化的多模态组合。  <br/>4. **实验验证与公开资源**：在多个基准上验证模型效果，证明其对多模态视频分析的潜力，并开放代码和数据集供研究复现。|
|2505.17536v2|[Multimodal Conversation Structure Understanding](http://arxiv.org/abs/2505.17536v2)|总结：  <br/>本研究提出对话角色归属与线程划分任务框架，构建大规模人工标注数据集，评估多模态模型在理解对话结构上的性能差异，揭示关键影响因素，为改进多模态语言模型的对话理解能力提供基础。<br/><br/>贡献点：  <br/>1. **构建多模态对话结构标注数据集**：提供首个包含4,398条对话角色标注、5,755条收件人信息、3,142条旁观者信息的语料库，涵盖多参与者、多模态场景。  <br/>2. **定义关键任务框架**：提出针对对话角色分配（说话人、收件人、旁观者）和对话线程构建（语句关联与聚类）的系统性任务，结合会话分析与社会语言学理论。  <br/>3. **评估模型性能差异**：对比音频-视觉LLM与视觉-语言模型，发现前者在说话人/收件人识别上更优，但匿名化参与者时性能显著下降。  <br/>4. **揭示关键影响因素**：通过实验发现对话参与者数量是角色识别的主要负向预测因子，而声学清晰度（音调、频谱质心）和面部覆盖情况与性能呈正相关。  <br/>5. **推动未来研究方向**：为多模态LLM在对话结构建模和推理能力的提升提供基准与启示。|
|2505.15670v2|[Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](http://arxiv.org/abs/2505.15670v2)|总结：  <br/>本文提出首个无需语音预训练的双工S2S模型，通过连续输入输出与信道融合实现实时对话，降低比特率并提升推理、回合管理及打断处理能力，同时减少数据需求，开源代码促进复现。<br/><br/>贡献点：  <br/>1. **双工S2S架构设计**：支持连续用户输入与同步代理输出，引入信道融合机制实现真实场景下的实时交互（如用户打断）。  <br/>2. **无需语音预训练**：首次提出仅依赖流式编码器的双工模型，消除对专用语音预训练的需求。  <br/>3. **高效编码与微调**：采用独立的代理和用户建模架构，支持编码微调以优化代理语音质量，将比特率降至0.6kbps。  <br/>4. **性能提升**：在推理能力、回合控制和打断处理等关键指标上超越现有双工模型。  <br/>5. **数据需求降低**：跳过语音预训练环节，显著减少所需语音数据量，简化模型构建流程。  <br/>6. **开源与可复现性**：首个公开完整训练与推理代码的双工S2S模型，推动领域研究复现与验证。|
|2505.13338v2|[Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data   Condensation and Spoken QA Generation](http://arxiv.org/abs/2505.13338v2)|**贡献点分点列出：**  <br/>1. **提出首个融合上下文推理与语音伴随信息的框架**：首次设计结合两者的数据集生成方法，解决传统Speech-LLMs在综合理解上的不足。  <br/>2. **创新性的数据生成机制**：包含两阶段技术——基于伪语音伴随标签的野外语音数据压缩，以及LLM驱动的上下文语音伴随问答（CPQA）生成。  <br/>3. **验证框架有效性**：通过Qwen2-Audio-7B-Instruct在自动生成与人工标注CPQA数据集上的强相关性评估，证明其能力。  <br/>4. **揭示Speech-LLMs的局限性**：明确指出模型在共情推理任务中的缺陷，强调需针对性数据集与更优模型。  <br/>5. **潜在应用价值**：为训练具备语音伴随推理能力的鲁棒Speech-LLMs提供基础，推动语音理解研究发展。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出首个融合上下文推理与语音伴随信息的数据集生成框架，揭示Speech-LLMs在共情任务中的局限，验证框架对模型训练的有效性，为提升语音AI能力提供新方向。|
|2505.09439v2|[Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](http://arxiv.org/abs/2505.09439v2)|总结：  <br/>本文提出Omni-R1，通过GRPO强化学习微调Qwen2.5-Omni，在MMAU和MMAR基准上取得SOTA性能，揭示了文本推理能力对音频任务的关键作用，并意外发现文本数据微调可提升音频表现。<br/><br/>贡献点：  <br/>1. **提出Omni-R1模型**：通过GRPO强化学习方法对Qwen2.5-Omni进行微调，聚焦音频问答任务。  <br/>2. **SOTA性能突破**：在MMAU和MMAR基准测试中达到当前最优结果，尤其在声音、音乐、语音及总体平均类别均表现最佳。  <br/>3. **因果分析**：验证了性能提升主要源于增强的文本推理能力，而非单纯依赖音频数据。  <br/>4. **意外发现**：发现仅以文本数据集进行微调也能有效提升模型的音频表现，为多模态训练提供新思路。|
|2505.05335v2|[FLAM: Frame-Wise Language-Audio Modeling](http://arxiv.org/abs/2505.05335v2)|总结：  <br/>本文提出FLAM，解决帧级音频理解与开放词汇定位难题，通过logit调整和大规模数据集提升模型性能，保持全局检索能力。<br/><br/>贡献点：  <br/>1. **首次提出开放词汇帧级音频定位模型**：突破传统模型对预定义类别的依赖，实现对真实场景中未见事件的泛化定位。  <br/>2. **设计记忆高效且校准的帧级目标函数**：引入logit调整机制，有效缓解训练中的虚假相关（如事件依赖、标签不平衡）。  <br/>3. **构建多源帧级监督数据集**：结合LLM生成字幕与模拟数据，提供多样化、细粒度的音频事件标注。  <br/>4. **验证模型多任务能力**：在保持文本-音频检索性能的同时，显著提升帧级定位效果及下游任务表现。|
|2504.20007v3|[Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from   Police Body-Worn Camera Footage](http://arxiv.org/abs/2504.20007v3)|总结：  <br/>本研究提出一种跨学科框架，整合AI与机器学习技术分析警用摄像头数据，通过多模态方法提取警民互动行为动态，构建结构化总结系统，并建立评估流程以提升执法审查与知识发现效率。<br/><br/>贡献点：  <br/>1. **跨学科框架创新**：首次将人工智能（AI）与统计机器学习（ML）技术结合，专门针对警用摄像头（BWC）视频进行模式分析，突破传统单一技术应用的局限。  <br/>2. **多模态数据融合**：综合图像、音频和自然语言处理（NLP）技术，实现对警民互动场景的全面分析，提取尊重/不尊重、冲突升级等复杂行为动态。  <br/>3. **结构化总结生成**：引入说话人分离、转录及大语言模型（LLMs），构建可解释的警民接触事件摘要，提升数据分析的可操作性。  <br/>4. **定制化评估体系**：开发适用于高风险执法场景的自动评估流程，量化转录质量与行为检测准确率，支持实际应用验证。  <br/>5. **执法应用场景落地**：方法论与实证结果为执法部门提供审查、培训及问责的实用工具，推动AI技术在警务实践中的落地应用。  <br/>6. **知识发现前沿推进**：通过系统化分析复杂BWC数据，拓展法律与技术交叉领域的研究边界，为公共安全数据分析提供新范式。|
|2504.08961v2|[A Fully Automated Pipeline for Conversational Discourse Annotation: Tree   Scheme Generation and Labeling with Large Language Models](http://arxiv.org/abs/2504.08961v2)|**贡献点总结：**  <br/>1. 提出基于LLM的全自动决策树构建与标注流水线，替代传统人工设计流程。  <br/>2. 首次将频率引导的决策树与LLM结合，提升语音功能标注性能。  <br/>3. 通过实验验证不同设计选择的效果，展示方法优于手动方案及人类标注。  <br/>4. 开源代码、标注方案及结果，促进语音领域对话标注研究。|