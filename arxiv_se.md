|Source|Title|Summary|
|---|---|---|
|2506.02908v1|[Diffusion Buffer: Online Diffusion-based Speech Enhancement with   Sub-Second Latency](http://arxiv.org/abs/2506.02908v1)|**贡献点总结（100字以内）**  <br/>本文提出滑动窗口扩散模型，通过时间逐步加噪和延迟优化，实现高效、低延迟的实时语音增强，是首个实用的解决方案。<br/><br/>**分点贡献：**  <br/>1. **提出滑动窗口扩散框架**：首次将扩散模型应用于实时语音增强，通过滑动窗口机制处理流数据。  <br/>2. **时间腐蚀机制**：按时间顺序逐步对语音帧添加噪声，提高近期帧的噪声比例以优化生成效果。  <br/>3. **延迟-性能权衡设计**：通过缓冲区控制输出延迟，实现推理效率与语音增强质量的平衡。  <br/>4. **实验验证有效性**：在GPU上实现0.3-1秒的延迟，性能优于标准扩散模型，证明其实际应用潜力。|
|2506.01611v1|[Lessons Learned from the URGENT 2024 Speech Enhancement Challenge](http://arxiv.org/abs/2506.01611v1)|总结：  <br/>该论文通过URGENT 2024挑战，揭示了语音增强技术中数据清洗与评估指标的两个关键问题，指出传统流程存在的三大缺陷，并提出多维度评估的重要性，旨在推动更优SE系统设计。<br/><br/>贡献点：  <br/>1. **系统分析关键问题**：深入探讨了数据清洗与评估指标在语音增强系统开发中的重要性，指出其被低估的研究价值。  <br/>2. **揭示传统缺陷**：  <br/>   - 强调声明与实际音频带宽的不匹配及高质量语料库中的标签噪声问题；  <br/>   - 指出缺乏应对极端条件（如语音重叠、强噪声/混响）的有效系统及样本难度评估方法；  <br/>   - 提出多维度评估指标对全面、符合人类判断的性能评估的必要性。  <br/>3. **推动技术改进**：通过挑战成果启发，为未来语音增强管道的设计提供改进方向。|
|2506.01460v1|[Few-step Adversarial Schrödinger Bridge for Generative Speech   Enhancement](http://arxiv.org/abs/2506.01460v1)|**总结（100字以内）**  <br/>该论文提出结合Schrödinger Bridge与GAN的新框架，显著降低语音增强模型的采样步骤需求（单步推理），在低SNR条件下仍保持高质量输出，优于现有基线方法，适用于全频带数据集。<br/><br/>**贡献点分点列出**  <br/>1. **方法创新**：首次将Schrödinger Bridge与GAN结合，解决语音增强领域中扩散模型采样步骤过多的问题。  <br/>2. **效率提升**：通过该框架减少所需采样步骤（甚至降至单步推理），显著提高推理效率。  <br/>3. **性能保障**：在低信噪比（low-SNR）等挑战性条件下，模型性能优于传统基线方法。  <br/>4. **实验验证**：在全频带语音数据集上验证了方法的有效性，表明其在降噪和消混响任务中的优越性。|
|2505.23212v2|[Interspeech 2025 URGENT Speech Enhancement Challenge](http://arxiv.org/abs/2505.23212v2)|贡献点：<br/>1. 引入Interspeech 2025 URGENT Challenge，延续并拓展了通用语音增强研究框架；<br/>2. 系统评估语言依赖性、更广泛失真类型适应性、数据可扩展性及噪声训练数据有效性四个关键方向；<br/>3. 收集了32个系统提交，揭示了判别模型与混合方法在性能上的差异；<br/>4. 发现生成/混合方法在主观评价中优于判别模型，同时指出纯生成模型存在语言依赖问题。<br/><br/>总结（99字）：  <br/>该研究通过Interspeech 2025 URGENT挑战，系统探索通用语音增强的四个核心问题，揭示生成和混合方法在主观评价中的优势，同时指出纯生成模型存在语言依赖性，为领域发展提供新方向。|
|2506.01023v1|[A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech   Enhancement](http://arxiv.org/abs/2506.01023v1)|总结：  <br/>本文提出HDF-Net模型，通过子带处理与深度滤波的整合，结合两阶段框架和TAConv模块，有效提升单通道语音增强性能，同时减少资源消耗。<br/><br/>贡献点：  <br/>1. **提出新型音频处理框架**：首次将子带处理与深度滤波相结合，利用目标时频(bin)及其周围时频信息进行联合建模。  <br/>2. **分解深度滤波模块**：将深度滤波拆分为时域和频域两个子模块，并采用两阶段框架，显著降低滤波系数预测复杂度。  <br/>3. **设计TAConv模块**：提出专门增强卷积特征提取的TAConv模块，提升模型对时频特征的学习能力。  <br/>4. **实验验证有效性**：在资源受限条件下，HDF-Net在语音增强任务中超越现有先进系统，证明了其高效性和优越性。|
|2506.00809v1|[FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse   Compression and Token Generation Models for the URGENT 2025 Challenge](http://arxiv.org/abs/2506.00809v1)|**贡献点总结（100字以内）:**  <br/>本研究提出多阶段语音增强框架，融合稀疏压缩网络、自监督生成模型与信号融合策略，创新性引入时间位移技巧与输出混合方法，在多语言、多采样率数据集上有效提升语音质量与鲁棒性。<br/><br/>**分点贡献:**  <br/>1. **多阶段框架设计**：提出包含源分离、生成模型重构和信号融合的分阶段处理流程，针对URGENT Challenge优化多语言语音增强任务。  <br/>2. **稀疏压缩网络**：首次利用稀疏压缩技术从噪声中分离声源并提取初始干净语音估计，提升抗噪能力。  <br/>3. **自监督生成模型**：开发基于神经音频编解码器的生成模型，通过自监督特征与掩码语言建模目标优化语音质量。  <br/>4. **信号融合策略**：设计融合网络，结合多阶段输出与原始噪声信号，平衡信号保真度与感知质量。  <br/>5. **时间位移技巧**：引入时间位移策略，整合多时间步预测结果以增强模型鲁棒性。  <br/>6. **输出混合方法**：提出输出混合技术，进一步优化最终语音质量并提升系统性能。  <br/>7. **多场景验证**：在包含多语言、多采样率及多种失真类型的复杂数据集上验证方法的有效性。|
|2505.19314v2|[SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline](http://arxiv.org/abs/2505.19314v2)|总结：  <br/>提出SoloSpeech，一种无需说话人嵌入的生成式语音提取框架，在保持高自然度的同时解决环境敏感性问题，并在多个任务中取得SOTA性能。<br/><br/>贡献点：  <br/>1. **提出新型生成式框架**：构建了包含压缩、提取、重建和修正的级联生成模型，替代传统判别模型。  <br/>2. **无说话人嵌入设计**：采用条件信息对齐混音与cue音频潜在空间，消除对说话人嵌入的依赖。  <br/>3. **提升性能与泛化能力**：在Libri2Mix数据集上实现语音提取与分离的SOTA，且对未标注数据和实际场景具有更强适应性。|
|2505.24576v1|[A Composite Predictive-Generative Approach to Monaural Universal Speech   Enhancement](http://arxiv.org/abs/2505.24576v1)|总结：  <br/>提出PGUSE模型，结合预测与生成建模，通过输出融合和截断扩散技术提升语音增强性能，实验验证其优于现有方法。<br/><br/>贡献点：  <br/>1. **提出统一框架PGUSE**：首次整合预测模型与扩散生成模型，解决单一模型难以兼顾多种失真抑制的难题。  <br/>2. **双分支协同机制**：预测分支直接生成干净语音，生成分支优化扩散模型的去噪目标，实现互补性增强。  <br/>3. **创新融合技术**：采用输出融合策略直接结合双分支结果，并通过截断扩散方案利用预测初期估计优化逆过程。  <br/>4. **有效实验验证**：在多种数据集上对比实验表明，该模型在语音质量与计算效率方面均优于现有技术。|
|2505.24450v1|[SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition](http://arxiv.org/abs/2505.24450v1)|**贡献点**  <br/>1. **提出直接声场估计（DSE）**：用于估计真实远场数据中的理想直达声，解决实际数据缺乏目标语音标注的问题。  <br/>2. **设计伪监督学习方法SuPseudo**：通过DSE生成伪标签，使语音增强模型能够直接适应真实数据，提升泛化能力。  <br/>3. **开发FARNET模型**：基于SuPseudo方法，构建专门用于远场语音增强的模型，实现对真实数据的全面利用。  <br/>4. **实验证明有效性**：在MISP2023数据集上验证SuPseudo和FARNET的性能，系统显著优于先前的SOTA方法。  <br/><br/>**总结**：  <br/>本研究提出DSE与SuPseudo方法，解决远场语音增强模型训练数据不足的问题，设计FARNET模型并验证其有效性，显著提升真实场景下的语音增强性能。|
|2505.24446v1|[Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge](http://arxiv.org/abs/2505.24446v1)|**贡献点：**  <br/>1. 提出G-SpatialNet，作为新型语音增强模型，提升指导性声分离（GSS）信号的清晰度。  <br/>2. 设计TLS框架（时间对齐、水平对齐、信噪比滤波），生成信号级伪标签以优化真实远场语音数据的SE模型训练。  <br/>3. 探索微调策略、数据增强及多模态信息融合，有效提升预训练ASR模型在会议场景中的性能。  <br/><br/>**总结（100字以内）：**  <br/>本研究针对会议语音挑战提出三项创新方法，包括语音增强模型、伪标签生成框架及多模态ASR优化策略，显著提升了鲁棒性和识别率，并在挑战赛中取得第二名成绩。|
|2505.23744v1|[Boosting Domain Incremental Learning: Selecting the Optimal Parameters   is All You Need](http://arxiv.org/abs/2505.23744v1)|**总结（100字以内）:**  <br/>提出SOYO轻量框架，通过GMC和DFR优化领域数据存储与平衡，结合MDFN提升特征提取能力，支持多种PEFT方法，在多任务和基准中验证其有效性，代码开源以促进研究。<br/><br/>---<br/><br/>**贡献点分点列表:**  <br/>1. **提出SOYO框架**：首次设计轻量级解决方案，解决PIDIL中参数选择不准确导致的知识冲突问题。  <br/>2. **引入高效数据处理机制**：通过Gaussian Mixture Compressor (GMC)和Domain Feature Resampler (DFR)，优化领域数据的存储与分布平衡。  <br/>3. **开发多级特征融合网络**：Multi-level Domain Feature Fusion Network (MDFN)提升领域特征提取能力，增强模型泛化性。  <br/>4. **兼容多种PEFT方法**：支持参数高效微调技术，实现动态环境下的轻量模型更新与迁移学习。  <br/>5. **跨任务验证有效性**：在图像分类、目标检测和语音增强等任务中进行实验，证明其在复杂环境中的鲁棒性和适应性。  <br/>6. **开源代码促进研究**：发布代码库，提升方法可复现性，推动语音领域的相关研究与应用。|
|2505.23515v1|[DeepFilterGAN: A Full-band Real-time Speech Enhancement System with   GAN-based Stochastic Regeneration](http://arxiv.org/abs/2505.23515v1)|总结：  <br/>该论文提出了一种基于GAN的实时全频语音增强系统，通过结合预测模型与生成模型降低失真，采用轻量架构实现高效处理，并验证了噪声条件对性能的重要性。<br/><br/>贡献点：  <br/>1. **提出全频实时语音增强系统**：首次将GAN-based stochastic regeneration框架应用于实时语音流处理。  <br/>2. **结合预测与生成模型**：通过联合预测模型（估计均值）和生成模型（学习完整分布）减少过抑制和输出失真。  <br/>3. **轻量级高效架构**：设计参数量为358万、延迟低的模型，适用于实时性要求高的场景。  <br/>4. **实验验证性能提升**：在NISQA-MOS指标上超越传统方法，证明系统有效性。  <br/>5. **消融实验突出噪声条件**：证明噪声输入对语音增强结果的关键作用。  <br/>6. **挑战赛应用与改进**：在Urgent Challenge中部署模型并基于反馈进行性能优化。|
|2505.15914v2|[A Novel Deep Learning Framework for Efficient Multichannel Acoustic   Feedback Control](http://arxiv.org/abs/2505.15914v2)|**总结**：  <br/>提出一种基于深度学习的多通道声学反馈控制框架，结合卷积循环网络与三种创新训练方法，有效提升语音增强性能，降低计算需求，并具备良好的可扩展性。<br/><br/>**贡献点**：  <br/>1. **框架设计**：开发了首个深度学习框架，用于控制多通道声学反馈，解决了传统数字信号处理方法在处理高度相关噪声时的收敛难题。  <br/>2. **模型创新**：引入卷积循环网络（CRN），创新性地融合空间和时序信息处理，显著提高了语音增强效率并降低了计算负担。  <br/>3. **训练策略**：提出三种互补的训练方法（In-a-Loop Training、Teacher Forcing、与多通道维纳滤波器结合的混合策略），优化复杂声学环境下的系统性能。  <br/>4. **实用性突破**：构建了可扩展性强、适用于实际场景的框架，推动声学反馈控制技术向高效、鲁棒方向发展。|
|2505.22051v2|[ARiSE: Auto-Regressive Multi-Channel Speech Enhancement](http://arxiv.org/abs/2505.22051v2)|总结：  <br/>提出ARiSE自回归算法，通过前帧语音特征与波束形成混合提升多通道语音增强性能，并设计并行训练机制加速训练，验证了其在噪声回声环境下的有效性。<br/><br/>贡献点：  <br/>1. **引入自回归机制**：在多通道语音增强中，利用前帧估计的语音作为额外输入特征，提升当前帧的估计精度。  <br/>2. **双路径特征生成**：提出两种方式提取额外输入特征：（a）基于前帧估计语音；（b）结合波束形成技术，使用前帧估计结果计算混合信号。  <br/>3. **并行训练策略**：设计平行训练机制，克服自回归训练效率低的问题，显著缩短训练时间。  <br/>4. **有效验证**：在噪声-混响环境下的实验结果表明该算法具有显著性能提升和实际应用潜力。|
|2505.21198v1|[Universal Speech Enhancement with Regression and Generative Mamba](http://arxiv.org/abs/2505.21198v1)|总结：  <br/>提出USEMamba状态空间模型，实现跨语言、多失真类型的通用语音增强，在部分数据下仍取得优异盲测成绩。<br/><br/>贡献点：  <br/>1. **统一多任务框架**：首次整合七种失真类型与五种语言，构建跨条件语音增强统一挑战平台（URGENT）  <br/>2. **创新模型架构**：设计支持长程序列建模、时频结构化处理及采样频率独立特征提取的Mamba模型  <br/>3. **混合建模策略**：采用回归建模应对常规失真，针对数据包丢失与带宽扩展等缺失内容场景开发生成式变种  <br/>4. **高效泛化能力**：仅使用部分训练数据即在盲测中取得第二名，验证模型对未知条件的强适应性|
|2505.21156v1|[Model as Loss: A Self-Consistent Training Paradigm](http://arxiv.org/abs/2505.21156v1)|总结：  <br/>提出Model as Loss新范式，利用同一模型编码器作为损失函数，提升语音增强的感知质量与跨域泛化能力，优于传统手工/深度特征损失方法。<br/><br/>贡献点：<br/>1. **创新损失函数设计**：首次将模型自身编码器作为损失函数，替代传统手工或预训练深度特征损失，更精准捕捉语音信号细节。  <br/>2. **任务相关特征对齐**：通过编码器的语境感知特征空间，优化解码器输出，使增强语音与原始干净信号在任务关键属性上保持一致。  <br/>3. **自一致性约束**：强制清洁参考语音与增强输出之间在特征空间内的一致性，提升模型训练的内在逻辑性与稳定性。  <br/>4. **性能突破**：在多个标准语音增强数据集上验证，显著优于现有方法，实现更优感知质量与对域内外数据的鲁棒泛化。|
|2505.21057v1|[Study of Lightweight Transformer Architectures for Single-Channel Speech   Enhancement](http://arxiv.org/abs/2505.21057v1)|**贡献点：**<br/>1. 提出轻量化因果变压器架构LCT-GAN，通过Frequency-Time-Frequency（FTF）堆叠方式高效建模全局时频依赖，显著降低计算复杂度和参数量。<br/>2. 引入对抗训练机制，在训练阶段提升语音增强效果，且不增加推理时的计算负担。<br/>3. 在多个基准测试中验证LCT-GAN的优越性：相比DeepFilterNet2减少95%参数（仅需6%），相比CCFNet+(Lite)降低10%乘加操作并提升性能，且超越常见复杂基线模型。<br/><br/>**总结（100字以内）：**  <br/>本文提出轻量化因果Transformer架构LCT-GAN，通过优化时频建模与对抗训练，在保持SotA性能的同时大幅降低计算成本，实现在边缘设备上的高效语音增强应用。|
|2505.19476v2|[FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching](http://arxiv.org/abs/2505.19476v2)|总结：  <br/>本文提出FlowSE，一种基于流匹配的语音增强模型，解决了传统方法的延迟与质量问题，支持文本辅助和不依赖文本的推理，并通过实验证明其有效性，推动生成式语音增强的发展。<br/><br/>贡献点：  <br/>1. **提出FlowSE模型**：首个将流匹配（flow matching）应用于语音增强的生成模型，突破传统方法的局限。  <br/>2. **减少推理延迟**：通过单次通过学习连续变换，显著降低推理耗时并保持高质量语音重建。  <br/>3. **多模态训练策略**：支持以噪声mel谱图和可选文本序列作为输入，优化条件流匹配损失以提升性能。  <br/>4. **隐式学习语音结构**：无需显式设计，自动捕捉语音的时频结构与文本-语音对齐特性。  <br/>5. **文本辅助的灵活性**：推理阶段可使用或不使用文本信息，兼顾通用性与特定场景的性能提升。  <br/>6. **实验证明效果优越**：在多个指标上超越现有生成式语音增强方法，验证流匹配的潜力。|
|2505.20741v1|[Uni-VERSA: Versatile Speech Assessment with a Unified Network](http://arxiv.org/abs/2505.20741v1)|总结：  <br/>提出Uni-VERSA统一模型，整合多指标评估，提升全面性与实用性，与人类感知一致，为语音质量评估提供新方案。<br/><br/>贡献点：  <br/>1. **提出多任务统一评估模型**：Uni-VERSA首次同时预测自然度、可懂度、说话人特征、韵律和噪声等多维度语音质量指标，突破传统单一指标评估的局限。  <br/>2. **建立统一框架与协议**：系统化设计评估框架、验证协议及在语音增强、合成和质量控制中的应用，推动客观评估方法的标准化。  <br/>3. **验证有效性**：基于URGENT24挑战基准测试及自监督表示基线，证明其优于单指标方法，在多任务场景中表现优异。  <br/>4. **提高与人类感知的一致性**：通过多维度评估与主观测试结果对比，展现其对真实听觉体验的高匹配度，为未来研究提供可靠方向。|
|2505.19597v1|[A Lightweight Hybrid Dual Channel Speech Enhancement System under   Low-SNR Conditions](http://arxiv.org/abs/2505.19597v1)|总结：  <br/>提出轻量级混合双通道语音增强系统，融合IVA与改进GTCRN，优化低SNR下的语音质量与计算效率。<br/><br/>贡献点：  <br/>1. **轻量化设计**：构建低参数量、低计算复杂度的双通道语音增强系统，适配资源受限场景。  <br/>2. **混合方法创新**：结合独立矢量分析（IVA）与改进的双通道分组时序卷积循环网络（GTCRN），实现粗估计与精细优化的协同。  <br/>3. **信息利用优化**：提出多组改进策略，提升原始语音与辅助噪声信息的融合效率。  <br/>4. **实验验证有效性**：通过实验验证系统在低SNR条件下的性能优势，证明其实际部署潜力。|
|2505.03273v2|[SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation](http://arxiv.org/abs/2505.03273v2)|**贡献点**  <br/>1. **提出SepALM框架**：首次将音频语言模型（ALMs）引入语音分离后处理流程，实现分离后的语音校正与文本域重合成。  <br/>2. **四阶段处理流程**：包含分离器、校正器、合成器和对齐器四个核心模块，系统化解决现实环境中的语音分离挑战。  <br/>3. **端到端错误校正机制**：通过ALM消除了传统方法中ASR与LLMs结合的误差累积问题，提升准确性与鲁棒性。  <br/>4. **创新提示与训练技术**：开发Chain-of-Thought（CoT）提示及知识蒸馏技术，优化ALM的推理能力和训练效率。  <br/>5. **实验验证效果**：在噪声和混响环境下，SepALM显著提升语音分离精度并增强对新声学场景的适应性。  <br/><br/>**总结（100字内）**  <br/>本文提出SepALM，将音频语言模型应用于语音分离后处理，通过分离-校正-合成-对齐四阶段流程，有效解决现实场景中的混响和噪声问题，结合CoT提示与知识蒸馏技术，提升模型精度与适应性。|
|2505.19576v1|[Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech   Enhancement](http://arxiv.org/abs/2505.19576v1)|总结：  <br/>本文提出Mel-McNet框架，在Mel频率域处理多通道语音增强，通过STFT-to-Mel模块和修改的McNet结构显著降低计算复杂度并提升性能，验证了Mel尺度在语音增强中的有效性。<br/><br/>贡献点：  <br/>1. **首次在Mel频率域应用多通道语音增强框架**：提出Mel-McNet，突破传统线性域方法的局限性。  <br/>2. **双组件设计**：包含STFT-to-Mel模块（压缩多通道STFT至Mel表示）和Mel域优化的McNet骨干网络（生成增强LogMel谱）。  <br/>3. **高效性与性能平衡**：实验证明在CHiME-3数据集上，计算复杂度降低60%的同时保持与原McNet相当的增强和ASR效果。  <br/>4. **超越SOTA方法**：在性能上优于当前主流技术，验证Mel尺度语音增强的潜力。|
|2505.19534v1|[Training-Free Multi-Step Audio Source Separation](http://arxiv.org/abs/2505.19534v1)|总结：  <br/>本文提出无需额外训练的多步音频分离方法，通过迭代融合和优化比例提升性能，理论证明其有效性并关联到扩散模型，实验证明在语音和音乐分离任务中均优于单步方法，可实现类似大模型的扩展效果。<br/><br/>贡献点：  <br/>1. **多步推理框架**：首次提出利用预训练单步模型进行多步音频分离，无需额外训练即可提升分离性能。  <br/>2. **优化融合策略**：设计迭代分离方法，通过动态调整输入混合物与前一步分离结果的融合比例最大化分离效果。  <br/>3. **理论保障**：证明多步方法优于单步推理，提供基于模型平滑性和度量鲁棒性的误差界限，并建立与线性插值路径去噪的理论联系。  <br/>4. **跨任务有效性**：实验证明方法在语音增强和音乐分离任务中均显著优于单步方法，且具备类似大模型的扩展能力。  <br/>5. **非优化指标提升**：方法改进不仅体现在优化指标上，还扩展到几乎所有非优化指标（唯一代价指标除外）。  <br/>6. **研究局限与展望**：讨论了方法的局限性并提出未来研究方向。|
|2505.19401v1|[Stack Less, Repeat More: A Block Reusing Approach for Progressive Speech   Enhancement](http://arxiv.org/abs/2505.19401v1)|总结：  <br/>本论文提出一种通过重复单个处理块实现语音增强的高效方法，减少参数冗余并优化处理阶段设计，验证了其在性能和效率上的优势。<br/><br/>贡献点：  <br/>1. **提出重用单块模型结构**：采用重复单一处理块而非传统堆叠多块的架构，提升模型效率并减少参数冗余。  <br/>2. **序列建模块重用策略**：通过保持编码器/解码器浅层并重复单一序列建模块，降低领域变换复杂度。  <br/>3. **处理阶段数量优先于块数量**：实验表明，增加处理阶段数量比增加块数量更显著提升性能。  <br/>4. **单块内逐步细化机制**：揭示单个处理块可通过内部迭代逐步优化噪声输入，无需多块协作。  <br/>5. **编码器/解码器深度优化**：证明加深编码器和解码器在块重用框架下可能冗余，简化复杂表征学习。|
|2505.16351v2|[Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection](http://arxiv.org/abs/2505.16351v2)|**贡献点总结（100字以内）:**  <br/>本研究提出Dysfluent-WFST，实现零样本解码，同时进行音素转录与语音不流畅性检测，无需额外训练。在模拟及真实数据上达到SOTA性能，具备轻量、可解释性，强调显式建模发音行为对提升系统性能的关键作用。<br/><br/>**分点贡献:**  <br/>1. **提出Dysfluent-WFST**：首个零样本解码器，同步完成音素转录与不流畅性检测，集成上游编码器（如WavLM）无需额外训练。  <br/>2. **解决传统方法局限**：突破仅依赖分类的临床洞察不足，以及文本无关模型在上下文相关场景的误判问题。  <br/>3. **性能突破**：在语音错误率与不流畅性检测任务上均达到SOTA，验证模型有效性。  <br/>4. **轻量化与可解释性**：方法设计简洁，适合实际应用，便于临床理解和部署。  <br/>5. **理论贡献**：证明显式建模发音行为（而非复杂架构）是提升不流畅性处理系统性能的核心。|
|2505.18533v1|[TS-URGENet: A Three-stage Universal Robust and Generalizable Speech   Enhancement Network](http://arxiv.org/abs/2505.18533v1)|**贡献点分点列出：**  <br/>1. **提出三阶段架构（Filling-SEparation-Restoration）**：通过填充、分离、恢复三阶段协同处理语音信号，系统性解决多种干扰问题。  <br/>2. **增强鲁棒性与通用性**：填充阶段在噪声下补全丢失段，分离阶段联合抑制噪声、混响和 clipping，恢复阶段补偿带宽限制和编解码伪影，实现多场景适应。  <br/>3. **显著提升性能**：在Interspeech 2025 URGENT Challenge中取得Track 1第二名，验证了方法在复杂语音增强任务中的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出TS-URGENet，通过三阶段架构（补全、去噪、恢复）解决语音增强中的多样干扰，兼顾鲁棒性与通用性，实验在国际挑战赛中表现优异，为通用语音增强提供了新方案。|
|2505.16911v2|[Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation](http://arxiv.org/abs/2505.16911v2)|总结:  <br/>提出主动语音增强(ASE)新范式，结合Transformer-Mamba架构与任务特定损失函数，实现语音信号的主动调控，在去噪、降混响等任务中超越现有基线。<br/><br/>贡献点:  <br/>1. **提出主动语音增强新框架**：区别于传统ANC，ASE通过同时抑制噪声和增强语音频率提升清晰度与感知质量。  <br/>2. **设计混合架构**：提出基于Transformer-Mamba的创新模型，融合Transformer的长期依赖建模与Mamba的高效状态空间处理。  <br/>3. **开发联合优化损失函数**：构建任务特定损失函数，同步优化干扰抑制与信号增强目标。  <br/>4. **验证多任务有效性**：在去噪、降混响、降限幅等任务中超越现有方法，证明主动调制在复杂声学环境中的优势。|
|2505.16607v1|[Attractor-Based Speech Separation of Multiple Utterances by Unknown   Number of Speakers](http://arxiv.org/abs/2505.16607v1)|**贡献点（分点）：**  <br/>1. 提出首个同时处理多说话人语音分离、动态说话人数估计和个体说话人活动检测的集成模型。  <br/>2. 引入基于吸引子（attractor）的架构，有效融合局部与全局时序建模能力，提升多语句场景下的分离效果。  <br/>3. 构建多说话人多语句合成数据集（结合Librispeech与WHAM!噪声），用于验证方法在混响和噪声环境下的鲁棒性。  <br/>4. 实验结果表明，系统在已知及未知说话人数场景中均能准确估计声源数量并生成正确的分离输出。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种集成模型，通过吸引子架构融合时序建模能力，实现动态说话人数估计与多语句语音分离，并构建合成数据集验证其在复杂环境下的有效性，显著提升了单通道语音分离性能。|
|2505.13830v2|[Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising](http://arxiv.org/abs/2505.13830v2)|总结：  <br/>本文提出基于神经编码器的语音降噪模块，结合LauraTTS实现噪声鲁棒的零样本语音合成，在降噪效果和模型性能上均优于现有技术。<br/><br/>贡献点：  <br/>1. **提出神经编码器语音降噪器**：设计包含音频编码、令牌降噪和嵌入优化的三阶段降噪框架。  <br/>2. **实现噪声鲁棒的零样本TTS**：将降噪模块与LauraTTS集成，显著提升噪声环境下的语音合成质量。  <br/>3. **改进模型性能**：通过令牌降噪预测前两组干净的声学标记，结合嵌入优化器和解码器生成高质量语音，超越现有SE方法及基于额外SE模型的替代方案。|
|2505.15254v1|[Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice   Conversion Framework](http://arxiv.org/abs/2505.15254v1)|总结（100字以内）：<br/>该研究提出结合生成性语音修复和语音转换的双阶段系统，通过前端噪声抑制和后端说话人嵌入引导修复，有效解决VC模型在噪声环境下的脆弱性，实现与SOTA相当的语音质量提升。<br/><br/>贡献点：<br/>1. 提出双阶段语音增强框架：将说话人无关的生成性语音修复（GSR）与语音转换（VC）相结合，形成分层处理流程。<br/>2. 解决VC模型噪声敏感问题：在VC前端添加GSR模块，直接处理噪声干扰和语音损伤。<br/>3. 创新性引入嵌入引导机制：VC阶段利用干净说话人嵌入作为指导信号，提升语音质量。<br/>4. 达到SOTA性能表现：在多数据集上验证，其语音质量客观指标与当前最佳方法相当。|
|2505.13029v2|[MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for   Speech Enhancement](http://arxiv.org/abs/2505.13029v2)|**贡献点**  <br/>1. **提出混合模型MDDM**：结合多视角判别式增强与扩散模型，突破传统单一流派方法的局限。  <br/>2. **三域特征输入**：通过时间、频率和噪声多模态特征协同提升频谱生成质量。  <br/>3. **优化采样策略**：利用判别式输出与干净目标的分布交集，减少采样步骤以提升效率。  <br/>4. **跨数据集验证**：在公开数据集和真实场景数据集上验证模型有效性，兼具主观与客观指标表现。  <br/><br/>**总结**（100字以内）:  <br/>本文提出MDDM，融合多视角判别式增强与扩散模型，通过多模态特征输入和优化采样策略提升语音增强效果，显著减少计算成本并在多数据集验证中表现优异。|
|2505.14433v1|[Single-Channel Target Speech Extraction Utilizing Distance and Room   Clues](http://arxiv.org/abs/2505.14433v1)|总结：  <br/>该论文提出一种结合距离线索和房间信息的单通道目标语音提取模型，通过可学习的嵌入提升系统在不同环境下的泛化能力，并在仿真与真实数据上验证其有效性。<br/><br/>贡献点：  <br/>1. **引入房间环境信息**：首次将房间维度和混响时间等环境参数纳入距离线索的TSE框架，以增强系统对不同声学环境的适应性。  <br/>2. **提出时间频率域模型**：设计基于时频域的可学习距离与房间嵌入模型，有效融合两种信息进行目标语音提取。  <br/>3. **提升泛化能力**：通过结合环境信息，解决传统仅依赖距离线索的TSE系统在房间变化时性能下降的问题。  <br/>4. **实验验证可行性**：在仿真和真实数据集上验证方法的有效性，证明其在实际场景中的适用性。  <br/>5. **提供可复现资源**：公开演示材料，便于研究者进一步验证和应用该方法。|
|2505.13983v1|[Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding   for Diffusion-Based Speech Enhancement](http://arxiv.org/abs/2505.13983v1)|总结：  <br/>本论文提出双流编码修复扩散模型（DERDM-SE），通过结合粗粒度与细粒度处理及两种条件（仅确定性特征和混合确定性-噪声特征）提升语音增强效果，实验验证其在客观指标和稳定性上均优于现有扩散模型。<br/><br/>贡献点：  <br/>1. **分析条件影响**：系统评估不同确定性语音增强模型作为扩散条件的效果，发现其对听觉体验的提升作用。  <br/>2. **提出双流编码框架**：设计双条件输入的DERDM-SE模型，有效融合确定性特征与噪声特征，增强预测准确性。  <br/>3. **混合处理机制**：引入结合粗粒度和细粒度处理的确定性模型，在保持扩散稳定性的同时提升客观评价指标。  <br/>4. **实验证明优势**：在CHiME4数据集上验证了模型的高效性，实现更优的语音增强评分及更稳定的性能表现。|
|2505.13843v1|[A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model](http://arxiv.org/abs/2505.13843v1)|总结：  <br/>本文提出一种基于语义信息的分步因子化语音增强方法，结合因子化编解码器与扩散模型，通过层次化建模提升复杂环境下的语音恢复效果，并显著增强下游TTS任务的性能，实验结果优于SOTA基线。<br/><br/>贡献点：  <br/>1. **提出语义驱动的分步因子化SE框架**：首次将语义信息直接整合进语音增强流水线，结合因子化编解码器与扩散模型，实现对语音信号的分阶段重构。  <br/>2. **层次化建模语义与声学属性**：通过解耦语义内容和声学细节，增强模型在复杂噪声环境中的鲁棒性，突破传统方法仅依赖频谱或掩码的局限。  <br/>3. **提升下游TTS任务性能**：改进后的语音增强结果显著优化了噪声环境下的语音到文本生成效果，拓展了语音处理技术的实际应用价值。  <br/>4. **实验验证优越性**：在语音质量指标（如PESQ）和TTS任务表现上均超越现有SOTA基线，证明方法在复杂场景下的有效性。|
|2505.13094v1|[Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech   Separation](http://arxiv.org/abs/2505.13094v1)|**贡献点：**  <br/>1. 提出Time-Frequency Attention Cache Memory (TFACM)模型，通过结合注意力机制与缓存内存解决因果语音分离中历史信息保留问题。  <br/>2. 引入LSTM层处理频率相关的位置关系，同时通过局部与全局表示实现时间维度上的因果建模。  <br/>3. 设计缓存内存模块（CM）存储历史信息，并采用因果注意力细化（CAR）模块增强时间特征表示。  <br/>4. 实验证明TFACM在性能上与现有SOTA模型（TF-GridNet-Causal）相当，但显著降低模型复杂度和参数量。  <br/><br/>**总结（100字内）：**  <br/>本研究提出TFACM模型，通过融合注意力机制与缓存内存解决因果语音分离中历史信息丢失问题，实现性能与复杂度的优化，为高效语音分离提供了新思路。|
|2505.12686v1|[RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations](http://arxiv.org/abs/2505.12686v1)|总结：  <br/>提出了RoVo，通过在高维嵌入向量注入对抗性扰动并重构语音，显著提升防御成功率，同时增强对语音增强攻击的抵抗力，并保持语音自然性。<br/><br/>贡献点：  <br/>1. **方法创新**：提出RoVo（Robust Voice），通过注入对抗性扰动到音频信号的高维嵌入向量中，而非直接作用于原始音频，有效规避语音增强方法的抵消作用。  <br/>2. **防御效果提升**：在四类先进语音合成模型上，将防御成功率（DSR）提升超70%，在商用语音验证API中达到99.5%的DSR，显著优于传统方法。  <br/>3. **抗语音增强能力**：扰动在强语音增强条件下仍保持鲁棒性，有效抵御二次攻击威胁。  <br/>4. **用户体验保障**：通过用户研究验证，所保护的语音在自然性和功能性上无明显损失，适用于复杂威胁场景。|
|2505.12288v1|[Unified Architecture and Unsupervised Speech Disentanglement for Speaker   Embedding-Free Enrollment in Personalized Speech Enhancement](http://arxiv.org/abs/2505.12288v1)|**贡献点总结（100字以内）：**  <br/>本研究提出统一模型USEF-PNet，整合语音增强与个性化增强任务；创新性地引入DSEF-PNet的无监督语音解缠绕方法，提升PSE鲁棒性；并通过LSEP策略分析参考语音时长的影响，验证了其有效性。  <br/><br/>**具体贡献点分点：**  <br/>1. **统一模型架构**：设计USEF-PNet，首次将传统语音增强（SE）与个性化语音增强（PSE）整合为单一框架，简化部署并提升性能。  <br/>2. **无监督语音解缠绕**：在DSEF-PNet中，通过混合语音与双参考语句的配对，强制一致性约束，有效分离目标说话人身份信息，减少情感和内容干扰，增强PSE的鲁棒性。  <br/>3. **长-短参考语音策略**：提出LSEP策略，研究参考语音时长对训练和评估的动态影响，验证了随机时长参考语音在PSE中的优越性。|
|2505.12079v1|[SepPrune: Structured Pruning for Efficient Deep Speech Separation](http://arxiv.org/abs/2505.12079v1)|**贡献点分点列出：**  <br/>1. **首个结构化剪枝框架**：提出SepPrune，是首个专门针对语音分离模型的结构化压缩方法，兼顾分离质量与计算效率。  <br/>2. **计算结构分析与关键层识别**：通过分析模型计算结构，定位高计算负担的层，优化剪枝策略。  <br/>3. **可微分掩码策略**：引入基于梯度的通道选择机制，实现动态调整通道的可微分优化。  <br/>4. **高效性能恢复机制**：结合冗余通道剪枝与参数微调，在极少的训练轮次下（1轮）恢复原模型85%性能。  <br/>5. **显著提升收敛速度**：剪枝模型收敛速度比从头训练快36倍，降低实时应用的计算成本。  <br/>6. **开源实现支持**：提供开源代码，推动方法在语音分离领域的研究与实际应用。  <br/><br/>**总结（100字以内）:**  <br/>提出SepPrune框架，首次将结构化剪枝应用于语音分离，通过计算结构分析与可微分通道选择实现高效模型压缩，显著提升性能恢复效率与收敛速度，代码开源以促进应用。|
|2505.05657v2|[ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior](http://arxiv.org/abs/2505.05657v2)|**总结（100字以内）:**  <br/>提出ArrayDPS方法，实现无监督、阵列无关的盲语音分离。通过改进扩散后验采样，引入独立优化问题近似似然，建模房间声学与麦克风传递函数，仅需单人语音扩散模型作为先验。实验表明其SDR优于无监督方法，接近有监督水平。<br/><br/>**贡献点分点列出:**  <br/>1. **提出ArrayDPS方法**：首次在盲语音分离中引入无监督、阵列无关、生成式的框架，无需麦克风阵列几何信息。  <br/>2. **改进扩散后验采样**：通过建立独立优化问题近似似然，解决盲逆问题中房间声学和相对传输函数建模难题。  <br/>3. **简化先验需求**：仅依赖单人语音扩散模型作为先验，降低对复杂后验分布的依赖。  <br/>4. **性能优势**：在无监督方法中表现最优，同时SDR指标接近有监督方法，兼具高效与高精度。  <br/>5. **提供可复现实验**：配套音频演示，便于验证方法的实际效果与应用潜力。|
|2505.03186v2|[CoGenAV: Versatile Audio-Visual Representation Learning via   Contrastive-Generative Synchronization](http://arxiv.org/abs/2505.03186v2)|总结（100字以内）:<br/>CoGenAV通过融合对比学习和生成式文本预测，提出一种数据高效的跨模态同步模型，显著提升AVSR和VSR性能，并在噪声环境中表现优异，模型开源以推动学术与工业应用。<br/><br/>贡献点：<br/>1. **多模态同步建模**：首次整合唇部运动、语音与语言内容的天然同步性，构建跨模态关联学习框架。<br/>2. **双目标优化策略**：结合对比特征对齐（contrastive feature alignment）与生成文本预测（generative text prediction），提升表征学习能力。<br/>3. **数据高效性**：仅需LRS2数据集的223小时标注数据，实现低数据成本的高泛化能力。<br/>4. **任务泛化能力**：在AVSR（WER 1.27）、VSR（WER 20.5）、语音增强/分离以及ASD等任务中均取得领先性能。<br/>5. **噪声鲁棒性**：在噪声环境中性能提升超70%，解决传统音频系统在复杂场景下的脆弱性。<br/>6. **开源推动**：模型开放源码，促进跨领域研究与实际应用的发展。|
|2505.08694v1|[A Survey of Deep Learning for Complex Speech Spectrograms](http://arxiv.org/abs/2505.08694v1)|**贡献点：**  <br/>1. 系统性总结了深度学习在复杂频谱图处理中的关键技术与方法。  <br/>2. 提出复杂频谱图及其特征在语音分析任务中的具体应用与必要性。  <br/>3. 分析复杂值神经网络（CVNN）的关键组件和架构设计。  <br/>4. 探讨针对复杂频谱图的定制化训练策略与损失函数。  <br/>5. 概述深度学习在相位恢复、语音增强、语音分离等领域的应用进展。  <br/>6. 研究复杂频谱图与生成模型的结合，拓展其在语音处理中的潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文系统梳理了深度学习处理复杂频谱图的技术，涵盖网络设计、训练策略及应用，为语音领域研究提供全面参考。|
|2505.05216v1|[Normalize Everything: A Preconditioned Magnitude-Preserving Architecture   for Diffusion-Based Speech Enhancement](http://arxiv.org/abs/2505.05216v1)|**贡献点**  <br/>1. **方法创新**：提出基于Schroedinger桥的扩散模型框架，将噪声语音分布转化为干净语音分布。  <br/>2. **预处理技术**：引入时间依赖的输入/输出缩放（preconditioning）以提升训练稳定性与效果。  <br/>3. **网络结构设计**：设计两种跳连配置，分别预测环境噪声或干净语音，适应不同增强需求。  <br/>4. **幅度保持架构**：采用归一化激活与权重的网络结构，维持训练中的幅度平衡。  <br/>5. **输入条件优化**：在每个网络块中学习噪声输入的贡献，增强输入条件化效果。  <br/>6. **EMA策略分析**：提出近似EMA曲线并验证其效果，发现较短EMA长度在语音增强任务中表现更优。  <br/>7. **资源开放**：提供代码、音频示例和预训练模型，便于复现与研究。  <br/><br/>**总结**：本研究提出基于Schroedinger桥的扩散模型语音增强框架，结合预处理、跳连设计与EMA优化，提升了多维度语音质量，并开放了相关资源。|