|Source|Title|Summary|
|---|---|---|
|2511.10232v1|[VocalNet-M2: Advancing Low-Latency Spoken Language Modeling via Integrated Multi-Codebook Tokenization and Multi-Token Prediction](https://arxiv.org/abs/2511.10232v1)|总结（100字以内）:  <br/>VocalNet-M2通过多codebook tokenizer和多token预测策略，显著降低语音模型延迟（从725ms至350ms），并在保持性能的同时避免flow-matching模型，为实时语音交互提供了高效解决方案。<br/><br/>贡献点:<br/>1. **提出多codebook tokenizer**：直接生成多codebook语音token，摒弃传统自回归生成流程和flow-matching模型，降低延迟。<br/>2. **创新MTP生成策略**：通过多token预测机制提升生成效率，优化模型性能。<br/>3. **实验证明有效性**：在主流SLMs中验证了延迟降低与性能保持的平衡，首次实现显著的首块延迟优化。<br/>4. **策略对比分析**：系统比较单codebook与多codebook方法，为低延迟SLMs设计提供理论依据与实践指导。|
|2511.10112v1|[FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features](https://arxiv.org/abs/2511.10112v1)|总结（100字以内）:  <br/>提出FabasedVC，融合文本模态信息与音素级SSL特征，结合持续预测器对齐语速韵律，提升音色、韵律、时长相似性及内容完整性，实验验证其优越性。<br/><br/>**贡献点分点：**  <br/>1. **端到端系统设计**：基于VITS构建集成文本模态（文本、音素、语调、BERT）、音素级SSL特征及持续预测器的VC系统，实现语义完整性与目标特征建模。  <br/>2. **多模态文本特征编码**：通过文本特征编码器融合多源文本属性，增强内容表征能力。  <br/>3. **音素级SSL特征处理**：采用平均池化与注意力机制（基于音素时长）将帧级SSL特征转换为音素级特征，提升特征准确性。  <br/>4. **持续预测器集成**：引入持续预测器精准对齐目标说话人语速与韵律，增强输出与目标的匹配度。  <br/>5. **全面性能验证**：实验表明方法在自然度、相似性及内容完整性等指标上显著优于现有系统。|
|2511.10090v1|[ELYADATA & LIA at NADI 2025: ASR and ADI Subtasks](https://arxiv.org/abs/2511.10090v1)|**贡献点分点总结：**  <br/>1. **提出混合方言语音处理系统**：联合Elyadata与LIA团队，在NADI 2025多方言阿拉伯语语音处理任务中首次实现跨任务的系统性研究。  <br/>2. **创新性模型架构**：针对ADI任务，采用微调后的Whisper-large-v3编码器结合数据增强技术，以79.83%的准确率登顶官方测试集。  <br/>3. **多方言ASR优化方法**：为八个阿拉伯语方言分别微调SeamlessM4T-v2 Large（埃及变种），实现平均WER 38.54%和CER 14.53%的显著性能。  <br/>4. **验证预训练模型有效性**：实验证明大型预训练语音模型（如Whisper、SeamlessM4T）通过精细化微调可有效提升多方言阿拉伯语音处理的准确性和鲁棒性。  <br/><br/>**总结（100字以内）**：  <br/>本研究通过微调Whisper和SeamlessM4T模型，结合数据增强技术，在多方言阿拉伯语语音处理任务中取得优异成绩，验证了预训练模型在方言识别与ASR中的高效性。|
|2511.09085v1|[Context-Aware Dynamic Chunking for Streaming Tibetan Speech Recognition](https://arxiv.org/abs/2511.09085v1)|**贡献点总结：**  <br/>1. 提出针对Amdo Tibetan的流式语音识别框架，基于混合CTC/Attention架构与上下文感知动态分块机制。  <br/>2. 动态分块策略根据编码状态自适应调整分块宽度，实现灵活接收场、跨分块信息交互及适应不同说话速率。  <br/>3. 构建基于藏语正字法的词典，提供语言驱动的建模单元以捕捉藏语语言特征。  <br/>4. 集成外部语言模型提升语义一致性与长句识别能力。  <br/>5. 实验验证：框架在测试集上达到6.23%的WER，较固定分块基线提升48.15%，同时减少延迟且性能接近全局解码。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出基于动态分块机制的Amdo Tibetan流式语音识别框架，结合混合CTC/Attention模型与语言学词典，集成语言模型提升性能，实现高效、准确的语音识别。|
|2511.08230v1|[VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230v1)|贡献点：<br/>1. 提出首个针对中文的系统性S2S基准VocalBench-zh，填补领域空白<br/>2. 构建包含10个细分能力维度的评估体系，覆盖12类用户导向场景<br/>3. 收集整理超过10,000个高质量语料实例，确保数据多样性与规范性<br/>4. 通过14个主流模型的实验证明，揭示当前技术的共性挑战与改进方向<br/>5. 开源评估工具与数据集，推动中文语音交互技术的公平比较与持续发展<br/><br/>总结：本研究创建首个中文S2S评估基准VocalBench-zh，通过10个能力维度和10K高质量数据，系统揭示当前语音交互技术挑战，为模型改进提供方向，推动领域发展。|
|2511.08132v2|[National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution](https://arxiv.org/abs/2511.08132v2)|总结（100字以内）: <br/>SpeechCARE通过多模态融合和预训练模型提升ADRD早期检测，结合SHAP解释性模块与偏见缓解技术，实现高准确率分类，并计划拓展至真实医疗场景与EHR系统。<br/><br/>贡献点:<br/>1. 提出SpeechCARE系统：首个基于预训练多语言语音-语言Transformer模型的多模态语音处理框架，整合语音特征、语言结构和人口统计信息。<br/>2. 动态融合架构：采用Mixture of Experts范式，通过动态权重机制实现模态间有效融合，支持扩展社交因素、影像等多模态数据。<br/>3. 影像级预处理：包含自动转录、LLM异常检测与任务识别模块，提升数据质量与任务适应性。<br/>4. 可解释性模块：通过SHAP分析和LLM推理可视化各模态对诊断决策的贡献，增强模型可信度。<br/>5. 性能突破：在AD/MDA分类中达到AUC=0.88，MCI检测AUC=0.90，较传统方法显著提升。<br/>6. 偏见缓解：针对年龄偏见提出过采样和加权损失策略，提升模型公平性（对80岁以上群体的偏差最小化）。<br/>7. 临床落地规划：计划部署至真实医疗场景（如VNS Health）和整合EHR系统，服务纽约市少数群体。|
|2511.07883v2|[SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition](https://arxiv.org/abs/2511.07883v2)|总结：  <br/>本文提出MSTASA模块和SpikCommander架构，通过结合时序感知注意力与多视角学习，提升SNN在语音命令识别中的时序建模与通道特征整合能力，验证了其在参数更少情况下优于SOTA方法的高效性。<br/><br/>贡献点：  <br/>1. **提出MSTASA模块**：融合spiking temporal-aware attention与多视角学习框架，有效建模语音中互补的时序依赖关系，解决传统SNN对复杂时序模式捕捉不足的问题。  <br/>2. **设计SpikCommander架构**：构建全spike驱动的transformer模型，集成MSTASA与spiking contextual refinement channel MLP（SCR-MLP），联合优化时序上下文建模与通道特征整合。  <br/>3. **验证方法有效性**：在SHD、SSC、GSC三个基准数据集上的实验表明，SpikCommander在参数量更少的情况下，仍显著优于现有SOTA SNN方法，证明其高效率与鲁棒性。|
|2511.07677v1|[Speech Separation for Hearing-Impaired Children in the Classroom](https://arxiv.org/abs/2511.07677v1)|**贡献点：**  <br/>1. **提出MIMO-TasNet模型**：设计紧凑、低延迟、多通道架构，适合实时部署于双耳助听器/人工耳蜗等设备。  <br/>2. **模拟真实教室场景**：通过动态儿童-儿童/儿童-成人对话与噪声/距离变化，还原复杂教室环境。  <br/>3. **数据高效适应策略**：对比成人语音、教室数据及微调模型，验证教室特定训练对儿童语音分离的提升效果。  <br/>4. **微调与噪声增强**：仅需半量教室数据即可实现有效迁移学习，并通过扩散噪声提升模型鲁棒性。  <br/>5. **空间感知与泛化能力**：模型在保持空间线索的同时，有效泛化到未见过的距离场景。  <br/><br/>**总结：**  <br/>提出MIMO-TasNet模型，结合空间线索与数据迁移学习，提升儿童教室语音分离效果，推动实时助听设备发展。|
|2511.07107v1|[MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107v1)|总结：  <br/>MENTOR提出了一种元认知驱动的LGM框架，通过动态知识图、领域数据集和激活引导技术，有效解决LLMs在专业领域中的隐性风险问题，并提升其安全性和价值对齐能力。<br/><br/>贡献点：  <br/>1. **框架创新**：提出MENTOR框架，首次将元认知能力引入LLM的自我进化机制，用于识别和缓解领域任务中的隐性风险。  <br/>2. **自评工具**：设计新型元认知自评工具，替代传统人工评估，通过视角转换与后果推理实现模型自我反思。  <br/>3. **领域数据集**：发布包含9,000个风险查询的跨领域数据集（教育、金融、管理），强化模型对领域特定风险的识别能力。  <br/>4. **动态规则扩展**：基于元认知反馈动态生成补充规则知识图，扩展静态规则树，提升泛化能力并降低维护成本。  <br/>5. **推理强化技术**：采用激活引导方法，在推理阶段高效约束模型遵循规则，确保隐性风险缓解的适用性与稳定性。|
|2511.07065v1|[Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065v1)|**贡献点：**<br/>1. 提出SRA框架，通过显式对齐模型注意力与人类理据提升解释性与公平性  <br/>2. 首次将监督注意力机制与transformer分类器结合，优化联合目标函数（分类损失+对齐损失）  <br/>3. 在英语（HateXplain）和葡萄牙语（HateBRXplain）基准测试中验证方法有效性  <br/>4. 实现2.4倍于基线的可解释性提升，生成更忠实且符合人类预期的token级解释  <br/>5. 在公平性指标上表现优异，尤其在检测针对身份群体的有毒帖子中达到第二佳效果  <br/><br/>**总结：**  <br/>本研究提出SRA框架，通过监督注意力对齐提升仇恨言论检测的可解释性和公平性，实验证明其在多语言场景下兼具高解释性与竞争力的公平性表现。|
|2511.06988v1|[HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection](https://arxiv.org/abs/2511.06988v1)|**贡献点总结**（100字以内）：  <br/>提出HCFSLN框架，整合多模态数据进行焦虑检测；引入双曲嵌入与跨模态注意力增强特征可分性；构建首个大规模多模态焦虑数据集（108人）；在Few-Shot场景下实现88%准确率，优于现有基线14%。<br/><br/>**分点贡献**：  <br/>1. **提出HCFSLN框架**：首个结合语音、生理信号和视频的多模态Few-Shot焦虑检测模型，解决小样本数据下的过拟合问题。  <br/>2. **创新技术设计**：通过双曲嵌入、跨模态注意力机制和自适应门控网络提升特征分离能力，增强模型在低数据量下的鲁棒性。  <br/>3. **构建新数据集**：首次公开108人参与的多模态焦虑数据集，为相关研究提供高质量基准数据。  <br/>4. **实验验证效果**：在对比六种Few-Shot基线模型时，准确率达88%，性能提升14%，验证双曲空间建模焦虑语音模式的有效性。|
|2511.06150v1|[BSCodec: A Band-Split Neural Codec for High-Quality Universal Audio Reconstruction](https://arxiv.org/abs/2511.06150v1)|总结：  <br/>提出BSCodec架构，通过分频段独立压缩解决语音与非语音音频的频谱差异问题，提升多类型音频重建质量并保持语音性能优势，验证其在下游任务中的广泛应用潜力。<br/><br/>贡献点：  <br/>1. **提出分频段压缩架构**：首次将频谱维度拆分为独立频段，针对不同内容类型（语音/音乐/声音）分别优化压缩策略。  <br/>2. **解决频谱特征差异问题**：通过适配语音的窄带谐波和非语音的全频谱需求，克服传统全频带方法对不同音频类型的适应性不足。  <br/>3. **多类型音频性能提升**：在统一数据集中训练后，BSCodec在非语音音频（音乐、声音）重建质量上超越现有基线，同时保持语音领域的竞争力。  <br/>4. **下游任务有效性验证**：实验表明该方法在语音识别、语音合成等任务中具有更强的泛化能力和应用潜力。|
|2511.06036v1|[Towards Human-AI-Robot Collaboration and AI-Agent based Digital Twins for Parkinson's Disease Management: Review and Outlook](https://arxiv.org/abs/2511.06036v1)|**贡献点**：  <br/>1. **整合多模态传感与机器人干预**：首次提出将PD筛查/诊断的多模态传感（如IMU、语音分析）与康复机器人（SAR、VR平台）融合，打破传统研究领域的割裂。  <br/>2. **闭环AI-机器人框架设计**：构建“传感器-AI-机器人”闭环系统，通过AI代理实现患者、护理者与机器人（及医生）的动态交互，支持个性化干预。  <br/>3. **多AI模型协同应用**：引入LLMs、强化学习、持续学习等先进AI技术，实现多模态数据融合与决策耦合，提升系统自适应性。  <br/>4. **数字孪生技术应用**：提出基于AI的PD患者数字孪生体，实现疾病状态动态建模与智能化、患者中心化的长期管理。  <br/>5. **可解释性与情境感知**：强调系统需具备解释性（Explainable AI）和情境感知能力，推动AI在医疗场景中的可信部署。  <br/><br/>**总结**（100字内）：  <br/>论文提出整合PD多模态传感与机器人康复的闭环AI框架，融合LLMs等先进AI技术，构建患者数字孪生体，实现个性化、可解释的智能干预，推动医疗AI的跨领域协同与精准化应用。|
|2511.05945v1|[Loud-loss: A Perceptually Motivated Loss Function for Speech Enhancement Based on Equal-Loudness Contours](https://arxiv.org/abs/2511.05945v1)|总结：本研究提出一种基于心理声学的感知加权损失函数，通过等响曲线调节频域权重，解决了传统MSE对高频信息建模不足的问题，在语音增强任务中显著提升感知质量。<br/><br/>贡献点：<br/>1. 提出感知加权损失函数：基于心理声学原理，利用等响曲线为不同频段分配权重，更贴合人耳听觉特性。<br/>2. 解决MSE频段偏差问题：通过频率依赖的权重机制，缓解传统MSE对低频过度关注导致的高频建模不足。<br/>3. 模型通用性设计：该损失函数可应用于多种语音增强模型（如GTCRN），具有跨模型的泛化能力。<br/>4. 实验验证有效性：在VoiceBank+DEMAND数据集上实现WB-PESQ得分2.17→2.93的显著提升，证明感知质量优化效果。|
|2511.04995v1|[Enhancing Public Speaking Skills in Engineering Students Through AI](https://arxiv.org/abs/2511.04995v1)|总结：  <br/>开发多模态AI模型，融合语音、视觉与情感分析，提供个性化公共演讲评估，解决传统教学难以持续、高效反馈的难题。<br/><br/>贡献点：  <br/>1. **提出多模态评估框架**：整合语音分析（音高、音量、语速、语调）与计算机视觉（面部表情、手势、姿势）技术，首次将表达连贯性作为新型评估维度。  <br/>2. **实现个性化与可扩展反馈**：突破传统分离式评估，通过AI融合多模态数据，提供持续、定制化的学习建议。  <br/>3. **验证模型有效性**：初步测试显示AI反馈与专家评价具有中度一致性，Gemini Pro在LLM中表现最优，体现模型设计的可靠性。  <br/>4. **支持自主训练**：无需人工评估，允许学生反复练习以自然优化语音与肢体语言的协调性，提升专业沟通能力。|
|2511.04366v1|[Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction](https://arxiv.org/abs/2511.04366v1)|总结（100字以内）:<br/>本研究首次探索MLLM与语音语言治疗师在亲子联合注意分析中的对齐方法，通过观察-判断双阶段提示验证其可行性，揭示专家共识与分歧对模型性能的影响，为社会情境交互分析的AI应用提供实证参考。<br/><br/>贡献点分点:<br/>1. **提出跨领域对齐框架**：构建MLLM与语音语言治疗师（SLPs）协作分析亲子互动的新型对齐方法，聚焦早期社会沟通关键指标联合注意。<br/><br/>2. **开发双阶段提示策略**：创新性设计观察与判断分离的两阶段提示流程，模拟SLPs的专业分析流程，提升模型对社会互动的解析能力。<br/><br/>3. **揭示专家判断差异**：通过对比SLPs的观察共识与判断标准差异，发现模型在观察层表现更稳健，但判断层存在显著可信度挑战。<br/><br/>4. **建立行为分析基准**：基于三名SLPs的访谈与视频标注数据，构建具有临床意义的社会互动分析基准，为AI模型训练提供实证依据。<br/><br/>5. **推动AI应用实证研究**：作为专家-AI对齐的案例研究，强调将MLLM应用于社会情境交互分析的潜力与局限，为未来发展提供方向指引。|
|2511.03310v1|[TASU: Text-Only Alignment for Speech Understanding](https://arxiv.org/abs/2511.03310v1)|**贡献点分点列出：**  <br/>1. 提出TASU（Text-only Alignment for Speech Understanding），突破传统语音模型依赖音频-文本配对数据的限制，仅使用文本数据进行跨模态对齐。  <br/>2. 实现零样本语音识别的竞争力，无需对齐数据即可有效迁移至语音理解任务。  <br/>3. 将TASU作为课程学习的预训练阶段，提升语音识别模型对未见领域的泛化能力。  <br/>4. 在MMSU基准上显著优于GLM-4-Voice和Step-Audio等主流语音大模型，验证其高效性和可扩展性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出TASU，仅用文本数据实现语音模型跨模态对齐，突破传统依赖配对数据的瓶颈，显著提升零样本语音识别与领域泛化能力，并在MMSU基准上超越主流模型，展现高效、可扩展的语音理解潜力。|
|2511.03084v1|[Quantifying Articulatory Coordination as a Biomarker for Schizophrenia](https://arxiv.org/abs/2511.03084v1)|**贡献点总结（100字以内）：**  <br/>提出可解释的语音框架，利用特征值光谱差图和WSED方法量化声带协调性，有效区分复杂与简单模式，并关联BPRS整体严重度及正负症状平衡，为精神分裂症提供透明、敏感的生物标志物，推动临床可解释语音评估工具发展。  <br/><br/>**分点贡献：**  <br/>1. **方法创新**：开发结合特征值光谱差图与WSED（加权指数衰减）的新框架，量化语音中的发声机制协调性。  <br/>2. **特征区分**：利用特征值光谱图有效识别复杂与简单协调模式，WSED分数可靠分组，模糊范围局限。  <br/>3. **症状关联性**：WSED分数与BPRS整体严重度及正负症状平衡显著相关，揭示症状类型对协调模式的影响。  <br/>4. **临床价值**：提供透明、严重度敏感的生物标志物，促进基于语音的精神分裂症临床评估工具发展。|
|2511.02454v1|[Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech Enhancement on Discrete Codec Token](https://arxiv.org/abs/2511.02454v1)|**贡献点：**  <br/>1. **替代FAVOR+机制**：用双向选择结构状态空间模型（Hydra）替代FAVOR+，消除其近似误差，提升全局序列建模能力。  <br/>2. **线性复杂度设计**：通过结构化矩阵混合框架实现线性时间复杂度，避免自注意力的二次复杂度问题。  <br/>3. **性能验证**：在Genhancer模型上实验表明，所提方法显著优于DF-Conformer，验证其在语音增强任务中的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于双向结构状态空间模型的DF-Conformer替代方案，消除FAVOR+近似并保持线性复杂度，通过Hydra模型在语音增强任务中实现性能提升，验证了其高效性与有效性。|
|2510.18938v2|[StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction](https://arxiv.org/abs/2510.18938v2)|**贡献点：**  <br/>1. 提出首个端到端波形-波形模型StutterZero和StutterFormer，直接将口吃语音转换为流畅语音并联合预测转录，突破传统分阶段ASR-TTS流程的局限性。  <br/>2. 设计两种创新架构：StutterZero采用卷积-BiLSTM编码器-解码器与注意力机制，StutterFormer引入双流Transformer实现音素与语义的共享表示。  <br/>3. 在SEP-28K和LibriStutter数据合成的训练集上验证模型，首次在FluencyBank未见说话者数据上评估，体现泛化能力。  <br/>4. 实验结果显著优于基准模型（Whisper-Medium）：StutterZero WER降低24%，BERTScore提升31%；StutterFormer进一步优化至WER降28%，BERTScore升34%。  <br/>5. 验证端到端口吃-流畅语音转换的可行性，为无障碍AI、言语治疗及包容性人机交互提供新方法。  <br/><br/>**总结（100字以内）：**  <br/>该论文首次提出端到端波形-波形模型StutterZero和StutterFormer，直接转换口吃语音为流畅语音并同步预测转录，显著优于传统方法，在WER和语义相似度指标上取得突破，为无障碍AI与言语治疗提供新方向。|
|2510.04157v1|[GDiffuSE: Diffusion-based speech enhancement with noise model guidance](https://arxiv.org/abs/2510.04157v1)|**摘要（<100字）**  <br/>本文提出Guided diffusion for speech enhancement (GDiffuSE)，利用轻量辅助模型估计噪声分布并在扩散去噪过程中加入引导，实现对未见噪声的鲁棒适应，复用大规模生成式DDPM，实验在BBC噪声+LibriSpeech上显著超越现有基线。<br/><br/>**主要贡献**  <br/>- 首创将去噪扩散概率模型（DDPM）用于语音增强，并通过引导机制结合噪声分布估计。  <br/>- 设计轻量级辅助模型预测噪声分布，使扩散过程获得噪声信息的明确指引。  <br/>- 引入噪声引导显著提升对未见噪声类型的鲁棒性，实现无缝适配不同噪声场景。  <br/>- 能直接复用在大规模语音生成任务上预训练的 DDPM，降低专门针对 SE 的训练成本。  <br/>- 在 BBC 声效库噪声混入 LibriSpeech 的实验设定下，针对噪声不匹配条件，取得了持续且显著的性能提升，超过所有对比的最新基线。|
|2509.20799v2|[AuthGlass: Enhancing Voice Authentication on Smart Glasses via Air-Bone Acoustic Features](https://arxiv.org/abs/2509.20799v2)|**贡献点总结：**  <br/>1. 提出AuthGlass方法，融合空气传声和骨传导语音特征，提升语音认证的准确性与活体检测能力。  <br/>2. 设计并构建具有冗余同步麦克风的智能眼镜原型（14个空气传导麦克风+2个骨传导单元），实现多模态特征采集。  <br/>3. 通过42人实验验证，证明结合声场与振动特征显著增强认证鲁棒性与抗攻击能力。  <br/>4. 实验表明AuthGlass在复杂场景下保持竞争力，具备实际部署的适用性与可扩展性。  <br/><br/>**摘要总结（100字内）：**  <br/>AuthGlass通过融合空气和骨传导语音特征，提升智能眼镜语音认证的准确性与抗攻击能力，经实验证明其在真实场景中表现优异，具有广泛适用性与可扩展性。|