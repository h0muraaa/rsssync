|Source|Title|Summary|
|---|---|---|
|2510.26372v1|[UniTok-Audio: A Unified Audio Generation Framework via Generative   Modeling on Discrete Codec Tokens](http://arxiv.org/abs/2510.26372v1)|**总结（100字以内）:**  <br/>UniTok-Audio 提出一个统一音频生成框架，支持多种任务，具备高质量和可扩展性，通过条件连续特征提取、任务标识符和双流编解码器实现，开源促进研究。<br/><br/>**贡献点分点列出：**<br/><br/>1. **提出统一音频生成框架**：UniTok-Audio 是首个统一处理多种音频生成任务的框架，提升模型的通用性和可扩展性。  <br/>2. **条件连续特征提取与离散生成**：采用自回归方式从连续条件特征生成离散目标音频token，增强生成过程的可控性。  <br/>3. **任务标识符统一学习模式**：引入特殊任务标识符token，统一处理不同任务的学习模式，简化框架结构。  <br/>4. **双流音频编解码器**：设计包含声学与语义分支的双流编解码器，提升波形重建的保真度与任务适应性。  <br/>5. **跨任务性能验证**：在五个时间对齐的音频生成任务中验证性能，展示其与最新任务特定或多任务系统的竞争力。  <br/>6. **开源促进研究**：开放代码库，推动语音领域音频生成技术的进一步发展和应用。|
|2510.26299v1|[Modeling strategies for speech enhancement in the latent space of a   neural audio codec](http://arxiv.org/abs/2510.26299v1)|总结（100字以内）:  <br/>本研究比较了连续向量和离散token作为神经音频编码器训练目标在语音增强中的效果，发现连续表示更优；自回归模型质量高但效率低，非自回归更实用；微调编码器整体性能最佳，但重建质量受损，代码和数据公开可用。<br/><br/>**贡献点分点列出（缩写版）**:  <br/>1. **首次系统对比**：比较连续向量序列与离散token目标在监督语音增强中的性能差异。  <br/>2. **模型结构分析**：评估基于Conformer架构的自回归与非自回归模型的增强效果及效率。  <br/>3. **基线方法提出**：验证直接微调NAC编码器在语音增强任务中的潜力，发现其整体指标最优。  <br/>4. **权衡研究**：揭示自回归模型在质量提升与可理解性、效率之间的矛盾，为实际应用提供选择依据。  <br/>5. **公开数据支持**：提供代码和音频样本，促进技术复现与进一步研究。  <br/><br/>（注：实际贡献点可根据论文全文细节进一步细化，此处为基于摘要的归纳）|
|2510.23312v2|[Low-Resource Audio Codec (LRAC): 2025 Challenge Description](http://arxiv.org/abs/2510.23312v2)|总结：  <br/>提出2025低资源音频编解码器挑战，推动边缘部署下高效编码研究，提供标准数据集与基线系统，构建全面评估框架，促进编码与语音增强的结合，为相关领域提供实用见解。<br/><br/>贡献点：  <br/>1. 针对低资源场景和声学失真问题，提出全新挑战（2025 Low-Resource Audio Codec Challenge）；  <br/>2. 提供标准化训练数据集与两个基线系统，降低研究门槛；  <br/>3. 构建综合评估框架，量化低资源编解码器的性能与鲁棒性；  <br/>4. 促进神经编解码器与语音增强技术的集成研究；  <br/>5. 为边缘计算环境下的音频处理提供理论与技术指导。|
|2510.23312v1|[Low-Resource Audio Codec (LRAC): 2025 Challenge Description](http://arxiv.org/abs/2510.23312v1)|总结：提出2025低资源音频编解码器挑战，推动神经编解码器在资源受限场景下的鲁棒性与效率研究，提供标准化数据集、基线系统及评估框架，促进编解码器设计与下游应用的协同优化。<br/><br/>贡献点：<br/>1. 突出低资源操作与声学鲁棒性问题，为领域研究提供明确方向；<br/>2. 提出标准化训练数据集与评估框架，规范研究基准；<br/>3. 提供两个基线系统，降低参与门槛并促进技术对比；<br/>4. 设计面向边缘计算场景的挑战目标，融合低延迟、低比特率需求；<br/>5. 预期推动编解码器设计及语音增强等下游任务的协同发展。|
|2510.23141v1|[Treble10: A high-quality dataset for far-field speech recognition,   dereverberation, and enhancement](http://arxiv.org/abs/2510.23141v1)|总结：  <br/>Treble10是首个结合真实物理建模与大规模可扩展性的远场语音数据集，提供多场景RIRs与LibriSpeech配对数据，推动语音处理任务的物理基础研究与评估。<br/><br/>贡献点：<br/>1. **提出混合仿真方法**：结合波导与几何声学求解器，精准模拟复杂环境中的扩散、散射、干涉等物理声学现象，突破传统模拟的简化限制。<br/>2. **构建大规模物理准确数据集**：基于10个真实房间的3000+宽带RIRs，覆盖远场语音任务的多种场景需求（单声道、Ambisonics、六通道设备）。<br/>3. **实现低频-高频全频段建模**：在32kHz采样率下，完整模拟低频波传播与高频反射效应，提升声学细节的真实性。<br/>4. **提供配对干净与混响语音数据**：与LibriSpeech整合，生成可复现的预混响语音场景，支持端到端模型训练与评估。<br/>5. **开源共享促进研究**：通过Hugging Face Hub公开发布，构建语音处理领域的基准测试平台与下一代研究模板。|
|2510.22637v1|[HyBeam: Hybrid Microphone-Beamforming Array-Agnostic Speech Enhancement   for Wearables](http://arxiv.org/abs/2510.22637v1)|**贡献点：**  <br/>1. **提出HyBeam混合框架**：结合低频原始麦克风信号与高频波束成形器信号，利用两者互补优势提升语音增强效果。  <br/>2. **解决阵列几何限制问题**：通过混合信号处理，实现高度阵列无关性，适应多样化的麦克风配置和移动设备场景。  <br/>3. **实验验证性能优势**：在多种房间环境和可穿戴麦克风阵列配置下，HyBeam在PESQ、STOI和SI-SDR指标上均优于单一方法基线。  <br/>4. **频带分析揭示机制**：证明混合方法在低频利用麦克风方位信息，在高频依赖波束成形器方向性，全面优化各频段表现。  <br/><br/>**总结（100字以内）**：  <br/>HyBeam通过融合低频麦克风信号和高频波束成形器信号，突破传统方法对固定阵列的依赖，实现高度阵列无关性，并在多场景实验中显著提升语音增强性能。|
|2510.21485v1|[FlexIO: Flexible Single- and Multi-Channel Speech Separation and   Enhancement](http://arxiv.org/abs/2510.21485v1)||
|2510.21014v2|[ReFESS-QI: Reference-Free Evaluation For Speech Separation With Joint   Quality And Intelligibility Scoring](http://arxiv.org/abs/2510.21014v2)|**贡献点：**  <br/>1. **提出无参考评估框架**：首次构建基于自监督学习（SSL）的文本无关且参考音频无关的语音分离评估体系，解决无参考混合音频无法使用传统指标（如WAVERT、SI-SNR）评估的难题。  <br/>2. **联合预测质量与可懂度**：同步利用混合和分离后的音轨，通过SI-SNR评估音频质量、通过WER评估语音可懂度，实现多目标统一评价。  <br/>3. **高精度实验验证**：在WHAMR!数据集上取得显著结果，WER预测MAE为17%、PCC达0.77；SI-SNR预测MAE为1.38、PCC达0.95，验证方法有效性。  <br/>4. **鲁棒性分析**：通过对比多种SSL表示，证明框架对不同预训练模型的适应性与稳定性，拓宽其应用范围。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出一种无参考、文本无关的语音分离评估框架，基于自监督学习表示联合预测音频质量和可懂度，并通过实验验证其有效性与鲁棒性，为真实场景下的语音处理任务提供新工具。|
|2510.21014v1|[refess-qi: reference-free evaluation for speech separation with joint   quality and intelligibility scoring](http://arxiv.org/abs/2510.21014v1)||
|2510.20441v1|[UniSE: A Unified Framework for Decoder-only Autoregressive LM-based   Speech Enhancement](http://arxiv.org/abs/2510.20441v1)|总结：提出UniSE统一框架，基于自回归语言模型处理多语音增强任务，实现跨任务兼容性，验证其性能优于现有基线。<br/><br/>贡献点：<br/>1. **统一框架设计**：首次构建解码器-only语言模型（LM）框架UniSE，整合语音恢复、目标说话人提取和语音分离等多任务。<br/>2. **跨任务兼容性**：通过自回归建模将不同SE任务的学习模式兼容，突破传统多任务模型需独立设计的限制。<br/>3. **性能验证**：在多个基准测试中证明UniSE的竞争力，展现语言模型在语音增强任务统一建模的有效性。<br/>4. **开源实现**：提供demo页面展示框架应用，促进研究复现与实际部署。|
|2510.19439v1|[Relative Transfer Matrix Estimator using Covariance Subtraction](http://arxiv.org/abs/2510.19439v1)|总结：  <br/>本文提出基于协方差减法的ReTM估计方法，验证其在混响环境下的说话人分离性能，尤其在低信噪比下优于传统方法。<br/><br/>贡献点：  <br/>1. **提出新方法**：基于协方差矩阵减法，提供一种灵活且实用的ReTM估计方案，适用于特定独立声源。  <br/>2. **方法验证**：通过说话人分离实验验证该方法的适用性，尤其在混响条件下表现优异。  <br/>3. **性能对比**：在低信噪比环境下与现有ReTM基方法和相对传输函数基方法进行对比，证明其有效性。  <br/>4. **多场景应用**：实验覆盖模拟与真实环境，体现方法的鲁棒性和通用性。|
|2510.18744v1|[Diffusion Buffer for Online Generative Speech Enhancement](http://arxiv.org/abs/2510.18744v1)|总结：  <br/>提出在线Diffusion Buffer模型，通过时间对齐机制单次调用降低延迟，结合改进UNet架构和新型损失函数，显著提升性能并验证其在未见过噪声数据上的优越性。<br/><br/>贡献点：  <br/>1. **首次提出在线生成式语音增强框架**：设计Diffusion Buffer，仅需一次神经网络调用实现在线增强，突破传统生成模型高延迟限制。  <br/>2. **时间对齐机制**：将物理时间与扩散时间步对齐，逐步降噪过去帧，提升增强质量。  <br/>3. **专用2D UNet架构**：针对性设计UNet结构，与Diffusion Buffer的预览功能匹配，优化性能。  <br/>4. **Data Prediction损失函数**：替代传统Denoising Score Matching损失，实现算法延迟与质量的灵活平衡。  <br/>5. **显著性能提升与延迟降低**：将延迟从320-960ms降至32-176ms，同时增强效果更优。  <br/>6. **验证生成模型在线有效性**：证实在线Diffusion Buffer在未见过噪声数据上优于传统预测模型。|
|2510.16997v1|[Towards Real-Time Generative Speech Restoration with Flow-Matching](http://arxiv.org/abs/2510.16997v1)|**贡献点**  <br/>1. **低延迟实时系统**：提出首个基于流匹配（Flow-Matching）的低延迟、实时语音修复方法，总延迟仅20ms，适合实时通信场景。  <br/>2. **多任务兼容性**：支持降噪、回声消除和生成修复等多样化实际任务。  <br/>3. **因果架构设计**：无需时间下采样，通过因果架构实现有效实时处理。  <br/>4. **高效采样策略**：仅需5次函数评估（NFEs）即可达到与20 NFEs相当的修复质量。  <br/>5. **参数与轨迹关系分析**：揭示小模型在长反向轨迹中性能下降的现象，指导模型设计优化。  <br/>6. **对比实验**：对比流匹配与对抗损失训练方法，验证FM在相同架构下的优势。  <br/><br/>**总结**  <br/>本文提出低延迟实时语音修复系统，基于流匹配方法实现多任务处理与高效采样，在减少计算开销的同时保持高质量，并对比分析了不同训练策略的效果。|
|2510.16995v1|[Adaptive Deterministic Flow Matching for Target Speaker Extraction](http://arxiv.org/abs/2510.16995v1)|**贡献点总结：**  <br/>1. 提出AD-FlowTSE，采用自适应步长优化流匹配，提升TSE效率与质量。  <br/>2. 首次将传输路径定义为背景与源的混合比例（MR）驱动的轨迹。  <br/>3. 引入MR感知初始化，动态调整起始点以适应不同噪声条件。  <br/>4. 实验证明单步即可实现高效TSE，结合MR估计显著提升准确性。  <br/>5. 验证了对齐混合成分与自适应步长对TSE任务的关键作用。  <br/><br/>**（总结：100字以内）**  <br/>AD-FlowTSE通过自适应步长和MR感知初始化，在流匹配框架下实现高效精准的目标说话人提取，实验表明其单步性能优异，并结合MR估计进一步提升效果。|
|2510.16834v1|[Schrödinger Bridge Mamba for One-Step Speech Enhancement](http://arxiv.org/abs/2510.16834v1)|总结：  <br/>提出基于Schrödinger Bridge与Mamba的训练-推理框架SBM，实现高效语音增强，验证其在降噪与回声消除任务中超越传统方法，具备最佳实时性，并探讨其在深度生成模型中的通用潜力。  <br/><br/>贡献点：  <br/>1. **提出SBM框架**：结合Schrödinger Bridge训练范式与Mamba的可变长度状态空间结构，构建新型训练-推理体系。  <br/>2. **生成性语音增强应用**：以语音降噪与回声消除任务为例，验证SBM在单步推理下的有效性。  <br/>3. **性能优势验证**：实验表明SBM在多个基准数据集上优于现有基线，实现最佳实时因子（RTF）。  <br/>4. **通用性拓展**：探讨SB范式与选择性状态空间模型的理论对齐，为广泛深度生成任务提供新方向。|
|2510.16437v1|[Audio-Visual Speech Enhancement for Spatial Audio - Spatial-VisualVoice   and the MAVE Database](http://arxiv.org/abs/2510.16437v1)|总结：  <br/>本文提出多通道AVSE框架和MAVe数据库，解决低SNR下空间音频增强问题，实验验证了方法在语音质量指标上的显著提升及空间线索的保留效果。<br/><br/>贡献点：  <br/>1. **提出多通道AVSE框架**：基于VisualVoice，整合麦克风阵列的空间信息与视觉数据，有效提升低SNR环境下的目标说话者语音增强效果。  <br/>2. **构建MAVe数据库**：首个包含多通道音频-视觉信号的数据库，提供受控、可复现的房间环境和广泛SNR范围的实验数据。  <br/>3. **验证性能提升**：在SI-SDR、STOI、PESQ等关键指标上表现优异，尤其在低SNR场景下显著优于现有方法。  <br/>4. **保留空间线索与可懂性**：通过双耳信号分析证明，增强后的语音既保持空间感知特性，又提升语音可懂度。|
|2510.12485v1|[I-DCCRN-VAE: An Improved Deep Representation Learning Framework for   Complex VAE-based Single-channel Speech Enhancement](http://arxiv.org/abs/2510.12485v1)|**贡献点分点：**<br/><br/>1. **改进DCCRN-VAE结构**：移除预训练VAE的skip connections，提升语音与噪声潜在表示的信息量。  <br/>2. **引入β-VAE预训练策略**：通过β-VAE优化潜在空间的正则化与重建平衡，增强模型鲁棒性。  <br/>3. **双潜在表示生成机制**：NSVAE同时生成语音和噪声潜在表示，提升任务相关性。  <br/>4. **泛化能力提升**：在未匹配数据集（WSJ0-QUT、Voicebank-DEMEND）上表现优于基线，验证更强的适应性。  <br/>5. **简化训练流程**：证明经典微调可替代对抗训练，实现同等性能的更简单训练方法。  <br/><br/>**总结（100字以内）：**  <br/>本文改进DCCRN-VAE，通过移除skip连接、引入β-VAE和双潜在表示生成等方法，提升语音增强系统的泛化能力。实验表明新方法在不匹配数据集上优于基线，并简化训练流程。|
|2510.11395v1|[Dynamically Slimmable Speech Enhancement Network with Metric-Guided   Training](http://arxiv.org/abs/2510.11395v1)|总结：  <br/>提出基于门控机制的动态轻量化网络DSN，通过Metric-Guided Training（MGT）实现动态组件资源分配，在保持与现有轻量模型相当的语音增强效果的同时，显著降低计算负载（平均73%）。<br/><br/>贡献点：  <br/>1. **DSN架构创新**：设计包含静态和动态组件的网络结构，通过动态结构（分组循环神经网络、多头注意力、卷积与全连接层）实现架构无关的轻量化应用。  <br/>2. **动态资源分配机制**：引入策略模块，根据输入信号质量在帧级动态控制动态组件的使用，降低计算负载。  <br/>3. **MGT训练方法**：提出指标引导的训练策略，显式指导策略模块评估输入语音质量，提升动态资源分配的准确性。  <br/>4. **性能与效率平衡**：实验证明DSN在保持与先进轻量模型相近的客观指标效果的同时，平均计算负载降低至73%。  <br/>5. **自适应资源分配验证**：通过分析动态组件使用比例，验证MGT-DSN能根据输入信号失真程度自适应分配网络资源。|
|2510.10687v1|[LSZone: A Lightweight Spatial Information Modeling Architecture for   Real-time In-car Multi-zone Speech Separation](http://arxiv.org/abs/2510.10687v1)|总结：  <br/>LSZone提出轻量级架构，结合SpaIEC与CNP模块，在复杂场景下实现高效实时车载多区域语音分离，显著降低计算复杂度（0.56G MACs）和提升实时因子（0.37）。<br/><br/>贡献点：  <br/>1. **轻量级架构设计**：提出LSZone，专为车载多区域语音分离优化，解决SpatialNet计算成本高、难以实时应用的问题。  <br/>2. **多模态空间信息融合**：设计SpaIEC模块，整合梅尔谱图与双耳相位差（IPD），降低计算负担同时保持分离性能。  <br/>3. **超轻量化跨频带处理**：引入CNP模块，采用Conv-GRU实现高效跨频带与窄带信息建模，进一步提升推理效率。  <br/>4. **复杂场景有效性验证**：在真实噪声和多说话人场景中实验证明，LSZone在保持性能的同时显著降低计算复杂度和延迟。|
|2510.09974v1|[Universal Discrete-Domain Speech Enhancement](http://arxiv.org/abs/2510.09974v1)|总结：  <br/>本文提出UDSE，通过离散域分类任务解决多干扰语音增强问题，显著提升语音增强的通用性和实用性。<br/><br/>贡献点：  <br/>1. **任务范式创新**：首次将语音增强重新定义为离散域分类任务，而非传统回归模型的波形或连续特征预测。  <br/>2. **技术框架突破**：引入预训练神经语音编解码器的残差向量量化器（RVQ），通过量化干净离散标记实现增强。  <br/>3. **动态预测机制**：设计基于全局特征的逐VQ预测链，使每个VQ的输出依赖前序结果，提升干扰处理的连贯性。  <br/>4. **多干扰鲁棒性验证**：在传统与非传统干扰（含混合干扰）下均取得优异效果，证明模型的广泛适用性。|
|2510.08914v1|[VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR   Virtual Microphone Arrays](http://arxiv.org/abs/2510.08914v1)|总结（100字以内）:  <br/>VM-UNSSOR通过引入虚拟麦克风信号和增强MC损失，有效提升无监督语音分离性能，解决麦克风数量不足及频率排列问题，在多麦克风场景中显著优于UNSSOR。<br/><br/>### 贡献点分点列出:<br/>1. **提出虚拟麦克风（VM）信号增强策略**：针对麦克风数量不足导致混合一致性（MC）约束失效的问题，通过生成高信噪比的虚拟麦克风信号提升分离性能。  <br/>2. **改进混合一致性损失**：利用虚拟信号计算额外的MC损失，增强模型鲁棒性，并缓解频率排列问题（frequency permutation）。  <br/>3. **结合线性空间解混技术**：通过IVA和空间聚类等方法生成虚拟信号，为无监督学习提供额外的监督信息。  <br/>4. **实验验证有效性**：在SMS-WSJ数据集上，展示VM-UNSSOR在六麦克风（过确定）和两麦克风（确定）场景下分别优于UNSSOR，SI-SDR提升显著（17.1 dB vs. 14.7 dB；10.7 dB vs. -2.7 dB）。|
|2510.05295v1|[AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture   with Cross-Attention and Squeezeformer for Speech Enhancement](http://arxiv.org/abs/2510.05295v1)|总结：  <br/>本文提出AUREXA-SE框架，融合音频和视觉模态，结合U-Net和Swin Transformer V2，引入双向交叉注意力和轻量级Squeezeformer模块，显著提升语音增强性能。<br/><br/>贡献点：  <br/>1. **提出新型双模态框架**：设计AUREXA-SE，采用U-Net结构处理音频，Swin Transformer V2提取视觉特征，实现音频-视觉联合建模。  <br/>2. **创新双向交叉注意力机制**：开发双向跨模态注意力模块，深度融合音频与视觉上下文信息，增强特征互补性。  <br/>3. **整合轻量级Squeezeformer模块**：通过堆叠Squeezeformer模块（结合卷积与注意力），有效捕捉融合嵌入中的时序依赖性。  <br/>4. **优化解码器结构**：采用U-Net式解码器直接重建波形，确保输出语音在感知一致性和可理解性方面表现优异。  <br/>5. **验证性能提升**：在噪声环境中实现显著改进，在STOI、PESQ和SI-SDR指标上优于基线模型。|
|2510.04157v1|[GDiffuSE: Diffusion-based speech enhancement with noise model guidance](http://arxiv.org/abs/2510.04157v1)|**贡献点：**  <br/>1. **提出GDiffuSE方法**：首次将DDPM应用于语音增强，通过轻量辅助模型估计噪声分布并融入扩散过程。  <br/>2. **噪声分布估计机制**：区别于直接映射方法，采用辅助模型动态捕捉噪声特性，提升模型对未知噪声的适应能力。  <br/>3. **跨任务迁移应用**：利用原本为语音生成训练的DDPM，通过指导机制迁移至语音增强领域，增强模型泛化性。  <br/>4. **鲁棒性提升**：在未见过的噪声条件下（如BBC声效混合噪声），实验显示优于现有方法的性能。  <br/>5. **公开实验数据与复现**：提供噪声数据集及项目网页，支持对比实验与方法复现。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出GDiffuSE，基于DDPM的语音增强方法，通过噪声分布估计与辅助模型引导机制提升鲁棒性，成功迁移预训练DDPM至SE任务，并在混合噪声数据上超越现有方法。|
|2510.02187v1|[High-Fidelity Speech Enhancement via Discrete Audio Tokens](http://arxiv.org/abs/2510.02187v1)|总结（100字以内）:  <br/>DAC-SE1提出基于离散高分辨率音频表示的简化框架，保持声学细节与语义连贯，超越现有自回归语音增强方法，在客观指标和MUSHRA测试中表现优异，并开源代码与模型促进研究。<br/><br/>贡献点:  <br/>1. 引入离散高分辨率音频表示，替代传统低采样率编码，提升声学细节保留能力。  <br/>2. 构建简化语言模型驱动的单阶段语音增强框架，降低复杂度并扩展应用范围。  <br/>3. 在客观感知指标和MUSHRA人眼评估中均超越当前最优自回归SE方法。  <br/>4. 开源代码与模型检查点，推动可扩展、统一、高质量语音增强技术发展。|
|2510.01958v1|[Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for   Improved Cross-Corpus Speech Enhancement](http://arxiv.org/abs/2510.01958v1)|**贡献点：**  <br/>1. 提出RWSA-MambaUNet，创新性融合Mamba与多头注意力机制，优化U-Net结构以提升语音增强性能。  <br/>2. 引入分辨率级共享注意力（RWSA）机制，实现跨时间与频率分辨率的层间注意力信息共享，增强模型泛化能力。  <br/>3. 在两个模型外测试集（DNS 2020与EARS-WHAM_v2）中均取得SOTA泛化性能，且最小模型参数量仅为基线的一半，FLOPs显著降低。  <br/>4. 验证了RWSA-MambaUNet在PESQ、SSNR、ESTOI及SI-SDR等关键指标上的全面优势，尤其在DNS 2020测试集上超越所有基线模型。  <br/><br/>**总结（100字以内）：**  <br/>提出RWSA-MambaUNet混合模型，结合Mamba与多头注意力机制，通过分辨率级共享注意力提升跨语料泛化性能，同时降低参数与计算量，在DNS2020和EARS-WHAM_v2测试集上超越基线，实现高效语音增强。|
|2510.01130v2|[Learning Time-Graph Frequency Representation for Monaural Speech   Enhancement](http://arxiv.org/abs/2510.01130v2)|总结：  <br/>本文提出可学习GFT-SVD框架，通过1-D卷积构建动态图拓扑与基，消除矩阵求逆带来的数值不稳定问题，提升语音增强的自适应性和效果。  <br/><br/>贡献点：  <br/>1. **提出可学习图拓扑**：引入图移位算子构建动态可学习图结构，替代传统固定图拓扑以增强表示的适应性与灵活性。  <br/>2. **设计可学习图傅里叶基**：利用1-D卷积神经层定义图傅里叶基，避免显式矩阵求逆操作，解决GFT-SVD和GFT-EVD的数值误差问题。  <br/>3. **框架稳定性提升**：消除矩阵求逆带来的不稳定性，增强模型鲁棒性。  <br/>4. **简化计算流程**：通过学习方式替代复杂分解过程，降低计算复杂度并改善语音增强性能。|
|2509.25982v1|[An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction](http://arxiv.org/abs/2509.25982v1)|**贡献点总结（100字以内）：**  <br/>本研究提出通过非线性深度神经网络进行联合空间-时频谱处理，有效解决传统线性滤波器因空间混叠导致的性能下降问题。相比单独处理或分离式处理方法，联合处理显著提升系统对空间混叠的鲁棒性，并为多通道语音增强提供新的技术动机，特别是在大间距麦克风阵列应用中。|
|2509.25275v1|[VoiceBridge: Designing Latent Bridge Models for General Speech   Restoration at Scale](http://arxiv.org/abs/2509.25275v1)|**贡献点总结：**  <br/>VoiceBridge提出了一种基于潜在桥模型的语音增强系统，通过统一潜在到潜在生成过程、能量保持的VAE和感知导向的微调，解决了多任务和跨数据集的高质量语音重建问题，提升了感知质量。  <br/><br/>**分点贡献：**  <br/>1. **提出VoiceBridge系统**：基于潜在桥模型（LBMs）实现从多种失真中重建全频带（48kHz）高质量语音的通用语音恢复（GSR）。  <br/>2. **统一潜在生成过程**：通过可扩展的Transformer架构，将多样化的低质量-高质量（LQ-to-HQ）任务整合为单一潜在到潜在生成流程。  <br/>3. **能量保持的VAE设计**：引入能量保持变分自编码器，增强波形与潜在空间在不同能量水平下的对齐能力。  <br/>4. **联合神经先验提出**：构建统一的神经先验，缓解因不同LQ先验导致的HQ重建困难，降低LBM的重建负担。  <br/>5. **感知导向的微调阶段**：设计专门的微调策略，优化生成结果的人类感知质量，减少级联失配问题。|
|2509.24708v1|[SenSE: Semantic-Aware High-Fidelity Universal Speech Enhancement](http://arxiv.org/abs/2509.24708v1)|**贡献点（分点）：**  <br/>1. 提出SenSE框架，结合语言模型与流匹配模型，首次将高阶语义信息引入语音增强任务。  <br/>2. 开发语义感知的语音语言模型，生成语义标记以捕捉扭曲语音的上下文语义。  <br/>3. 设计语义引导机制，通过语义信息优化流匹配过程，缓解增强语音的语义模糊问题。  <br/>4. 提出提示引导机制，利用短参考语音维持说话者相似性，应对严重失真场景。  <br/>5. 在多个基准数据集上验证方法有效性，证明其在感知质量、语音保真度和鲁棒性上的综合提升。  <br/>6. 提供代码和演示，促进方法的实际应用与研究复现。  <br/><br/>**总结（100字以内）：**  <br/>SenSE通过融合语言模型与流匹配框架，引入语义引导和提示引导机制，有效解决语音增强中的语义模糊与失真问题，显著提升语音质量与鲁棒性，代码开源便于推广。|
|2509.24395v1|[Unsupervised Single-Channel Speech Separation with a Diffusion Prior   under Speaker-Embedding Guidance](http://arxiv.org/abs/2509.24395v1)|总结：  <br/>提出基于说话人嵌入引导的扩散模型，解决无监督语音分离中的时序不一致问题，并设计专用求解器，有效提升性能。<br/><br/>贡献点：  <br/>1. **源模型框架的创新应用**：首次将扩散生成模型（Diffusion Model）应用于无监督语音分离，仅依赖无混响（anechoic）语音数据，避免合成数据偏差。  <br/>2. **说话人一致性引导策略**：提出Speaker-Embedding引导方法，在反向扩散过程中通过保留说话人特征保持分离语音的时序一致性，同时增强不同说话人嵌入的分离度。  <br/>3. **专用求解器设计**：开发针对语音分离任务的新型求解器，与说话人引导策略结合，显著提升无监督源模型语音分离的性能。  <br/>4. **实验验证与开源支持**：通过大量实验验证方法有效性，并提供音频样本和代码开源，便于复现与研究。|
|2509.23832v1|[LORT: Locally Refined Convolution and Taylor Transformer for Monaural   Speech Enhancement](http://arxiv.org/abs/2509.23832v1)|总结：  <br/>本文提出LORT模型，结合空间通道增强的泰勒变换器与局部精细卷积，实现轻量化高效语音增强，参数仅0.96M，在保持低复杂度的同时达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出T-MSA模块**：引入空间-通道增强注意力机制，优化泰勒变换器的全局建模能力，解决空间注意力局限。  <br/>2. **设计LRC结构**：融合卷积前馈层、时频密集局部卷积与门控单元，增强对细粒度局部特征的捕捉。  <br/>3. **轻量化编码器设计**：基于U-Net结构，仅使用16个输出通道，显著降低参数量与计算复杂度。  <br/>4. **多分辨率处理机制**：通过交替的下采样与上采样操作，提升多尺度特征提取能力。  <br/>5. **复合损失函数优化**：联合幅度、复数、相位、判别器与一致性目标，提升增强结果的鲁棒性与质量。|
|2509.23610v1|[Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and   Multi-Scale Global-Local Attention](http://arxiv.org/abs/2509.23610v1)|贡献点总结（100字以内）:  <br/>提出高效AVSS方法Dolphin，包含轻量双路径视觉编码器DP-LipCoder和带GLA块的轻量音频分离器，实现50%参数减少、2.4倍MACs降低、6倍推理加速，同时超越SOTA性能，并开源代码。  <br/><br/>分点贡献：  <br/>1. **双路径轻量化视频编码器（DP-LipCoder）**：将唇部运动转化为离散的音频对齐语义标记，显著减少模型参数和计算量。  <br/>2. **全局-局部注意力（GLA）模块**：在音频分离器中引入GLA块，高效捕捉多尺度音频依赖关系。  <br/>3. **端到端效率提升**：在保持高分离性能的同时，实现超过50%的参数减少、2.4倍MACs降低和6倍GPU推理加速。  <br/>4. **跨场景验证**：在三个基准数据集上验证方法有效性，证明其在复杂真实环境中的实用性。  <br/>5. **开源与可复现性**：提供完整代码及演示页面，便于研究者复现和应用。|
|2509.22942v1|[Unsupervised Speech Enhancement using Data-defined Priors](http://arxiv.org/abs/2509.22942v1)|总结：  <br/>提出双分支编码器-解码器架构，利用对抗训练和无配对数据定义先验，解决无监督语音增强中的数据配对问题，揭示同域数据选择对性能的影响，实验验证其效果可媲美现有方法。<br/><br/>贡献点：  <br/>1. **创新架构**：设计双分支编码器-解码器框架，首次将输入信号分离为干净语音与残余噪声，提升无监督增强的模型表征能力。  <br/>2. **对抗训练机制**：引入对抗训练策略，通过无配对干净语音和噪声数据为各分支定义先验，减少合成数据与真实数据间的分布差异。  <br/>3. **无配对数据利用**：无需依赖配对数据，仅需单通道干净语音和噪声数据即可训练模型，突破传统方法的数据限制。  <br/>4. **性能对比验证**：实验表明方法性能接近当前主流无监督语音增强技术，证明其有效性与实用性。  <br/>5. **数据选择分析**：首次系统探讨同域数据用于先验定义的局限性，揭示其可能引发的性能虚高问题，为研究提供新视角。|
|2509.22425v2|[From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation](http://arxiv.org/abs/2509.22425v2)|**贡献点：**  <br/>1. **提出CSFNet框架**：采用“粗到细”的双阶段分离策略，结合粗分离（基于混合信号与视觉输入重建粗音频）和细分离（将粗音频反馈至音频-视觉语音识别模型并融合视觉信息），实现更精准的语音分离。  <br/>2. **引入递归语义增强范式**：通过反馈机制生成更具判别性的语义表示，提升分离效果并验证其必要性。  <br/>3. **设计说话者感知的感知融合模块**：跨模态编码说话者身份，增强多模态信息融合能力。  <br/>4. **构建多范围频时分离网络**：联合捕捉局部与全局时间-频率模式，提升对复杂音频结构的建模能力。  <br/>5. **实验验证SOTA性能**：在三类基准数据集和两类噪声数据集上取得显著的粗到细提升效果，证明方法有效性。  <br/><br/>**总结（100字以内）：**  <br/>提出CSFNet，结合递归语义增强和多模态融合，实现高效音频-视觉语音分离，通过双阶段策略与多范围网络设计，在多个数据集上达到SOTA性能。|
|2509.22425v1|[From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation](http://arxiv.org/abs/2509.22425v1)|总结：  <br/>提出CSFNet，通过递归语义增强框架显著提升音频-视觉语音分离性能，结合动态视觉引导与双阶段分离策略，设计跨模态感知融合与多范围频谱-时序网络，验证了其在多个数据集上的SOTA效果。<br/><br/>贡献点：  <br/>1. 提出**Coarse-to-Separate-Fine Network (CSFNet)**，首次采用递归语义增强范式，通过粗分离和细分离两阶段迭代提升分离效果。  <br/>2. 引入**动态视觉语义引导**，突破传统静态视觉表示的局限，增强音频与视觉信息的协同建模能力。  <br/>3. 设计**speaker-aware perceptual fusion block**，通过跨模态编码说话者身份信息，提高分离的语义一致性。  <br/>4. 提出**multi-range spectro-temporal separation network**，同步捕捉局部和全局时间-频率特征，增强分离鲁棒性。  <br/>5. 验证**递归框架的有效性**，在三个基准及两个噪声数据集上实现显著的粗到细性能提升，达到SOTA水平。|
|2509.21867v1|[FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement](http://arxiv.org/abs/2509.21867v1)|总结：  <br/>本文提出FastEnhancer，一种高效流式语音增强模型，通过简化架构和RNNFormer模块在保证质量的同时实现最快处理速度，并开源代码和预训练权重。<br/><br/>贡献点：  <br/>1. 提出FastEnhancer模型，采用轻量级编码器-解码器结构与高效RNNFormer模块，显著降低处理延迟。  <br/>2. 在语音质量和可懂度指标上达到当前最优水平，且在单CPU线程下实现最快的推理速度。  <br/>3. 针对流式场景优化模型设计，平衡性能与实时性需求。  <br/>4. 提供开源代码和预训练权重，便于复现与应用。|
|2509.21833v1|[Lightweight Front-end Enhancement for Robust ASR via Frame Resampling   and Sub-Band Pruning](http://arxiv.org/abs/2509.21833v1)|贡献点：<br/>1. 提出结合帧重采样与子带修剪的ASR噪声鲁棒性优化方案，降低计算成本同时保持性能；<br/>2. 创新性采用层间帧重采样技术，利用残差连接缓解信息损失；<br/>3. 引入渐进式子带修剪策略，动态剔除低信息量频率带以减少计算需求；<br/>4. 通过合成与真实噪声数据集验证，相比标准BSRNN降低66%的SE计算开销。 <br/><br/>总结：该研究通过帧重采样与子带修剪技术优化语音增强，显著降低计算成本且保持ASR性能，在多种噪声环境下验证了有效性。|
|2509.21522v1|[Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via   single stage training](http://arxiv.org/abs/2509.21522v1)|总结：提出SFMSE方法，通过单步不变模型与流匹配技术，实现低延迟高质量语音增强，并分析随机性对性能的影响。<br/><br/>贡献点：<br/>1. 提出Shortcut Flow Matching for Speech Enhancement（SFMSE）方法，结合流匹配与扩散模型优势，实现高效语音增强；<br/>2. 创新性设计单步不变模型架构，无需架构调整即可支持单步/少步/多步去噪过程；<br/>3. 通过条件化速度场于目标时间步，显著降低神经函数评估次数（NFE）至单步，RTF达0.013；<br/>4. 实证展示SFMSE在保持感知质量的同时，实现与传统扩散模型相当的去噪效果；<br/>5. 系统性分析训练与推理阶段的随机性作用，为高质量与低延迟生成模型的平衡提供理论依据。|
|2509.21214v1|[MeanSE: Efficient Generative Speech Enhancement with Mean Flows](http://arxiv.org/abs/2509.21214v1)|总结：<br/>本文提出MeanSE，通过建模平均速度场替代传统流匹配，实现高效高质量语音增强，显著降低函数评估次数并提升越域泛化能力。<br/><br/>贡献点：<br/>1. 提出MeanSE框架：首次将平均流概念引入语音增强领域，通过建模平均速度场而非传统流匹配的复杂梯度场，实现单次函数评估（1-NFE）的高效生成。<br/>2. 降低计算复杂度：相较于需大量NFE的流匹配方法，MeanSE通过平均流策略大幅减少计算开销，提升模型运行效率。<br/>3. 提升感知质量：在保持单次NFE的前提下，MeanSE在语音增强任务中实现与传统方法相当甚至更好的语音质量。<br/>4. 增强越域泛化能力：实验验证MeanSE在未知数据分布场景下表现出优于基线的泛化性能，提升模型鲁棒性。|
|2509.21185v1|[Hybrid Real- And Complex-Valued Neural Network Concept For   Low-Complexity Phase-Aware Speech Enhancement](http://arxiv.org/abs/2509.21185v1)|**贡献点总结：**  <br/>1. 提出混合实值与复值神经网络（Hybrid Real-Complex Networks）用于语音增强，结合两种模型的优势。  <br/>2. 设计一种简单直接的方法，将实值网络扩展为混合网络架构。  <br/>3. 基于语音可懂度和质量指标，系统性比较混合模型与实值/复值模型的性能差异。  <br/>4. 实验证明混合模型在相同参数量下性能优于传统模型，并显著降低计算复杂度（乘积累加操作）。  <br/><br/>**摘要精简（100字以内）：**  <br/>本文提出混合实值-复值神经网络用于语音增强，设计简单扩展方法，通过对比实值、复值及混合模型的性能与复杂度，验证其在相同参数量下显著提升语音质量和效率。|
|2509.21087v2|[Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](http://arxiv.org/abs/2509.21087v2)|**贡献点总结（100字以内）**：  <br/>该论文揭示语音增强模型的对抗脆弱性，指出对抗噪声可篡改语义，实验证明预测模型易受攻击，并提出扩散模型与随机采样器具备内在鲁棒性。<br/><br/>---<br/><br/>**分点贡献列表**：  <br/>1. **揭示对抗脆弱性**：指出语音增强模型的高表达性可能导致其易受对抗攻击，存在安全风险。  <br/>2. **语义篡改攻击**：提出对抗噪声通过心理声学掩蔽技术，可诱导模型生成与原始语义完全不同的增强语音。  <br/>3. **实验验证**：验证当前预测性语音增强模型确实可被此类攻击成功操控，证明其安全性问题。  <br/>4. **鲁棒性设计**：提出扩散模型结合随机采样器在架构上具有抗对抗攻击的天然优势，为改进模型安全性提供新方向。|
|2509.21087v1|[Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](http://arxiv.org/abs/2509.21087v1)|**贡献点：**  <br/>1. 指出现有语音增强模型的高表达性可能带来安全漏洞，易受对抗攻击影响。  <br/>2. 提出对抗噪声的构造方法：通过心理声学掩码技术注入精心设计的扰动。  <br/>3. 实验证明现代预测性语音增强模型可被操控，导致语义偏移。  <br/>4. 首次揭示扩散模型因随机采样机制具有对抗攻击的固有鲁棒性。  <br/><br/>**总结（100字以内）：**  <br/>该论文揭示语音增强模型的对抗脆弱性，提出心理声学掩码注入方法，并证明扩散模型因随机采样具备内生鲁棒性，为安全语音处理提供了新思路。|
|2509.20875v1|[PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice   Pickup in Hearables](http://arxiv.org/abs/2509.20875v1)|总结：  <br/>本文提出两种语音增强策略（PSE和AS-SE）的对比研究，通过训练时数据增强提升跨数据集泛化能力，并验证结合个性化与辅助传感器的混合方法（PAS-SE）在噪声环境下仍能保持性能优势。<br/><br/>贡献点：  <br/>1. **策略对比**：系统分析PSE（基于用户自定义语音）与AS-SE（基于耳内麦克风辅助信号）在单通道语音增强中的适用性与局限性。  <br/>2. **跨数据集泛化研究**：在两个公开数据集上评估不同辅助传感器阵列的性能，探究两种策略的通用性。  <br/>3. **训练增强方法**：提出训练时的数据增强技术，改进AS-SE系统的跨数据集泛化能力。  <br/>4. **混合方法优势**：验证PSE与AS-SE结合的PAS-SE方法能互补提升性能，尤其在使用耳内麦克风录制的入籍语音时效果显著。  <br/>5. **噪声个性化有效性**：证明用噪声耳内语音进行个性化训练的PAS-SE方法仍优于纯AS-SE系统，增强实际应用鲁棒性。|
|2509.20741v1|[Real-Time System for Audio-Visual Target Speech Enhancement](http://arxiv.org/abs/2509.20741v1)|**贡献点**  <br/>1. **全CPU实时系统**：设计了一个完全运行在CPU上的实时音频-视觉语音增强系统RAVEN，避免对GPU的依赖。  <br/>2. **视觉线索整合**：引入唇部运动等视觉信息，提升在干扰说话者环境下的语音增强鲁棒性。  <br/>3. **填补研究空白**：首次实现基于CPU的交互式实时音频-视觉语音增强系统，解决现有研究缺乏实际应用的痛点。  <br/>4. **预训练模型应用**：利用预训练的音频-视觉语音识别模型生成的视觉嵌入，高效编码唇部信息以增强性能。  <br/>5. **多场景鲁棒性**：系统可处理环境噪声、瞬态声音、歌唱等复杂干扰，具备广泛适用性。  <br/>6. **用户交互体验**：通过麦克风和网络摄像头提供实时演示，允许用户亲身体验语音增强效果。  <br/><br/>**总结**  <br/>提出首个全CPU实时音频-视觉语音增强系统，结合视觉线索提升鲁棒性，并实现多干扰场景的通用适配与用户交互演示。|
|2509.19881v1|[MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model](http://arxiv.org/abs/2509.19881v1)|**贡献点：**  <br/>1. **创新掩码策略**：提出基于稀缺性感知的粗到细（coarse-to-fine）掩码方法，优先处理高频语音内容，后续优化低频内容，提升效率与泛化能力。  <br/>2. **轻量级校正模块**：设计轻量校正模块，通过检测低置信度预测并重新掩码以优化结果，增强推理稳定性。  <br/>3. **参数量优化**：基于BigCodec和Qwen2.5-0.5B模型，通过选择性层保留策略将参数量压缩至200M，实现高效模型结构。  <br/>4. **性能优越性**：在DNS Challenge和noisy LibriSpeech数据集上取得SOTA感知质量，显著降低词错误率，优于更大规模基线模型。  <br/><br/>**总结（100字以内）:**  <br/>MAGE通过稀缺性感知掩码策略与轻量校正模块，优化语音增强效率与质量，在少量参数下实现SOTA性能，显著提升下游任务表现。|
|2509.19495v1|[ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based   Speech Enhancement](http://arxiv.org/abs/2509.19495v1)|总结：  <br/>提出基于语义一致性的集成推理方法，结合语音嵌入方差分析，有效降低低信噪比下的识别错误，并通过自适应扩散步数优化伪影与延迟的平衡，为扩散模型语音增强提供新思路。<br/><br/>贡献点：  <br/>1. 系统性研究扩散模型语音增强中的伪影预测与抑制问题  <br/>2. 提出利用语音嵌入方差预测音素错误的创新方法  <br/>3. 开发基于语义一致性的集成推理框架，降低低SNR场景下的WER达15%  <br/>4. 揭示扩散步数对伪影抑制与推理延迟的权衡关系，提出自适应步数策略  <br/>5. 强调语义先验在引导生成过程、消除伪影中的关键作用|
|2509.18890v1|[Generalizability of Predictive and Generative Speech Enhancement Models   to Pathological Speakers](http://arxiv.org/abs/2509.18890v1)|总结：  <br/>本文提出针对病理语音的三种改进策略，验证了模型通过病理数据训练和微调的有效性，发现多说话人微调效果优于个性化，揭示了数据量对性能的影响，为提升病理语音增强模型提供了方向。<br/><br/>贡献点：  <br/>1. 提出三种针对病理语音的语音增强改进策略：从零训练、预训练模型微调、个体个性化。  <br/>2. 系统验证了病理数据对模型训练/微调的有效性，克服数据量小的挑战。  <br/>3. 通过对比发现，多病理说话人微调效果显著优于单一个性化，揭示策略优先级。  <br/>4. 指出个体个性化效果有限，主要受单说话人数据量不足的制约。  <br/>5. 为病理语音增强领域提供了可操作的改进方向和理论依据。|
|2509.18885v1|[Influence of Clean Speech Characteristics on Speech Enhancement   Performance](http://arxiv.org/abs/2509.18885v1)|总结：  <br/>该研究系统分析了清洁语音特征对SE性能的影响，发现共振峰幅值关键作用及说话者内部声学差异显著影响效果，为SE数据集、评估和模型设计提供新视角。<br/><br/>贡献点：  <br/>1. **系统分析**：首次系统研究清洁语音的内在特性（如音高、共振峰、响度、频谱波动）对语音增强（SE）效果的影响，覆盖多模型、语言和噪声条件。  <br/>2. **关键特征识别**：发现共振峰幅值是预测SE性能的一致性指标，更高且更稳定的共振峰导致更显著的增强增益。  <br/>3. **说话者内差异揭示**：证明同一说话者的不同语音片段性能差异显著，凸显了说话者内部声学变化对SE挑战的重要性。  <br/>4. **方法论建议**：提出在SE研究中需考虑清洁语音的固有特性，为构建更合理的数据集、评估体系和模型设计提供理论依据。|
|2509.16979v1|[Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners](http://arxiv.org/abs/2509.16979v1)|总结（100字以内）:  <br/>提出非侵入式语音可懂度预测框架，结合语音增强器与数据增强策略，解决无参考信号场景下的评估难题，显著提升跨数据集泛化能力，验证了增强器引导方法在现实应用中的有效性。<br/><br/>贡献点分项：  <br/>1. **提出非侵入式预测框架**：首次通过语音增强器构建并行增强信号路径，无需参考信号即可评估听力障碍者语音可懂度。  <br/>2. **验证增强器选择影响**：对比三种SOTA增强器，发现强增强器集成可显著提升预测性能，揭示增强器对可懂度评估的关键作用。  <br/>3. **创新2-clips增强策略**：引入剪辑增强方法，增强听众特异性变化，提升模型对未见数据集的鲁棒性。  <br/>4. **超越非侵入式基线模型**：在多数据集测试中，方法性能优于CPC2 Champion基线，证明增强器引导的非侵入式评估潜力。|
|2509.16945v1|[DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time   UAV Speech Enhancement](http://arxiv.org/abs/2509.16945v1)|总结：  <br/>提出DroFiT模型，通过频率-wise Transformer、混合编码器-解码器与TCN后端结合，实现轻量高效无人机自噪声语音增强，适用于资源受限的UAV实时处理。<br/><br/>贡献点：  <br/>1. **提出轻量级模型架构**：设计DroFiT（Drone Frequency lightweight Transformer），专为单麦克风下的严重无人机自噪声场景优化，兼顾性能与资源效率。  <br/>2. **融合频率与时间信息处理**：采用全频/子频混合编码器-解码器结构，结合TCN（Temporal Convolutional Network）后端，提升内存效率和流处理能力。  <br/>3. **创新的跳连-门控融合机制**：引入可学习的skip-and-gate融合模块，通过联合频谱-时域损失函数优化语音重建质量。  <br/>4. **针对无人机噪声的训练数据**：基于VoiceBank-DEMAND数据集，混合真实无人机噪声（-5至-25 dB SNR），提升模型对特定噪声的适应性。  <br/>5. **实验证明有效性**：在语音增强指标和计算效率上均取得优异表现，验证了模型在资源受限UAV中的实时处理可行性。|
|2509.16705v1|[Reverse Attention for Lightweight Speech Enhancement on Edge Devices](http://arxiv.org/abs/2509.16705v1)|总结：提出轻量级实时语音增强模型，结合U-Net架构与软注意门，提升语音质量和可懂度指标，较基准模型提升6.24% WER和0.64 PESQ。<br/><br/>贡献点：<br/>1. 构建轻量化U-Net架构，适应资源受限设备的实时语音增强需求<br/>2. 引入基于软注意力机制的注意力门，优化特征提取效率<br/>3. 在保持模型规模前提下，实现比基线模型提升6.24% WER和0.64 PESQ的增强效果|
|2509.16481v1|[TF-CorrNet: Leveraging Spatial Correlation for Continuous Speech   Separation](http://arxiv.org/abs/2509.16481v1)|总结（100字以内）:  <br/>本文提出TF-CorrNet，通过结合相关性与PHAT-Beta、双路径时频处理及频谱模块实现高效语音分离，在LibriCSS数据集上展现高性能与低成本。<br/><br/>贡献点:  <br/>1. **引入基于相关性的相位变换（PHAT-beta）输入**：直接利用麦克风间相关性（而非传统相位差或幅度信息）作为分离滤波器估计的核心输入，更精准捕捉空间信息。  <br/>2. **设计时频交替的双路径策略**：通过交替处理时间与频率轴特征，优化对空间信息的建模方式，提升模型对多通道信号的适应性。  <br/>3. **加入频谱模块建模源相关时频模式**：增强对语音源特征的表达能力，进一步改善分离效果，使系统在保持低计算成本的同时实现高效语音分离。|
|2509.15952v2|[Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement](http://arxiv.org/abs/2509.15952v2)|总结：  <br/>本文提出COSE，一种面向语音增强的一步流匹配框架，通过引入速度组成恒等式优化JVP计算，显著提升采样速度并降低训练成本，同时保持语音质量。<br/><br/>贡献点：  <br/>1. **提出COSE框架**：基于一步流匹配模型，专为语音增强设计，克服传统多步生成模型的计算效率瓶颈。  <br/>2. **速度组成恒等式**：优化Jacobian-向量乘积（JVP）计算，高效求解平均速度场，降低训练成本并保持理论一致性。  <br/>3. **显著性能提升**：实验表明，COSE采样速度提升5倍，训练成本下降40%，且未牺牲语音增强质量。|
|2509.15952v1|[Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement](http://arxiv.org/abs/2509.15952v1)|**贡献点：**  <br/>1. 提出COSE框架，首次将一步流匹配模型应用于语音增强，解决传统多步生成的高计算成本和离散化误差问题。  <br/>2. 引入速度组合恒等式，高效计算平均速度场，显著降低Jacobian-vector product（JVP）训练开销，同时保持理论一致性。  <br/>3. 实验验证COSE在标准基准上实现5倍采样加速和40%训练成本降低，且未牺牲语音增强质量。  <br/>4. 提供开源代码，促进模型复现与进一步研究。|
|2509.15922v1|[DISPATCH: Distilling Selective Patches for Speech Enhancement](http://arxiv.org/abs/2509.15922v1)|总结：  <br/>提出DISPatch框架，针对性地在教师优于学生的频谱块进行蒸馏，结合MSSP多尺度方法提升性能，有效优化模型压缩效果。<br/><br/>贡献点：  <br/>1. **DISPatch框架**：首次提出基于Knowledge Gap Score的动态损失分配策略，仅在教师表现优于学生的频谱块进行蒸馏，避免学生模型重复学习教师错误预测区域。  <br/>2. **MSSP方法**：设计频率依赖的多尺度频谱块处理机制，针对低、高频带采用不同大小的块，适应语音频谱的异质性特征。  <br/>3. **兼容性验证**：将DISPatch整合至传统KD方法中，验证其在多种模型压缩场景下的有效性，实现更高效的性能提升。  <br/>4. **性能突破**：结合DISPatch与MSSP的混合方案，显著优于现有最先进的频率依赖KD方法，全面提升语音增强效果。|
|2509.15666v3|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v3)|总结：<br/>本论文提出TISDiSS框架，实现训练与推理时间的双扩展，通过多损失监督、参数共享和动态推理重复技术，显著降低参数量并提升低延迟应用性能，实验验证其在语音分离任务中的SOTA表现。<br/><br/>贡献点：<br/>1. 提出统一框架TISDiSS，实现训练时间和推理时间的双可扩展性<br/>2. 创新性整合早拆多损失监督、共享参数设计和动态推理重复机制<br/>3. 开发无需额外训练即能调整推理深度的灵活速度-性能调控方法<br/>4. 系统性分析架构与训练策略，揭示推理重复对浅层模型性能的提升作用<br/>5. 在标准数据集上实现参数量减少的SOTA性能，验证框架的实用性和可扩展性|
|2509.15666v2|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v2)|总结：提出TISDiSS框架，通过多损失监督、共享参数和动态推理重复实现训练与推理的可扩展性，降低参数量并提升低延迟应用的性能。<br/><br/>贡献点：<br/>1. 提出统一的TISDiSS框架，集成了早期分割多损失监督、共享参数设计和动态推理重复机制<br/>2. 实现训练时与推理时的可扩展性，允许通过调整推理深度灵活平衡速度与性能<br/>3. 通过系统性分析证明增加训练时推理重复次数可提升浅层推理性能<br/>4. 在标准语音分离基准上达到SOTA性能，同时显著减少模型参数量<br/>5. 为低延迟应用场景提供有效的可扩展解决方案，降低训练和部署成本|
|2509.15666v1|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v1)|总结（100字以内）:  <br/>本文提出TISDiSS框架，通过早期分割多损失监督、参数共享和动态推理重复实现训练与推理时间的可扩展性，可在不重新训练的情况下调整推理深度以平衡性能与速度，并在减少参数量的前提下达到SOTA效果，适用于低延迟源分离任务。<br/><br/>贡献点:  <br/>1. **统一框架设计**：首次将早期分割多损失监督、共享参数机制与动态推理重复结合，构建TISDiSS框架，实现训练与推理时间的分离可扩展。  <br/>2. **灵活速度-性能权衡**：通过调整推理深度而非重新训练模型，支持无需额外计算资源即可动态优化实时性与分离效果。  <br/>3. **系统性分析**：深入研究架构与训练策略，证明增加推理重复次数能提升浅层推理性能，为低延迟应用场景提供理论依据。  <br/>4. **高效性能验证**：在标准语音分离基准上实现SOTA性能，同时显著减少参数量，验证框架的可扩展性与实用性。|
|2509.14858v2|[MeanFlowSE: one-step generative speech enhancement via conditional mean   flow](http://arxiv.org/abs/2509.14858v2)|**贡献点总结**（100字以内）:  <br/>提出MeanFlowSE模型，通过单步生成和Jacobian-向量乘积技术，解决实时语音增强中多步骤推理瓶颈，无需外部教师，提升效率与质量，开源实现。<br/><br/>---<br/><br/>**详细贡献点分点**:<br/>1. **提出MeanFlowSE模型**：  <br/>   首次引入基于有限时间区间平均速度的条件生成模型，替代传统基于瞬时速度场的流/扩散框架，解决多步骤推理的计算效率问题。<br/><br/>2. **创新训练目标设计**：  <br/>   利用Jacobian-向量乘积（JVP）构建MeanFlow标识，推导出直接监督有限区间位移的本地训练目标，同时保持与瞬时场约束在对角线上的一致性。<br/><br/>3. **单步推理机制**：  <br/>   通过时间倒退的单步生成策略，避免依赖迭代ODE求解器，显著降低实时生成的计算成本。<br/><br/>4. **可选少步优化**：  <br/>   提供一种可选的少步生成变体，用于进一步提升生成质量，兼顾效率与精度。<br/><br/>5. **实验证明有效性**：  <br/>   在VoiceBank-DEMAND数据集上验证，单步MeanFlowSE在可懂度、保真度和感知质量上优于多步骤基线，且计算开销更低。<br/><br/>6. **无需外部依赖**：  <br/>   不依赖知识蒸馏或外部教师，实现高效、高保真实时语音增强框架，简化部署流程。<br/><br/>7. **开源实现**：  <br/>   提供开源代码（GitHub链接），推动技术复用与进一步研究。|
|2509.14858v1|[MeanFlowSE: one-step generative speech enhancement via conditional mean   flow](http://arxiv.org/abs/2509.14858v1)|**贡献点：**  <br/>1. 提出MeanFlowSE，通过学习有限时间间隔的平均速度场替代传统流/扩散模型的瞬时速度场，解决实时语音增强中的多步推理瓶颈。  <br/>2. 利用Jacobian-向量积（JVP）构建MeanFlow身份，设计局部训练目标直接优化有限区间位移，保持对角线瞬时场约束一致性。  <br/>3. 引入单步推理机制，通过反向时间位移生成语音，摆脱对多步ODE求解器的依赖；提供可选少步变体以提升生成质量。  <br/>4. 在VoiceBank-DEMAND数据集上验证方法有效性，单步模型在计算成本显著降低的同时保持高可懂性、保真度和感知质量。  <br/>5. 方法无需知识蒸馏或外部教师，实现高效、高保真的实时语音增强框架。  <br/><br/>**总结：**  <br/>提出MeanFlowSE模型，通过平均速度场与单步推理机制解决实时语音增强的多步计算问题，实现高效率和高质量性能，无需额外资源。|
|2509.14855v1|[AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning](http://arxiv.org/abs/2509.14855v1)|总结：  <br/>提出AmbiDrop框架，通过Ambisonics编码和通道丢弃技术实现阵列无关的语音增强，无需依赖多样化麦克风阵列数据库，在未见过的阵列布局上保持高性能。<br/><br/>贡献点：  <br/>1. **提出阵列无关的语音增强框架**：基于Ambisonics Signal Matching（ASM）将任意麦克风阵列录音转换为球面调和域，解决了传统方法对特定阵列几何的依赖问题。  <br/>2. **简化数据需求**：利用模拟Ambisonics数据训练深度神经网络，并通过通道丢弃增强鲁棒性，无需真实多几何数据库。  <br/>3. **验证泛化能力**：在未见过的阵列布局上对SI-SDR、PESQ和STOI指标均实现提升，证明了方法的通用性和实际应用潜力。|
|2509.14379v1|[Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy   Environments with Noise Prior](http://arxiv.org/abs/2509.14379v1)|贡献点：  <br/>1. 提出生成式无监督单麦克风语音分离方法，直接建模干净语音与结构化噪音，而非依赖混音信号；  <br/>2. 引入音频-视觉评分模型，利用视觉线索作为强语音生成先验；  <br/>3. 显式建模语音与噪音分布，结合逆问题范式实现有效分解；  <br/>4. 通过反向扩散过程从后验分布采样，直接估计并消除噪音以恢复语音信号；  <br/>5. 实验验证该方法在复杂声学环境下的有效性。  <br/><br/>总结：  <br/>本文提出基于音频-视觉评分模型的生成式无监督语音分离方法，通过显式建模语音与噪音分布并结合反向扩散过程，有效解决了单麦克风环境下语音分离问题，验证了直接建模噪声的优势。|
|2509.14076v1|[A Lightweight Fourier-based Network for Binaural Speech Enhancement with   Spatial Cue Preservation](http://arxiv.org/abs/2509.14076v1)|**贡献点：**  <br/>1. 提出Global Adaptive Fourier Network (GAF-Net)，首次结合轻量化设计与双耳语音增强性能平衡。  <br/>2. 创新性地采用双特征编码器（STFT + gammatone特征），提升声学表征的鲁棒性。  <br/>3. 设计通道无关的全局自适应傅里叶调制器，高效捕捉长时序依赖并保留空间信息。  <br/>4. 引入动态门控机制，显著降低处理伪影，提升输出质量。  <br/>5. 实验验证GAF-Net在双耳线索（ILD/IPD误差）和客观可懂度（MBSTOI）指标上表现优异，且参数与计算成本显著低于现有方法。  <br/><br/>**总结（100字以内）：**  <br/>GAF-Net通过双特征编码、全局自适应调制与动态门控机制，实现轻量化双耳语音增强，在保持高性能的同时降低计算成本，适用于资源受限设备。|
|2509.13825v1|[Neural Speech Separation with Parallel Amplitude and Phase Spectrum   Estimation](http://arxiv.org/abs/2509.13825v1)|**贡献点：**  <br/>1. 提出APSS模型，首次在语音分离中显式估计相位谱以提升分离完整性与准确性。  <br/>2. 引入特征融合器，将时间和频率信息整合为联合表示以增强模型表征能力。  <br/>3. 采用并行振幅与相位分离器，结合时间频率Transformer捕捉复杂时频依赖关系。  <br/>4. 通过iSTFT重构分离信号，实现端到端高效语音分离。  <br/>5. 实验验证APSS在时域方法和隐式相位估计方法上均表现更优，具备强泛化能力与实用价值。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出APSS模型，通过显式相位谱估计与并行分离器设计，在语音分离任务中实现更准确的信号重构，实验表明其在多数据集上均优于时域和隐式相位方法，具备良好通用性与应用潜力。|
|2509.12974v1|[The CCF AATC 2025: Speech Restoration Challenge](http://arxiv.org/abs/2509.12974v1)|总结：  <br/>本文提出Speech Restoration Challenge，构建包含多类复合失真的测试数据集，设计全面的评估框架，推动语音增强在复杂环境下的研究。<br/><br/>贡献点：  <br/>1. 提出首个针对多重复合语音失真的研究挑战（Speech Restoration Challenge），聚焦非稳态噪声、混响及预处理伪影的联合修复。  <br/>2. 构建综合性语音退化数据集，涵盖复杂声学失真、信号链伪影和二次增强伪影三种主流退化类型。  <br/>3. 设计兼顾客观性能评估与模型复杂度的双维度评价协议，促进算法的实用性与效果平衡。  <br/>4. 提供标准化任务框架与公开资源，推动语音增强领域的研究范式革新。|
|2509.10143v1|[Error Analysis in a Modular Meeting Transcription System](http://arxiv.org/abs/2509.10143v1)|**贡献点：**<br/>1. **框架扩展**：提出改进的语音分离泄漏分析框架，增强对时间局部性的敏感性。  <br/>2. **泄漏现象揭示**：发现仅主说话人活动时存在显著交叉通道泄漏，但VAD将其忽略，故对最终性能影响有限。  <br/>3. **分割方法对比**：比较不同分割策略，证明先进对齐技术可将分割效果与理想分割的差距缩小至三分之一。  <br/>4. **差异因素分析**：阐明剩余性能差距的成因，为优化提供理论依据。  <br/>5. **性能突破**：在仅使用LibriSpeech训练的系统中，于LibriCSS数据集上取得最先进性能。  <br/><br/>**总结：**  <br/>该研究通过优化框架和分割方法，揭示语音分离中泄漏的影响机制，并在LibriCSS上实现现有系统中最佳性能。|
|2509.09201v1|[DeCodec: Rethinking Audio Codecs as Universal Disentangled   Representation Learners](http://arxiv.org/abs/2509.09201v1)|总结：  <br/>提出DeCodec模型，实现语音与背景音的解耦表示，支持灵活特征选择，提升语音增强、ASR鲁棒性和TTS背景音控制等应用效果。<br/><br/>贡献点：  <br/>1. **提出新型通用音频编解码器DeCodec**：首次将音频表示解耦为语音和背景音的正交子空间，并在语音层进一步分解为语义和语用成分，实现分层解纠缠。  <br/>2. **双技术创新**：  <br/>   - 引入子空间正交投影模块，将输入音频分解为独立的正交子空间；  <br/>   - 设计表示交换训练过程，确保子空间分别对应语音和背景音。  <br/>3. **语义引导的语音子空间分解**：通过语义指导实现语音成分的语义-语用分离，增强模型对语音信号的理解。  <br/>4. **多功能应用验证**：在保持高质量信号重建的基础上，展示语音增强、单次语音转换、ASR鲁棒性和TTS的背景音控制等新能力。|
|2509.08470v1|[Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition](http://arxiv.org/abs/2509.08470v1)|总结：  <br/>提出Sparse MERIT框架，结合自监督表示与动态专家路由机制，有效解决噪声环境下SER与SE任务的性能下降问题，实现参数高效且任务自适应的表示学习。<br/><br/>贡献点：<br/>1. **首次提出框架融合**：提出将语音增强（SE）与情感识别（SER）任务结合的多任务学习框架Sparse MERIT，通过帧级专家路由机制优化任务协同。<br/>2. **动态专家选择机制**：引入任务特定的门控网络，动态从共享专家池中选择适配当前语音帧的专家，减少冗余计算与表示冲突。<br/>3. **参数高效性**：通过稀疏专家路由实现参数共享与任务适应性，降低模型复杂度与训练成本，同时保持高任务性能。<br/>4. **噪声鲁棒性验证**：在MSP-Podcast数据集上验证，Sparse MERIT在-5 dB极端噪声条件下显著提升SER F1-macro（+12.0%）和SE SSNR（+28.2%），并在未知噪声场景中具备统计显著性。|
|2509.07521v1|[Target matching based generative model for speech enhancement](http://arxiv.org/abs/2509.07521v1)|总结：本文提出基于目标的生成框架，优化均值/方差时间表设计，消除训练损失中的随机项，创新扩散骨干网络，显著提升语音生成效率与质量。<br/><br/>贡献点：<br/>1. 提出新型目标导向生成框架，兼顾均值/方差时间表灵活性与训练推理效率<br/>2. 通过重构训练损失去除随机项，实现更稳定高效的语音增强流程<br/>3. 引入logistic均值调度与bridge方差调度，优化信噪比轨迹提升扰动策略效率<br/>4. 开发新型音频扩散骨干网络，通过建模长时帧相关和跨频段依赖显著降低NCSN++的计算复杂度|
|2509.07341v1|[Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation](http://arxiv.org/abs/2509.07341v1)|总结：  <br/>提出AFN-HearNet模型，通过跨域特征融合解决噪声抑制与听力补偿任务分离问题，显著提升性能与效率，验证了各模块的有效性。<br/><br/>贡献点：  <br/>1. **提出联合处理框架AFN-HearNet**：首次将噪声抑制（NR）与听力损失补偿（HLC）任务统一处理，突破传统分离处理的局限。  <br/>2. **设计audiogram-specific编码器**：将稀疏的audiogram特征转化为深层表示，解决跨域特征对齐难题。  <br/>3. **引入affine modulation机制**：通过频率-时间Conformer模块实现NR与HLC特征的自适应融合，增强任务间交互建模。  <br/>4. **添加VAD辅助训练任务**：隐式嵌入语音与非语音模式，优化统一表示的学习效果。  <br/>5. **实验证明有效性**：跨多数据集验证模型模块的贡献，显著提升HASQI和PESQ等关键指标性能。|
|2509.05079v1|[Lightweight DNN for Full-Band Speech Denoising on Mobile Devices:   Exploiting Long and Short Temporal Patterns](http://arxiv.org/abs/2509.05079v1)|总结（100字以内）:  <br/>本文提出一种低延迟、轻量化且适用于全带信号的语音降噪方法，结合修改UNet架构与RNN，有效提取短时和长时时序特征，并在移动端实现实时因子低于0.02，优于现有方法。<br/><br/>---  <br/>**贡献点**  <br/>1. **针对资源受限平台优化**：设计了轻量化且低延迟的DNN模型，适用于移动端等计算资源有限的场景。  <br/>2. **全带信号处理能力**：支持48kHz采样率的全带（FB）语音信号降噪，解决现有方法对高频信号处理不足的问题。  <br/>3. **时序模式融合**：通过look-back帧、卷积核时序跨度和RNN模块，同时建模短时与长时语音时序特征。  <br/>4. **因果框架设计**：基于因果性框架的帧级处理，确保实时性并避免未来信息泄露。  <br/>5. **高效网络结构**：结合MobileNet的倒置瓶颈设计与因果实例归一化，提升计算效率与性能。  <br/>6. **实验证明优越性**：在公开数据集上验证方法有效性，SI-SDR指标优于现有全带与低延迟SD方法。|
|2509.04280v1|[Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation](http://arxiv.org/abs/2509.04280v1)|总结： <br/>LaDen提出首个针对语音增强的测试时适应方法，通过预训练表示与线性转换实现跨领域泛化，并建立综合基准验证效果。<br/><br/>贡献点：<br/>1. 提出LaDen（latent denoising）方法，首次解决语音增强模型在域移场景下的测试时适应问题<br/>2. 构建跨领域语音表示转换框架，利用线性映射将噪声嵌入转化为干净语音表示<br/>3. 创建包含多数据集、多种域移类型的综合基准测试平台（噪声类型/说话人特征/语言变化）<br/>4. 通过大量实验证明LaDen在感知指标上显著优于传统方法，尤其对说话人和语言域移表现突出|