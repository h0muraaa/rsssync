|Source|Title|Summary|
|---|---|---|
|2510.02187v1|[High-Fidelity Speech Enhancement via Discrete Audio Tokens](http://arxiv.org/abs/2510.02187v1)|总结（100字以内）:  <br/>DAC-SE1提出基于离散高分辨率音频表示的简化框架，保持声学细节与语义连贯，超越现有自回归语音增强方法，在客观指标和MUSHRA测试中表现优异，并开源代码与模型促进研究。<br/><br/>贡献点:  <br/>1. 引入离散高分辨率音频表示，替代传统低采样率编码，提升声学细节保留能力。  <br/>2. 构建简化语言模型驱动的单阶段语音增强框架，降低复杂度并扩展应用范围。  <br/>3. 在客观感知指标和MUSHRA人眼评估中均超越当前最优自回归SE方法。  <br/>4. 开源代码与模型检查点，推动可扩展、统一、高质量语音增强技术发展。|
|2510.01958v1|[Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for   Improved Cross-Corpus Speech Enhancement](http://arxiv.org/abs/2510.01958v1)|**贡献点：**  <br/>1. 提出RWSA-MambaUNet，创新性融合Mamba与多头注意力机制，优化U-Net结构以提升语音增强性能。  <br/>2. 引入分辨率级共享注意力（RWSA）机制，实现跨时间与频率分辨率的层间注意力信息共享，增强模型泛化能力。  <br/>3. 在两个模型外测试集（DNS 2020与EARS-WHAM_v2）中均取得SOTA泛化性能，且最小模型参数量仅为基线的一半，FLOPs显著降低。  <br/>4. 验证了RWSA-MambaUNet在PESQ、SSNR、ESTOI及SI-SDR等关键指标上的全面优势，尤其在DNS 2020测试集上超越所有基线模型。  <br/><br/>**总结（100字以内）：**  <br/>提出RWSA-MambaUNet混合模型，结合Mamba与多头注意力机制，通过分辨率级共享注意力提升跨语料泛化性能，同时降低参数与计算量，在DNS2020和EARS-WHAM_v2测试集上超越基线，实现高效语音增强。|
|2510.01130v2|[Learning Time-Graph Frequency Representation for Monaural Speech   Enhancement](http://arxiv.org/abs/2510.01130v2)|总结：  <br/>本文提出可学习GFT-SVD框架，通过1-D卷积构建动态图拓扑与基，消除矩阵求逆带来的数值不稳定问题，提升语音增强的自适应性和效果。  <br/><br/>贡献点：  <br/>1. **提出可学习图拓扑**：引入图移位算子构建动态可学习图结构，替代传统固定图拓扑以增强表示的适应性与灵活性。  <br/>2. **设计可学习图傅里叶基**：利用1-D卷积神经层定义图傅里叶基，避免显式矩阵求逆操作，解决GFT-SVD和GFT-EVD的数值误差问题。  <br/>3. **框架稳定性提升**：消除矩阵求逆带来的不稳定性，增强模型鲁棒性。  <br/>4. **简化计算流程**：通过学习方式替代复杂分解过程，降低计算复杂度并改善语音增强性能。|
|2509.25982v1|[An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction](http://arxiv.org/abs/2509.25982v1)|**贡献点总结（100字以内）：**  <br/>本研究提出通过非线性深度神经网络进行联合空间-时频谱处理，有效解决传统线性滤波器因空间混叠导致的性能下降问题。相比单独处理或分离式处理方法，联合处理显著提升系统对空间混叠的鲁棒性，并为多通道语音增强提供新的技术动机，特别是在大间距麦克风阵列应用中。|
|2509.25275v1|[VoiceBridge: Designing Latent Bridge Models for General Speech   Restoration at Scale](http://arxiv.org/abs/2509.25275v1)|**贡献点总结：**  <br/>VoiceBridge提出了一种基于潜在桥模型的语音增强系统，通过统一潜在到潜在生成过程、能量保持的VAE和感知导向的微调，解决了多任务和跨数据集的高质量语音重建问题，提升了感知质量。  <br/><br/>**分点贡献：**  <br/>1. **提出VoiceBridge系统**：基于潜在桥模型（LBMs）实现从多种失真中重建全频带（48kHz）高质量语音的通用语音恢复（GSR）。  <br/>2. **统一潜在生成过程**：通过可扩展的Transformer架构，将多样化的低质量-高质量（LQ-to-HQ）任务整合为单一潜在到潜在生成流程。  <br/>3. **能量保持的VAE设计**：引入能量保持变分自编码器，增强波形与潜在空间在不同能量水平下的对齐能力。  <br/>4. **联合神经先验提出**：构建统一的神经先验，缓解因不同LQ先验导致的HQ重建困难，降低LBM的重建负担。  <br/>5. **感知导向的微调阶段**：设计专门的微调策略，优化生成结果的人类感知质量，减少级联失配问题。|
|2509.24708v1|[SenSE: Semantic-Aware High-Fidelity Universal Speech Enhancement](http://arxiv.org/abs/2509.24708v1)|**贡献点（分点）：**  <br/>1. 提出SenSE框架，结合语言模型与流匹配模型，首次将高阶语义信息引入语音增强任务。  <br/>2. 开发语义感知的语音语言模型，生成语义标记以捕捉扭曲语音的上下文语义。  <br/>3. 设计语义引导机制，通过语义信息优化流匹配过程，缓解增强语音的语义模糊问题。  <br/>4. 提出提示引导机制，利用短参考语音维持说话者相似性，应对严重失真场景。  <br/>5. 在多个基准数据集上验证方法有效性，证明其在感知质量、语音保真度和鲁棒性上的综合提升。  <br/>6. 提供代码和演示，促进方法的实际应用与研究复现。  <br/><br/>**总结（100字以内）：**  <br/>SenSE通过融合语言模型与流匹配框架，引入语义引导和提示引导机制，有效解决语音增强中的语义模糊与失真问题，显著提升语音质量与鲁棒性，代码开源便于推广。|
|2509.24395v1|[Unsupervised Single-Channel Speech Separation with a Diffusion Prior   under Speaker-Embedding Guidance](http://arxiv.org/abs/2509.24395v1)|总结：  <br/>提出基于说话人嵌入引导的扩散模型，解决无监督语音分离中的时序不一致问题，并设计专用求解器，有效提升性能。<br/><br/>贡献点：  <br/>1. **源模型框架的创新应用**：首次将扩散生成模型（Diffusion Model）应用于无监督语音分离，仅依赖无混响（anechoic）语音数据，避免合成数据偏差。  <br/>2. **说话人一致性引导策略**：提出Speaker-Embedding引导方法，在反向扩散过程中通过保留说话人特征保持分离语音的时序一致性，同时增强不同说话人嵌入的分离度。  <br/>3. **专用求解器设计**：开发针对语音分离任务的新型求解器，与说话人引导策略结合，显著提升无监督源模型语音分离的性能。  <br/>4. **实验验证与开源支持**：通过大量实验验证方法有效性，并提供音频样本和代码开源，便于复现与研究。|
|2509.23832v1|[LORT: Locally Refined Convolution and Taylor Transformer for Monaural   Speech Enhancement](http://arxiv.org/abs/2509.23832v1)|总结：  <br/>本文提出LORT模型，结合空间通道增强的泰勒变换器与局部精细卷积，实现轻量化高效语音增强，参数仅0.96M，在保持低复杂度的同时达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出T-MSA模块**：引入空间-通道增强注意力机制，优化泰勒变换器的全局建模能力，解决空间注意力局限。  <br/>2. **设计LRC结构**：融合卷积前馈层、时频密集局部卷积与门控单元，增强对细粒度局部特征的捕捉。  <br/>3. **轻量化编码器设计**：基于U-Net结构，仅使用16个输出通道，显著降低参数量与计算复杂度。  <br/>4. **多分辨率处理机制**：通过交替的下采样与上采样操作，提升多尺度特征提取能力。  <br/>5. **复合损失函数优化**：联合幅度、复数、相位、判别器与一致性目标，提升增强结果的鲁棒性与质量。|
|2509.23610v1|[Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and   Multi-Scale Global-Local Attention](http://arxiv.org/abs/2509.23610v1)|贡献点总结（100字以内）:  <br/>提出高效AVSS方法Dolphin，包含轻量双路径视觉编码器DP-LipCoder和带GLA块的轻量音频分离器，实现50%参数减少、2.4倍MACs降低、6倍推理加速，同时超越SOTA性能，并开源代码。  <br/><br/>分点贡献：  <br/>1. **双路径轻量化视频编码器（DP-LipCoder）**：将唇部运动转化为离散的音频对齐语义标记，显著减少模型参数和计算量。  <br/>2. **全局-局部注意力（GLA）模块**：在音频分离器中引入GLA块，高效捕捉多尺度音频依赖关系。  <br/>3. **端到端效率提升**：在保持高分离性能的同时，实现超过50%的参数减少、2.4倍MACs降低和6倍GPU推理加速。  <br/>4. **跨场景验证**：在三个基准数据集上验证方法有效性，证明其在复杂真实环境中的实用性。  <br/>5. **开源与可复现性**：提供完整代码及演示页面，便于研究者复现和应用。|
|2509.22942v1|[Unsupervised Speech Enhancement using Data-defined Priors](http://arxiv.org/abs/2509.22942v1)|总结：  <br/>提出双分支编码器-解码器架构，利用对抗训练和无配对数据定义先验，解决无监督语音增强中的数据配对问题，揭示同域数据选择对性能的影响，实验验证其效果可媲美现有方法。<br/><br/>贡献点：  <br/>1. **创新架构**：设计双分支编码器-解码器框架，首次将输入信号分离为干净语音与残余噪声，提升无监督增强的模型表征能力。  <br/>2. **对抗训练机制**：引入对抗训练策略，通过无配对干净语音和噪声数据为各分支定义先验，减少合成数据与真实数据间的分布差异。  <br/>3. **无配对数据利用**：无需依赖配对数据，仅需单通道干净语音和噪声数据即可训练模型，突破传统方法的数据限制。  <br/>4. **性能对比验证**：实验表明方法性能接近当前主流无监督语音增强技术，证明其有效性与实用性。  <br/>5. **数据选择分析**：首次系统探讨同域数据用于先验定义的局限性，揭示其可能引发的性能虚高问题，为研究提供新视角。|
|2509.22425v1|[From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation](http://arxiv.org/abs/2509.22425v1)|总结：  <br/>提出CSFNet，通过递归语义增强框架显著提升音频-视觉语音分离性能，结合动态视觉引导与双阶段分离策略，设计跨模态感知融合与多范围频谱-时序网络，验证了其在多个数据集上的SOTA效果。<br/><br/>贡献点：  <br/>1. 提出**Coarse-to-Separate-Fine Network (CSFNet)**，首次采用递归语义增强范式，通过粗分离和细分离两阶段迭代提升分离效果。  <br/>2. 引入**动态视觉语义引导**，突破传统静态视觉表示的局限，增强音频与视觉信息的协同建模能力。  <br/>3. 设计**speaker-aware perceptual fusion block**，通过跨模态编码说话者身份信息，提高分离的语义一致性。  <br/>4. 提出**multi-range spectro-temporal separation network**，同步捕捉局部和全局时间-频率特征，增强分离鲁棒性。  <br/>5. 验证**递归框架的有效性**，在三个基准及两个噪声数据集上实现显著的粗到细性能提升，达到SOTA水平。|
|2509.21867v1|[FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement](http://arxiv.org/abs/2509.21867v1)|总结：  <br/>本文提出FastEnhancer，一种高效流式语音增强模型，通过简化架构和RNNFormer模块在保证质量的同时实现最快处理速度，并开源代码和预训练权重。<br/><br/>贡献点：  <br/>1. 提出FastEnhancer模型，采用轻量级编码器-解码器结构与高效RNNFormer模块，显著降低处理延迟。  <br/>2. 在语音质量和可懂度指标上达到当前最优水平，且在单CPU线程下实现最快的推理速度。  <br/>3. 针对流式场景优化模型设计，平衡性能与实时性需求。  <br/>4. 提供开源代码和预训练权重，便于复现与应用。|
|2509.21833v1|[Lightweight Front-end Enhancement for Robust ASR via Frame Resampling   and Sub-Band Pruning](http://arxiv.org/abs/2509.21833v1)|贡献点：<br/>1. 提出结合帧重采样与子带修剪的ASR噪声鲁棒性优化方案，降低计算成本同时保持性能；<br/>2. 创新性采用层间帧重采样技术，利用残差连接缓解信息损失；<br/>3. 引入渐进式子带修剪策略，动态剔除低信息量频率带以减少计算需求；<br/>4. 通过合成与真实噪声数据集验证，相比标准BSRNN降低66%的SE计算开销。 <br/><br/>总结：该研究通过帧重采样与子带修剪技术优化语音增强，显著降低计算成本且保持ASR性能，在多种噪声环境下验证了有效性。|
|2509.21522v1|[Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via   single stage training](http://arxiv.org/abs/2509.21522v1)|总结：提出SFMSE方法，通过单步不变模型与流匹配技术，实现低延迟高质量语音增强，并分析随机性对性能的影响。<br/><br/>贡献点：<br/>1. 提出Shortcut Flow Matching for Speech Enhancement（SFMSE）方法，结合流匹配与扩散模型优势，实现高效语音增强；<br/>2. 创新性设计单步不变模型架构，无需架构调整即可支持单步/少步/多步去噪过程；<br/>3. 通过条件化速度场于目标时间步，显著降低神经函数评估次数（NFE）至单步，RTF达0.013；<br/>4. 实证展示SFMSE在保持感知质量的同时，实现与传统扩散模型相当的去噪效果；<br/>5. 系统性分析训练与推理阶段的随机性作用，为高质量与低延迟生成模型的平衡提供理论依据。|
|2509.21214v1|[MeanSE: Efficient Generative Speech Enhancement with Mean Flows](http://arxiv.org/abs/2509.21214v1)|总结：<br/>本文提出MeanSE，通过建模平均速度场替代传统流匹配，实现高效高质量语音增强，显著降低函数评估次数并提升越域泛化能力。<br/><br/>贡献点：<br/>1. 提出MeanSE框架：首次将平均流概念引入语音增强领域，通过建模平均速度场而非传统流匹配的复杂梯度场，实现单次函数评估（1-NFE）的高效生成。<br/>2. 降低计算复杂度：相较于需大量NFE的流匹配方法，MeanSE通过平均流策略大幅减少计算开销，提升模型运行效率。<br/>3. 提升感知质量：在保持单次NFE的前提下，MeanSE在语音增强任务中实现与传统方法相当甚至更好的语音质量。<br/>4. 增强越域泛化能力：实验验证MeanSE在未知数据分布场景下表现出优于基线的泛化性能，提升模型鲁棒性。|
|2509.21185v1|[Hybrid Real- And Complex-Valued Neural Network Concept For   Low-Complexity Phase-Aware Speech Enhancement](http://arxiv.org/abs/2509.21185v1)|**贡献点总结：**  <br/>1. 提出混合实值与复值神经网络（Hybrid Real-Complex Networks）用于语音增强，结合两种模型的优势。  <br/>2. 设计一种简单直接的方法，将实值网络扩展为混合网络架构。  <br/>3. 基于语音可懂度和质量指标，系统性比较混合模型与实值/复值模型的性能差异。  <br/>4. 实验证明混合模型在相同参数量下性能优于传统模型，并显著降低计算复杂度（乘积累加操作）。  <br/><br/>**摘要精简（100字以内）：**  <br/>本文提出混合实值-复值神经网络用于语音增强，设计简单扩展方法，通过对比实值、复值及混合模型的性能与复杂度，验证其在相同参数量下显著提升语音质量和效率。|
|2509.21087v1|[Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](http://arxiv.org/abs/2509.21087v1)|**贡献点：**  <br/>1. 指出现有语音增强模型的高表达性可能带来安全漏洞，易受对抗攻击影响。  <br/>2. 提出对抗噪声的构造方法：通过心理声学掩码技术注入精心设计的扰动。  <br/>3. 实验证明现代预测性语音增强模型可被操控，导致语义偏移。  <br/>4. 首次揭示扩散模型因随机采样机制具有对抗攻击的固有鲁棒性。  <br/><br/>**总结（100字以内）：**  <br/>该论文揭示语音增强模型的对抗脆弱性，提出心理声学掩码注入方法，并证明扩散模型因随机采样具备内生鲁棒性，为安全语音处理提供了新思路。|
|2509.20875v1|[PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice   Pickup in Hearables](http://arxiv.org/abs/2509.20875v1)|总结：  <br/>本文提出两种语音增强策略（PSE和AS-SE）的对比研究，通过训练时数据增强提升跨数据集泛化能力，并验证结合个性化与辅助传感器的混合方法（PAS-SE）在噪声环境下仍能保持性能优势。<br/><br/>贡献点：  <br/>1. **策略对比**：系统分析PSE（基于用户自定义语音）与AS-SE（基于耳内麦克风辅助信号）在单通道语音增强中的适用性与局限性。  <br/>2. **跨数据集泛化研究**：在两个公开数据集上评估不同辅助传感器阵列的性能，探究两种策略的通用性。  <br/>3. **训练增强方法**：提出训练时的数据增强技术，改进AS-SE系统的跨数据集泛化能力。  <br/>4. **混合方法优势**：验证PSE与AS-SE结合的PAS-SE方法能互补提升性能，尤其在使用耳内麦克风录制的入籍语音时效果显著。  <br/>5. **噪声个性化有效性**：证明用噪声耳内语音进行个性化训练的PAS-SE方法仍优于纯AS-SE系统，增强实际应用鲁棒性。|
|2509.20741v1|[Real-Time System for Audio-Visual Target Speech Enhancement](http://arxiv.org/abs/2509.20741v1)|**贡献点**  <br/>1. **全CPU实时系统**：设计了一个完全运行在CPU上的实时音频-视觉语音增强系统RAVEN，避免对GPU的依赖。  <br/>2. **视觉线索整合**：引入唇部运动等视觉信息，提升在干扰说话者环境下的语音增强鲁棒性。  <br/>3. **填补研究空白**：首次实现基于CPU的交互式实时音频-视觉语音增强系统，解决现有研究缺乏实际应用的痛点。  <br/>4. **预训练模型应用**：利用预训练的音频-视觉语音识别模型生成的视觉嵌入，高效编码唇部信息以增强性能。  <br/>5. **多场景鲁棒性**：系统可处理环境噪声、瞬态声音、歌唱等复杂干扰，具备广泛适用性。  <br/>6. **用户交互体验**：通过麦克风和网络摄像头提供实时演示，允许用户亲身体验语音增强效果。  <br/><br/>**总结**  <br/>提出首个全CPU实时音频-视觉语音增强系统，结合视觉线索提升鲁棒性，并实现多干扰场景的通用适配与用户交互演示。|
|2509.19881v1|[MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model](http://arxiv.org/abs/2509.19881v1)|**贡献点：**  <br/>1. **创新掩码策略**：提出基于稀缺性感知的粗到细（coarse-to-fine）掩码方法，优先处理高频语音内容，后续优化低频内容，提升效率与泛化能力。  <br/>2. **轻量级校正模块**：设计轻量校正模块，通过检测低置信度预测并重新掩码以优化结果，增强推理稳定性。  <br/>3. **参数量优化**：基于BigCodec和Qwen2.5-0.5B模型，通过选择性层保留策略将参数量压缩至200M，实现高效模型结构。  <br/>4. **性能优越性**：在DNS Challenge和noisy LibriSpeech数据集上取得SOTA感知质量，显著降低词错误率，优于更大规模基线模型。  <br/><br/>**总结（100字以内）:**  <br/>MAGE通过稀缺性感知掩码策略与轻量校正模块，优化语音增强效率与质量，在少量参数下实现SOTA性能，显著提升下游任务表现。|
|2509.19495v1|[ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based   Speech Enhancement](http://arxiv.org/abs/2509.19495v1)|总结：  <br/>提出基于语义一致性的集成推理方法，结合语音嵌入方差分析，有效降低低信噪比下的识别错误，并通过自适应扩散步数优化伪影与延迟的平衡，为扩散模型语音增强提供新思路。<br/><br/>贡献点：  <br/>1. 系统性研究扩散模型语音增强中的伪影预测与抑制问题  <br/>2. 提出利用语音嵌入方差预测音素错误的创新方法  <br/>3. 开发基于语义一致性的集成推理框架，降低低SNR场景下的WER达15%  <br/>4. 揭示扩散步数对伪影抑制与推理延迟的权衡关系，提出自适应步数策略  <br/>5. 强调语义先验在引导生成过程、消除伪影中的关键作用|
|2509.18890v1|[Generalizability of Predictive and Generative Speech Enhancement Models   to Pathological Speakers](http://arxiv.org/abs/2509.18890v1)|总结：  <br/>本文提出针对病理语音的三种改进策略，验证了模型通过病理数据训练和微调的有效性，发现多说话人微调效果优于个性化，揭示了数据量对性能的影响，为提升病理语音增强模型提供了方向。<br/><br/>贡献点：  <br/>1. 提出三种针对病理语音的语音增强改进策略：从零训练、预训练模型微调、个体个性化。  <br/>2. 系统验证了病理数据对模型训练/微调的有效性，克服数据量小的挑战。  <br/>3. 通过对比发现，多病理说话人微调效果显著优于单一个性化，揭示策略优先级。  <br/>4. 指出个体个性化效果有限，主要受单说话人数据量不足的制约。  <br/>5. 为病理语音增强领域提供了可操作的改进方向和理论依据。|
|2509.18885v1|[Influence of Clean Speech Characteristics on Speech Enhancement   Performance](http://arxiv.org/abs/2509.18885v1)|总结：  <br/>该研究系统分析了清洁语音特征对SE性能的影响，发现共振峰幅值关键作用及说话者内部声学差异显著影响效果，为SE数据集、评估和模型设计提供新视角。<br/><br/>贡献点：  <br/>1. **系统分析**：首次系统研究清洁语音的内在特性（如音高、共振峰、响度、频谱波动）对语音增强（SE）效果的影响，覆盖多模型、语言和噪声条件。  <br/>2. **关键特征识别**：发现共振峰幅值是预测SE性能的一致性指标，更高且更稳定的共振峰导致更显著的增强增益。  <br/>3. **说话者内差异揭示**：证明同一说话者的不同语音片段性能差异显著，凸显了说话者内部声学变化对SE挑战的重要性。  <br/>4. **方法论建议**：提出在SE研究中需考虑清洁语音的固有特性，为构建更合理的数据集、评估体系和模型设计提供理论依据。|
|2509.16979v1|[Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners](http://arxiv.org/abs/2509.16979v1)|总结（100字以内）:  <br/>提出非侵入式语音可懂度预测框架，结合语音增强器与数据增强策略，解决无参考信号场景下的评估难题，显著提升跨数据集泛化能力，验证了增强器引导方法在现实应用中的有效性。<br/><br/>贡献点分项：  <br/>1. **提出非侵入式预测框架**：首次通过语音增强器构建并行增强信号路径，无需参考信号即可评估听力障碍者语音可懂度。  <br/>2. **验证增强器选择影响**：对比三种SOTA增强器，发现强增强器集成可显著提升预测性能，揭示增强器对可懂度评估的关键作用。  <br/>3. **创新2-clips增强策略**：引入剪辑增强方法，增强听众特异性变化，提升模型对未见数据集的鲁棒性。  <br/>4. **超越非侵入式基线模型**：在多数据集测试中，方法性能优于CPC2 Champion基线，证明增强器引导的非侵入式评估潜力。|
|2509.16945v1|[DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time   UAV Speech Enhancement](http://arxiv.org/abs/2509.16945v1)|总结：  <br/>提出DroFiT模型，通过频率-wise Transformer、混合编码器-解码器与TCN后端结合，实现轻量高效无人机自噪声语音增强，适用于资源受限的UAV实时处理。<br/><br/>贡献点：  <br/>1. **提出轻量级模型架构**：设计DroFiT（Drone Frequency lightweight Transformer），专为单麦克风下的严重无人机自噪声场景优化，兼顾性能与资源效率。  <br/>2. **融合频率与时间信息处理**：采用全频/子频混合编码器-解码器结构，结合TCN（Temporal Convolutional Network）后端，提升内存效率和流处理能力。  <br/>3. **创新的跳连-门控融合机制**：引入可学习的skip-and-gate融合模块，通过联合频谱-时域损失函数优化语音重建质量。  <br/>4. **针对无人机噪声的训练数据**：基于VoiceBank-DEMAND数据集，混合真实无人机噪声（-5至-25 dB SNR），提升模型对特定噪声的适应性。  <br/>5. **实验证明有效性**：在语音增强指标和计算效率上均取得优异表现，验证了模型在资源受限UAV中的实时处理可行性。|
|2509.16705v1|[Reverse Attention for Lightweight Speech Enhancement on Edge Devices](http://arxiv.org/abs/2509.16705v1)|总结：提出轻量级实时语音增强模型，结合U-Net架构与软注意门，提升语音质量和可懂度指标，较基准模型提升6.24% WER和0.64 PESQ。<br/><br/>贡献点：<br/>1. 构建轻量化U-Net架构，适应资源受限设备的实时语音增强需求<br/>2. 引入基于软注意力机制的注意力门，优化特征提取效率<br/>3. 在保持模型规模前提下，实现比基线模型提升6.24% WER和0.64 PESQ的增强效果|
|2509.16481v1|[TF-CorrNet: Leveraging Spatial Correlation for Continuous Speech   Separation](http://arxiv.org/abs/2509.16481v1)|总结（100字以内）:  <br/>本文提出TF-CorrNet，通过结合相关性与PHAT-Beta、双路径时频处理及频谱模块实现高效语音分离，在LibriCSS数据集上展现高性能与低成本。<br/><br/>贡献点:  <br/>1. **引入基于相关性的相位变换（PHAT-beta）输入**：直接利用麦克风间相关性（而非传统相位差或幅度信息）作为分离滤波器估计的核心输入，更精准捕捉空间信息。  <br/>2. **设计时频交替的双路径策略**：通过交替处理时间与频率轴特征，优化对空间信息的建模方式，提升模型对多通道信号的适应性。  <br/>3. **加入频谱模块建模源相关时频模式**：增强对语音源特征的表达能力，进一步改善分离效果，使系统在保持低计算成本的同时实现高效语音分离。|
|2509.15952v2|[Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement](http://arxiv.org/abs/2509.15952v2)|总结：  <br/>本文提出COSE，一种面向语音增强的一步流匹配框架，通过引入速度组成恒等式优化JVP计算，显著提升采样速度并降低训练成本，同时保持语音质量。<br/><br/>贡献点：  <br/>1. **提出COSE框架**：基于一步流匹配模型，专为语音增强设计，克服传统多步生成模型的计算效率瓶颈。  <br/>2. **速度组成恒等式**：优化Jacobian-向量乘积（JVP）计算，高效求解平均速度场，降低训练成本并保持理论一致性。  <br/>3. **显著性能提升**：实验表明，COSE采样速度提升5倍，训练成本下降40%，且未牺牲语音增强质量。|
|2509.15952v1|[Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement](http://arxiv.org/abs/2509.15952v1)|**贡献点：**  <br/>1. 提出COSE框架，首次将一步流匹配模型应用于语音增强，解决传统多步生成的高计算成本和离散化误差问题。  <br/>2. 引入速度组合恒等式，高效计算平均速度场，显著降低Jacobian-vector product（JVP）训练开销，同时保持理论一致性。  <br/>3. 实验验证COSE在标准基准上实现5倍采样加速和40%训练成本降低，且未牺牲语音增强质量。  <br/>4. 提供开源代码，促进模型复现与进一步研究。|
|2509.15922v1|[DISPATCH: Distilling Selective Patches for Speech Enhancement](http://arxiv.org/abs/2509.15922v1)|总结：  <br/>提出DISPatch框架，针对性地在教师优于学生的频谱块进行蒸馏，结合MSSP多尺度方法提升性能，有效优化模型压缩效果。<br/><br/>贡献点：  <br/>1. **DISPatch框架**：首次提出基于Knowledge Gap Score的动态损失分配策略，仅在教师表现优于学生的频谱块进行蒸馏，避免学生模型重复学习教师错误预测区域。  <br/>2. **MSSP方法**：设计频率依赖的多尺度频谱块处理机制，针对低、高频带采用不同大小的块，适应语音频谱的异质性特征。  <br/>3. **兼容性验证**：将DISPatch整合至传统KD方法中，验证其在多种模型压缩场景下的有效性，实现更高效的性能提升。  <br/>4. **性能突破**：结合DISPatch与MSSP的混合方案，显著优于现有最先进的频率依赖KD方法，全面提升语音增强效果。|
|2509.15666v2|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v2)|总结：提出TISDiSS框架，通过多损失监督、共享参数和动态推理重复实现训练与推理的可扩展性，降低参数量并提升低延迟应用的性能。<br/><br/>贡献点：<br/>1. 提出统一的TISDiSS框架，集成了早期分割多损失监督、共享参数设计和动态推理重复机制<br/>2. 实现训练时与推理时的可扩展性，允许通过调整推理深度灵活平衡速度与性能<br/>3. 通过系统性分析证明增加训练时推理重复次数可提升浅层推理性能<br/>4. 在标准语音分离基准上达到SOTA性能，同时显著减少模型参数量<br/>5. 为低延迟应用场景提供有效的可扩展解决方案，降低训练和部署成本|
|2509.15666v1|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v1)|总结（100字以内）:  <br/>本文提出TISDiSS框架，通过早期分割多损失监督、参数共享和动态推理重复实现训练与推理时间的可扩展性，可在不重新训练的情况下调整推理深度以平衡性能与速度，并在减少参数量的前提下达到SOTA效果，适用于低延迟源分离任务。<br/><br/>贡献点:  <br/>1. **统一框架设计**：首次将早期分割多损失监督、共享参数机制与动态推理重复结合，构建TISDiSS框架，实现训练与推理时间的分离可扩展。  <br/>2. **灵活速度-性能权衡**：通过调整推理深度而非重新训练模型，支持无需额外计算资源即可动态优化实时性与分离效果。  <br/>3. **系统性分析**：深入研究架构与训练策略，证明增加推理重复次数能提升浅层推理性能，为低延迟应用场景提供理论依据。  <br/>4. **高效性能验证**：在标准语音分离基准上实现SOTA性能，同时显著减少参数量，验证框架的可扩展性与实用性。|
|2509.14858v2|[MeanFlowSE: one-step generative speech enhancement via conditional mean   flow](http://arxiv.org/abs/2509.14858v2)|**贡献点总结**（100字以内）:  <br/>提出MeanFlowSE模型，通过单步生成和Jacobian-向量乘积技术，解决实时语音增强中多步骤推理瓶颈，无需外部教师，提升效率与质量，开源实现。<br/><br/>---<br/><br/>**详细贡献点分点**:<br/>1. **提出MeanFlowSE模型**：  <br/>   首次引入基于有限时间区间平均速度的条件生成模型，替代传统基于瞬时速度场的流/扩散框架，解决多步骤推理的计算效率问题。<br/><br/>2. **创新训练目标设计**：  <br/>   利用Jacobian-向量乘积（JVP）构建MeanFlow标识，推导出直接监督有限区间位移的本地训练目标，同时保持与瞬时场约束在对角线上的一致性。<br/><br/>3. **单步推理机制**：  <br/>   通过时间倒退的单步生成策略，避免依赖迭代ODE求解器，显著降低实时生成的计算成本。<br/><br/>4. **可选少步优化**：  <br/>   提供一种可选的少步生成变体，用于进一步提升生成质量，兼顾效率与精度。<br/><br/>5. **实验证明有效性**：  <br/>   在VoiceBank-DEMAND数据集上验证，单步MeanFlowSE在可懂度、保真度和感知质量上优于多步骤基线，且计算开销更低。<br/><br/>6. **无需外部依赖**：  <br/>   不依赖知识蒸馏或外部教师，实现高效、高保真实时语音增强框架，简化部署流程。<br/><br/>7. **开源实现**：  <br/>   提供开源代码（GitHub链接），推动技术复用与进一步研究。|
|2509.14858v1|[MeanFlowSE: one-step generative speech enhancement via conditional mean   flow](http://arxiv.org/abs/2509.14858v1)|**贡献点：**  <br/>1. 提出MeanFlowSE，通过学习有限时间间隔的平均速度场替代传统流/扩散模型的瞬时速度场，解决实时语音增强中的多步推理瓶颈。  <br/>2. 利用Jacobian-向量积（JVP）构建MeanFlow身份，设计局部训练目标直接优化有限区间位移，保持对角线瞬时场约束一致性。  <br/>3. 引入单步推理机制，通过反向时间位移生成语音，摆脱对多步ODE求解器的依赖；提供可选少步变体以提升生成质量。  <br/>4. 在VoiceBank-DEMAND数据集上验证方法有效性，单步模型在计算成本显著降低的同时保持高可懂性、保真度和感知质量。  <br/>5. 方法无需知识蒸馏或外部教师，实现高效、高保真的实时语音增强框架。  <br/><br/>**总结：**  <br/>提出MeanFlowSE模型，通过平均速度场与单步推理机制解决实时语音增强的多步计算问题，实现高效率和高质量性能，无需额外资源。|
|2509.14855v1|[AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning](http://arxiv.org/abs/2509.14855v1)|总结：  <br/>提出AmbiDrop框架，通过Ambisonics编码和通道丢弃技术实现阵列无关的语音增强，无需依赖多样化麦克风阵列数据库，在未见过的阵列布局上保持高性能。<br/><br/>贡献点：  <br/>1. **提出阵列无关的语音增强框架**：基于Ambisonics Signal Matching（ASM）将任意麦克风阵列录音转换为球面调和域，解决了传统方法对特定阵列几何的依赖问题。  <br/>2. **简化数据需求**：利用模拟Ambisonics数据训练深度神经网络，并通过通道丢弃增强鲁棒性，无需真实多几何数据库。  <br/>3. **验证泛化能力**：在未见过的阵列布局上对SI-SDR、PESQ和STOI指标均实现提升，证明了方法的通用性和实际应用潜力。|
|2509.14379v1|[Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy   Environments with Noise Prior](http://arxiv.org/abs/2509.14379v1)|贡献点：  <br/>1. 提出生成式无监督单麦克风语音分离方法，直接建模干净语音与结构化噪音，而非依赖混音信号；  <br/>2. 引入音频-视觉评分模型，利用视觉线索作为强语音生成先验；  <br/>3. 显式建模语音与噪音分布，结合逆问题范式实现有效分解；  <br/>4. 通过反向扩散过程从后验分布采样，直接估计并消除噪音以恢复语音信号；  <br/>5. 实验验证该方法在复杂声学环境下的有效性。  <br/><br/>总结：  <br/>本文提出基于音频-视觉评分模型的生成式无监督语音分离方法，通过显式建模语音与噪音分布并结合反向扩散过程，有效解决了单麦克风环境下语音分离问题，验证了直接建模噪声的优势。|
|2509.14076v1|[A Lightweight Fourier-based Network for Binaural Speech Enhancement with   Spatial Cue Preservation](http://arxiv.org/abs/2509.14076v1)|**贡献点：**  <br/>1. 提出Global Adaptive Fourier Network (GAF-Net)，首次结合轻量化设计与双耳语音增强性能平衡。  <br/>2. 创新性地采用双特征编码器（STFT + gammatone特征），提升声学表征的鲁棒性。  <br/>3. 设计通道无关的全局自适应傅里叶调制器，高效捕捉长时序依赖并保留空间信息。  <br/>4. 引入动态门控机制，显著降低处理伪影，提升输出质量。  <br/>5. 实验验证GAF-Net在双耳线索（ILD/IPD误差）和客观可懂度（MBSTOI）指标上表现优异，且参数与计算成本显著低于现有方法。  <br/><br/>**总结（100字以内）：**  <br/>GAF-Net通过双特征编码、全局自适应调制与动态门控机制，实现轻量化双耳语音增强，在保持高性能的同时降低计算成本，适用于资源受限设备。|
|2509.13825v1|[Neural Speech Separation with Parallel Amplitude and Phase Spectrum   Estimation](http://arxiv.org/abs/2509.13825v1)|**贡献点：**  <br/>1. 提出APSS模型，首次在语音分离中显式估计相位谱以提升分离完整性与准确性。  <br/>2. 引入特征融合器，将时间和频率信息整合为联合表示以增强模型表征能力。  <br/>3. 采用并行振幅与相位分离器，结合时间频率Transformer捕捉复杂时频依赖关系。  <br/>4. 通过iSTFT重构分离信号，实现端到端高效语音分离。  <br/>5. 实验验证APSS在时域方法和隐式相位估计方法上均表现更优，具备强泛化能力与实用价值。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出APSS模型，通过显式相位谱估计与并行分离器设计，在语音分离任务中实现更准确的信号重构，实验表明其在多数据集上均优于时域和隐式相位方法，具备良好通用性与应用潜力。|
|2509.12974v1|[The CCF AATC 2025: Speech Restoration Challenge](http://arxiv.org/abs/2509.12974v1)|总结：  <br/>本文提出Speech Restoration Challenge，构建包含多类复合失真的测试数据集，设计全面的评估框架，推动语音增强在复杂环境下的研究。<br/><br/>贡献点：  <br/>1. 提出首个针对多重复合语音失真的研究挑战（Speech Restoration Challenge），聚焦非稳态噪声、混响及预处理伪影的联合修复。  <br/>2. 构建综合性语音退化数据集，涵盖复杂声学失真、信号链伪影和二次增强伪影三种主流退化类型。  <br/>3. 设计兼顾客观性能评估与模型复杂度的双维度评价协议，促进算法的实用性与效果平衡。  <br/>4. 提供标准化任务框架与公开资源，推动语音增强领域的研究范式革新。|
|2509.10143v1|[Error Analysis in a Modular Meeting Transcription System](http://arxiv.org/abs/2509.10143v1)|**贡献点：**<br/>1. **框架扩展**：提出改进的语音分离泄漏分析框架，增强对时间局部性的敏感性。  <br/>2. **泄漏现象揭示**：发现仅主说话人活动时存在显著交叉通道泄漏，但VAD将其忽略，故对最终性能影响有限。  <br/>3. **分割方法对比**：比较不同分割策略，证明先进对齐技术可将分割效果与理想分割的差距缩小至三分之一。  <br/>4. **差异因素分析**：阐明剩余性能差距的成因，为优化提供理论依据。  <br/>5. **性能突破**：在仅使用LibriSpeech训练的系统中，于LibriCSS数据集上取得最先进性能。  <br/><br/>**总结：**  <br/>该研究通过优化框架和分割方法，揭示语音分离中泄漏的影响机制，并在LibriCSS上实现现有系统中最佳性能。|
|2509.09201v1|[DeCodec: Rethinking Audio Codecs as Universal Disentangled   Representation Learners](http://arxiv.org/abs/2509.09201v1)|总结：  <br/>提出DeCodec模型，实现语音与背景音的解耦表示，支持灵活特征选择，提升语音增强、ASR鲁棒性和TTS背景音控制等应用效果。<br/><br/>贡献点：  <br/>1. **提出新型通用音频编解码器DeCodec**：首次将音频表示解耦为语音和背景音的正交子空间，并在语音层进一步分解为语义和语用成分，实现分层解纠缠。  <br/>2. **双技术创新**：  <br/>   - 引入子空间正交投影模块，将输入音频分解为独立的正交子空间；  <br/>   - 设计表示交换训练过程，确保子空间分别对应语音和背景音。  <br/>3. **语义引导的语音子空间分解**：通过语义指导实现语音成分的语义-语用分离，增强模型对语音信号的理解。  <br/>4. **多功能应用验证**：在保持高质量信号重建的基础上，展示语音增强、单次语音转换、ASR鲁棒性和TTS的背景音控制等新能力。|
|2509.08470v1|[Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition](http://arxiv.org/abs/2509.08470v1)|总结：  <br/>提出Sparse MERIT框架，结合自监督表示与动态专家路由机制，有效解决噪声环境下SER与SE任务的性能下降问题，实现参数高效且任务自适应的表示学习。<br/><br/>贡献点：<br/>1. **首次提出框架融合**：提出将语音增强（SE）与情感识别（SER）任务结合的多任务学习框架Sparse MERIT，通过帧级专家路由机制优化任务协同。<br/>2. **动态专家选择机制**：引入任务特定的门控网络，动态从共享专家池中选择适配当前语音帧的专家，减少冗余计算与表示冲突。<br/>3. **参数高效性**：通过稀疏专家路由实现参数共享与任务适应性，降低模型复杂度与训练成本，同时保持高任务性能。<br/>4. **噪声鲁棒性验证**：在MSP-Podcast数据集上验证，Sparse MERIT在-5 dB极端噪声条件下显著提升SER F1-macro（+12.0%）和SE SSNR（+28.2%），并在未知噪声场景中具备统计显著性。|
|2509.07521v1|[Target matching based generative model for speech enhancement](http://arxiv.org/abs/2509.07521v1)|总结：本文提出基于目标的生成框架，优化均值/方差时间表设计，消除训练损失中的随机项，创新扩散骨干网络，显著提升语音生成效率与质量。<br/><br/>贡献点：<br/>1. 提出新型目标导向生成框架，兼顾均值/方差时间表灵活性与训练推理效率<br/>2. 通过重构训练损失去除随机项，实现更稳定高效的语音增强流程<br/>3. 引入logistic均值调度与bridge方差调度，优化信噪比轨迹提升扰动策略效率<br/>4. 开发新型音频扩散骨干网络，通过建模长时帧相关和跨频段依赖显著降低NCSN++的计算复杂度|
|2509.07341v1|[Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation](http://arxiv.org/abs/2509.07341v1)|总结：  <br/>提出AFN-HearNet模型，通过跨域特征融合解决噪声抑制与听力补偿任务分离问题，显著提升性能与效率，验证了各模块的有效性。<br/><br/>贡献点：  <br/>1. **提出联合处理框架AFN-HearNet**：首次将噪声抑制（NR）与听力损失补偿（HLC）任务统一处理，突破传统分离处理的局限。  <br/>2. **设计audiogram-specific编码器**：将稀疏的audiogram特征转化为深层表示，解决跨域特征对齐难题。  <br/>3. **引入affine modulation机制**：通过频率-时间Conformer模块实现NR与HLC特征的自适应融合，增强任务间交互建模。  <br/>4. **添加VAD辅助训练任务**：隐式嵌入语音与非语音模式，优化统一表示的学习效果。  <br/>5. **实验证明有效性**：跨多数据集验证模型模块的贡献，显著提升HASQI和PESQ等关键指标性能。|
|2509.05079v1|[Lightweight DNN for Full-Band Speech Denoising on Mobile Devices:   Exploiting Long and Short Temporal Patterns](http://arxiv.org/abs/2509.05079v1)|总结（100字以内）:  <br/>本文提出一种低延迟、轻量化且适用于全带信号的语音降噪方法，结合修改UNet架构与RNN，有效提取短时和长时时序特征，并在移动端实现实时因子低于0.02，优于现有方法。<br/><br/>---  <br/>**贡献点**  <br/>1. **针对资源受限平台优化**：设计了轻量化且低延迟的DNN模型，适用于移动端等计算资源有限的场景。  <br/>2. **全带信号处理能力**：支持48kHz采样率的全带（FB）语音信号降噪，解决现有方法对高频信号处理不足的问题。  <br/>3. **时序模式融合**：通过look-back帧、卷积核时序跨度和RNN模块，同时建模短时与长时语音时序特征。  <br/>4. **因果框架设计**：基于因果性框架的帧级处理，确保实时性并避免未来信息泄露。  <br/>5. **高效网络结构**：结合MobileNet的倒置瓶颈设计与因果实例归一化，提升计算效率与性能。  <br/>6. **实验证明优越性**：在公开数据集上验证方法有效性，SI-SDR指标优于现有全带与低延迟SD方法。|
|2509.04280v1|[Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation](http://arxiv.org/abs/2509.04280v1)|总结： <br/>LaDen提出首个针对语音增强的测试时适应方法，通过预训练表示与线性转换实现跨领域泛化，并建立综合基准验证效果。<br/><br/>贡献点：<br/>1. 提出LaDen（latent denoising）方法，首次解决语音增强模型在域移场景下的测试时适应问题<br/>2. 构建跨领域语音表示转换框架，利用线性映射将噪声嵌入转化为干净语音表示<br/>3. 创建包含多数据集、多种域移类型的综合基准测试平台（噪声类型/说话人特征/语言变化）<br/>4. 通过大量实验证明LaDen在感知指标上显著优于传统方法，尤其对说话人和语言域移表现突出|
|2509.02571v1|[Gaussian Process Regression of Steering Vectors With Physics-Aware Deep   Composite Kernels for Augmented Listening](http://arxiv.org/abs/2509.02571v1)|总结：  <br/>本文提出基于神经场与高斯过程的物理感知复合核模型，有效解决声场转向向量超分辨率中的过拟合问题，在数据不足条件下实现高精度空间滤波和双耳渲染，显著减少测量需求。<br/><br/>贡献点：  <br/>1. 提出物理感知的复合核模型，同时建模入射波方向与声场散射效应，突破传统代数方法的局限；  <br/>2. 首次将神经场（NF）与高斯过程（GP）结合，构建基于概率框架的声场连续表示方法；  <br/>3. 通过引入不确定性建模，有效缓解超分辨率过程中的过拟合问题；  <br/>4. 在数据不足场景下（如少于十倍测量）实现接近理想性能的声场重建，验证方法有效性；  <br/>5. 适用于空间滤波、双耳渲染等下游任务，推动增强听觉技术的实际应用。|
|2509.01889v2|[From Evaluation to Optimization: Neural Speech Assessment for Downstream   Applications](http://arxiv.org/abs/2509.01889v2)|总结：  <br/>该论文系统总结了基于神经网络的语音评估模型在语音处理中的双重应用价值，并探讨了其局限性及未来发展方向，推动了语音评估与合成/增强技术的深度融合。<br/><br/>贡献点：  <br/>1. **提出非侵入性评估方法**：通过无需干净参考信号的神经网络模型，解决传统客观指标在实际应用中因参考信号缺失导致的局限性。  <br/>2. **构建可微分感知代理**：将语音评估模型作为可微分的感知代理，既用于评估语音质量与可懂度，又直接指导语音增强和合成模型的优化。  <br/>3. **实现高效特征检测**：利用模型检测语音中的显著特征，为下游处理任务（如降噪、语音识别）提供更精准的输入，提升处理效率。  <br/>4. **分析集成挑战与未来方向**：系统梳理当前语音评估模型在处理流程中的应用瓶颈，并提出未来研究重点，如模型泛化性与实时性改进。|
|2509.01399v1|[CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech   Separation with Distributed Heterogeneous Arrays](http://arxiv.org/abs/2509.01399v1)|总结（100字以内）:  <br/>CabinSep通过融合通道信息、应用MVDR策略和混合数据增强方法，显著提升语音分离效果，降低ASR错误率，且具备极低计算复杂度。<br/><br/>贡献点分点列出:  <br/>1. **创新性空间特征提取**：首次将通道信息用于提取空间特征，提升语音与噪声掩码估计的准确性，增强分离效果。  <br/>2. **ASR优化的MVDR应用**：在推理阶段引入MVDR技术，减少语音失真并提升ASR友好性，降低后端识别错误率。  <br/>3. **混合数据增强方法**：结合模拟与真实环境下的脉冲响应（IRs），改善说话人定位精度，进一步减少识别错误。  <br/>4. **高效计算性能**：模型计算复杂度仅0.4 GMACs，相比SOTA模型DualSep实现17.5%的相对识别错误率下降。|
|2509.00988v1|[A Unified Denoising and Adaptation Framework for Self-Supervised Bengali   Dialectal ASR](http://arxiv.org/abs/2509.00988v1)|总结：该研究提出针对孟加拉语方言和噪声的统一ASR框架，结合WavLM去噪预训练与多阶段微调策略，显著提升识别性能，建立新SOTA并为低资源语言ASR提供可扩展方案。<br/><br/>贡献点：<br/>1. 首次针对孟加拉语方言多样性与环境噪声双重挑战，提出统一解决方案框架<br/>2. 构建基于WavLM的去噪预训练模型，通过掩码语音去噪目标增强对声学失真的鲁棒性<br/>3. 创新性地设计多阶段微调策略：先标准化方言适配，后通过特定数据增强实现噪声鲁棒性优化<br/>4. 在涵盖多方言与多噪声场景的综合基准上进行系统性验证，建立全面评估体系<br/>5. 实验结果超越现有主流模型（wav2vec2.0、Whisper），推动低资源语言ASR技术发展<br/>6. 提供可复用的框架范式，为其他具有高变异性的低资源语言ASR系统设计提供参考 blueprint|
|2509.00405v2|[SaD: A Scenario-Aware Discriminator for Speech Enhancement](http://arxiv.org/abs/2509.00405v2)|总结（100字以内）:  <br/>本文提出场景感知判别器，通过捕捉场景特征和频域分割提升语音增强质量评估，实验证明该方法可适配多种生成器架构，有效提升不同场景下的性能表现。<br/><br/>贡献点：<br/>1. **场景感知机制创新**：设计新型判别器，融入场景特征提取能力，突破传统模型对场景上下文的忽视。<br/>2. **频域分割评估策略**：首次在语音增强中引入频域划分技术，实现对生成语音质量的精细化、场景化判断。<br/>3. **架构无关的通用性**：验证方法对多种生成器架构（如3个代表性模型）均适用，无需修改生成器结构即可提升性能，拓展了模型应用范围。|
|2508.20859v1|[Leveraging Discriminative Latent Representations for Conditioning   GAN-Based Speech Enhancement](http://arxiv.org/abs/2508.20859v1)|**贡献点：**<br/>1. **提出DisCoGAN方法**：首次将判别式语音增强模型的潜在特征作为通用条件特征，用于改进GAN-based语音增强性能。  <br/>2. **系统评估传统架构**：对比分析不同GAN结构（端到端、预处理阶段、后滤波）及判别式模型在低SNR下的表现。  <br/>3. **消融实验分析**：深入探究DisCoGAN内部组件对性能的影响，验证判别式条件方法的关键作用。  <br/>4. **性能优化成果**：在低SNR场景和实际录音中显著优于现有方法，同时保持高SNR和高精度下的竞争力。  <br/><br/>**总结（100字以内）**：  <br/>提出DisCoGAN方法，利用判别式模型潜在特征提升GAN语音增强性能；系统评估不同架构与条件方法，验证其在低SNR下的有效性及优势。|
|2508.20584v1|[Flowing Straighter with Conditional Flow Matching for Accurate Speech   Enhancement](http://arxiv.org/abs/2508.20584v1)|总结：  <br/>该论文提出独立条件流匹配方法，量化路径直度对语音增强的影响，并设计一步推断方案，证明时间独立路径优于时间依赖弯曲路径。<br/><br/>贡献点：  <br/>1. **提出独立条件流匹配框架**：直接建模噪声与干净语音之间的直路径，提升生成效果。  <br/>2. **量化路径直度影响**：通过实验验证路径直度对语音增强质量的关键作用。  <br/>3. **改进Schrodinger桥配置**：展示特定设置可使概率路径更接近直线，增强模型表现。  <br/>4. **一步推断解决方案**：减少传统多步推断的复杂性，将训练模型视为直接预测模型。  <br/>5. **对比路径类型优势**：基于实验发现，证明时间独立路径在生成语音增强中优于时间依赖路径。|
|2508.20474v1|[Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](http://arxiv.org/abs/2508.20474v1)|总结：  <br/>提出一种统一的多说话人编码器（UME），通过共享基础模型实现多任务联合训练，创新性引入RWSE机制提升任务间对齐效果，在LibriMix数据集上显著优于单任务基线。<br/><br/>贡献点：  <br/>1. **提出多任务统一架构**：首次将speaker diarization（SD）、speech separation（SS）、multi-speaker ASR整合到单一模型中，通过共享语音基础编码器提升参数效率与表征能力。  <br/>2. **设计残差加权和编码（RWSE）**：利用多层隐藏表征的加权组合，有效融合不同语义层次信息，促进任务间的自底向上对齐与协作学习。  <br/>3. **提升多任务性能**：联合训练策略捕捉任务间的内在依赖，显著增强在重叠语音数据上的整体处理效果，并在SD任务中取得1.37%（Libri2Mix）和2.29%（Libri3Mix）的SOTA diarization error rates。|
|2508.19583v1|[Lightweight speech enhancement guided target speech extraction in noisy   multi-speaker scenarios](http://arxiv.org/abs/2508.19583v1)|**贡献点总结：**  <br/>1. 提出轻量级语音增强模型GTCRN，用于改善目标语音提取（TSE）在复杂噪声环境下的性能。  <br/>2. 基于SEF-PNet框架开发两个扩展方法LGTSE与D-LGTSE，通过噪声-无关植入和扩展训练条件提升鲁棒性。  <br/>3. 设计两阶段训练策略，结合GTCRN预训练与联合微调，最大化模型潜力。  <br/>4. 在Libri2Mix数据集上实现显著性能提升（SISDR+0.89 dB，PESQ+0.16，STOI+1.97%）。  <br/><br/>**100字内总结：**  <br/>本文提出轻量级GTCRN模型及LGTSE/D-LGTSE扩展方法，通过前期降噪与动态训练条件增强，结合两阶段训练策略，显著提升了TSE在复杂多说话人噪声场景下的性能，并在数据集上验证了有效性。|
|2508.19528v1|[FLASepformer: Efficient Speech Separation with Gated Focused Linear   Attention Transformer](http://arxiv.org/abs/2508.19528v1)|总结:  <br/>提出Focused Linear Attention架构，通过线性复杂度改进实现高效语音分离，结合新Gated模块提升性能，并在多个数据集上验证了模型的效率优势。<br/><br/>贡献点:  <br/>1. 提出Focused Linear Attention机制，突破Transformer注意力模块的二次时间复杂度瓶颈  <br/>2. 构建FLASepformer模型，首次实现语音分离任务的线性时间复杂度处理  <br/>3. 开发FLA-SepReformer和FLA-TFLocoformer两个变体，扩展应用场景并优化性能  <br/>4. 引入新型Gated模块，显著提升分离效果和模型稳定性  <br/>5. 在多个基准数据集上验证模型有效性，证实可保持SOTA性能同时降低内存消耗和提升推理速度|
|2508.19483v1|[Audio-Visual Feature Synchronization for Robust Speech Enhancement in   Hearing Aids](http://arxiv.org/abs/2508.19483v1)|总结：  <br/>本研究提出轻量级跨注意力模型，实现音频-视觉同步提升实时语音增强性能，显著优化噪声抑制与可懂度。<br/><br/>贡献点：  <br/>1. **提出轻量级跨注意力模型**：利用大规模数据和简单架构，学习鲁棒的音频-视觉特征表示，提升多模态融合效率。  <br/>2. **构建高效特征对齐模块**：优化音频-视觉特征同步机制，增强特征对齐的准确性与实时性。  <br/>3. **实现低延迟实时处理**：达到36ms的最小延迟和低能耗，适用于助听器等实际应用场景。  <br/>4. **显著提升性能指标**：在AVSEC3数据集上优于基线，在感知质量（PESQ:0.52）、可懂度（STOI:19%）和保真度（SI-SDR:10.10dB）方面均有显著改进。|
|2508.18913v1|[A Framework for Robust Speaker Verification in Highly Noisy Environments   Leveraging Both Noisy and Enhanced Audio](http://arxiv.org/abs/2508.18913v1)|贡献点：<br/>1. 提出基于Siamese架构的新型神经网络框架，首次结合噪声和增强语音的说话人嵌入以增强验证鲁棒性  <br/>2. 构建轻量级系统，无需修改现有说话人验证和语音增强技术即可兼容多种SOTA解决方案  <br/>3. 解决语音增强可能破坏说话人独特特征的问题，通过互补信息融合提升抗噪性能  <br/>4. 通过实验验证了框架的有效性，证明其在低信噪比环境下的优越表现  <br/><br/>总结：  <br/>本文提出一种轻量级Siamese框架，通过融合噪声与增强语音的说话人嵌入，有效提升说话人验证在恶劣环境下的鲁棒性，兼容多种技术并验证其性能优势。|
|2508.17980v1|[Objective and Subjective Evaluation of Diffusion-Based Speech   Enhancement for Dysarthric Speech](http://arxiv.org/abs/2508.17980v1)|总结：  <br/>本研究首次提出将扩散模型应用于构音障碍语音增强，通过对比两种扩散模型和一种传统信号处理方法，验证其对提升语音质量和ASR性能的有效性，并探索微调ASR模型的潜在优化路径。<br/><br/>贡献点：  <br/>1. **方法创新**：首次系统探索扩散模型在构音障碍语音增强中的应用，提出通过扩散过程将异常语音分布逼近正常语音分布的新思路。  <br/>2. **算法对比**：对比两种扩散模型与一种信号处理算法在构音障碍语音增强中的效果，评估其对语音质量与识别性能的影响。  <br/>3. **多维度评估**：结合主观与客观指标，全面分析增强前后语音质量及ASR性能（使用Whisper-Turbo），验证技术有效性。  <br/>4. **模型优化**：通过微调Whisper-Turbo模型，探讨语音增强对识别性能的进一步提升潜力，拓展ASR系统的适应性。|
|2508.15473v1|[EffortNet: A Deep Learning Framework for Objective Assessment of Speech   Enhancement Technologies Using EEG-Based Alpha Oscillations](http://arxiv.org/abs/2508.15473v1)|总结：本文提出EffortNet，通过融合自监督、增量和迁移学习解决EEG个体差异问题，验证alpha波作为听力努力指标的有效性，并在数据效率和应用价值方面取得创新突破。<br/><br/>贡献点：<br/>1. 提出EffortNet框架：首个结合自监督、增量和迁移学习的深度学习模型，专门用于从EEG信号中解码个体听力努力。<br/>2. 验证alpha震荡作为生物标志物：通过实验证明alpha频段（8-13Hz）功率在嘈杂语音处理中显著升高，确立其作为客观听力努力指标的有效性。<br/>3. 提高模型泛化能力：通过仅需40%新受试者数据即可达到80.9%准确率，显著超越传统CNN和STAnet模型。<br/>4. 揭示语音增强效果差异：发现Transformer增强语音的神经反应更接近自然语音，与主观评价相反但符合客观指标。<br/>5. 提供实际应用价值：为个性化听觉技术评估和认知感知的语音增强系统设计提供新思路。|
|2508.14709v1|[Improving Resource-Efficient Speech Enhancement via Neural   Differentiable DSP Vocoder Refinement](http://arxiv.org/abs/2508.14709v1)|**贡献点总结：**  <br/>1. **高效端到端框架**：提出结合DDSP vocoder的轻量级端到端语音增强系统，适应嵌入式设备。  <br/>2. **紧凑神经网络预测特征**：通过小型化网络提取噪声语音的频谱包络、F0和周期性等关键声学特征。  <br/>3. **混合损失训练策略**：采用STFT与对抗损失联合优化，实现特征与波形层面的直接提升。  <br/>4. **低计算成本与高效果平衡**：在不显著增加计算量的前提下，提升可懂度（+4% STOI）和质量（+19% DNSMOS）。  <br/>5. **实际应用验证**：证明方法适合实时场景，为可穿戴设备中的语音增强提供可行性方案。  <br/><br/>**摘要总结（100字内）：**  <br/>提出高效端到端语音增强框架，结合紧凑神经网络与DDSP vocoder，通过STFT和对抗损失优化，在保持低计算成本的同时显著提升语音可懂度与质量，适用于可穿戴设备的实时应用。|
|2508.14623v1|[A Study of the Scale Invariant Signal to Distortion Ratio in Speech   Separation with Noisy References](http://arxiv.org/abs/2508.14623v1)|**贡献点总结（100字以内）：**  <br/>本研究提出通过WHAM!增强参考信号和混合数据，解决监督语音分离中噪声参考对SI-SDR的限制问题，揭示SI-SDR与主观感知噪声的负相关，验证了数据预处理对提升分离质量的关键作用。  <br/><br/>**分点贡献：**  <br/>1. **理论分析**：推导SI-SDR在噪声参考下的性能限制，指出噪声会导致分离结果的失真或降低SI-SDR值。  <br/>2. **方法创新**：提出基于WHAM!的参考信号增强与混合数据噪声增强策略，旨在避免模型学习噪声参考。  <br/>3. **实验验证**：在WSJ0-2Mix和Libri2Mix数据集上验证方法效果，发现分离语音噪声减少，但处理可能引入伪影，影响整体质量。  <br/>4. **指标关联性**：揭示SI-SDR与感知噪声间的负相关关系，强调SI-SDR作为质量评估指标的局限性。|
|2508.14525v1|[EffiFusion-GAN: Efficient Fusion Generative Adversarial Network for   Speech Enhancement](http://arxiv.org/abs/2508.14525v1)|总结：  <br/>EffiFusion-GAN通过轻量化网络结构、增强注意力机制和动态剪枝技术，在语音增强任务中实现高效性能，显著优于同参数量模型。<br/><br/>贡献点：  <br/>1. **轻量化网络设计**：采用深度可分离卷积与多尺度块结合，高效提取多样化的声学特征。  <br/>2. **改进注意力机制**：引入双归一化与残差细化模块，提升模型训练稳定性和收敛效率。  <br/>3. **动态剪枝技术**：通过动态剪枝降低模型规模，保持性能的同时适配资源受限环境。  <br/>4. **性能验证**：在VoiceBank+DEMAND数据集上取得3.45 PESQ得分，验证了模型在相同参数设置下的优越性。|
|2508.13624v2|[Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement](http://arxiv.org/abs/2508.13624v2)|**贡献点：**  <br/>1. **提出AVSEMamba模型**：首次将全脸视觉线索与Mamba-based时间主干结合，解决多说话者语音增强难题。  <br/>2. **增强时空感知能力**：通过融合视觉和音频的时空信息，提升复杂环境（如鸡尾酒会问题）下目标语音的提取准确性。  <br/>3. **实验验证优越性**：在AVSEC-4数据集上超越单通道基线模型，在STOI、PESQ和UTMOS指标上均取得最优表现，获得单通道榜单第一名。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出AVSEMamba，融合全脸视觉与Mamba时间主干，解决多说话者语音增强问题。在复杂场景下显著提升语音提取效果，并在AVSEC-4数据集上取得单通道榜单首位。|
|2508.13624v1|[Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement](http://arxiv.org/abs/2508.13624v1)|**贡献点：**  <br/>1. **提出AVSEMamba模型**：首次将全脸视觉信息与Mamba时序架构结合，解决多说话人语音增强问题（如鸡尾酒会场景）。  <br/>2. **性能突破**：在AVSEC-4挑战的开发与盲测数据集上，显著提升语音可懂度（STOI）、感知质量（PESQ）和非侵入式质量（UTMOS）指标。  <br/>3. **榜单领先**：在单耳模型排行榜上取得第一名，验证了其在复杂环境下的优越性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出AVSEMamba，融合音频与全脸视觉信息，解决多说话人语音增强难题，性能优于现有单耳基线，并在排行榜上夺冠。|
|2508.13576v1|[End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in   Noisy Environments](http://arxiv.org/abs/2508.13576v1)|总结：  <br/>该研究提出了一种结合音频-视觉语音增强与深度学习的新型助听器系统AVSE-ECS，通过端到端联合训练显著提升嘈杂环境下的语音可懂度，验证了多模态数据整合在CI技术中的有效性。<br/><br/>贡献点：  <br/>1. **提出新型CI系统**：设计AVSE-ECS系统，将音频-视觉语音增强（AVSE）作为预处理模块与深度学习的ElectrodeNet-CS策略结合。  <br/>2. **端到端联合训练方法**：首次采用联合训练框架实现AVSE与深度学习模型的协同优化，提升系统整体性能。  <br/>3. **视觉信息整合**：创新性地引入视觉辅助数据，增强多模态语音处理在嘈杂环境中的鲁棒性。  <br/>4. **实验验证有效性**：在噪声条件下验证了该方法相较于传统ECS策略的优越性，指标提升显著。|
|2508.13028v1|[Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic   Speech Synthesis](http://arxiv.org/abs/2508.13028v1)|总结：  <br/>本研究提出结合讽刺检测反馈损失与两阶段迁移微调方法，提升TTS生成讽刺语音的能力，显著改善质量、自然度与讽刺传达效果。<br/><br/>贡献点：  <br/>1. **引入双模讽刺检测反馈机制**：将语音-文本双模讽刺检测模型的反馈损失融入TTS训练，提升对讽刺语调特征的建模能力。  <br/>2. **两段式迁移学习策略**：  <br/>   - 第一阶段：以多样化语音风格数据集（含讽刺语）进行预训练模型微调，增强通用表达能力；  <br/>   - 第二阶段：以专门讽刺语音数据集进一步优化，提升讽刺语音生成的针对性。  <br/>3. **系统性改进合成效果**：通过客观与主观评估验证，显著提升合成语音的自然度、质量及讽刺意识表达水平。|
|2508.12009v1|[Optimizing Neural Architectures for Hindi Speech Separation and   Enhancement in Noisy Environments](http://arxiv.org/abs/2508.12009v1)|**贡献点总结（100字以内）:**  <br/>本文提出适用于边缘设备的Hindi语音分离与增强方案，改进DEMUCS模型结合U-Net和LSTM，利用大规模数据集与数据增强技术提升性能，通过量化技术优化计算效率，并强调定制AI算法在印度语境中的应用价值。  <br/><br/>**分点贡献:**  <br/>1. **模型改进**：基于DEMUCS提出优化方法，提升Hindi语音清晰度与可懂度，解决传统技术局限性。  <br/>2. **结构融合**：引入U-Net和LSTM层对模型微调，增强对多变声学环境的适应性。  <br/>3. **数据增强**：构建包含40万Hindi语音剪辑的数据库，并结合ESC-50和MS-SNSD进行多样化环境训练。  <br/>4. **部署优化**：探索量化技术，降低计算需求以适配资源受限设备（如TWS耳机）。  <br/>5. **应用与展望**：验证定制AI算法在印度语音处理领域的有效性，提出边缘架构优化方向。|
|2508.10830v1|[Advances in Speech Separation: Techniques, Challenges, and Future Trends](http://arxiv.org/abs/2508.10830v1)|**贡献点：**  <br/>1. **全面视角**：系统分析DNN语音分离的学习范式、场景（已知/未知说话人）、监督/自监督/无监督框架对比及架构组件（编码器至估计策略）。  <br/>2. **及时性**：覆盖当前前沿技术及基准，确保读者获取最新创新与成果。  <br/>3. **独特见解**：深入探讨技术发展路径，识别新兴趋势，提出领域鲁棒、高效架构、多模态整合等未来方向。  <br/>4. **公正评估**：基于标准数据集进行定量分析，客观揭示不同方法的能力与局限性。  <br/><br/>**总结（100字以内）：**  <br/>本综述系统梳理DNN语音分离技术，涵盖学习范式、场景对比与架构分析，全面总结前沿成果并揭示技术趋势，为研究者提供权威、公正的评估与未来方向参考。|
|2508.10436v1|[Alternating Approach-Putt Models for Multi-Stage Speech Enhancement](http://arxiv.org/abs/2508.10436v1)|**贡献点总结：**  <br/>1. 提出PuttNet作为语音增强后处理框架，针对性消除增强模型引入的artifacts。  <br/>2. 首次建立语音增强模型与PuttNet的交替应用机制，显著提升PESQ、STOI、CBAK等客观质量指标。  <br/>3. 通过可视化分析直观解释交替策略的优势，揭示其优于单模型重复应用的理论依据。|
|2508.08468v1|[Audio-Visual Speech Enhancement: Architectural Design and Deployment   Strategies](http://arxiv.org/abs/2508.08468v1)|总结：  <br/>提出AI驱动的多模态语音增强系统，对比分析云、边缘辅助与独立设备三种部署架构，揭示其在延迟、计算开销与语音质量的平衡特性，为实际应用提供部署优化指南。  <br/><br/>贡献点：  <br/>1. **提出新型AVSE系统架构**：结合CNN（谱特征提取）与LSTM（时序建模），实现音频与视觉信息的多模态融合，提升语音增强效果。  <br/>2. **系统化部署场景对比**：分析云、边缘辅助和独立设备三种部署方式在语音质量、延迟、计算开销等方面的性能差异。  <br/>3. **实测跨网络环境分析**：在Ethernet、Wi-Fi 4/5、5G等实际网络条件下验证系统表现，量化处理延迟与通信延迟的权衡关系。  <br/>4. **提出实用部署指导**：明确边缘辅助架构在5G/Wi-Fi 6下可兼顾低延迟与语音可懂度，为助听设备、远程通信等场景提供选型依据。|
|2508.07563v1|[Exploring Efficient Directional and Distance Cues for Regional Speech   Separation](http://arxiv.org/abs/2508.07563v1)|贡献点总结（100字以内）:  <br/>本研究提出基于麦克风阵列的神经网络区域语音分离方法，创新性融合空间线索与直接-混响比率特征，改进延迟-求和技术提升方向识别，实现目标方向与距离的精准分离，在CHiME-8数据集上取得SOTA性能，并验证了其在真实场景的应用价值。<br/><br/>分点贡献:  <br/>1. **方法创新**：提出结合神经网络与麦克风阵列的区域语音分离框架，引入新型空间线索提取机制。  <br/>2. **技术改进**：改进传统延迟-求和技术，增强目标方向信号分离能力，同时融合直接-混响比率（D/R ratio）作为输入特征。  <br/>3. **距离判别**：通过D/R比率区分声源的远近，提升模型对距离信息的感知与分离效果。  <br/>4. **实验验证**：在真实场景数据集（CHiME-8）上实现最先进性能，验证方法在实际应用的有效性。|
|2508.07558v1|[UniFlow: Unifying Speech Front-End Tasks via Continuous Generative   Modeling](http://arxiv.org/abs/2508.07558v1)|**贡献点总结（100字以内）**  <br/>提出UniFlow统一框架，整合VAE与DiT处理多语音前端任务，通过条件嵌入实现参数共享与任务适配，比较三种生成目标优化性能，开源代码促进研究。<br/><br/>**详细贡献点**  <br/>1. **统一框架设计**：首次构建基于连续生成模型的统一框架UniFlow，整合多语音前端任务（如语音增强、目标说话人提取等）于共享潜在空间，减少冗余工程。  <br/>2. **混合生成模型架构**：将波形变分自编码器（VAE）与扩散变压器（DiT）结合，通过VAE学习紧凑潜在表示，DiT预测潜在更新，提升生成效率与质量。  <br/>3. **条件嵌入机制**：引入可学习条件嵌入（通过任务ID索引），实现模型参数最大化共享的同时保留任务特定的适应性，增强灵活性。  <br/>4. **生成目标对比研究**：系统性比较噪声消除扩散、流匹配和潜在域平均流三种生成目标，探索其在计算效率与性能间的平衡，提出优化策略。  <br/>5. **广泛基准验证**：在多个公开语音数据集上验证UniFlow，证明其在性能上优于现有最佳方法（SOTA基线），并展示良好的可扩展性。  <br/>6. **开源促进研究**：公开代码库，推动生成式语音处理技术的进一步发展与应用。|
|2508.07219v1|[ParaNoise-SV: Integrated Approach for Noise-Robust Speaker Verification   with Parallel Joint Learning of Speech Enhancement and Noise Extraction](http://arxiv.org/abs/2508.07219v1)|总结：  <br/>提出ParaNoise-SV模型，通过显式建模噪声和双U-Net结构设计，有效提升说话人验证的抗噪性能，实验显示EER降低8.4%。<br/><br/>贡献点：  <br/>1. **显式建模噪声**：首次将噪声提取（NE）与语音增强（SE）结合，替代传统隐式噪声抑制方法，更精确分离噪音与说话人特征。  <br/>2. **双U-Net架构**：引入噪声提取U-Net与语音增强U-Net的协同机制，通过并行连接实现噪声与语音的联合优化。  <br/>3. **特征保留策略**：在语音增强过程中利用噪声提取结果进行引导，有效保留学说话人相关特征，避免信息损失。  <br/>4. **性能提升验证**：在基准数据集上实现显著效果提升，相较现有联合SE-SV方法将EER降低8.4%。|
|2508.06842v3|[Speech Enhancement based on cascaded two flows](http://arxiv.org/abs/2508.06842v3)|贡献点总结（100字以内）:  <br/>提出通过单一flow matching模型实现语音增强及初始值生成，减少NFE需求并保持性能优势，有效简化流程，提升计算效率。|
|2508.06842v1|[Speech Enhancement based on cascaded two flow](http://arxiv.org/abs/2508.06842v1)|总结：  <br/>该论文提出基于流匹配的语音增强方法，通过统一模型结构降低计算需求，实现与基线相当或更优性能，无需额外预测模型。<br/><br/>贡献点：<br/>1. **方法创新**：首次将流匹配模型用于语音增强（SE）的两个关键步骤（均值漂移生成与初始值采样），实现统一模型架构。<br/>2. **计算效率提升**：在保持或超越传统基线性能的前提下，降低函数评估次数（NFE），减少计算开销。<br/>3. **消融额外模型**：无需依赖独立的预测模型进行初始语音增强，简化系统架构并避免额外训练成本。<br/>4. **实验验证**：通过对比实验证明，所提方法在单个或两个级联生成模型中均可实现高效性能，验证了其有效性。|
|2508.06840v1|[FlowSE: Flow Matching-based Speech Enhancement](http://arxiv.org/abs/2508.06840v1)|总结：  <br/>本文提出基于条件流匹配的语音增强方法，显著减少推理所需的函数评估次数（NFE），在性能上媲美传统扩散模型，且无需额外微调或优化传输条件向量场即可实现高效处理。<br/><br/>贡献点：  <br/>1. **引入条件流匹配框架**：首次将流匹配技术应用于语音增强任务，通过建模条件概率路径实现高效的生成过程。  <br/>2. **降低计算复杂度**：在保持性能的前提下，将NFE从60降至5，有效减少计算开销。  <br/>3. **无需额外微调**：相比于传统扩散模型需微调修正反向过程，所提方法无需额外细调即可实现相似性能。  <br/>4. **优化扩散模型结构**：基于修改后的最优传输条件向量场，构建扩散模型在NFE=5时仍保持良好效果。|
|2508.06393v1|[Robust Target Speaker Diarization and Separation via Augmented Speaker   Embedding Sampling](http://arxiv.org/abs/2508.06393v1)|总结：  <br/>本研究提出无需注册的语音分离与说话人辨识方法，通过双阶段训练和重叠频谱损失函数提升鲁棒性和准确性，实验验证在DER和cpWER指标上均优于现有SOTA基线。<br/><br/>贡献点：  <br/>1. **提出无需注册的方法**：首次无需依赖目标说话人先验信息或固定参与人数，直接通过自动识别目标说话人嵌入实现语音分离与说话人辨识。  <br/>2. **设计双阶段训练框架**：引入分阶段的训练流程，学习对背景噪声具有鲁棒性的说话人表征特征。  <br/>3. **开发重叠频谱损失函数**：针对重叠语音帧场景，优化损失函数以提升说话人辨识的准确性。  <br/>4. **实验证明性能提升**：在混合语音数据上验证，实现71% DER和69% cpWER的相对性能提升，超越当前SOTA基线。|
|2508.06310v1|[Egonoise Resilient Source Localization and Speech Enhancement for Drones   Using a Hybrid Model and Learning-Based Approach](http://arxiv.org/abs/2508.06310v1)|贡献点总结：  <br/>1. 提出混合方法：结合阵列信号处理（ASP）与深度神经网络（DNN），解决无人机麦克风低信噪比（SNR）下的自噪声抑制问题。  <br/>2. 设计麦克风阵列：采用六麦克风均匀圆形阵列与波束成形技术，提升目标语音的定位与增强效果。  <br/>3. 优化语音增强模型：引入改进型GSC-DF2系统，实现更高效的噪声消除与语音质量提升。  <br/>4. 验证效果显著：通过DREGON数据集与实测数据，证明该方法在-30 dB极端SNR条件下优于四种基线技术。|
|2508.05293v1|[Investigation of Speech and Noise Latent Representations in   Single-channel VAE-based Speech Enhancement](http://arxiv.org/abs/2508.05293v1)|总结：  <br/>本研究提出基于贝叶斯排列训练的VAE框架，通过分离语音和噪声的潜在表示提升单通道语音增强性能，验证了显式区分潜在空间对增强效果的显著优势。<br/><br/>贡献点：  <br/>1. **提出贝叶斯排列训练与VAE结合的新框架**：首次将贝叶斯排列训练应用于单通道语音增强，利用两个预训练VAE分别建模语音和噪声的潜在表示。  <br/>2. **显式分离潜在空间**：通过修改预训练VAE损失函数，实现语音和噪声潜在表示的分离，解决传统VAE中潜在空间重叠的问题。  <br/>3. **验证分离表示的有效性**：在DNS3、WSJ0-QUT和VoiceBank-DEMAND数据集上，实验证明分离潜在空间显著优于标准VAE的语音增强效果。|