|Source|Title|Summary|
|---|---|---|
|2507.06179v1|[Dynamic Slimmable Networks for Efficient Speech Separation](http://arxiv.org/abs/2507.06179v1)|总结：  <br/>该论文提出动态可剪枝网络（DSN）在语音分离中实现性能与效率平衡，通过信号感知的动态结构调整和轻量门控模块优化资源使用，并引入信号依赖复杂度损失提升模型适应性。<br/><br/>贡献点：  <br/>1. **提出动态可剪枝网络（DSN）架构**：首次设计可自适应调整计算复杂度的网络，根据输入信号特性动态变化网络宽度，突破传统固定结构限制。  <br/>2. **引入轻量级门控模块**：通过分析局部输入特征动态决策网络宽度，显著降低冗余计算，提升实时性与资源利用率。  <br/>3. **创新信号依赖复杂度损失**：基于段落重建误差设计损失函数，针对性抑制非必要计算，优化性能-效率权衡。  <br/>4. **验证有效性**：在WSJ0-2mix和WHAM!数据集的洁净/噪声场景下，证明DSN优于多组静态网络，实现更优的分离效果与计算效率。|
|2507.06116v1|[Speech Quality Assessment Model Based on Mixture of Experts:   System-Level Performance Enhancement and Utterance-Level Challenge Analysis](http://arxiv.org/abs/2507.06116v1)|总结：  <br/>该论文提出基于自监督学习和MoE架构的增强MOS预测系统，构建大规模合成数据集，揭示句子级评估的局限性，为语音质量评估提供新方法和技术路径。<br/><br/>贡献点：  <br/>1. **提出增强MOS预测系统**：结合自监督学习模型（如wav2vec2）与Mixture of Experts（MoE）分类头，优化语音质量评估性能。  <br/>2. **设计专用MoE架构**：针对不同类型的语音质量任务，开发适配的MoE结构以提升多任务处理能力。  <br/>3. **构建多模态合成数据集**：整合最新TTS、语音转换及语音增强系统的合成数据，扩展训练样本多样性。  <br/>4. **揭示性能差异根源**：分析不同评估粒度（如句子级）下模型表现差异的根本原因，指出现有方法的局限性。|
|2507.05688v1|[Robust One-step Speech Enhancement via Consistency Distillation](http://arxiv.org/abs/2507.05688v1)|**贡献点总结（100字以内）**:  <br/>提出ROSE-CD，通过随机学习轨迹与双时域辅助损失提升噪声鲁棒性与性能，首次实现纯单步一致性蒸馏，显著加速推理（54倍）并达到SOTA，验证其在合成与真实场景的泛化能力。<br/><br/>**分点贡献（详细）**:  <br/>1. **首次纯单步一致性蒸馏模型**：ROSE-CD是首个完全基于单步采样的扩散语音增强一致性蒸馏框架，突破传统多步迭代限制，实现高效实时处理。  <br/>2. **增强噪声鲁棒性**：引入**随机化学习轨迹**，使模型在噪声环境下表现更稳健，减少对教师模型采样路径的依赖。  <br/>3. **联合优化辅助损失**：结合**两个时域辅助损失**（如增强损失和掩码损失），有效修正教师模型的误差并提升整体性能。  <br/>4. **显著性能提升**：在VoiceBank-DEMAND数据集上达到SOTA，且比30步教师模型快54倍；同时验证了对跨域数据和真实噪声的泛化能力。|
|2507.04879v1|[Adaptive Slimming for Scalable and Efficient Speech Enhancement](http://arxiv.org/abs/2507.04879v1)|**贡献点总结：**<br/>1. 提出动态修剪技术，使DEMUCS语音增强模型实现可扩展的输入自适应，无需额外存储成本即可模拟多种模型规模。<br/>2. 设计端到端训练的路由子网络，根据输入动态选择最优利用率因子（UF），实现资源自适应优化。<br/>3. 实验证明：在10%平均计算能力下，语音质量与静态25%利用率模型相当甚至更优，MACs减少29%，显著提升计算效率。<br/><br/>**摘要总结（100字以内）：**  <br/>该研究提出动态剪枝方法，使DEMUCS语音增强模型适应不同计算需求，通过路由子网络智能选择利用率因子，在保持语音质量的同时降低计算量29%，显著提升资源受限场景下的效率与性能。|
|2507.04368v1|[Long-Context Modeling Networks for Monaural Speech Enhancement: A   Comparative Study](http://arxiv.org/abs/2507.04368v1)|**贡献点：**  <br/>1. 提出统一的语音增强框架，系统比较Transformer、Conformer、Mamba和xLSTM等长时序建模网络的性能。  <br/>2. 首次评估xLSTM在语音增强任务中的有效性，拓展其应用场景。  <br/>3. 验证Mamba在长语音输入中显著优于Transformer和Conformer的训练与推理效率。  <br/>4. 探究因果与非因果配置对模型表现的影响，为实际部署提供参考。  <br/><br/>**总结（100字内）：**  <br/>本文构建统一框架，系统比较多类长时序模型在语音增强中的性能，发现Mamba和xLSTM优于传统模型，并分析其效率差异，为高效语音处理提供新思路。|
|2507.02791v2|[Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient   Extraction of Moving Speakers under Weak Guidance](http://arxiv.org/abs/2507.02791v2)|1. **提出低复杂度替代方案**：基于粒子滤波的轻量级跟踪算法，替代传统资源密集型数据驱动方法，解决动态场景下计算成本过高的问题。  <br/>2. **引入时序反馈机制**：通过因果顺序处理框架，利用空间滤波器输出的增强信号补偿粒子滤波器的建模不足，提升动态适应性。  <br/>3. **验证算法协同效果**：在合成数据集中证明，空间滤波器与粒子滤波器的自回归交互显著改善跟踪精度和增强性能。  <br/>4. **实验证明实用性**：通过真实场景录音的听测验证，表明所提自引导管道在实际应用中优于对比方法，具有更高偏好度。  <br/><br/>**总结**: 本文提出低复杂度粒子滤波跟踪方法，结合时序反馈优化动态场景下的语音增强性能，实验验证其优于现有技术。|
|2507.02791v1|[Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient   Extraction of Moving Speakers under Weak Guidance](http://arxiv.org/abs/2507.02791v1)|**贡献点：**  <br/>1. 提出低复杂度的粒子滤波追踪算法替代传统资源密集型方法，降低动态场景下的计算开销。  <br/>2. 引入时序反馈机制，利用空间选择滤波器的增强信号补偿粒子滤波的建模局限。  <br/>3. 首次在合成数据集上验证自回归交互对动态语音增强性能的提升作用。  <br/>4. 通过实际录音的听力测试证明所提自引导框架在动态场景中的优越性。  <br/><br/>**总结（100字内）：**  <br/>本文提出低复杂度粒子滤波与空间选择滤波器结合的自回归框架，降低动态场景计算开销并提升增强性能，通过合成与实际测试验证其有效性，优于现有方法。|
|2507.02562v1|[Multi-Utterance Speech Separation and Association Trained on Short   Segments](http://arxiv.org/abs/2507.02562v1)|**贡献点：**  <br/>1. 提出频率-时序递归神经网络（FTRNN），结合全带模块与子带模块分别建模频率依赖性和时序模式。  <br/>2. 实现跨长音频（21-121秒）的稳健语音分离，超出训练数据长度限制。  <br/>3. 保持说话人关联性，解决长音频中utterance间隙的语义断裂问题。  <br/>4. 设计轻量级架构（0.9M参数），无需分段处理即可完成长音频推理，消除边界失真。  <br/>5. 验证FTRNN在多utterance场景下的泛化能力，优于传统分段分离拼接方法。  <br/><br/>**总结（100字内）：**  <br/>本文提出FTRNN模型，通过全带与子带模块协同处理频时信息，实现长音频的稳健语音分离与说话人关联，无需分段操作。模型轻量化且避免边界失真，实验验证其在多utterance场景下的优越泛化性能。|
|2507.02391v1|[Posterior Transition Modeling for Unsupervised Diffusion-Based Speech   Enhancement](http://arxiv.org/abs/2507.02391v1)|总结：  <br/>本文提出两种无监督语音增强方法，通过扩散模型建模条件逆转移分布，消除超参数依赖并实现精确似然计算，实验验证了其在增强性能和领域适应性上的优势。<br/><br/>贡献点：  <br/>1. **提出新的无监督方法**：利用扩散模型作为生成先验，直接建模条件逆扩散过程的转移分布，而非依赖近似噪声扰动的似然分数与无条件分数的组合。  <br/>2. **方法一：联合建模优化**：将扩散先验与观测模型原理性整合，无需超参数调优，提升模型的自动性和鲁棒性。  <br/>3. **方法二：端到端扩散建模**：在噪声语音本身上定义扩散过程，实现完全可计算且精确的条件 likelihood score，提升生成质量。  <br/>4. **实验验证优势**：在 WSJ0-QUT 和 VoiceBank-DEMAND 数据集上，方法在增强指标和跨领域适应性上均优于监督及无监督基线。|
|2507.02192v1|[An Investigation on Combining Geometry and Consistency Constraints into   Phase Estimation for Speech Enhancement](http://arxiv.org/abs/2507.02192v1)|总结（100字以内）:  <br/>提出MSGLA算法，利用STFT一致性约束解决相位符号模糊问题，结合正弦/余弦定律构建几何约束框架，实验验证其在背景噪声抑制方面优于现有方法。<br/><br/>贡献点分点：  <br/>1. **提出多源Griffin-Lim算法（MSGLA）**：首次将多源信息整合到相位估计框架中，用于加性噪声环境下的语音增强。  <br/>2. **解决符号模糊问题**：通过复值STFT谱图的ad-hoc一致性约束，克服几何相位估计中的相位符号不确定性挑战。  <br/>3. **创新几何约束方法**：基于正弦和余弦定律设计新型几何约束框架，提出结合噪声相位估计的相位重建算法。  <br/>4. **实验证明有效性**：在理想条件和真实数据集（VB-DMD、WSJ0-CHiME3）上验证，显示MSGLA在背景噪声抑制方面表现优异或优于现有方法（如直接相位估计、DNN预测）。|
|2507.00966v1|[MambAttention: Mamba with Multi-Head Attention for Generalizable   Single-Channel Speech Enhancement](http://arxiv.org/abs/2507.00966v1)|贡献点总结（100字以内）:  <br/>本文提出MambAttention混合架构，融合Mamba与时频多头注意力模块，设计更具挑战性的VB-DemandEx数据集，并验证其在领域外语音增强任务中的优越性能，同时揭示权重共享对模型泛化的关键作用。|
|2506.23874v1|[URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech   Enhancement Competition](http://arxiv.org/abs/2506.23874v1)|**贡献点总结**：  <br/>1. 提出URGENT-PK，一种基于成对比较的语音质量系统级排名方法，无需绝对得分评估。  <br/>2. 创新性地利用系统间的所有成对排列作为训练实例，有效提升有限数据下的模型性能。  <br/>3. 在多个开放测试集上验证，表现优于现有深度学习基线模型（如DNSMOS、UTMOS）。  <br/>4. 采用简单网络架构，证明高效率和低数据需求下的优异性能，推动语音增强系统比较的实用性。|
|2506.23859v1|[Less is More: Data Curation Matters in Scaling Speech Enhancement](http://arxiv.org/abs/2506.23859v1)|总结：  <br/>本文挑战传统数据量与性能正相关观点，揭示语音增强中“干净”标签质量问题，提出质量优先于数量的数据选择策略，并通过实验验证精挑细选的小数据集优于大规模低质量数据集，强调数据整理对系统扩展的关键作用。<br/><br/>贡献点：  <br/>1. **挑战传统观点**：首次系统论证语音增强领域中数据量扩展的边际效益递减现象，推翻"数据越大效果越好"的假设。  <br/>2. **识别标签质量问题**：深入分析大规模数据集中"clean"训练标签存在的系统性缺陷，揭示其对模型训练的负面影响。  <br/>3. **提出质量优先策略**：创新性地主张选择高质量小数据集优于单纯扩大数据量，建立新的数据选择范式。  <br/>4. **实验证明有效性**：通过对比实验（700h vs 2500h）量化验证数据质量的重要性，为领域实践提供实证依据。  <br/>5. **强调数据整理价值**：突出数据筛选在提升模型性能和推动技术发展的核心作用，指导未来数据集构建方向。|
|2506.22321v1|[A Practical Approach to Power Saving in Hearables Using Sub-Nyquist   Sampling with Bandwidth Extension](http://arxiv.org/abs/2506.22321v1)|**贡献点总结（100字以内）：**  <br/>提出SUBARU方法，解决低功耗多模态语音增强中的采样率、GAN质量及信号重建问题，实现显著功耗降低与高效流式处理。<br/><br/>**分点贡献：**  <br/>1. **低功耗设计**：采用子奈奎斯特采样和低位分辨率ADC，实现3.31倍的功耗降低。  <br/>2. **无对抗训练的GAN质量提升**：引入多尺度、多周期虚拟判别器，无需实际GAN判别器即可生成GAN-like音频质量。  <br/>3. **实时噪声环境处理**：基于窄带部分的宽带重建方法，支持子奈奎斯特采样下的流式处理，推理时间1.74ms，内存占用<13.77MB。|
|2506.22001v1|[WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with   Spatial Cues Peservation](http://arxiv.org/abs/2506.22001v1)|总结：  <br/>本文提出WTFormer网络，结合小波变换与多维协作注意力机制，实现多通道语音增强中的空间-时间信号保护，并通过多任务损失策略优化，以更少参数取得与先进系统相当的降噪效果。<br/><br/>贡献点：  <br/>1. 提出WTFormer架构，融合波let变换的多分辨率特性与多维协作注意力机制，有效捕捉全局空间特征。  <br/>2. 引入Conformer模型进行时频建模，优化时间信息处理能力。  <br/>3. 设计多任务损失策略结合MUSIC算法，最大保护空间信息完整性。  <br/>4. 在LibriSpeech数据集上验证，仅需0.98M参数即可实现与先进系统相当的降噪性能并保留更多空间信息。|
|2506.19398v1|[ClearerVoice-Studio: Bridging Advanced Speech Processing Research and   Practical Deployment](http://arxiv.org/abs/2506.19398v1)|总结：  <br/>本研究提出ClearerVoice-Studio，作为连接语音研究与应用的开源工具包，专注于语音增强、分离等多任务处理，整合了高使用量预训练模型与实用工具，推动学术与工业应用。<br/><br/>贡献点：  <br/>1. **多任务整合**：专精语音增强、分离、超分辨率及多模态目标说话人提取等关联任务，形成系统性处理框架。  <br/>2. **高使用量预训练模型**：集成FRCRN（300万次使用）和MossFormer（250万次使用），优化实际场景应用效果。  <br/>3. **功能完备性**：提供模型优化工具、多格式音频支持、SpeechScore评估体系及用户友好界面，覆盖研究、开发与用户需求。  <br/>4. **社区影响**：开源后迅速获得3000星、239次fork，体现学术与工业的广泛应用价值。  <br/>5. **全面文档披露**：公开能力细节、架构设计、训练策略、基准测试及未来规划，促进技术共享与协作。|
|2506.18714v1|[Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech   Enhancement](http://arxiv.org/abs/2506.18714v1)|**贡献点列表：**  <br/>1. **提出感知引导的SDR损失函数框架**：通过时频域建模，引入频率依赖的加权机制，强调语音信号显著或干扰噪声强烈的频谱区域。  <br/>2. **设计多类频率加权策略**：包括固定策略（如ANSI带重要性权重）、基于频谱幅度的权重和自适应动态加权（根据语音与噪声比例调整）。  <br/>3. **验证模型有效性**：将所提损失函数应用于多通道语音增强模型FaSNet，实验表明感知加权指标（如SDR）显著优于传统方法。  <br/>4. **揭示语音特征提升机制**：通过频谱与音素级分析，证明方法在辅音重建和细粒度频谱线索保留方面的效果，改善语音可懂度。  <br/><br/>**总结（100字以内）**：  <br/>本文提出感知引导的SDR损失函数，结合多种频率加权策略，显著提升多通道语音增强模型的语音可懂度，尤其在辅音重建和频谱线索保留方面表现突出。|
|2506.18691v1|[Evaluating Multichannel Speech Enhancement Algorithms at the Phoneme   Scale Across Genders](http://arxiv.org/abs/2506.18691v1)|**贡献点总结（100字以内）:**  <br/>该论文揭示性别和音素类别对多通道语音增强性能的影响差异，提出音素-性别特定的频谱特征分析方法，并通过实验验证女性语音在减少干扰和伪影、提升感知与识别质量方面具有更优表现。<br/><br/>---<br/><br/>**分点贡献:**  <br/>1. **引入性别与语音内容的差异化分析**：首次系统探讨性别（男/女）及不同音素类别（塞音、擦音、元音等）对多通道语音增强算法性能的影响，突破传统以句子为单位的评估局限。  <br/>2. **提出音素-性别特定的频谱特征**：明确区分音素和性别相关的声学特征，为个性化语音增强提供理论依据。  <br/>3. **发现音素层级的显著性能差异**：实验表明，尽管性别间的句子级性能差异较小，但音素级存在显著差异，强调需细化评估粒度。  <br/>4. **验证女性语音的优化效果**：证明女性语音在干扰抑制（尤其在复杂音素）和感知质量（如语音识别指标）上表现更优，具有实际应用价值。  <br/>5. **推动算法性能评估方法升级**：为语音增强领域的性别和语音内容敏感性研究提供新的视角，促进更精准的算法设计与优化。|
|2506.16231v1|[EDNet: A Distortion-Agnostic Speech Enhancement Framework with Gating   Mamba Mechanism and Phase Shift-Invariant Training](http://arxiv.org/abs/2506.16231v1)|**贡献点：**  <br/>1. 提出Erase and Draw Network (EDNet)框架，实现对多种干扰（噪声、混响、带宽限制）的通用性处理，无需预设任务或输入特性。  <br/>2. 引入可学习的Gating Mamba (GM)模块，动态融合掩码（抑制非语音成分）与映射（重构语音）策略，根据局部信号特征自适应选择操作模式。  <br/>3. 设计Phase Shift-Invariant Training (PSIT)训练策略，通过动态对齐提升相位估计精度，兼容标准损失函数。  <br/>4. 实验证明EDNet在去噪、去混响、带宽扩展及多干扰增强任务中表现优异，展示其架构灵活性和跨任务适应性。  <br/><br/>**总结：**  <br/>EDNet通过动态融合掩码与映射策略、兼容相位对齐的训练方法，实现对多种语音干扰的统一处理，显著提升跨任务的增强性能。|
|2506.15000v1|[A Comparative Evaluation of Deep Learning Models for Speech Enhancement   in Real-World Noisy Environments](http://arxiv.org/abs/2506.15000v1)|总结（100字以内）:  <br/>该研究系统比较了三种语音增强模型在不同数据集上的性能，量化了其在降噪、感知质量及说话人特征保持上的优劣，揭示了平衡三者的技术方案，为语音生物识别等应用提供了优化方向。<br/><br/>贡献点:  <br/>1. **多模型基准对比**：首次系统评估Wave-U-Net、CMGAN和U-Net在SpEAR、VPQAD、Clarkson等多数据集上的综合性能，填补了现有研究在模型比较方面的空白。  <br/>2. **性能指标量化分析**：明确量化各模型在噪声抑制（SNR）、感知质量（PESQ）及说话人特征保留（VeriSpeak）指标上的表现差异，为技术选择提供数据支持。  <br/>3. **权衡优化研究**：揭示三种模型在噪声抑制、感知质量与说话人特征保持之间的权衡机制，提出优化方法以满足实际应用场景需求。  <br/>4. **应用导向结论**：基于评估结果，指出CMGAN适合对语音自然度要求高的场景，U-Net在降噪表现突出，Wave-U-Net平衡说话人特征保留，为语音生物识别、司法音频分析等应用提供参考。|
|2506.14204v1|[Improving Practical Aspects of End-to-End Multi-Talker Speech   Recognition for Online and Offline Scenarios](http://arxiv.org/abs/2506.14204v1)|**贡献点：**  <br/>1. **SOT框架扩展**：结合连续语音分离（CSS）单通道前端与端到端（E2E）系统，提升多说话人重叠场景下的ASR准确率，挑战传统E2E与级联框架的对比认知。  <br/>2. **双模型架构**：提出流式场景用Conformer Transducer、离线场景用Sequence-to-Sequence模型，或级联编码器的两遍模型，优化实时性与准确性平衡。  <br/>3. **segSOT方法**：开发基于段的SOT（segSOT），专为离线场景设计，同时增强多说话人转录的可读性。  <br/><br/>**总结（100字内）：**  <br/>该论文提出扩展SOT框架，整合CSS与双模型（流式/离线）及segSOT，兼顾实时与离线ASR的延迟与准确率，提升多说话人场景下的语音识别性能与转录可读性。|
|2506.13127v1|[I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency   Calibration for Speech Enhancement](http://arxiv.org/abs/2506.13127v1)|**贡献点总结（分点）**  <br/>1. **提出I²S-TFCKD框架**：结合时间-频率校准和知识蒸馏，优化SE模型复杂度压缩与性能平衡。  <br/>2. **双流交叉校准方法**：通过时域与频域独立相似性权重计算及跨加权，实现基于语音特性的多层蒸馏贡献分配。  <br/>3. **协同蒸馏范式**：构建同集与异集相关性处理机制，利用残差融合生成融合特征集，促进跨集知识交互。  <br/>4. **实证验证**：应用于冠军模型DPDCRN，显著提升低复杂度学生模型性能并超越其他蒸馏方案。  <br/><br/>**摘要总结（100字内）**  <br/>提出一种结合时间-频率校准和知识蒸馏的I²S-TFCKD框架，创新性地通过双流交叉校准与协同蒸馏机制，有效优化语音增强模型的复杂度与性能，验证在DPDCRN上取得显著效果。|
|2506.12260v1|[Improving Speech Enhancement with Multi-Metric Supervision from Learned   Quality Assessment](http://arxiv.org/abs/2506.12260v1)|总结（100字以内）：  <br/>提出利用多指标SQA模型作为监督信号指导SE训练的方法，解决传统目标与感知质量不一致的问题，并实现无参考数据下的训练，实验验证了其在多种质量指标上的有效性提升。  <br/><br/>贡献点：  <br/>1. **引入SQA模型作为SE训练的监督信号**：首次将SQA模型（用于多质量评估指标预测）应用于SE训练框架，突破传统方法仅依赖单一评价指标的局限。  <br/>2. **解决SE目标与感知质量的不匹配问题**：通过SQA模型对多种评价指标的统一预测，克服了传统目标（如SI-SNR）无法有效反映主观质量感知的缺陷。  <br/>3. **支持无参考数据的SE训练**：提出在缺乏干净语音数据的情况下，利用SQA模型对实际数据进行训练，拓展了SE训练的数据适用性。  <br/>4. **验证多指标性能提升效果**：在模拟与真实测试集上的实验表明，该方法能显著提高SE系统在多个质量评估指标（如PESQ、STOI）上的表现。|
|2506.11542v1|[Amplifying Artifacts with Speech Enhancement in Voice Anti-spoofing](http://arxiv.org/abs/2506.11542v1)|总结（100字以内）:  <br/>本研究提出模型无关的伪影增强方法，通过噪声添加、提取与放大步骤提升合成语音检测性能，在ASVspoof2019和2021数据集上分别提高44.44%和26.34%。<br/><br/>贡献点:  <br/>1. **提出模型无关的伪影增强框架**：不同于传统基于模型架构的改进方法，设计通用流程提升合成语音中伪影的可检测性。  <br/>2. **分阶段增强伪影存在性**：通过噪声添加、噪声提取与噪声放大三步骤系统性增强伪影特征，提升检测效果。  <br/>3. **兼容性设计**：支持多种语音增强模型与对抗措施架构，扩展了方法的应用范围。  <br/>4. **显著性能提升**：在ASVspoof2019和ASVspoof2021数据集上分别实现44.44%和26.34%的检测性能提升。|
|2506.11514v1|[Efficient Speech Enhancement via Embeddings from Pre-trained Generative   Audioencoders](http://arxiv.org/abs/2506.11514v1)|总结（100字以内）:  <br/>该论文提出一种基于生成式音频编码器的高效语音增强系统，通过紧凑编码器提升参数效率，并在语音质量和说话人保真度上超越现有方法，实验与主观测试均验证其优越性。<br/><br/>贡献点分点:  <br/>1. **提出生成式SE新范式**：首次将生成式音频编码器与紧凑编码器网络结合，替代传统时频掩码和信号预测方法，实现语音增强。  <br/>2. **参数效率优化**：通过消融实验验证使用预训练音频编码器和声码器的参数效率，提升模型轻量化程度。  <br/>3. **性能优势**：在语音增强任务和说话人保真度指标上，相比判别式音频编码器模型取得更好效果。  <br/>4. **感知质量验证**：通过主观听觉测试证明系统在感知质量上优于现有SOTA模型，增强实际应用价值。|
|2506.08457v1|[A Review on Score-based Generative Models for Audio Applications](http://arxiv.org/abs/2506.08457v1)|**贡献点总结（100字以内）:**  <br/>该论文系统梳理了音频扩散model的设计原则与实现方法，提出基于分数建模的统一框架，构建开源代码库并开展多场景应用与基准评估，为音频生成领域提供了可复现的研究基础。<br/><br/>**分点贡献:**  <br/>1. **系统性综述**  <br/>   - 首次对音频扩散模型的设计选择进行深度分析，聚焦质量提升与条件机制在音频任务中的关键作用，弥补现有文献的不足。<br/><br/>2. **统一框架提出**  <br/>   - 采用分数建模视角作为核心框架，整合多种扩散模型范式（如流匹配），为不同应用场景提供普适性指导。<br/><br/>3. **开源代码库建设**  <br/>   - 提供开源实现平台（https://github.com/gzhu06/AudioDiffuser），支持音频生成、语音增强和文本到语音等任务，推动可复现性研究。<br/><br/>4. **多应用案例验证**  <br/>   - 通过音频生成、语音增强和文本到语音合成的三个案例研究，验证框架的有效性与灵活性。<br/><br/>5. **基准评估体系**  <br/>   - 在标准数据集上进行系统性评估，为不同扩散模型设计选择提供定量比较依据，促进技术选型与优化研究。|
|2506.06697v1|[Exploring Length Generalization For Transformer-based Speech Enhancement](http://arxiv.org/abs/2506.06697v1)|总结：  <br/>本文提出LearnLin，一种通过单一参数优化注意力头长期依赖的位置编码方案，验证相对位置编码优于绝对编码，提升模型对长语音的泛化能力。<br/><br/>贡献点：  <br/>1. 提出一种新型位置编码方案LearnLin，仅用单个可训练参数针对每个注意力头进行优化  <br/>2. 通过系统对比实验验证相对位置编码在长度泛化上的有效性，优于传统绝对位置编码  <br/>3. 建立位置编码对语音增强长度泛化能力的直接影响机制，揭示其在长时序处理中的关键作用  <br/>4. 在四种训练目标和五项指标上验证模型性能，为长语音增强提供全面评估框架  <br/>5. 实证结果显示LearnLin在保持性能的同时显著提升长度泛化能力，优于现有SOTA方法|
|2506.06689v1|[A Fast and Lightweight Model for Causal Audio-Visual Speech Separation](http://arxiv.org/abs/2506.06689v1)|贡献点总结（100字以内）:  <br/>提出Swift-Net流式AVSS模型，解决复杂架构和依赖未来上下文的问题，结合轻量视觉模块、高效融合机制及Grouped SRUs历史信息整合，引入因果转换模板，实验证明其在实时场景下具备优异性能。<br/><br/>分点贡献：  <br/>1. **提出流式AVSS框架Swift-Net**：首次设计针对实时应用的流式模型，克服传统方法需离线处理、依赖未来帧的局限性。  <br/>2. **轻量化视觉特征提取与高效融合**：采用简化视觉模块和高效跨模态融合策略，降低计算复杂度并提升音频-视频信息整合效率。  <br/>3. **引入Grouped SRUs历史信息整合机制**：通过分组结构的SRUs（Shifted Recurrent Units）有效融合多特征空间的历史数据，增强时间上下文建模能力。  <br/>4. **设计因果转换模板**：提供将非因果模型转换为因果模型的方法，提高模型在实时场景下的适用性与通用性。|
|2506.02908v1|[Diffusion Buffer: Online Diffusion-based Speech Enhancement with   Sub-Second Latency](http://arxiv.org/abs/2506.02908v1)|**贡献点：**<br/>1. **滑动窗口扩散框架创新**：首次将滑动窗口机制引入扩散模型，实现对流式语音信号的在线增强处理。<br/>2. **时间依赖性噪声注入**：提出基于时间的噪声注入策略，对缓冲区中接近当前的帧赋予更多噪声，提升渐进式重建效果。<br/>3. **性能-延迟平衡设计**：通过调整缓冲区大小，实现输入输出延迟（0.3-1秒）与增强性能的可控权衡。<br/>4. **实证有效性验证**：在GPU上实现高效推理，实验结果表明该方法在语音增强任务上优于标准扩散模型，为实时应用提供可行方案。<br/><br/>**总结（100字内）**：  <br/>提出滑动窗口扩散模型，通过时间逐步降噪与缓冲区设计实现低延迟的在线语音增强，突破扩散模型实时性瓶颈，首次在语音领域实现高效实用的扩散方法。|
|2506.01611v1|[Lessons Learned from the URGENT 2024 Speech Enhancement Challenge](http://arxiv.org/abs/2506.01611v1)|总结：  <br/>该论文通过分析URGENT 2024挑战，揭示了语音增强系统中数据清洗和评估指标的两大关键问题，提出需解决带宽不匹配、标签噪声、恶劣条件处理能力不足等挑战，并倡导结合多维度指标以更贴近人类判断的综合评估方法。<br/><br/>贡献点：  <br/>1. 提出语音增强技术需具备更大通用性、鲁棒性和泛化能力，并引入更广泛的任务定义、大规模多领域数据集和全面的评估指标框架。  <br/>2. 深入分析数据清洗和评估指标两个长期被忽视的问题，揭示传统SE流程中的核心缺陷（如带宽不匹配、标签噪声）。  <br/>3. 强调需开发能应对极端条件（如语音重叠、强噪声/混响）的SE系统，并建立可靠的语音样本难度度量方法。  <br/>4. 主张结合多方面评估指标进行综合评价，以提升与人类主观判断的一致性。|
|2506.01460v1|[Few-step Adversarial Schrödinger Bridge for Generative Speech   Enhancement](http://arxiv.org/abs/2506.01460v1)|总结：本文提出将薛定谔桥与GAN结合的语音增强框架，显著减少采样步骤并提升低信噪比下的性能，实验证明其在去噪与消回声任务中优于现有方法。<br/><br/>贡献点：  <br/>1. **创新方法**：首次将可处理的Schödinger Bridge与生成对抗网络（GANs）结合，构建新型语音增强框架。  <br/>2. **效率提升**：通过改进模型设计，将传统需50+步的采样过程减少至单步推理，显著降低计算成本。  <br/>3. **性能优化**：在低信噪比（SNR）条件下保持高质量输出，解决现有模型在减少采样步数时性能退化的缺陷。  <br/>4. **实验验证**：在全带数据集上实验证明，新方法在去噪和消回声任务中均优于现有基线模型。|
|2506.01023v1|[A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech   Enhancement](http://arxiv.org/abs/2506.01023v1)|总结：  <br/>本文提出结合子带处理与深度滤波的HDF-Net模型，通过分解滤波结构和引入TAConv模块提升单通道语音增强性能，在资源效率与效果上优于现有方法。<br/><br/>贡献点：  <br/>1. **提出级联式深度滤波架构**：首次将子带处理与深度滤波结合，利用目标TF bin及其邻近区域信息进行语音增强。  <br/>2. **两阶段框架优化**：将深度滤波拆分为时域和频域组件，降低每阶段滤波系数预测复杂度，提升计算效率。  <br/>3. **设计TAConv模块**：增强卷积特征提取能力，优化TF域信息建模，提升语音增强效果。  <br/>4. **验证资源效率优势**：实验表明模型在保持高性能的同时，消耗更少计算资源，优于现有先进系统。|
|2506.00809v1|[FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse   Compression and Token Generation Models for the URGENT 2025 Challenge](http://arxiv.org/abs/2506.00809v1)|总结（100字以内）:  <br/>提出多阶段语音增强框架，结合自监督生成模型与时间移位技巧，有效提升多语言及多采样率下的语音质量。<br/><br/>贡献点:<br/>1. **多阶段框架设计**：首次将语音增强任务拆分为源分离、生成模型优化和融合三个阶段，系统化提升处理效果。<br/>2. **Sparse Compression Network**：引入稀疏压缩网络实现噪声环境下的源分离与初步干净语音估计，增强鲁棒性。<br/>3. **自监督生成模型**：基于神经音频编解码器的声学token，通过自监督特征和masked语言建模目标优化语音质量。<br/>4. **融合策略创新**：设计融合网络结合多阶段输出与原始噪声信号，平衡信号保真度与感知质量。<br/>5. **时间移位技巧**：提出shift trick聚合多时间尺度预测，结合output blending提升模型鲁棒性和输出质量。<br/>6. **多语言验证**：在多语言、多采样率和多样化失真数据集上验证框架有效性，体现跨语言适用性。|
|2505.24576v1|[A Composite Predictive-Generative Approach to Monaural Universal Speech   Enhancement](http://arxiv.org/abs/2505.24576v1)|1. **提出PGUSE模型**：首次结合预测性建模与扩散生成建模，设计出具有预测与生成双重能力的通用语音增强框架，解决传统方法在处理不同类型失真时的局限性。  <br/>2. **双分支结构创新**：构建预测分支（直接从退化信号生成干净语音）与生成分支（优化扩散模型去噪目标），实现预测效率与生成质量的协同提升。  <br/>3. **融合策略改进**：引入输出融合与截断扩散方案，前者整合两分支结果，后者通过预测分支初始估计优化逆扩散过程，降低计算复杂度并减少伪影。  <br/>4. **实验验证优势**：在多数据集上验证PGUSE优于现有SOTA方法，证明了混合建模在语音增强任务中的互补性和有效性。  <br/><br/>**总结**（100字内）：  <br/>提出PGUSE模型，融合预测与生成建模，通过双分支结构和创新融合策略提升语音增强效果，解决计算效率与伪影问题，验证其优越性。|
|2505.24450v2|[SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition](http://arxiv.org/abs/2505.24450v2)|**贡献点：**  <br/>1. 提出直接声音估计（DSE）方法，填补了真实远场对话数据中缺乏理想直接声音注释的空白。  <br/>2. 开发伪监督学习框架SuPseudo，通过DSE生成伪标签，实现SE模型对真实数据的直接学习与适应。  <br/>3. 设计端到端的SE模型FARNET，深度融合SuPseudo策略以提升语音增强性能。  <br/>4. 通过MISP2023语料库实验验证方法有效性，系统性能显著超越现有最优技术。  <br/><br/>**总结：** 提出DSE和SuPseudo方法，构建FARNET模型，实现真实远场语音增强的高性能突破。|
|2505.24450v1|[SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition](http://arxiv.org/abs/2505.24450v1)|**贡献点**  <br/>1. **提出直接声场估计（DSE）**：用于估计真实远场数据中的理想直达声，解决实际数据缺乏目标语音标注的问题。  <br/>2. **设计伪监督学习方法SuPseudo**：通过DSE生成伪标签，使语音增强模型能够直接适应真实数据，提升泛化能力。  <br/>3. **开发FARNET模型**：基于SuPseudo方法，构建专门用于远场语音增强的模型，实现对真实数据的全面利用。  <br/>4. **实验证明有效性**：在MISP2023数据集上验证SuPseudo和FARNET的性能，系统显著优于先前的SOTA方法。  <br/><br/>**总结**：  <br/>本研究提出DSE与SuPseudo方法，解决远场语音增强模型训练数据不足的问题，设计FARNET模型并验证其有效性，显著提升真实场景下的语音增强性能。|
|2505.24446v2|[Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge](http://arxiv.org/abs/2505.24446v2)|总结：  <br/>本论文提出三项创新方法：G-SpatialNet语音增强模型、TLS伪标签生成框架、以及结合微调、数据增强和多模态信息的ASR优化策略，显著提升会议场景下的语音识别性能，获得挑战赛第二名。<br/><br/>贡献点：  <br/>1. **G-SpatialNet语音增强模型设计**：针对强背景噪声、回声和语音重叠问题，开发专门的SE模型改进引导源分离（GSS）信号。  <br/>2. **TLS伪标签生成框架**：提出包含时间对齐、电平等对齐和信噪比过滤的框架，为真实远场音频数据生成信号级伪标签，辅助SE模型训练。  <br/>3. **多模态与训练策略优化**：探索预训练ASR模型在会议场景的微调策略、数据增强及多模态信息融合，显著提升识别性能。|
|2505.24446v1|[Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge](http://arxiv.org/abs/2505.24446v1)|**贡献点：**  <br/>1. 提出G-SpatialNet，作为新型语音增强模型，提升指导性声分离（GSS）信号的清晰度。  <br/>2. 设计TLS框架（时间对齐、水平对齐、信噪比滤波），生成信号级伪标签以优化真实远场语音数据的SE模型训练。  <br/>3. 探索微调策略、数据增强及多模态信息融合，有效提升预训练ASR模型在会议场景中的性能。  <br/><br/>**总结（100字以内）：**  <br/>本研究针对会议语音挑战提出三项创新方法，包括语音增强模型、伪标签生成框架及多模态ASR优化策略，显著提升了鲁棒性和识别率，并在挑战赛中取得第二名成绩。|
|2505.23744v1|[Boosting Domain Incremental Learning: Selecting the Optimal Parameters   is All You Need](http://arxiv.org/abs/2505.23744v1)|总结（100字以内）:  <br/>提出SOYO轻量框架，通过GMC、DFR和MDFN解决PIDIL参数选择难题，支持多种PEFT方法，实验验证在语音及多任务场景中的优越性，代码开源。<br/><br/>贡献点：  <br/>1. **提出SOYO框架**：首次设计专门针对PIDIL的轻量级框架，有效提升领域选择准确性。  <br/>2. **引入GMC与DFR**：通过高斯混合压缩器和领域特征重采样器，优化历史领域数据的存储与平衡。  <br/>3. **开发MDFN模块**：多级领域特征融合网络增强特征提取能力，提升模型适应性。  <br/>4. **兼容多种PEFT方法**：支持参数高效微调技术，适应不同任务需求。  <br/>5. **多任务验证**：在图像分类、目标检测、语音增强等任务中验证框架有效性。  <br/>6. **广泛基准测试**：在六个基准数据集上的实验表明其在动态环境中的鲁棒性与适应性。  <br/>7. **开源实现**：代码公开便于复现与进一步研究。|
|2505.23515v1|[DeepFilterGAN: A Full-band Real-time Speech Enhancement System with   GAN-based Stochastic Regeneration](http://arxiv.org/abs/2505.23515v1)|**贡献点**：  <br/>1. **提出全频带实时语音增强系统**：基于GAN的随机再生框架，实现全频段实时语音增强。  <br/>2. **结合预测与生成模型**：通过预测模型估计目标分布均值，生成模型学习完整分布，减少过抑制和失真。  <br/>3. **轻量高效架构设计**：仅含3.58M参数，低延迟，适配实时流处理需求。  <br/>4. **性能验证与改进**：实验表明系统在NISQA-MOS指标上优于基线，消融研究强调噪声条件的重要性。  <br/>5. **实际应用成果**：参与2025 Urgent Challenge并基于挑战结果进一步优化模型。  <br/><br/>**总结**（100字以内）：  <br/>本研究提出一种基于GAN的实时语音增强系统，结合预测与生成模型以减少失真，设计轻量架构并验证其在NISQA-MOS指标上的有效性，同时通过消融研究和挑战赛应用证明了噪声条件的重要性及模型的实用性。|
|2505.23212v2|[Interspeech 2025 URGENT Speech Enhancement Challenge](http://arxiv.org/abs/2505.23212v2)|**贡献点：**<br/>1. 提出Interspeech 2025 URGENT Challenge，扩展研究范围至语言依赖性、更广泛失真类型、数据可扩展性及噪声训练数据有效性。<br/>2. 首次系统评估混合方法与判别模型在通用语音增强中的表现差异。<br/>3. 发现生成/混合方法在主观评价中优于判别模型，纯生成模型存在语言依赖性。<br/>4. 构建多样化数据集与多维度评估指标，推动语音增强技术的标准化与普适性研究。  <br/><br/>**总结：**  <br/>本研究通过Interspeech 2025 URGENT Challenge探索语音增强的多方面挑战，揭示混合方法在主观评价中的优势和纯生成模型的语言依赖性，推动技术发展。|
|2505.22051v2|[ARiSE: Auto-Regressive Multi-Channel Speech Enhancement](http://arxiv.org/abs/2505.22051v2)|**贡献点分点列表：**  <br/>1. 提出ARiSE算法：首次将自回归机制引入多通道语音增强领域，通过利用前帧估计的目标语音作为额外输入，显著提升模型性能。  <br/>2. 创新输入特征设计：引入两种额外输入方式——(a)直接使用前帧估计结果；(b)基于前帧估计的波束形成混合信号，增强模型对噪声和混响的鲁棒性。  <br/>3. 提出高效训练机制：设计并行训练策略，解决传统自回归训练过程缓慢的问题，提升训练效率。  <br/>4. 实验证明有效性：在噪声-混响条件下验证算法性能，展示其在实际场景中的应用潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出ARiSE算法，通过自回归连接和双输入特征设计提升多通道语音增强性能，引入并行训练机制加速训练，并验证其在复杂环境下的有效性及潜力。|
|2505.21198v1|[Universal Speech Enhancement with Regression and Generative Mamba](http://arxiv.org/abs/2505.21198v1)|贡献点：  <br/>1. 提出USEMamba模型：基于状态空间模型，实现长序列建模、时频结构化处理和采样频率无关的特征提取，解决语音增强中的多条件处理需求。  <br/>2. 方法创新：结合回归建模与生成性变体，兼顾通用失真（回归）和缺失内容生成（如数据包丢失、带宽扩展）的性能优势。  <br/>3. 高泛化能力：仅使用部分训练数据即在盲测阶段取得Track 1第二名，验证了模型在复杂场景下的鲁棒性和推广性。  <br/>4. 多条件统一：支持七种失真类型、五种语言的跨场景语音增强任务，推动技术标准化与适用性提升。  <br/><br/>总结（100字以内）：  <br/>该研究提出USEMamba模型，通过状态空间架构与混合建模策略，实现高效语音增强，支持多失真类型和语言，显著提升模型泛化能力，仅用部分数据即在基准测试中取得优异成绩。|
|2505.21156v1|[Model as Loss: A Self-Consistent Training Paradigm](http://arxiv.org/abs/2505.21156v1)|总结：  <br/>提出Model as Loss新范式，利用编码器特征空间优化解码器，提升语音增强的感知质量和泛化能力，超越传统深度特征损失方法。<br/><br/>贡献点：  <br/>1. **提出新训练范式**：首次将同一模型的编码器作为损失函数，替代传统手工设计或预训练的深度特征损失，实现端到端训练。  <br/>2. **利用特征空间引导优化**：通过编码器的语义特征空间约束解码器输出，使增强结果更符合真实语音的感知与任务相关特性。  <br/>3. **提升性能与泛化能力**：在标准语音增强任务中表现优于现有方法，在域内和域外数据集均实现更优的感知质量与鲁棒性。|
|2505.21057v1|[Study of Lightweight Transformer Architectures for Single-Channel Speech   Enhancement](http://arxiv.org/abs/2505.21057v1)|总结：该论文提出LCT-GAN架构，通过因果约束与对抗训练，在降低参数量和计算开销的同时，实现了优于现有轻量模型和传统基线模型的语音增强性能。<br/><br/>贡献点：<br/>1. 提出轻量化因果约束的Transformer架构（LCT-GAN），通过流线型Frequency-Time-Frequency (FTF)堆叠结构高效建模全局时序依赖，显著降低计算开销。<br/>2. 引入判别器进行对抗训练优化，提升语音增强效果且不增加推理阶段的计算复杂度。<br/>3. 在参数量和计算效率上优于DeepFilterNet2（仅需6%参数）及CCFNet+Lite（参数减少9%、MAC操作降低10%），同时保持性能优势。<br/>4. 在主流测试数据集上，LCT-GAN超越更复杂的传统基线模型，达成当前轻量模型中的SotA性能。|
|2505.20741v1|[Uni-VERSA: Versatile Speech Assessment with a Unified Network](http://arxiv.org/abs/2505.20741v1)|**贡献点总结（100字以内）：**  <br/>提出Uni-VERSA，首次统一预测多维度语音质量指标（自然度、可懂度、说话人特征等），构建评估框架与协议，应用于语音增强、合成与质量控制，并通过URGENT24基准验证其有效性及与人类感知的一致性。  <br/><br/>**分点贡献：**  <br/>1. **统一评估模型**：开发Uni-VERSA网络，首次同时预测自然度、可懂度、说话人特征、语调和噪声等多种语音质量指标，突破传统单指标评估方法的局限。  <br/>2. **框架与协议标准化**：系统化定义模型框架、评估流程及多任务应用场景（增强、合成、质量控制），推动语音质量评估方法的标准化。  <br/>3. **自监督基准验证**：基于URGENT24挑战赛构建基准测试，结合自监督表示的基线模型，证明Uni-VERSA在性能与可扩展性上优于现有方法。  <br/>4. **人类感知对齐**：通过实验验证模型预测结果与人类主观评价高度一致，为未来语音质量评估提供更可靠的客观替代方案。|
|2505.19597v1|[A Lightweight Hybrid Dual Channel Speech Enhancement System under   Low-SNR Conditions](http://arxiv.org/abs/2505.19597v1)|**贡献点总结（100字以内）**：  <br/>提出轻量级混合双通道语音增强系统，结合IVA与改进GTCRN，有效降低计算复杂度并提升低SNR下的语音质量。<br/><br/>**分点贡献**：  <br/>1. **提出轻量级混合架构**：设计结合独立向量分析（IVA）与改进的双通道分组时序卷积循环网络（GTCRN）的混合系统，优化计算资源受限场景下的语音增强效率。  <br/>2. **IVA作为粗估计器**：利用IVA提取语音和噪声的辅助信息，为后续网络提供基础估计以提升整体性能。  <br/>3. **改进GTCRN网络结构**：针对原始和辅助信息进行优化设计，进一步细化语音质量，增强系统鲁棒性。  <br/>4. **全面信息融合策略**：研究多种修改方法，确保系统对原始信号与辅助信息的高效利用，提升处理效果。  <br/>5. **实验验证有效性**：在低SNR条件下，通过实验证明系统在参数量和计算复杂度上的优势，兼顾性能与实用性。|
|2505.19576v1|[Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech   Enhancement](http://arxiv.org/abs/2505.19576v1)|总结：  <br/>本文提出Mel-McNet框架，在Mel频率域处理多通道语音增强，通过STFT-to-Mel模块和修改的McNet结构显著降低计算复杂度并提升性能，验证了Mel尺度在语音增强中的有效性。<br/><br/>贡献点：  <br/>1. **首次在Mel频率域应用多通道语音增强框架**：提出Mel-McNet，突破传统线性域方法的局限性。  <br/>2. **双组件设计**：包含STFT-to-Mel模块（压缩多通道STFT至Mel表示）和Mel域优化的McNet骨干网络（生成增强LogMel谱）。  <br/>3. **高效性与性能平衡**：实验证明在CHiME-3数据集上，计算复杂度降低60%的同时保持与原McNet相当的增强和ASR效果。  <br/>4. **超越SOTA方法**：在性能上优于当前主流技术，验证Mel尺度语音增强的潜力。|
|2505.19534v1|[Training-Free Multi-Step Audio Source Separation](http://arxiv.org/abs/2505.19534v1)|总结：  <br/>本文提出无需额外训练的多步音频分离方法，通过迭代融合和优化比例提升性能，理论证明其有效性并关联到扩散模型，实验证明在语音和音乐分离任务中均优于单步方法，可实现类似大模型的扩展效果。<br/><br/>贡献点：  <br/>1. **多步推理框架**：首次提出利用预训练单步模型进行多步音频分离，无需额外训练即可提升分离性能。  <br/>2. **优化融合策略**：设计迭代分离方法，通过动态调整输入混合物与前一步分离结果的融合比例最大化分离效果。  <br/>3. **理论保障**：证明多步方法优于单步推理，提供基于模型平滑性和度量鲁棒性的误差界限，并建立与线性插值路径去噪的理论联系。  <br/>4. **跨任务有效性**：实验证明方法在语音增强和音乐分离任务中均显著优于单步方法，且具备类似大模型的扩展能力。  <br/>5. **非优化指标提升**：方法改进不仅体现在优化指标上，还扩展到几乎所有非优化指标（唯一代价指标除外）。  <br/>6. **研究局限与展望**：讨论了方法的局限性并提出未来研究方向。|
|2505.19476v2|[FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching](http://arxiv.org/abs/2505.19476v2)|**贡献点：**  <br/>1. 提出FlowSE，首个基于流匹配的语音增强模型，解决语言模型量化损失和扩散模型高延迟的痛点；  <br/>2. 通过单次传递学习噪声与干净语音的连续分布转换，显著降低推理延迟并保持高保真重建；  <br/>3. 支持噪声mel谱图与文本序列的联合训练，隐式捕捉语音的时序-频谱结构及文本-语音对齐特性；  <br/>4. 推理阶段支持有无文本信息，均表现优异，文本辅助场景下进一步提升性能；  <br/>5. 实验验证FlowSE在语音增强任务中超越现有生成模型，开创生成式SE新范式并证明流匹配方法的潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于流匹配的FlowSE模型，有效解决语音增强中量化损失与推理延迟问题，支持文本辅助与无文本场景，实验验证其性能优于现有方法，为生成式语音增强提供新范式。|
|2505.19401v1|[Stack Less, Repeat More: A Block Reusing Approach for Progressive Speech   Enhancement](http://arxiv.org/abs/2505.19401v1)|总结：  <br/>本论文提出一种通过重复单个处理块实现语音增强的高效方法，减少参数冗余并优化处理阶段设计，验证了其在性能和效率上的优势。<br/><br/>贡献点：  <br/>1. **提出重用单块模型结构**：采用重复单一处理块而非传统堆叠多块的架构，提升模型效率并减少参数冗余。  <br/>2. **序列建模块重用策略**：通过保持编码器/解码器浅层并重复单一序列建模块，降低领域变换复杂度。  <br/>3. **处理阶段数量优先于块数量**：实验表明，增加处理阶段数量比增加块数量更显著提升性能。  <br/>4. **单块内逐步细化机制**：揭示单个处理块可通过内部迭代逐步优化噪声输入，无需多块协作。  <br/>5. **编码器/解码器深度优化**：证明加深编码器和解码器在块重用框架下可能冗余，简化复杂表征学习。|
|2505.19314v2|[SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline](http://arxiv.org/abs/2505.19314v2)|**贡献点：**<br/>1. **提出新型生成模型架构**：设计SoloSpeech，首次将压缩、提取、重建与修正过程整合为级联生成流程，突破传统单阶段生成模型的局限。  <br/>2. **无说话人嵌入的提取方法**：开发无需说话人嵌入的提取器，通过条件信息对齐提示音频与混合音频的潜在空间，避免模型对说话人特征的依赖。  <br/>3. **提升语音质量与可理解性**：在Libri2Mix数据集上实现目标语音提取和语音分离任务的新SOTA，显著优于现有判别模型和生成模型的性能。  <br/>4. **增强域外泛化能力**：在非目标域数据和现实场景中表现出色，解决了传统模型对环境差异敏感的问题。  <br/><br/>**总结：**  <br/>本文提出SoloSpeech，通过级联生成模型和潜在空间对齐，解决了TSE中传统模型的伪影、自然度下降及环境适应性问题，实现了质量与可理解性的突破，并具备优秀的泛化能力。|
|2505.18533v1|[TS-URGENet: A Three-stage Universal Robust and Generalizable Speech   Enhancement Network](http://arxiv.org/abs/2505.18533v1)|**贡献点分点列出：**  <br/>1. **提出三阶段架构（Filling-SEparation-Restoration）**：通过填充、分离、恢复三阶段协同处理语音信号，系统性解决多种干扰问题。  <br/>2. **增强鲁棒性与通用性**：填充阶段在噪声下补全丢失段，分离阶段联合抑制噪声、混响和 clipping，恢复阶段补偿带宽限制和编解码伪影，实现多场景适应。  <br/>3. **显著提升性能**：在Interspeech 2025 URGENT Challenge中取得Track 1第二名，验证了方法在复杂语音增强任务中的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出TS-URGENet，通过三阶段架构（补全、去噪、恢复）解决语音增强中的多样干扰，兼顾鲁棒性与通用性，实验在国际挑战赛中表现优异，为通用语音增强提供了新方案。|
|2505.16911v2|[Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation](http://arxiv.org/abs/2505.16911v2)|总结:  <br/>提出主动语音增强(ASE)新范式，结合Transformer-Mamba架构与任务特定损失函数，实现语音信号的主动调控，在去噪、降混响等任务中超越现有基线。<br/><br/>贡献点:  <br/>1. **提出主动语音增强新框架**：区别于传统ANC，ASE通过同时抑制噪声和增强语音频率提升清晰度与感知质量。  <br/>2. **设计混合架构**：提出基于Transformer-Mamba的创新模型，融合Transformer的长期依赖建模与Mamba的高效状态空间处理。  <br/>3. **开发联合优化损失函数**：构建任务特定损失函数，同步优化干扰抑制与信号增强目标。  <br/>4. **验证多任务有效性**：在去噪、降混响、降限幅等任务中超越现有方法，证明主动调制在复杂声学环境中的优势。|
|2505.16607v1|[Attractor-Based Speech Separation of Multiple Utterances by Unknown   Number of Speakers](http://arxiv.org/abs/2505.16607v1)|**贡献点（分点）：**  <br/>1. 提出首个同时处理多说话人语音分离、动态说话人数估计和个体说话人活动检测的集成模型。  <br/>2. 引入基于吸引子（attractor）的架构，有效融合局部与全局时序建模能力，提升多语句场景下的分离效果。  <br/>3. 构建多说话人多语句合成数据集（结合Librispeech与WHAM!噪声），用于验证方法在混响和噪声环境下的鲁棒性。  <br/>4. 实验结果表明，系统在已知及未知说话人数场景中均能准确估计声源数量并生成正确的分离输出。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种集成模型，通过吸引子架构融合时序建模能力，实现动态说话人数估计与多语句语音分离，并构建合成数据集验证其在复杂环境下的有效性，显著提升了单通道语音分离性能。|
|2505.16351v2|[Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection](http://arxiv.org/abs/2505.16351v2)|**贡献点总结（100字以内）:**  <br/>本研究提出Dysfluent-WFST，实现零样本解码，同时进行音素转录与语音不流畅性检测，无需额外训练。在模拟及真实数据上达到SOTA性能，具备轻量、可解释性，强调显式建模发音行为对提升系统性能的关键作用。<br/><br/>**分点贡献:**  <br/>1. **提出Dysfluent-WFST**：首个零样本解码器，同步完成音素转录与不流畅性检测，集成上游编码器（如WavLM）无需额外训练。  <br/>2. **解决传统方法局限**：突破仅依赖分类的临床洞察不足，以及文本无关模型在上下文相关场景的误判问题。  <br/>3. **性能突破**：在语音错误率与不流畅性检测任务上均达到SOTA，验证模型有效性。  <br/>4. **轻量化与可解释性**：方法设计简洁，适合实际应用，便于临床理解和部署。  <br/>5. **理论贡献**：证明显式建模发音行为（而非复杂架构）是提升不流畅性处理系统性能的核心。|
|2505.15914v2|[A Novel Deep Learning Framework for Efficient Multichannel Acoustic   Feedback Control](http://arxiv.org/abs/2505.15914v2)|总结：  <br/>提出结合空间与时间处理的卷积循环网络，创新三种训练方法并实现高效多通道语音反馈控制，推动实际应用技术发展。<br/><br/>贡献点：  <br/>1. **提出新模型**：开发卷积循环网络（Convolutional Recurrent Network），融合空间与时间处理，提升语音增强能力。  <br/>2. **创新训练方法**：采用三种训练策略（In-a-Loop Training、Teacher Forcing、混合多通道维纳滤波器），优化复杂声学环境下的性能。  <br/>3. **高效性与可扩展性**：框架在降低计算需求的同时保持高效，适用于实际场景，推动语音反馈控制技术进步。|
|2505.15254v1|[Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice   Conversion Framework](http://arxiv.org/abs/2505.15254v1)|总结（100字以内）：<br/>该研究提出结合生成性语音修复和语音转换的双阶段系统，通过前端噪声抑制和后端说话人嵌入引导修复，有效解决VC模型在噪声环境下的脆弱性，实现与SOTA相当的语音质量提升。<br/><br/>贡献点：<br/>1. 提出双阶段语音增强框架：将说话人无关的生成性语音修复（GSR）与语音转换（VC）相结合，形成分层处理流程。<br/>2. 解决VC模型噪声敏感问题：在VC前端添加GSR模块，直接处理噪声干扰和语音损伤。<br/>3. 创新性引入嵌入引导机制：VC阶段利用干净说话人嵌入作为指导信号，提升语音质量。<br/>4. 达到SOTA性能表现：在多数据集上验证，其语音质量客观指标与当前最佳方法相当。|
|2505.14433v1|[Single-Channel Target Speech Extraction Utilizing Distance and Room   Clues](http://arxiv.org/abs/2505.14433v1)|总结：  <br/>该论文提出一种结合距离线索和房间信息的单通道目标语音提取模型，通过可学习的嵌入提升系统在不同环境下的泛化能力，并在仿真与真实数据上验证其有效性。<br/><br/>贡献点：  <br/>1. **引入房间环境信息**：首次将房间维度和混响时间等环境参数纳入距离线索的TSE框架，以增强系统对不同声学环境的适应性。  <br/>2. **提出时间频率域模型**：设计基于时频域的可学习距离与房间嵌入模型，有效融合两种信息进行目标语音提取。  <br/>3. **提升泛化能力**：通过结合环境信息，解决传统仅依赖距离线索的TSE系统在房间变化时性能下降的问题。  <br/>4. **实验验证可行性**：在仿真和真实数据集上验证方法的有效性，证明其在实际场景中的适用性。  <br/>5. **提供可复现资源**：公开演示材料，便于研究者进一步验证和应用该方法。|
|2505.13983v1|[Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding   for Diffusion-Based Speech Enhancement](http://arxiv.org/abs/2505.13983v1)|总结：  <br/>本论文提出双流编码修复扩散模型（DERDM-SE），通过结合粗粒度与细粒度处理及两种条件（仅确定性特征和混合确定性-噪声特征）提升语音增强效果，实验验证其在客观指标和稳定性上均优于现有扩散模型。<br/><br/>贡献点：  <br/>1. **分析条件影响**：系统评估不同确定性语音增强模型作为扩散条件的效果，发现其对听觉体验的提升作用。  <br/>2. **提出双流编码框架**：设计双条件输入的DERDM-SE模型，有效融合确定性特征与噪声特征，增强预测准确性。  <br/>3. **混合处理机制**：引入结合粗粒度和细粒度处理的确定性模型，在保持扩散稳定性的同时提升客观评价指标。  <br/>4. **实验证明优势**：在CHiME4数据集上验证了模型的高效性，实现更优的语音增强评分及更稳定的性能表现。|
|2505.13843v1|[A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model](http://arxiv.org/abs/2505.13843v1)|总结：  <br/>本文提出一种基于语义信息的分步因子化语音增强方法，结合因子化编解码器与扩散模型，通过层次化建模提升复杂环境下的语音恢复效果，并显著增强下游TTS任务的性能，实验结果优于SOTA基线。<br/><br/>贡献点：  <br/>1. **提出语义驱动的分步因子化SE框架**：首次将语义信息直接整合进语音增强流水线，结合因子化编解码器与扩散模型，实现对语音信号的分阶段重构。  <br/>2. **层次化建模语义与声学属性**：通过解耦语义内容和声学细节，增强模型在复杂噪声环境中的鲁棒性，突破传统方法仅依赖频谱或掩码的局限。  <br/>3. **提升下游TTS任务性能**：改进后的语音增强结果显著优化了噪声环境下的语音到文本生成效果，拓展了语音处理技术的实际应用价值。  <br/>4. **实验验证优越性**：在语音质量指标（如PESQ）和TTS任务表现上均超越现有SOTA基线，证明方法在复杂场景下的有效性。|
|2505.13830v2|[Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising](http://arxiv.org/abs/2505.13830v2)|总结：  <br/>本文提出基于神经编码器的语音降噪模块，结合LauraTTS实现噪声鲁棒的零样本语音合成，在降噪效果和模型性能上均优于现有技术。<br/><br/>贡献点：  <br/>1. **提出神经编码器语音降噪器**：设计包含音频编码、令牌降噪和嵌入优化的三阶段降噪框架。  <br/>2. **实现噪声鲁棒的零样本TTS**：将降噪模块与LauraTTS集成，显著提升噪声环境下的语音合成质量。  <br/>3. **改进模型性能**：通过令牌降噪预测前两组干净的声学标记，结合嵌入优化器和解码器生成高质量语音，超越现有SE方法及基于额外SE模型的替代方案。|
|2505.13094v1|[Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech   Separation](http://arxiv.org/abs/2505.13094v1)|**贡献点：**  <br/>1. 提出Time-Frequency Attention Cache Memory (TFACM)模型，通过结合注意力机制与缓存内存解决因果语音分离中历史信息保留问题。  <br/>2. 引入LSTM层处理频率相关的位置关系，同时通过局部与全局表示实现时间维度上的因果建模。  <br/>3. 设计缓存内存模块（CM）存储历史信息，并采用因果注意力细化（CAR）模块增强时间特征表示。  <br/>4. 实验证明TFACM在性能上与现有SOTA模型（TF-GridNet-Causal）相当，但显著降低模型复杂度和参数量。  <br/><br/>**总结（100字内）：**  <br/>本研究提出TFACM模型，通过融合注意力机制与缓存内存解决因果语音分离中历史信息丢失问题，实现性能与复杂度的优化，为高效语音分离提供了新思路。|
|2505.13029v2|[MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for   Speech Enhancement](http://arxiv.org/abs/2505.13029v2)|**贡献点**  <br/>1. **提出混合模型MDDM**：结合多视角判别式增强与扩散模型，突破传统单一流派方法的局限。  <br/>2. **三域特征输入**：通过时间、频率和噪声多模态特征协同提升频谱生成质量。  <br/>3. **优化采样策略**：利用判别式输出与干净目标的分布交集，减少采样步骤以提升效率。  <br/>4. **跨数据集验证**：在公开数据集和真实场景数据集上验证模型有效性，兼具主观与客观指标表现。  <br/><br/>**总结**（100字以内）:  <br/>本文提出MDDM，融合多视角判别式增强与扩散模型，通过多模态特征输入和优化采样策略提升语音增强效果，显著减少计算成本并在多数据集验证中表现优异。|
|2505.12686v1|[RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations](http://arxiv.org/abs/2505.12686v1)|总结：  <br/>提出了RoVo，通过在高维嵌入向量注入对抗性扰动并重构语音，显著提升防御成功率，同时增强对语音增强攻击的抵抗力，并保持语音自然性。<br/><br/>贡献点：  <br/>1. **方法创新**：提出RoVo（Robust Voice），通过注入对抗性扰动到音频信号的高维嵌入向量中，而非直接作用于原始音频，有效规避语音增强方法的抵消作用。  <br/>2. **防御效果提升**：在四类先进语音合成模型上，将防御成功率（DSR）提升超70%，在商用语音验证API中达到99.5%的DSR，显著优于传统方法。  <br/>3. **抗语音增强能力**：扰动在强语音增强条件下仍保持鲁棒性，有效抵御二次攻击威胁。  <br/>4. **用户体验保障**：通过用户研究验证，所保护的语音在自然性和功能性上无明显损失，适用于复杂威胁场景。|
|2505.12288v1|[Unified Architecture and Unsupervised Speech Disentanglement for Speaker   Embedding-Free Enrollment in Personalized Speech Enhancement](http://arxiv.org/abs/2505.12288v1)|**贡献点总结（100字以内）：**  <br/>本研究提出统一模型USEF-PNet，整合语音增强与个性化增强任务；创新性地引入DSEF-PNet的无监督语音解缠绕方法，提升PSE鲁棒性；并通过LSEP策略分析参考语音时长的影响，验证了其有效性。  <br/><br/>**具体贡献点分点：**  <br/>1. **统一模型架构**：设计USEF-PNet，首次将传统语音增强（SE）与个性化语音增强（PSE）整合为单一框架，简化部署并提升性能。  <br/>2. **无监督语音解缠绕**：在DSEF-PNet中，通过混合语音与双参考语句的配对，强制一致性约束，有效分离目标说话人身份信息，减少情感和内容干扰，增强PSE的鲁棒性。  <br/>3. **长-短参考语音策略**：提出LSEP策略，研究参考语音时长对训练和评估的动态影响，验证了随机时长参考语音在PSE中的优越性。|
|2505.12079v1|[SepPrune: Structured Pruning for Efficient Deep Speech Separation](http://arxiv.org/abs/2505.12079v1)|**贡献点分点列出：**  <br/>1. **首个结构化剪枝框架**：提出SepPrune，是首个专门针对语音分离模型的结构化压缩方法，兼顾分离质量与计算效率。  <br/>2. **计算结构分析与关键层识别**：通过分析模型计算结构，定位高计算负担的层，优化剪枝策略。  <br/>3. **可微分掩码策略**：引入基于梯度的通道选择机制，实现动态调整通道的可微分优化。  <br/>4. **高效性能恢复机制**：结合冗余通道剪枝与参数微调，在极少的训练轮次下（1轮）恢复原模型85%性能。  <br/>5. **显著提升收敛速度**：剪枝模型收敛速度比从头训练快36倍，降低实时应用的计算成本。  <br/>6. **开源实现支持**：提供开源代码，推动方法在语音分离领域的研究与实际应用。  <br/><br/>**总结（100字以内）:**  <br/>提出SepPrune框架，首次将结构化剪枝应用于语音分离，通过计算结构分析与可微分通道选择实现高效模型压缩，显著提升性能恢复效率与收敛速度，代码开源以促进应用。|
|2505.08694v1|[A Survey of Deep Learning for Complex Speech Spectrograms](http://arxiv.org/abs/2505.08694v1)|**贡献点：**  <br/>1. 系统性总结了深度学习在复杂频谱图处理中的关键技术与方法。  <br/>2. 提出复杂频谱图及其特征在语音分析任务中的具体应用与必要性。  <br/>3. 分析复杂值神经网络（CVNN）的关键组件和架构设计。  <br/>4. 探讨针对复杂频谱图的定制化训练策略与损失函数。  <br/>5. 概述深度学习在相位恢复、语音增强、语音分离等领域的应用进展。  <br/>6. 研究复杂频谱图与生成模型的结合，拓展其在语音处理中的潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文系统梳理了深度学习处理复杂频谱图的技术，涵盖网络设计、训练策略及应用，为语音领域研究提供全面参考。|
|2505.05657v3|[ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior](http://arxiv.org/abs/2505.05657v3)|**贡献点总结（100字以内）：**  <br/>提出ArrayDPS方法，实现无监督、阵列无关的盲语音分离，通过扩散后验采样和优化问题近似似然，结合扩散先验和相对传输函数估计，无需麦克风信息，性能超越无监督基线并媲美监督方法。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **提出ArrayDPS框架**：首次将扩散后验采样（DPS）应用于盲语音分离（BSS），实现无监督、阵列无关和生成式分离，无需依赖麦克风阵列几何或房间冲激响应（RIR）。  <br/>2. **解决似然不可处理难题**：通过引入独立优化问题近似复杂似然，有效建模房间声学和麦克风间相对传输函数，突破传统DPS对可处理似然的依赖。  <br/>3. **联合估计声学参数与分离结果**：在采样过程中迭代更新扩散先验与声学模型，同时优化分离性能与房间特性估计，提升整体鲁棒性。  <br/>4. **简化先验需求**：仅依赖单讲者扩散模型作为先验，无需额外阵列信息，降低模型复杂度并提高实际应用适应性。  <br/>5. **性能优势验证**：在无监督方法中表现最优，且与监督方法在SDR指标上具有竞争力，证明了方法的有效性与泛化能力。|
|2505.05657v2|[ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior](http://arxiv.org/abs/2505.05657v2)|**总结（100字以内）:**  <br/>提出ArrayDPS方法，实现无监督、阵列无关的盲语音分离。通过改进扩散后验采样，引入独立优化问题近似似然，建模房间声学与麦克风传递函数，仅需单人语音扩散模型作为先验。实验表明其SDR优于无监督方法，接近有监督水平。<br/><br/>**贡献点分点列出:**  <br/>1. **提出ArrayDPS方法**：首次在盲语音分离中引入无监督、阵列无关、生成式的框架，无需麦克风阵列几何信息。  <br/>2. **改进扩散后验采样**：通过建立独立优化问题近似似然，解决盲逆问题中房间声学和相对传输函数建模难题。  <br/>3. **简化先验需求**：仅依赖单人语音扩散模型作为先验，降低对复杂后验分布的依赖。  <br/>4. **性能优势**：在无监督方法中表现最优，同时SDR指标接近有监督方法，兼具高效与高精度。  <br/>5. **提供可复现实验**：配套音频演示，便于验证方法的实际效果与应用潜力。|
|2505.05216v1|[Normalize Everything: A Preconditioned Magnitude-Preserving Architecture   for Diffusion-Based Speech Enhancement](http://arxiv.org/abs/2505.05216v1)|**贡献点**  <br/>1. **方法创新**：提出基于Schroedinger桥的扩散模型框架，将噪声语音分布转化为干净语音分布。  <br/>2. **预处理技术**：引入时间依赖的输入/输出缩放（preconditioning）以提升训练稳定性与效果。  <br/>3. **网络结构设计**：设计两种跳连配置，分别预测环境噪声或干净语音，适应不同增强需求。  <br/>4. **幅度保持架构**：采用归一化激活与权重的网络结构，维持训练中的幅度平衡。  <br/>5. **输入条件优化**：在每个网络块中学习噪声输入的贡献，增强输入条件化效果。  <br/>6. **EMA策略分析**：提出近似EMA曲线并验证其效果，发现较短EMA长度在语音增强任务中表现更优。  <br/>7. **资源开放**：提供代码、音频示例和预训练模型，便于复现与研究。  <br/><br/>**总结**：本研究提出基于Schroedinger桥的扩散模型语音增强框架，结合预处理、跳连设计与EMA优化，提升了多维度语音质量，并开放了相关资源。|
|2505.03273v2|[SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation](http://arxiv.org/abs/2505.03273v2)|**贡献点**  <br/>1. **提出SepALM框架**：首次将音频语言模型（ALMs）引入语音分离后处理流程，实现分离后的语音校正与文本域重合成。  <br/>2. **四阶段处理流程**：包含分离器、校正器、合成器和对齐器四个核心模块，系统化解决现实环境中的语音分离挑战。  <br/>3. **端到端错误校正机制**：通过ALM消除了传统方法中ASR与LLMs结合的误差累积问题，提升准确性与鲁棒性。  <br/>4. **创新提示与训练技术**：开发Chain-of-Thought（CoT）提示及知识蒸馏技术，优化ALM的推理能力和训练效率。  <br/>5. **实验验证效果**：在噪声和混响环境下，SepALM显著提升语音分离精度并增强对新声学场景的适应性。  <br/><br/>**总结（100字内）**  <br/>本文提出SepALM，将音频语言模型应用于语音分离后处理，通过分离-校正-合成-对齐四阶段流程，有效解决现实场景中的混响和噪声问题，结合CoT提示与知识蒸馏技术，提升模型精度与适应性。|
|2505.03186v2|[CoGenAV: Versatile Audio-Visual Representation Learning via   Contrastive-Generative Synchronization](http://arxiv.org/abs/2505.03186v2)|总结（100字以内）:<br/>CoGenAV通过融合对比学习和生成式文本预测，提出一种数据高效的跨模态同步模型，显著提升AVSR和VSR性能，并在噪声环境中表现优异，模型开源以推动学术与工业应用。<br/><br/>贡献点：<br/>1. **多模态同步建模**：首次整合唇部运动、语音与语言内容的天然同步性，构建跨模态关联学习框架。<br/>2. **双目标优化策略**：结合对比特征对齐（contrastive feature alignment）与生成文本预测（generative text prediction），提升表征学习能力。<br/>3. **数据高效性**：仅需LRS2数据集的223小时标注数据，实现低数据成本的高泛化能力。<br/>4. **任务泛化能力**：在AVSR（WER 1.27）、VSR（WER 20.5）、语音增强/分离以及ASD等任务中均取得领先性能。<br/>5. **噪声鲁棒性**：在噪声环境中性能提升超70%，解决传统音频系统在复杂场景下的脆弱性。<br/>6. **开源推动**：模型开放源码，促进跨领域研究与实际应用的发展。|