|Source|Title|Summary|
|---|---|---|
|2512.01512v1|[MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages](https://arxiv.org/abs/2512.01512v1)||
|2512.01450v1|[Model-Based Clustering of Functional Data Via Random Projection Ensembles](https://arxiv.org/abs/2512.01450v1)||
|2512.01267v1|[ZO-ASR: Zeroth-Order Fine-Tuning of Speech Foundation Models without Back-Propagation](https://arxiv.org/abs/2512.01267v1)||
|2511.23404v1|[LFM2 Technical Report](https://arxiv.org/abs/2511.23404v1)||
|2511.22863v1|[CoordSpeaker: Exploiting Gesture Captioning for Coordinated Caption-Empowered Co-Speech Gesture Generation](https://arxiv.org/abs/2511.22863v1)||
|2511.22769v1|[Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration](https://arxiv.org/abs/2511.22769v1)||
|2511.20534v1|[Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition](https://arxiv.org/abs/2511.20534v1)||
|2511.19947v1|[Towards Edge General Intelligence: Knowledge Distillation for Mobile Agentic AI](https://arxiv.org/abs/2511.19947v1)||
|2511.18632v1|[The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion](https://arxiv.org/abs/2511.18632v1)||
|2511.17477v1|[Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition](https://arxiv.org/abs/2511.17477v1)||
|2511.16114v1|[SceneGuard: Training-Time Voice Protection with Scene-Consistent Audible Background Noise](https://arxiv.org/abs/2511.16114v1)|总结：  <br/>SceneGuard通过场景一致的可听背景噪声实现训练时语音保护，有效抵御攻击且保持高语音可懂度，为隐私安全提供新方案。<br/><br/>贡献点：  <br/>1. **提出SceneGuard方法**：首次将场景一致的可听背景噪声作为训练时语音保护手段，利用自然场景声学环境增强防护。  <br/>2. **抗对抗性设计**：噪声与语音场景匹配，避免传统不可感知扰动易被预处理（如压缩、降噪）破坏的问题。  <br/>3. **显著防护效果**：在语音克隆攻击中使说话人相似度下降5.5%（p<10⁻¹⁵），同时保持98.6%语音可懂度（STOI=0.986）。  <br/>4. **鲁棒性验证**：对MP3压缩、谱减法等5种常见对抗措施均保持防护效果，证明其在真实场景中的可靠性。  <br/>5. **方法可扩展性**：为语音隐私保护提供新的研究方向，代码开源便于后续研究与应用。|
|2511.15253v2|[PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback](https://arxiv.org/abs/2511.15253v2)||
|2511.15253v1|[PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback](https://arxiv.org/abs/2511.15253v1)|**贡献点**  <br/>1. **提出双代理系统**：构建Ideal Presentation Agent（生成参考视频）和Coach Agent（评估与反馈）的协同框架，整合多模态处理能力。  <br/>2. **多模态技术整合**：融合幻灯片处理、视觉语言分析、语音合成与视频合成，实现从文本到动态演示的端到端生成。  <br/>3. **结构化反馈机制**：引入Observation-Impact-Suggestion（OIS）格式，提供清晰、可操作的个性化语音指导。  <br/>4. **听众模拟模块**：通过Audience Agent模拟人类视角，增强反馈的人性化与互动性，提升学习沉浸感。  <br/>5. **闭环学习循环**：实现“观察-练习-反馈”的完整闭环，支持持续迭代的语音技能培训。  <br/>6. **技术鲁棒性**：基于多模型集成、语音克隆与误差处理机制构建稳定后端，保障系统的实用性和可扩展性。  <br/><br/>**总结**（100字以内）：  <br/>本研究提出双代理系统，整合多模态技术生成参考视频并模拟听众反馈，提供结构化、人性化的语音技能培训，实现闭环学习，推动AI在教育与专业场景中的应用。|
|2511.14852v1|[PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852v1)|总结：  <br/>提出PolyKAN，首个通用开源KAN实现，通过四项技术优化实现高效GPU加速，显著提升推理与训练速度，在多种任务中保持精度。<br/><br/>贡献点：  <br/>1. **首个通用开源实现**：提供PolyKAN，是首个支持KAN及其变体的开放源码GPU加速库。  <br/>2. **融合计算优化**：将多项式KAN层的前向与反向传播合并为优化的CUDA内核，提高效率。  <br/>3. **四项关键技术**：  <br/>   - (i) 查找表+线性插值替代数学库函数；  <br/>   - (ii) 二维平铺提升线程并行性与内存局部性；  <br/>   - (iii) 两阶段归约减少原子更新开销；  <br/>   - (iv) 系数布局重排实现单位步长读取。  <br/>4. **性能提升**：在语音、音频增强和表格回归任务中，相比Triton+cuBLAS基线，推理速度提升1.2-10倍，训练速度提升1.4-12倍。|
|2511.14515v2|[IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention](https://arxiv.org/abs/2511.14515v2)||
|2511.14515v1|[IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention](https://arxiv.org/abs/2511.14515v1)|**总结**（100字以内）：  <br/>IMSE通过引入MALA和IDConv，在保持与SOTA相当的PESQ性能下，显著降低参数量（16.8%），为资源受限设备上的轻量语音增强提供了高效解决方案，平衡了模型规模与语音质量。<br/><br/>**贡献点**：  <br/>1. **提出IMSE网络**：系统性优化现有方法，设计超轻量级语音增强模型，解决资源受限设备上的性能与轻量化矛盾。  <br/>2. **MALA注意力机制**：替换MET模块，通过显式保留查询向量的范数信息，直接修复线性注意力的“幅度忽略”问题，实现高效全局建模，无需辅助补偿分支。  <br/>3. **IDConv模块**：采用Inception理念，将大核操作分解为并行分支（方形、水平、垂直条带），以极低参数冗余捕捉频谱特征，提升计算效率。  <br/>4. **性能验证**：在VoiceBank+DEMAND数据集上实验表明，IMSE参数量减少16.8%（0.513M→0.427M），PESQ指标达到3.373，与SOTA方法性能相当，推动轻量语音增强新基准。|
|2511.14219v1|[Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219v1)|总结（100字以内）:  <br/>提出两阶段框架，结合自适应层注意力（ALA）与多目标知识蒸馏，提升Whisper在噪声环境下的鲁棒性与可靠性，显著降低幻觉错误与词错误率，同时保持干净语音性能。<br/><br/>贡献点分点列出：<br/>1. **提出新型两阶段架构**：首创通过自适应层注意力（ALA）增强编码器鲁棒性，结合多目标知识蒸馏（KD）框架抑制幻觉，系统性解决噪声环境下的ASR可靠性问题。<br/>2. **创新性ALA模块**：通过层间相关性分析将编码器分组为语义连贯块，引入可学习多头注意力模块融合低/高阶特征，提升噪声场景下的语义表征能力。<br/>3. **多目标KD框架设计**：训练学生模型在噪声音频上对齐教师模型（处理干净输入）的语义和注意力分布，实现噪声与干净数据的联合优化，优化幻觉抑制效果。<br/>4. **实证效果显著**：在噪声语音基准测试中，有效降低幻觉错误和词错误率，同时保持对干净语音的高识别性能，验证了方法的实际有效性。|
|2511.13918v1|[Human-centric Maintenance Process Through Integration of AI, Speech, and AR](https://arxiv.org/abs/2511.13918v1)|**总结（100字以内）**：  <br/>该研究开发了一个基于HoloLens 2的AR演示系统，整合AI与语音处理技术，实现高精度语音转文本，支持无手操作的工业维护任务记录与交互，旨在提升人机协作效率、降低认知负荷并增强安全保障。<br/><br/>**贡献点**：  <br/>1. **技术整合**：首次将AI、AR与语音处理结合，提出用于工业维护的多模态技术支持方案。  <br/>2. **系统开发**：构建了基于微软HoloLens 2的交互式AR演示系统，采用Unity、C#及Azure Cognitive Services实现语音到文本的实时转换。  <br/>3. **高精度语音识别**：通过Azure语音识别技术，在AR环境下实现高效、准确的语音转文本，提升用户操作便捷性。  <br/>4. **实际应用验证**：探讨AR技术在工业维护场景中对减少认知负担、优化工作流程及提高安全性的潜在价值，为实际部署提供理论依据。|
|2511.12609v2|[Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609v2)||
|2511.12609v1|[Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609v1)|**贡献点总结：**  <br/>1. 提出动态容量MoE架构，支持10种跨模态输入，平衡效率与能力；  <br/>2. 采用渐进式监督微调与迭代GSPO-DPO策略增强训练稳定性；  <br/>3. 独创多模态数据匹配技术，集成语音/图像生成标记；  <br/>4. 引入Omni-Modality 3D RoPE实现跨模态时空对齐；  <br/>5. 在85个基准测试中超越Qwen2.5-Omni，提升视频理解、音频视觉推理等任务性能。  <br/><br/>**总结（100字以内）：**  <br/>Uni-MoE 2.0通过动态MoE架构、强化训练策略与多模态数据技术，实现跨模态理解与生成，超越Qwen2.5-Omni，在多项任务中取得SOTA性能，提升语音处理与可控生成能力。|
|2511.11686v3|[Regularized Schrödinger Bridge: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686v3)|贡献点：  <br/>1. 提出Regularized Schrödinger Bridge (RSB)，将Schrödinger Bridge理论扩展至语音逆问题求解，解决感知质量与重建保真度的平衡难题。  <br/>2. 设计新颖的正则化训练策略，通过扰动输入和目标数据模拟预测误差，缓解暴露偏差问题。  <br/>3. 利用后验均值的精心设计插值方式，有效降低失真并提升模型鲁棒性。  <br/>4. 在语音增强典型逆问题实验中验证，RSB显著优于现有方法，同时提升失真指标与降低暴露偏差影响。  <br/><br/>总结：本研究提出RSB方法，解决语音逆问题中的失真-感知权衡与暴露偏差挑战，实验证明其在质量与鲁棒性上均优于现有技术。|
|2511.11686v2|[Regularized Schrödinger Bridge: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686v2)|贡献点总结（100字以内）：  <br/>提出Regularized Schrödinger Bridge（RSB）方法，通过创新正则化策略解决扩散模型在语音增强中的两个核心问题：感知质量与重建保真度的权衡、训练与推理输入不匹配导致的暴露偏差，实验验证效果显著优于现有方法。<br/><br/>分点贡献列表：  <br/>1. **提出RSB方法**：基于Schrödinger Bridge理论，设计适用于语音逆问题的正则化框架，解决扩散模型在质量与保真度间的矛盾。  <br/>2. **缓解暴露偏差**：通过扰动输入和目标数据，模拟预测误差，提升模型对训练-推理输入差异的鲁棒性。  <br/>3. **降低失真度**：利用后验均值的优化插值策略，减少生成结果的失真，同时保持高感知质量。  <br/>4. **实验验证优势**：在典型语音增强任务中，RSB显著提升失真指标（如PESQ、STOI），验证其有效性。|
|2511.10935v1|[CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding](https://arxiv.org/abs/2511.10935v1)|**贡献点总结：**<br/>1. 提出首个跨受试多模态BCI语音解码框架，融合EEG与EMG信号以解决普通话声调分类难题；  <br/>2. 引入基于神经-肌肉协作机制的混合特征提取与跨注意力融合架构；  <br/>3. 首次采用领域对抗训练提升跨受试泛化能力；  <br/>4. 仅用20个EEG和5个EMG通道实现高精度声调解码，验证了轻量化数据采集的可行性；  <br/>5. 在可听和无声语音条件下均超越现有基线，跨受试准确率分别达83.27%和85.10%；  <br/>6. 通过消融实验验证各组件的重要性，为实用化BCI系统提供理论支持。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出跨受试多模态BCI框架，融合EEG与EMG信号实现普通话声调解码，通过轻量模块与跨注意力机制提升性能，验证了最小通道解码的可行性，为实用化语音交互系统奠定基础。|
|2511.10670v1|[Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670v1)|**贡献总结（100字内）**：  <br/>提出基于MoE的语音投影器解决代码转换翻译挑战，结合多阶段训练与双损失机制优化语义建模，引入过渡损失弥合数据缺口，提升跨语言语音-文本对齐与翻译效果，验证方法的有效性和通用性。<br/><br/>**分点贡献**：  <br/>1. **MoE语音投影器**：通过语言专精专家（Language-Specific Experts）实现细粒度语义建模，提升多语言语音特征处理能力。  <br/>2. **多阶段训练框架**：利用单语ASR与ST数据，逐步构建跨语言对齐能力，降低对稀缺代码转换数据的依赖。  <br/>3. **双损失机制**：设计语言特定损失（跨专家组）与组内负载均衡损失（内部分配），高效引导模型分配token到合适专家。  <br/>4. **过渡损失**：缓解训练阶段间数据分布差异，实现代码转换场景的平滑适应，增强模型泛化能力。|
|2511.10232v1|[VocalNet-M2: Advancing Low-Latency Spoken Language Modeling via Integrated Multi-Codebook Tokenization and Multi-Token Prediction](https://arxiv.org/abs/2511.10232v1)|总结（100字以内）:  <br/>VocalNet-M2通过多codebook tokenizer和多token预测策略，显著降低语音模型延迟（从725ms至350ms），并在保持性能的同时避免flow-matching模型，为实时语音交互提供了高效解决方案。<br/><br/>贡献点:<br/>1. **提出多codebook tokenizer**：直接生成多codebook语音token，摒弃传统自回归生成流程和flow-matching模型，降低延迟。<br/>2. **创新MTP生成策略**：通过多token预测机制提升生成效率，优化模型性能。<br/>3. **实验证明有效性**：在主流SLMs中验证了延迟降低与性能保持的平衡，首次实现显著的首块延迟优化。<br/>4. **策略对比分析**：系统比较单codebook与多codebook方法，为低延迟SLMs设计提供理论依据与实践指导。|
|2511.10112v1|[FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features](https://arxiv.org/abs/2511.10112v1)|总结（100字以内）:  <br/>提出FabasedVC，融合文本模态信息与音素级SSL特征，结合持续预测器对齐语速韵律，提升音色、韵律、时长相似性及内容完整性，实验验证其优越性。<br/><br/>**贡献点分点：**  <br/>1. **端到端系统设计**：基于VITS构建集成文本模态（文本、音素、语调、BERT）、音素级SSL特征及持续预测器的VC系统，实现语义完整性与目标特征建模。  <br/>2. **多模态文本特征编码**：通过文本特征编码器融合多源文本属性，增强内容表征能力。  <br/>3. **音素级SSL特征处理**：采用平均池化与注意力机制（基于音素时长）将帧级SSL特征转换为音素级特征，提升特征准确性。  <br/>4. **持续预测器集成**：引入持续预测器精准对齐目标说话人语速与韵律，增强输出与目标的匹配度。  <br/>5. **全面性能验证**：实验表明方法在自然度、相似性及内容完整性等指标上显著优于现有系统。|
|2511.10090v1|[ELYADATA & LIA at NADI 2025: ASR and ADI Subtasks](https://arxiv.org/abs/2511.10090v1)|**贡献点分点总结：**  <br/>1. **提出混合方言语音处理系统**：联合Elyadata与LIA团队，在NADI 2025多方言阿拉伯语语音处理任务中首次实现跨任务的系统性研究。  <br/>2. **创新性模型架构**：针对ADI任务，采用微调后的Whisper-large-v3编码器结合数据增强技术，以79.83%的准确率登顶官方测试集。  <br/>3. **多方言ASR优化方法**：为八个阿拉伯语方言分别微调SeamlessM4T-v2 Large（埃及变种），实现平均WER 38.54%和CER 14.53%的显著性能。  <br/>4. **验证预训练模型有效性**：实验证明大型预训练语音模型（如Whisper、SeamlessM4T）通过精细化微调可有效提升多方言阿拉伯语音处理的准确性和鲁棒性。  <br/><br/>**总结（100字以内）**：  <br/>本研究通过微调Whisper和SeamlessM4T模型，结合数据增强技术，在多方言阿拉伯语语音处理任务中取得优异成绩，验证了预训练模型在方言识别与ASR中的高效性。|
|2511.09085v1|[Context-Aware Dynamic Chunking for Streaming Tibetan Speech Recognition](https://arxiv.org/abs/2511.09085v1)|**贡献点总结：**  <br/>1. 提出针对Amdo Tibetan的流式语音识别框架，基于混合CTC/Attention架构与上下文感知动态分块机制。  <br/>2. 动态分块策略根据编码状态自适应调整分块宽度，实现灵活接收场、跨分块信息交互及适应不同说话速率。  <br/>3. 构建基于藏语正字法的词典，提供语言驱动的建模单元以捕捉藏语语言特征。  <br/>4. 集成外部语言模型提升语义一致性与长句识别能力。  <br/>5. 实验验证：框架在测试集上达到6.23%的WER，较固定分块基线提升48.15%，同时减少延迟且性能接近全局解码。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出基于动态分块机制的Amdo Tibetan流式语音识别框架，结合混合CTC/Attention模型与语言学词典，集成语言模型提升性能，实现高效、准确的语音识别。|
|2511.08230v2|[VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230v2)|总结：提出针对中文的S2S基准VocalBench-zh，结构化评估12项用户需求，揭示模型共性挑战，推动下一代语音交互系统发展，开源代码与数据集促进研究。<br/><br/>贡献点：<br/>1. 构建首个中文专用S2S基准VocalBench-zh，填补领域评估空白；<br/>2. 采用能力等级分层设计，实现结构化、系统化评估体系；<br/>3. 覆盖12项用户导向语音交互核心能力指标；<br/>4. 通过14个主流模型实验揭示当前技术瓶颈与改进方向；<br/>5. 开源代码与数据集，推动语音交互模型公平比较与技术迭代。|
|2511.08230v1|[VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230v1)|贡献点：<br/>1. 提出首个针对中文的系统性S2S基准VocalBench-zh，填补领域空白<br/>2. 构建包含10个细分能力维度的评估体系，覆盖12类用户导向场景<br/>3. 收集整理超过10,000个高质量语料实例，确保数据多样性与规范性<br/>4. 通过14个主流模型的实验证明，揭示当前技术的共性挑战与改进方向<br/>5. 开源评估工具与数据集，推动中文语音交互技术的公平比较与持续发展<br/><br/>总结：本研究创建首个中文S2S评估基准VocalBench-zh，通过10个能力维度和10K高质量数据，系统揭示当前语音交互技术挑战，为模型改进提供方向，推动领域发展。|
|2511.08132v2|[National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution](https://arxiv.org/abs/2511.08132v2)|总结（100字以内）: <br/>SpeechCARE通过多模态融合和预训练模型提升ADRD早期检测，结合SHAP解释性模块与偏见缓解技术，实现高准确率分类，并计划拓展至真实医疗场景与EHR系统。<br/><br/>贡献点:<br/>1. 提出SpeechCARE系统：首个基于预训练多语言语音-语言Transformer模型的多模态语音处理框架，整合语音特征、语言结构和人口统计信息。<br/>2. 动态融合架构：采用Mixture of Experts范式，通过动态权重机制实现模态间有效融合，支持扩展社交因素、影像等多模态数据。<br/>3. 影像级预处理：包含自动转录、LLM异常检测与任务识别模块，提升数据质量与任务适应性。<br/>4. 可解释性模块：通过SHAP分析和LLM推理可视化各模态对诊断决策的贡献，增强模型可信度。<br/>5. 性能突破：在AD/MDA分类中达到AUC=0.88，MCI检测AUC=0.90，较传统方法显著提升。<br/>6. 偏见缓解：针对年龄偏见提出过采样和加权损失策略，提升模型公平性（对80岁以上群体的偏差最小化）。<br/>7. 临床落地规划：计划部署至真实医疗场景（如VNS Health）和整合EHR系统，服务纽约市少数群体。|
|2511.07883v2|[SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition](https://arxiv.org/abs/2511.07883v2)|总结：  <br/>本文提出MSTASA模块和SpikCommander架构，通过结合时序感知注意力与多视角学习，提升SNN在语音命令识别中的时序建模与通道特征整合能力，验证了其在参数更少情况下优于SOTA方法的高效性。<br/><br/>贡献点：  <br/>1. **提出MSTASA模块**：融合spiking temporal-aware attention与多视角学习框架，有效建模语音中互补的时序依赖关系，解决传统SNN对复杂时序模式捕捉不足的问题。  <br/>2. **设计SpikCommander架构**：构建全spike驱动的transformer模型，集成MSTASA与spiking contextual refinement channel MLP（SCR-MLP），联合优化时序上下文建模与通道特征整合。  <br/>3. **验证方法有效性**：在SHD、SSC、GSC三个基准数据集上的实验表明，SpikCommander在参数量更少的情况下，仍显著优于现有SOTA SNN方法，证明其高效率与鲁棒性。|
|2511.07677v1|[Speech Separation for Hearing-Impaired Children in the Classroom](https://arxiv.org/abs/2511.07677v1)|**贡献点：**  <br/>1. **提出MIMO-TasNet模型**：设计紧凑、低延迟、多通道架构，适合实时部署于双耳助听器/人工耳蜗等设备。  <br/>2. **模拟真实教室场景**：通过动态儿童-儿童/儿童-成人对话与噪声/距离变化，还原复杂教室环境。  <br/>3. **数据高效适应策略**：对比成人语音、教室数据及微调模型，验证教室特定训练对儿童语音分离的提升效果。  <br/>4. **微调与噪声增强**：仅需半量教室数据即可实现有效迁移学习，并通过扩散噪声提升模型鲁棒性。  <br/>5. **空间感知与泛化能力**：模型在保持空间线索的同时，有效泛化到未见过的距离场景。  <br/><br/>**总结：**  <br/>提出MIMO-TasNet模型，结合空间线索与数据迁移学习，提升儿童教室语音分离效果，推动实时助听设备发展。|
|2511.07107v1|[MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107v1)|总结：  <br/>MENTOR提出了一种元认知驱动的LGM框架，通过动态知识图、领域数据集和激活引导技术，有效解决LLMs在专业领域中的隐性风险问题，并提升其安全性和价值对齐能力。<br/><br/>贡献点：  <br/>1. **框架创新**：提出MENTOR框架，首次将元认知能力引入LLM的自我进化机制，用于识别和缓解领域任务中的隐性风险。  <br/>2. **自评工具**：设计新型元认知自评工具，替代传统人工评估，通过视角转换与后果推理实现模型自我反思。  <br/>3. **领域数据集**：发布包含9,000个风险查询的跨领域数据集（教育、金融、管理），强化模型对领域特定风险的识别能力。  <br/>4. **动态规则扩展**：基于元认知反馈动态生成补充规则知识图，扩展静态规则树，提升泛化能力并降低维护成本。  <br/>5. **推理强化技术**：采用激活引导方法，在推理阶段高效约束模型遵循规则，确保隐性风险缓解的适用性与稳定性。|
|2511.07065v1|[Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065v1)|**贡献点：**<br/>1. 提出SRA框架，通过显式对齐模型注意力与人类理据提升解释性与公平性  <br/>2. 首次将监督注意力机制与transformer分类器结合，优化联合目标函数（分类损失+对齐损失）  <br/>3. 在英语（HateXplain）和葡萄牙语（HateBRXplain）基准测试中验证方法有效性  <br/>4. 实现2.4倍于基线的可解释性提升，生成更忠实且符合人类预期的token级解释  <br/>5. 在公平性指标上表现优异，尤其在检测针对身份群体的有毒帖子中达到第二佳效果  <br/><br/>**总结：**  <br/>本研究提出SRA框架，通过监督注意力对齐提升仇恨言论检测的可解释性和公平性，实验证明其在多语言场景下兼具高解释性与竞争力的公平性表现。|
|2511.06988v1|[HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection](https://arxiv.org/abs/2511.06988v1)|**贡献点总结**（100字以内）：  <br/>提出HCFSLN框架，整合多模态数据进行焦虑检测；引入双曲嵌入与跨模态注意力增强特征可分性；构建首个大规模多模态焦虑数据集（108人）；在Few-Shot场景下实现88%准确率，优于现有基线14%。<br/><br/>**分点贡献**：  <br/>1. **提出HCFSLN框架**：首个结合语音、生理信号和视频的多模态Few-Shot焦虑检测模型，解决小样本数据下的过拟合问题。  <br/>2. **创新技术设计**：通过双曲嵌入、跨模态注意力机制和自适应门控网络提升特征分离能力，增强模型在低数据量下的鲁棒性。  <br/>3. **构建新数据集**：首次公开108人参与的多模态焦虑数据集，为相关研究提供高质量基准数据。  <br/>4. **实验验证效果**：在对比六种Few-Shot基线模型时，准确率达88%，性能提升14%，验证双曲空间建模焦虑语音模式的有效性。|
|2511.06150v1|[BSCodec: A Band-Split Neural Codec for High-Quality Universal Audio Reconstruction](https://arxiv.org/abs/2511.06150v1)|总结：  <br/>提出BSCodec架构，通过分频段独立压缩解决语音与非语音音频的频谱差异问题，提升多类型音频重建质量并保持语音性能优势，验证其在下游任务中的广泛应用潜力。<br/><br/>贡献点：  <br/>1. **提出分频段压缩架构**：首次将频谱维度拆分为独立频段，针对不同内容类型（语音/音乐/声音）分别优化压缩策略。  <br/>2. **解决频谱特征差异问题**：通过适配语音的窄带谐波和非语音的全频谱需求，克服传统全频带方法对不同音频类型的适应性不足。  <br/>3. **多类型音频性能提升**：在统一数据集中训练后，BSCodec在非语音音频（音乐、声音）重建质量上超越现有基线，同时保持语音领域的竞争力。  <br/>4. **下游任务有效性验证**：实验表明该方法在语音识别、语音合成等任务中具有更强的泛化能力和应用潜力。|
|2511.06036v1|[Towards Human-AI-Robot Collaboration and AI-Agent based Digital Twins for Parkinson's Disease Management: Review and Outlook](https://arxiv.org/abs/2511.06036v1)|**贡献点**：  <br/>1. **整合多模态传感与机器人干预**：首次提出将PD筛查/诊断的多模态传感（如IMU、语音分析）与康复机器人（SAR、VR平台）融合，打破传统研究领域的割裂。  <br/>2. **闭环AI-机器人框架设计**：构建“传感器-AI-机器人”闭环系统，通过AI代理实现患者、护理者与机器人（及医生）的动态交互，支持个性化干预。  <br/>3. **多AI模型协同应用**：引入LLMs、强化学习、持续学习等先进AI技术，实现多模态数据融合与决策耦合，提升系统自适应性。  <br/>4. **数字孪生技术应用**：提出基于AI的PD患者数字孪生体，实现疾病状态动态建模与智能化、患者中心化的长期管理。  <br/>5. **可解释性与情境感知**：强调系统需具备解释性（Explainable AI）和情境感知能力，推动AI在医疗场景中的可信部署。  <br/><br/>**总结**（100字内）：  <br/>论文提出整合PD多模态传感与机器人康复的闭环AI框架，融合LLMs等先进AI技术，构建患者数字孪生体，实现个性化、可解释的智能干预，推动医疗AI的跨领域协同与精准化应用。|
|2511.05945v1|[Loud-loss: A Perceptually Motivated Loss Function for Speech Enhancement Based on Equal-Loudness Contours](https://arxiv.org/abs/2511.05945v1)|总结：本研究提出一种基于心理声学的感知加权损失函数，通过等响曲线调节频域权重，解决了传统MSE对高频信息建模不足的问题，在语音增强任务中显著提升感知质量。<br/><br/>贡献点：<br/>1. 提出感知加权损失函数：基于心理声学原理，利用等响曲线为不同频段分配权重，更贴合人耳听觉特性。<br/>2. 解决MSE频段偏差问题：通过频率依赖的权重机制，缓解传统MSE对低频过度关注导致的高频建模不足。<br/>3. 模型通用性设计：该损失函数可应用于多种语音增强模型（如GTCRN），具有跨模型的泛化能力。<br/>4. 实验验证有效性：在VoiceBank+DEMAND数据集上实现WB-PESQ得分2.17→2.93的显著提升，证明感知质量优化效果。|
|2511.04995v1|[Enhancing Public Speaking Skills in Engineering Students Through AI](https://arxiv.org/abs/2511.04995v1)|总结：  <br/>开发多模态AI模型，融合语音、视觉与情感分析，提供个性化公共演讲评估，解决传统教学难以持续、高效反馈的难题。<br/><br/>贡献点：  <br/>1. **提出多模态评估框架**：整合语音分析（音高、音量、语速、语调）与计算机视觉（面部表情、手势、姿势）技术，首次将表达连贯性作为新型评估维度。  <br/>2. **实现个性化与可扩展反馈**：突破传统分离式评估，通过AI融合多模态数据，提供持续、定制化的学习建议。  <br/>3. **验证模型有效性**：初步测试显示AI反馈与专家评价具有中度一致性，Gemini Pro在LLM中表现最优，体现模型设计的可靠性。  <br/>4. **支持自主训练**：无需人工评估，允许学生反复练习以自然优化语音与肢体语言的协调性，提升专业沟通能力。|
|2511.04366v1|[Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction](https://arxiv.org/abs/2511.04366v1)|总结（100字以内）:<br/>本研究首次探索MLLM与语音语言治疗师在亲子联合注意分析中的对齐方法，通过观察-判断双阶段提示验证其可行性，揭示专家共识与分歧对模型性能的影响，为社会情境交互分析的AI应用提供实证参考。<br/><br/>贡献点分点:<br/>1. **提出跨领域对齐框架**：构建MLLM与语音语言治疗师（SLPs）协作分析亲子互动的新型对齐方法，聚焦早期社会沟通关键指标联合注意。<br/><br/>2. **开发双阶段提示策略**：创新性设计观察与判断分离的两阶段提示流程，模拟SLPs的专业分析流程，提升模型对社会互动的解析能力。<br/><br/>3. **揭示专家判断差异**：通过对比SLPs的观察共识与判断标准差异，发现模型在观察层表现更稳健，但判断层存在显著可信度挑战。<br/><br/>4. **建立行为分析基准**：基于三名SLPs的访谈与视频标注数据，构建具有临床意义的社会互动分析基准，为AI模型训练提供实证依据。<br/><br/>5. **推动AI应用实证研究**：作为专家-AI对齐的案例研究，强调将MLLM应用于社会情境交互分析的潜力与局限，为未来发展提供方向指引。|
|2511.03310v1|[TASU: Text-Only Alignment for Speech Understanding](https://arxiv.org/abs/2511.03310v1)|**贡献点分点列出：**  <br/>1. 提出TASU（Text-only Alignment for Speech Understanding），突破传统语音模型依赖音频-文本配对数据的限制，仅使用文本数据进行跨模态对齐。  <br/>2. 实现零样本语音识别的竞争力，无需对齐数据即可有效迁移至语音理解任务。  <br/>3. 将TASU作为课程学习的预训练阶段，提升语音识别模型对未见领域的泛化能力。  <br/>4. 在MMSU基准上显著优于GLM-4-Voice和Step-Audio等主流语音大模型，验证其高效性和可扩展性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出TASU，仅用文本数据实现语音模型跨模态对齐，突破传统依赖配对数据的瓶颈，显著提升零样本语音识别与领域泛化能力，并在MMSU基准上超越主流模型，展现高效、可扩展的语音理解潜力。|
|2511.03084v1|[Quantifying Articulatory Coordination as a Biomarker for Schizophrenia](https://arxiv.org/abs/2511.03084v1)|**贡献点总结（100字以内）：**  <br/>提出可解释的语音框架，利用特征值光谱差图和WSED方法量化声带协调性，有效区分复杂与简单模式，并关联BPRS整体严重度及正负症状平衡，为精神分裂症提供透明、敏感的生物标志物，推动临床可解释语音评估工具发展。  <br/><br/>**分点贡献：**  <br/>1. **方法创新**：开发结合特征值光谱差图与WSED（加权指数衰减）的新框架，量化语音中的发声机制协调性。  <br/>2. **特征区分**：利用特征值光谱图有效识别复杂与简单协调模式，WSED分数可靠分组，模糊范围局限。  <br/>3. **症状关联性**：WSED分数与BPRS整体严重度及正负症状平衡显著相关，揭示症状类型对协调模式的影响。  <br/>4. **临床价值**：提供透明、严重度敏感的生物标志物，促进基于语音的精神分裂症临床评估工具发展。|
|2511.02454v1|[Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech Enhancement on Discrete Codec Token](https://arxiv.org/abs/2511.02454v1)|**贡献点：**  <br/>1. **替代FAVOR+机制**：用双向选择结构状态空间模型（Hydra）替代FAVOR+，消除其近似误差，提升全局序列建模能力。  <br/>2. **线性复杂度设计**：通过结构化矩阵混合框架实现线性时间复杂度，避免自注意力的二次复杂度问题。  <br/>3. **性能验证**：在Genhancer模型上实验表明，所提方法显著优于DF-Conformer，验证其在语音增强任务中的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于双向结构状态空间模型的DF-Conformer替代方案，消除FAVOR+近似并保持线性复杂度，通过Hydra模型在语音增强任务中实现性能提升，验证了其高效性与有效性。|
|2510.24821v2|[Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821v2)||
|2510.18938v2|[StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction](https://arxiv.org/abs/2510.18938v2)|**贡献点：**  <br/>1. 提出首个端到端波形-波形模型StutterZero和StutterFormer，直接将口吃语音转换为流畅语音并联合预测转录，突破传统分阶段ASR-TTS流程的局限性。  <br/>2. 设计两种创新架构：StutterZero采用卷积-BiLSTM编码器-解码器与注意力机制，StutterFormer引入双流Transformer实现音素与语义的共享表示。  <br/>3. 在SEP-28K和LibriStutter数据合成的训练集上验证模型，首次在FluencyBank未见说话者数据上评估，体现泛化能力。  <br/>4. 实验结果显著优于基准模型（Whisper-Medium）：StutterZero WER降低24%，BERTScore提升31%；StutterFormer进一步优化至WER降28%，BERTScore升34%。  <br/>5. 验证端到端口吃-流畅语音转换的可行性，为无障碍AI、言语治疗及包容性人机交互提供新方法。  <br/><br/>**总结（100字以内）：**  <br/>该论文首次提出端到端波形-波形模型StutterZero和StutterFormer，直接转换口吃语音为流畅语音并同步预测转录，显著优于传统方法，在WER和语义相似度指标上取得突破，为无障碍AI与言语治疗提供新方向。|
|2510.04157v1|[GDiffuSE: Diffusion-based speech enhancement with noise model guidance](https://arxiv.org/abs/2510.04157v1)|**摘要（<100字）**  <br/>本文提出Guided diffusion for speech enhancement (GDiffuSE)，利用轻量辅助模型估计噪声分布并在扩散去噪过程中加入引导，实现对未见噪声的鲁棒适应，复用大规模生成式DDPM，实验在BBC噪声+LibriSpeech上显著超越现有基线。<br/><br/>**主要贡献**  <br/>- 首创将去噪扩散概率模型（DDPM）用于语音增强，并通过引导机制结合噪声分布估计。  <br/>- 设计轻量级辅助模型预测噪声分布，使扩散过程获得噪声信息的明确指引。  <br/>- 引入噪声引导显著提升对未见噪声类型的鲁棒性，实现无缝适配不同噪声场景。  <br/>- 能直接复用在大规模语音生成任务上预训练的 DDPM，降低专门针对 SE 的训练成本。  <br/>- 在 BBC 声效库噪声混入 LibriSpeech 的实验设定下，针对噪声不匹配条件，取得了持续且显著的性能提升，超过所有对比的最新基线。|