|Source|Title|Summary|
|---|---|---|
|2509.15952v1|[Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement](http://arxiv.org/abs/2509.15952v1)|**贡献点：**  <br/>1. 提出COSE框架，首次将一步流匹配模型应用于语音增强，解决传统多步生成的高计算成本和离散化误差问题。  <br/>2. 引入速度组合恒等式，高效计算平均速度场，显著降低Jacobian-vector product（JVP）训练开销，同时保持理论一致性。  <br/>3. 实验验证COSE在标准基准上实现5倍采样加速和40%训练成本降低，且未牺牲语音增强质量。  <br/>4. 提供开源代码，促进模型复现与进一步研究。|
|2509.15922v1|[DISPATCH: Distilling Selective Patches for Speech Enhancement](http://arxiv.org/abs/2509.15922v1)|总结：  <br/>提出DISPatch框架，针对性地在教师优于学生的频谱块进行蒸馏，结合MSSP多尺度方法提升性能，有效优化模型压缩效果。<br/><br/>贡献点：  <br/>1. **DISPatch框架**：首次提出基于Knowledge Gap Score的动态损失分配策略，仅在教师表现优于学生的频谱块进行蒸馏，避免学生模型重复学习教师错误预测区域。  <br/>2. **MSSP方法**：设计频率依赖的多尺度频谱块处理机制，针对低、高频带采用不同大小的块，适应语音频谱的异质性特征。  <br/>3. **兼容性验证**：将DISPatch整合至传统KD方法中，验证其在多种模型压缩场景下的有效性，实现更高效的性能提升。  <br/>4. **性能突破**：结合DISPatch与MSSP的混合方案，显著优于现有最先进的频率依赖KD方法，全面提升语音增强效果。|
|2509.15666v1|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v1)|总结（100字以内）:  <br/>本文提出TISDiSS框架，通过早期分割多损失监督、参数共享和动态推理重复实现训练与推理时间的可扩展性，可在不重新训练的情况下调整推理深度以平衡性能与速度，并在减少参数量的前提下达到SOTA效果，适用于低延迟源分离任务。<br/><br/>贡献点:  <br/>1. **统一框架设计**：首次将早期分割多损失监督、共享参数机制与动态推理重复结合，构建TISDiSS框架，实现训练与推理时间的分离可扩展。  <br/>2. **灵活速度-性能权衡**：通过调整推理深度而非重新训练模型，支持无需额外计算资源即可动态优化实时性与分离效果。  <br/>3. **系统性分析**：深入研究架构与训练策略，证明增加推理重复次数能提升浅层推理性能，为低延迟应用场景提供理论依据。  <br/>4. **高效性能验证**：在标准语音分离基准上实现SOTA性能，同时显著减少参数量，验证框架的可扩展性与实用性。|
|2509.14858v2|[MeanFlowSE: one-step generative speech enhancement via conditional mean   flow](http://arxiv.org/abs/2509.14858v2)|**贡献点总结**（100字以内）:  <br/>提出MeanFlowSE模型，通过单步生成和Jacobian-向量乘积技术，解决实时语音增强中多步骤推理瓶颈，无需外部教师，提升效率与质量，开源实现。<br/><br/>---<br/><br/>**详细贡献点分点**:<br/>1. **提出MeanFlowSE模型**：  <br/>   首次引入基于有限时间区间平均速度的条件生成模型，替代传统基于瞬时速度场的流/扩散框架，解决多步骤推理的计算效率问题。<br/><br/>2. **创新训练目标设计**：  <br/>   利用Jacobian-向量乘积（JVP）构建MeanFlow标识，推导出直接监督有限区间位移的本地训练目标，同时保持与瞬时场约束在对角线上的一致性。<br/><br/>3. **单步推理机制**：  <br/>   通过时间倒退的单步生成策略，避免依赖迭代ODE求解器，显著降低实时生成的计算成本。<br/><br/>4. **可选少步优化**：  <br/>   提供一种可选的少步生成变体，用于进一步提升生成质量，兼顾效率与精度。<br/><br/>5. **实验证明有效性**：  <br/>   在VoiceBank-DEMAND数据集上验证，单步MeanFlowSE在可懂度、保真度和感知质量上优于多步骤基线，且计算开销更低。<br/><br/>6. **无需外部依赖**：  <br/>   不依赖知识蒸馏或外部教师，实现高效、高保真实时语音增强框架，简化部署流程。<br/><br/>7. **开源实现**：  <br/>   提供开源代码（GitHub链接），推动技术复用与进一步研究。|
|2509.14858v1|[MeanFlowSE: one-step generative speech enhancement via conditional mean   flow](http://arxiv.org/abs/2509.14858v1)|**贡献点：**  <br/>1. 提出MeanFlowSE，通过学习有限时间间隔的平均速度场替代传统流/扩散模型的瞬时速度场，解决实时语音增强中的多步推理瓶颈。  <br/>2. 利用Jacobian-向量积（JVP）构建MeanFlow身份，设计局部训练目标直接优化有限区间位移，保持对角线瞬时场约束一致性。  <br/>3. 引入单步推理机制，通过反向时间位移生成语音，摆脱对多步ODE求解器的依赖；提供可选少步变体以提升生成质量。  <br/>4. 在VoiceBank-DEMAND数据集上验证方法有效性，单步模型在计算成本显著降低的同时保持高可懂性、保真度和感知质量。  <br/>5. 方法无需知识蒸馏或外部教师，实现高效、高保真的实时语音增强框架。  <br/><br/>**总结：**  <br/>提出MeanFlowSE模型，通过平均速度场与单步推理机制解决实时语音增强的多步计算问题，实现高效率和高质量性能，无需额外资源。|
|2509.14855v1|[AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning](http://arxiv.org/abs/2509.14855v1)|总结：  <br/>提出AmbiDrop框架，通过Ambisonics编码和通道丢弃技术实现阵列无关的语音增强，无需依赖多样化麦克风阵列数据库，在未见过的阵列布局上保持高性能。<br/><br/>贡献点：  <br/>1. **提出阵列无关的语音增强框架**：基于Ambisonics Signal Matching（ASM）将任意麦克风阵列录音转换为球面调和域，解决了传统方法对特定阵列几何的依赖问题。  <br/>2. **简化数据需求**：利用模拟Ambisonics数据训练深度神经网络，并通过通道丢弃增强鲁棒性，无需真实多几何数据库。  <br/>3. **验证泛化能力**：在未见过的阵列布局上对SI-SDR、PESQ和STOI指标均实现提升，证明了方法的通用性和实际应用潜力。|
|2509.14379v1|[Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy   Environments with Noise Prior](http://arxiv.org/abs/2509.14379v1)|贡献点：  <br/>1. 提出生成式无监督单麦克风语音分离方法，直接建模干净语音与结构化噪音，而非依赖混音信号；  <br/>2. 引入音频-视觉评分模型，利用视觉线索作为强语音生成先验；  <br/>3. 显式建模语音与噪音分布，结合逆问题范式实现有效分解；  <br/>4. 通过反向扩散过程从后验分布采样，直接估计并消除噪音以恢复语音信号；  <br/>5. 实验验证该方法在复杂声学环境下的有效性。  <br/><br/>总结：  <br/>本文提出基于音频-视觉评分模型的生成式无监督语音分离方法，通过显式建模语音与噪音分布并结合反向扩散过程，有效解决了单麦克风环境下语音分离问题，验证了直接建模噪声的优势。|
|2509.14076v1|[A Lightweight Fourier-based Network for Binaural Speech Enhancement with   Spatial Cue Preservation](http://arxiv.org/abs/2509.14076v1)|**贡献点：**  <br/>1. 提出Global Adaptive Fourier Network (GAF-Net)，首次结合轻量化设计与双耳语音增强性能平衡。  <br/>2. 创新性地采用双特征编码器（STFT + gammatone特征），提升声学表征的鲁棒性。  <br/>3. 设计通道无关的全局自适应傅里叶调制器，高效捕捉长时序依赖并保留空间信息。  <br/>4. 引入动态门控机制，显著降低处理伪影，提升输出质量。  <br/>5. 实验验证GAF-Net在双耳线索（ILD/IPD误差）和客观可懂度（MBSTOI）指标上表现优异，且参数与计算成本显著低于现有方法。  <br/><br/>**总结（100字以内）：**  <br/>GAF-Net通过双特征编码、全局自适应调制与动态门控机制，实现轻量化双耳语音增强，在保持高性能的同时降低计算成本，适用于资源受限设备。|
|2509.13825v1|[Neural Speech Separation with Parallel Amplitude and Phase Spectrum   Estimation](http://arxiv.org/abs/2509.13825v1)|**贡献点：**  <br/>1. 提出APSS模型，首次在语音分离中显式估计相位谱以提升分离完整性与准确性。  <br/>2. 引入特征融合器，将时间和频率信息整合为联合表示以增强模型表征能力。  <br/>3. 采用并行振幅与相位分离器，结合时间频率Transformer捕捉复杂时频依赖关系。  <br/>4. 通过iSTFT重构分离信号，实现端到端高效语音分离。  <br/>5. 实验验证APSS在时域方法和隐式相位估计方法上均表现更优，具备强泛化能力与实用价值。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出APSS模型，通过显式相位谱估计与并行分离器设计，在语音分离任务中实现更准确的信号重构，实验表明其在多数据集上均优于时域和隐式相位方法，具备良好通用性与应用潜力。|
|2509.12974v1|[The CCF AATC 2025: Speech Restoration Challenge](http://arxiv.org/abs/2509.12974v1)|总结：  <br/>本文提出Speech Restoration Challenge，构建包含多类复合失真的测试数据集，设计全面的评估框架，推动语音增强在复杂环境下的研究。<br/><br/>贡献点：  <br/>1. 提出首个针对多重复合语音失真的研究挑战（Speech Restoration Challenge），聚焦非稳态噪声、混响及预处理伪影的联合修复。  <br/>2. 构建综合性语音退化数据集，涵盖复杂声学失真、信号链伪影和二次增强伪影三种主流退化类型。  <br/>3. 设计兼顾客观性能评估与模型复杂度的双维度评价协议，促进算法的实用性与效果平衡。  <br/>4. 提供标准化任务框架与公开资源，推动语音增强领域的研究范式革新。|
|2509.10143v1|[Error Analysis in a Modular Meeting Transcription System](http://arxiv.org/abs/2509.10143v1)|**贡献点：**<br/>1. **框架扩展**：提出改进的语音分离泄漏分析框架，增强对时间局部性的敏感性。  <br/>2. **泄漏现象揭示**：发现仅主说话人活动时存在显著交叉通道泄漏，但VAD将其忽略，故对最终性能影响有限。  <br/>3. **分割方法对比**：比较不同分割策略，证明先进对齐技术可将分割效果与理想分割的差距缩小至三分之一。  <br/>4. **差异因素分析**：阐明剩余性能差距的成因，为优化提供理论依据。  <br/>5. **性能突破**：在仅使用LibriSpeech训练的系统中，于LibriCSS数据集上取得最先进性能。  <br/><br/>**总结：**  <br/>该研究通过优化框架和分割方法，揭示语音分离中泄漏的影响机制，并在LibriCSS上实现现有系统中最佳性能。|
|2509.09201v1|[DeCodec: Rethinking Audio Codecs as Universal Disentangled   Representation Learners](http://arxiv.org/abs/2509.09201v1)|总结：  <br/>提出DeCodec模型，实现语音与背景音的解耦表示，支持灵活特征选择，提升语音增强、ASR鲁棒性和TTS背景音控制等应用效果。<br/><br/>贡献点：  <br/>1. **提出新型通用音频编解码器DeCodec**：首次将音频表示解耦为语音和背景音的正交子空间，并在语音层进一步分解为语义和语用成分，实现分层解纠缠。  <br/>2. **双技术创新**：  <br/>   - 引入子空间正交投影模块，将输入音频分解为独立的正交子空间；  <br/>   - 设计表示交换训练过程，确保子空间分别对应语音和背景音。  <br/>3. **语义引导的语音子空间分解**：通过语义指导实现语音成分的语义-语用分离，增强模型对语音信号的理解。  <br/>4. **多功能应用验证**：在保持高质量信号重建的基础上，展示语音增强、单次语音转换、ASR鲁棒性和TTS的背景音控制等新能力。|
|2509.08470v1|[Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition](http://arxiv.org/abs/2509.08470v1)|总结：  <br/>提出Sparse MERIT框架，结合自监督表示与动态专家路由机制，有效解决噪声环境下SER与SE任务的性能下降问题，实现参数高效且任务自适应的表示学习。<br/><br/>贡献点：<br/>1. **首次提出框架融合**：提出将语音增强（SE）与情感识别（SER）任务结合的多任务学习框架Sparse MERIT，通过帧级专家路由机制优化任务协同。<br/>2. **动态专家选择机制**：引入任务特定的门控网络，动态从共享专家池中选择适配当前语音帧的专家，减少冗余计算与表示冲突。<br/>3. **参数高效性**：通过稀疏专家路由实现参数共享与任务适应性，降低模型复杂度与训练成本，同时保持高任务性能。<br/>4. **噪声鲁棒性验证**：在MSP-Podcast数据集上验证，Sparse MERIT在-5 dB极端噪声条件下显著提升SER F1-macro（+12.0%）和SE SSNR（+28.2%），并在未知噪声场景中具备统计显著性。|
|2509.07521v1|[Target matching based generative model for speech enhancement](http://arxiv.org/abs/2509.07521v1)|总结：本文提出基于目标的生成框架，优化均值/方差时间表设计，消除训练损失中的随机项，创新扩散骨干网络，显著提升语音生成效率与质量。<br/><br/>贡献点：<br/>1. 提出新型目标导向生成框架，兼顾均值/方差时间表灵活性与训练推理效率<br/>2. 通过重构训练损失去除随机项，实现更稳定高效的语音增强流程<br/>3. 引入logistic均值调度与bridge方差调度，优化信噪比轨迹提升扰动策略效率<br/>4. 开发新型音频扩散骨干网络，通过建模长时帧相关和跨频段依赖显著降低NCSN++的计算复杂度|
|2509.07341v1|[Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation](http://arxiv.org/abs/2509.07341v1)|总结：  <br/>提出AFN-HearNet模型，通过跨域特征融合解决噪声抑制与听力补偿任务分离问题，显著提升性能与效率，验证了各模块的有效性。<br/><br/>贡献点：  <br/>1. **提出联合处理框架AFN-HearNet**：首次将噪声抑制（NR）与听力损失补偿（HLC）任务统一处理，突破传统分离处理的局限。  <br/>2. **设计audiogram-specific编码器**：将稀疏的audiogram特征转化为深层表示，解决跨域特征对齐难题。  <br/>3. **引入affine modulation机制**：通过频率-时间Conformer模块实现NR与HLC特征的自适应融合，增强任务间交互建模。  <br/>4. **添加VAD辅助训练任务**：隐式嵌入语音与非语音模式，优化统一表示的学习效果。  <br/>5. **实验证明有效性**：跨多数据集验证模型模块的贡献，显著提升HASQI和PESQ等关键指标性能。|
|2509.05079v1|[Lightweight DNN for Full-Band Speech Denoising on Mobile Devices:   Exploiting Long and Short Temporal Patterns](http://arxiv.org/abs/2509.05079v1)|总结（100字以内）:  <br/>本文提出一种低延迟、轻量化且适用于全带信号的语音降噪方法，结合修改UNet架构与RNN，有效提取短时和长时时序特征，并在移动端实现实时因子低于0.02，优于现有方法。<br/><br/>---  <br/>**贡献点**  <br/>1. **针对资源受限平台优化**：设计了轻量化且低延迟的DNN模型，适用于移动端等计算资源有限的场景。  <br/>2. **全带信号处理能力**：支持48kHz采样率的全带（FB）语音信号降噪，解决现有方法对高频信号处理不足的问题。  <br/>3. **时序模式融合**：通过look-back帧、卷积核时序跨度和RNN模块，同时建模短时与长时语音时序特征。  <br/>4. **因果框架设计**：基于因果性框架的帧级处理，确保实时性并避免未来信息泄露。  <br/>5. **高效网络结构**：结合MobileNet的倒置瓶颈设计与因果实例归一化，提升计算效率与性能。  <br/>6. **实验证明优越性**：在公开数据集上验证方法有效性，SI-SDR指标优于现有全带与低延迟SD方法。|
|2509.04280v1|[Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation](http://arxiv.org/abs/2509.04280v1)|总结： <br/>LaDen提出首个针对语音增强的测试时适应方法，通过预训练表示与线性转换实现跨领域泛化，并建立综合基准验证效果。<br/><br/>贡献点：<br/>1. 提出LaDen（latent denoising）方法，首次解决语音增强模型在域移场景下的测试时适应问题<br/>2. 构建跨领域语音表示转换框架，利用线性映射将噪声嵌入转化为干净语音表示<br/>3. 创建包含多数据集、多种域移类型的综合基准测试平台（噪声类型/说话人特征/语言变化）<br/>4. 通过大量实验证明LaDen在感知指标上显著优于传统方法，尤其对说话人和语言域移表现突出|
|2509.02571v1|[Gaussian Process Regression of Steering Vectors With Physics-Aware Deep   Composite Kernels for Augmented Listening](http://arxiv.org/abs/2509.02571v1)|总结：  <br/>本文提出基于神经场与高斯过程的物理感知复合核模型，有效解决声场转向向量超分辨率中的过拟合问题，在数据不足条件下实现高精度空间滤波和双耳渲染，显著减少测量需求。<br/><br/>贡献点：  <br/>1. 提出物理感知的复合核模型，同时建模入射波方向与声场散射效应，突破传统代数方法的局限；  <br/>2. 首次将神经场（NF）与高斯过程（GP）结合，构建基于概率框架的声场连续表示方法；  <br/>3. 通过引入不确定性建模，有效缓解超分辨率过程中的过拟合问题；  <br/>4. 在数据不足场景下（如少于十倍测量）实现接近理想性能的声场重建，验证方法有效性；  <br/>5. 适用于空间滤波、双耳渲染等下游任务，推动增强听觉技术的实际应用。|
|2509.01889v2|[From Evaluation to Optimization: Neural Speech Assessment for Downstream   Applications](http://arxiv.org/abs/2509.01889v2)|总结：  <br/>该论文系统总结了基于神经网络的语音评估模型在语音处理中的双重应用价值，并探讨了其局限性及未来发展方向，推动了语音评估与合成/增强技术的深度融合。<br/><br/>贡献点：  <br/>1. **提出非侵入性评估方法**：通过无需干净参考信号的神经网络模型，解决传统客观指标在实际应用中因参考信号缺失导致的局限性。  <br/>2. **构建可微分感知代理**：将语音评估模型作为可微分的感知代理，既用于评估语音质量与可懂度，又直接指导语音增强和合成模型的优化。  <br/>3. **实现高效特征检测**：利用模型检测语音中的显著特征，为下游处理任务（如降噪、语音识别）提供更精准的输入，提升处理效率。  <br/>4. **分析集成挑战与未来方向**：系统梳理当前语音评估模型在处理流程中的应用瓶颈，并提出未来研究重点，如模型泛化性与实时性改进。|
|2509.01399v1|[CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech   Separation with Distributed Heterogeneous Arrays](http://arxiv.org/abs/2509.01399v1)|总结（100字以内）:  <br/>CabinSep通过融合通道信息、应用MVDR策略和混合数据增强方法，显著提升语音分离效果，降低ASR错误率，且具备极低计算复杂度。<br/><br/>贡献点分点列出:  <br/>1. **创新性空间特征提取**：首次将通道信息用于提取空间特征，提升语音与噪声掩码估计的准确性，增强分离效果。  <br/>2. **ASR优化的MVDR应用**：在推理阶段引入MVDR技术，减少语音失真并提升ASR友好性，降低后端识别错误率。  <br/>3. **混合数据增强方法**：结合模拟与真实环境下的脉冲响应（IRs），改善说话人定位精度，进一步减少识别错误。  <br/>4. **高效计算性能**：模型计算复杂度仅0.4 GMACs，相比SOTA模型DualSep实现17.5%的相对识别错误率下降。|
|2509.00988v1|[A Unified Denoising and Adaptation Framework for Self-Supervised Bengali   Dialectal ASR](http://arxiv.org/abs/2509.00988v1)|总结：该研究提出针对孟加拉语方言和噪声的统一ASR框架，结合WavLM去噪预训练与多阶段微调策略，显著提升识别性能，建立新SOTA并为低资源语言ASR提供可扩展方案。<br/><br/>贡献点：<br/>1. 首次针对孟加拉语方言多样性与环境噪声双重挑战，提出统一解决方案框架<br/>2. 构建基于WavLM的去噪预训练模型，通过掩码语音去噪目标增强对声学失真的鲁棒性<br/>3. 创新性地设计多阶段微调策略：先标准化方言适配，后通过特定数据增强实现噪声鲁棒性优化<br/>4. 在涵盖多方言与多噪声场景的综合基准上进行系统性验证，建立全面评估体系<br/>5. 实验结果超越现有主流模型（wav2vec2.0、Whisper），推动低资源语言ASR技术发展<br/>6. 提供可复用的框架范式，为其他具有高变异性的低资源语言ASR系统设计提供参考 blueprint|
|2509.00405v2|[SaD: A Scenario-Aware Discriminator for Speech Enhancement](http://arxiv.org/abs/2509.00405v2)|总结（100字以内）:  <br/>本文提出场景感知判别器，通过捕捉场景特征和频域分割提升语音增强质量评估，实验证明该方法可适配多种生成器架构，有效提升不同场景下的性能表现。<br/><br/>贡献点：<br/>1. **场景感知机制创新**：设计新型判别器，融入场景特征提取能力，突破传统模型对场景上下文的忽视。<br/>2. **频域分割评估策略**：首次在语音增强中引入频域划分技术，实现对生成语音质量的精细化、场景化判断。<br/>3. **架构无关的通用性**：验证方法对多种生成器架构（如3个代表性模型）均适用，无需修改生成器结构即可提升性能，拓展了模型应用范围。|
|2508.20859v1|[Leveraging Discriminative Latent Representations for Conditioning   GAN-Based Speech Enhancement](http://arxiv.org/abs/2508.20859v1)|**贡献点：**<br/>1. **提出DisCoGAN方法**：首次将判别式语音增强模型的潜在特征作为通用条件特征，用于改进GAN-based语音增强性能。  <br/>2. **系统评估传统架构**：对比分析不同GAN结构（端到端、预处理阶段、后滤波）及判别式模型在低SNR下的表现。  <br/>3. **消融实验分析**：深入探究DisCoGAN内部组件对性能的影响，验证判别式条件方法的关键作用。  <br/>4. **性能优化成果**：在低SNR场景和实际录音中显著优于现有方法，同时保持高SNR和高精度下的竞争力。  <br/><br/>**总结（100字以内）**：  <br/>提出DisCoGAN方法，利用判别式模型潜在特征提升GAN语音增强性能；系统评估不同架构与条件方法，验证其在低SNR下的有效性及优势。|
|2508.20584v1|[Flowing Straighter with Conditional Flow Matching for Accurate Speech   Enhancement](http://arxiv.org/abs/2508.20584v1)|总结：  <br/>该论文提出独立条件流匹配方法，量化路径直度对语音增强的影响，并设计一步推断方案，证明时间独立路径优于时间依赖弯曲路径。<br/><br/>贡献点：  <br/>1. **提出独立条件流匹配框架**：直接建模噪声与干净语音之间的直路径，提升生成效果。  <br/>2. **量化路径直度影响**：通过实验验证路径直度对语音增强质量的关键作用。  <br/>3. **改进Schrodinger桥配置**：展示特定设置可使概率路径更接近直线，增强模型表现。  <br/>4. **一步推断解决方案**：减少传统多步推断的复杂性，将训练模型视为直接预测模型。  <br/>5. **对比路径类型优势**：基于实验发现，证明时间独立路径在生成语音增强中优于时间依赖路径。|
|2508.20474v1|[Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](http://arxiv.org/abs/2508.20474v1)|总结：  <br/>提出一种统一的多说话人编码器（UME），通过共享基础模型实现多任务联合训练，创新性引入RWSE机制提升任务间对齐效果，在LibriMix数据集上显著优于单任务基线。<br/><br/>贡献点：  <br/>1. **提出多任务统一架构**：首次将speaker diarization（SD）、speech separation（SS）、multi-speaker ASR整合到单一模型中，通过共享语音基础编码器提升参数效率与表征能力。  <br/>2. **设计残差加权和编码（RWSE）**：利用多层隐藏表征的加权组合，有效融合不同语义层次信息，促进任务间的自底向上对齐与协作学习。  <br/>3. **提升多任务性能**：联合训练策略捕捉任务间的内在依赖，显著增强在重叠语音数据上的整体处理效果，并在SD任务中取得1.37%（Libri2Mix）和2.29%（Libri3Mix）的SOTA diarization error rates。|
|2508.19583v1|[Lightweight speech enhancement guided target speech extraction in noisy   multi-speaker scenarios](http://arxiv.org/abs/2508.19583v1)|**贡献点总结：**  <br/>1. 提出轻量级语音增强模型GTCRN，用于改善目标语音提取（TSE）在复杂噪声环境下的性能。  <br/>2. 基于SEF-PNet框架开发两个扩展方法LGTSE与D-LGTSE，通过噪声-无关植入和扩展训练条件提升鲁棒性。  <br/>3. 设计两阶段训练策略，结合GTCRN预训练与联合微调，最大化模型潜力。  <br/>4. 在Libri2Mix数据集上实现显著性能提升（SISDR+0.89 dB，PESQ+0.16，STOI+1.97%）。  <br/><br/>**100字内总结：**  <br/>本文提出轻量级GTCRN模型及LGTSE/D-LGTSE扩展方法，通过前期降噪与动态训练条件增强，结合两阶段训练策略，显著提升了TSE在复杂多说话人噪声场景下的性能，并在数据集上验证了有效性。|
|2508.19528v1|[FLASepformer: Efficient Speech Separation with Gated Focused Linear   Attention Transformer](http://arxiv.org/abs/2508.19528v1)|总结:  <br/>提出Focused Linear Attention架构，通过线性复杂度改进实现高效语音分离，结合新Gated模块提升性能，并在多个数据集上验证了模型的效率优势。<br/><br/>贡献点:  <br/>1. 提出Focused Linear Attention机制，突破Transformer注意力模块的二次时间复杂度瓶颈  <br/>2. 构建FLASepformer模型，首次实现语音分离任务的线性时间复杂度处理  <br/>3. 开发FLA-SepReformer和FLA-TFLocoformer两个变体，扩展应用场景并优化性能  <br/>4. 引入新型Gated模块，显著提升分离效果和模型稳定性  <br/>5. 在多个基准数据集上验证模型有效性，证实可保持SOTA性能同时降低内存消耗和提升推理速度|
|2508.19483v1|[Audio-Visual Feature Synchronization for Robust Speech Enhancement in   Hearing Aids](http://arxiv.org/abs/2508.19483v1)|总结：  <br/>本研究提出轻量级跨注意力模型，实现音频-视觉同步提升实时语音增强性能，显著优化噪声抑制与可懂度。<br/><br/>贡献点：  <br/>1. **提出轻量级跨注意力模型**：利用大规模数据和简单架构，学习鲁棒的音频-视觉特征表示，提升多模态融合效率。  <br/>2. **构建高效特征对齐模块**：优化音频-视觉特征同步机制，增强特征对齐的准确性与实时性。  <br/>3. **实现低延迟实时处理**：达到36ms的最小延迟和低能耗，适用于助听器等实际应用场景。  <br/>4. **显著提升性能指标**：在AVSEC3数据集上优于基线，在感知质量（PESQ:0.52）、可懂度（STOI:19%）和保真度（SI-SDR:10.10dB）方面均有显著改进。|
|2508.18913v1|[A Framework for Robust Speaker Verification in Highly Noisy Environments   Leveraging Both Noisy and Enhanced Audio](http://arxiv.org/abs/2508.18913v1)|贡献点：<br/>1. 提出基于Siamese架构的新型神经网络框架，首次结合噪声和增强语音的说话人嵌入以增强验证鲁棒性  <br/>2. 构建轻量级系统，无需修改现有说话人验证和语音增强技术即可兼容多种SOTA解决方案  <br/>3. 解决语音增强可能破坏说话人独特特征的问题，通过互补信息融合提升抗噪性能  <br/>4. 通过实验验证了框架的有效性，证明其在低信噪比环境下的优越表现  <br/><br/>总结：  <br/>本文提出一种轻量级Siamese框架，通过融合噪声与增强语音的说话人嵌入，有效提升说话人验证在恶劣环境下的鲁棒性，兼容多种技术并验证其性能优势。|
|2508.17980v1|[Objective and Subjective Evaluation of Diffusion-Based Speech   Enhancement for Dysarthric Speech](http://arxiv.org/abs/2508.17980v1)|总结：  <br/>本研究首次提出将扩散模型应用于构音障碍语音增强，通过对比两种扩散模型和一种传统信号处理方法，验证其对提升语音质量和ASR性能的有效性，并探索微调ASR模型的潜在优化路径。<br/><br/>贡献点：  <br/>1. **方法创新**：首次系统探索扩散模型在构音障碍语音增强中的应用，提出通过扩散过程将异常语音分布逼近正常语音分布的新思路。  <br/>2. **算法对比**：对比两种扩散模型与一种信号处理算法在构音障碍语音增强中的效果，评估其对语音质量与识别性能的影响。  <br/>3. **多维度评估**：结合主观与客观指标，全面分析增强前后语音质量及ASR性能（使用Whisper-Turbo），验证技术有效性。  <br/>4. **模型优化**：通过微调Whisper-Turbo模型，探讨语音增强对识别性能的进一步提升潜力，拓展ASR系统的适应性。|
|2508.15473v1|[EffortNet: A Deep Learning Framework for Objective Assessment of Speech   Enhancement Technologies Using EEG-Based Alpha Oscillations](http://arxiv.org/abs/2508.15473v1)|总结：本文提出EffortNet，通过融合自监督、增量和迁移学习解决EEG个体差异问题，验证alpha波作为听力努力指标的有效性，并在数据效率和应用价值方面取得创新突破。<br/><br/>贡献点：<br/>1. 提出EffortNet框架：首个结合自监督、增量和迁移学习的深度学习模型，专门用于从EEG信号中解码个体听力努力。<br/>2. 验证alpha震荡作为生物标志物：通过实验证明alpha频段（8-13Hz）功率在嘈杂语音处理中显著升高，确立其作为客观听力努力指标的有效性。<br/>3. 提高模型泛化能力：通过仅需40%新受试者数据即可达到80.9%准确率，显著超越传统CNN和STAnet模型。<br/>4. 揭示语音增强效果差异：发现Transformer增强语音的神经反应更接近自然语音，与主观评价相反但符合客观指标。<br/>5. 提供实际应用价值：为个性化听觉技术评估和认知感知的语音增强系统设计提供新思路。|
|2508.14709v1|[Improving Resource-Efficient Speech Enhancement via Neural   Differentiable DSP Vocoder Refinement](http://arxiv.org/abs/2508.14709v1)|**贡献点总结：**  <br/>1. **高效端到端框架**：提出结合DDSP vocoder的轻量级端到端语音增强系统，适应嵌入式设备。  <br/>2. **紧凑神经网络预测特征**：通过小型化网络提取噪声语音的频谱包络、F0和周期性等关键声学特征。  <br/>3. **混合损失训练策略**：采用STFT与对抗损失联合优化，实现特征与波形层面的直接提升。  <br/>4. **低计算成本与高效果平衡**：在不显著增加计算量的前提下，提升可懂度（+4% STOI）和质量（+19% DNSMOS）。  <br/>5. **实际应用验证**：证明方法适合实时场景，为可穿戴设备中的语音增强提供可行性方案。  <br/><br/>**摘要总结（100字内）：**  <br/>提出高效端到端语音增强框架，结合紧凑神经网络与DDSP vocoder，通过STFT和对抗损失优化，在保持低计算成本的同时显著提升语音可懂度与质量，适用于可穿戴设备的实时应用。|
|2508.14623v1|[A Study of the Scale Invariant Signal to Distortion Ratio in Speech   Separation with Noisy References](http://arxiv.org/abs/2508.14623v1)|**贡献点总结（100字以内）：**  <br/>本研究提出通过WHAM!增强参考信号和混合数据，解决监督语音分离中噪声参考对SI-SDR的限制问题，揭示SI-SDR与主观感知噪声的负相关，验证了数据预处理对提升分离质量的关键作用。  <br/><br/>**分点贡献：**  <br/>1. **理论分析**：推导SI-SDR在噪声参考下的性能限制，指出噪声会导致分离结果的失真或降低SI-SDR值。  <br/>2. **方法创新**：提出基于WHAM!的参考信号增强与混合数据噪声增强策略，旨在避免模型学习噪声参考。  <br/>3. **实验验证**：在WSJ0-2Mix和Libri2Mix数据集上验证方法效果，发现分离语音噪声减少，但处理可能引入伪影，影响整体质量。  <br/>4. **指标关联性**：揭示SI-SDR与感知噪声间的负相关关系，强调SI-SDR作为质量评估指标的局限性。|
|2508.14525v1|[EffiFusion-GAN: Efficient Fusion Generative Adversarial Network for   Speech Enhancement](http://arxiv.org/abs/2508.14525v1)|总结：  <br/>EffiFusion-GAN通过轻量化网络结构、增强注意力机制和动态剪枝技术，在语音增强任务中实现高效性能，显著优于同参数量模型。<br/><br/>贡献点：  <br/>1. **轻量化网络设计**：采用深度可分离卷积与多尺度块结合，高效提取多样化的声学特征。  <br/>2. **改进注意力机制**：引入双归一化与残差细化模块，提升模型训练稳定性和收敛效率。  <br/>3. **动态剪枝技术**：通过动态剪枝降低模型规模，保持性能的同时适配资源受限环境。  <br/>4. **性能验证**：在VoiceBank+DEMAND数据集上取得3.45 PESQ得分，验证了模型在相同参数设置下的优越性。|
|2508.13624v1|[Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement](http://arxiv.org/abs/2508.13624v1)|**贡献点：**  <br/>1. **提出AVSEMamba模型**：首次将全脸视觉信息与Mamba时序架构结合，解决多说话人语音增强问题（如鸡尾酒会场景）。  <br/>2. **性能突破**：在AVSEC-4挑战的开发与盲测数据集上，显著提升语音可懂度（STOI）、感知质量（PESQ）和非侵入式质量（UTMOS）指标。  <br/>3. **榜单领先**：在单耳模型排行榜上取得第一名，验证了其在复杂环境下的优越性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出AVSEMamba，融合音频与全脸视觉信息，解决多说话人语音增强难题，性能优于现有单耳基线，并在排行榜上夺冠。|
|2508.13576v1|[End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in   Noisy Environments](http://arxiv.org/abs/2508.13576v1)|总结：  <br/>该研究提出了一种结合音频-视觉语音增强与深度学习的新型助听器系统AVSE-ECS，通过端到端联合训练显著提升嘈杂环境下的语音可懂度，验证了多模态数据整合在CI技术中的有效性。<br/><br/>贡献点：  <br/>1. **提出新型CI系统**：设计AVSE-ECS系统，将音频-视觉语音增强（AVSE）作为预处理模块与深度学习的ElectrodeNet-CS策略结合。  <br/>2. **端到端联合训练方法**：首次采用联合训练框架实现AVSE与深度学习模型的协同优化，提升系统整体性能。  <br/>3. **视觉信息整合**：创新性地引入视觉辅助数据，增强多模态语音处理在嘈杂环境中的鲁棒性。  <br/>4. **实验验证有效性**：在噪声条件下验证了该方法相较于传统ECS策略的优越性，指标提升显著。|
|2508.13028v1|[Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic   Speech Synthesis](http://arxiv.org/abs/2508.13028v1)|总结：  <br/>本研究提出结合讽刺检测反馈损失与两阶段迁移微调方法，提升TTS生成讽刺语音的能力，显著改善质量、自然度与讽刺传达效果。<br/><br/>贡献点：  <br/>1. **引入双模讽刺检测反馈机制**：将语音-文本双模讽刺检测模型的反馈损失融入TTS训练，提升对讽刺语调特征的建模能力。  <br/>2. **两段式迁移学习策略**：  <br/>   - 第一阶段：以多样化语音风格数据集（含讽刺语）进行预训练模型微调，增强通用表达能力；  <br/>   - 第二阶段：以专门讽刺语音数据集进一步优化，提升讽刺语音生成的针对性。  <br/>3. **系统性改进合成效果**：通过客观与主观评估验证，显著提升合成语音的自然度、质量及讽刺意识表达水平。|
|2508.12009v1|[Optimizing Neural Architectures for Hindi Speech Separation and   Enhancement in Noisy Environments](http://arxiv.org/abs/2508.12009v1)|**贡献点总结（100字以内）:**  <br/>本文提出适用于边缘设备的Hindi语音分离与增强方案，改进DEMUCS模型结合U-Net和LSTM，利用大规模数据集与数据增强技术提升性能，通过量化技术优化计算效率，并强调定制AI算法在印度语境中的应用价值。  <br/><br/>**分点贡献:**  <br/>1. **模型改进**：基于DEMUCS提出优化方法，提升Hindi语音清晰度与可懂度，解决传统技术局限性。  <br/>2. **结构融合**：引入U-Net和LSTM层对模型微调，增强对多变声学环境的适应性。  <br/>3. **数据增强**：构建包含40万Hindi语音剪辑的数据库，并结合ESC-50和MS-SNSD进行多样化环境训练。  <br/>4. **部署优化**：探索量化技术，降低计算需求以适配资源受限设备（如TWS耳机）。  <br/>5. **应用与展望**：验证定制AI算法在印度语音处理领域的有效性，提出边缘架构优化方向。|
|2508.10830v1|[Advances in Speech Separation: Techniques, Challenges, and Future Trends](http://arxiv.org/abs/2508.10830v1)|**贡献点：**  <br/>1. **全面视角**：系统分析DNN语音分离的学习范式、场景（已知/未知说话人）、监督/自监督/无监督框架对比及架构组件（编码器至估计策略）。  <br/>2. **及时性**：覆盖当前前沿技术及基准，确保读者获取最新创新与成果。  <br/>3. **独特见解**：深入探讨技术发展路径，识别新兴趋势，提出领域鲁棒、高效架构、多模态整合等未来方向。  <br/>4. **公正评估**：基于标准数据集进行定量分析，客观揭示不同方法的能力与局限性。  <br/><br/>**总结（100字以内）：**  <br/>本综述系统梳理DNN语音分离技术，涵盖学习范式、场景对比与架构分析，全面总结前沿成果并揭示技术趋势，为研究者提供权威、公正的评估与未来方向参考。|
|2508.10436v1|[Alternating Approach-Putt Models for Multi-Stage Speech Enhancement](http://arxiv.org/abs/2508.10436v1)|**贡献点总结：**  <br/>1. 提出PuttNet作为语音增强后处理框架，针对性消除增强模型引入的artifacts。  <br/>2. 首次建立语音增强模型与PuttNet的交替应用机制，显著提升PESQ、STOI、CBAK等客观质量指标。  <br/>3. 通过可视化分析直观解释交替策略的优势，揭示其优于单模型重复应用的理论依据。|
|2508.08468v1|[Audio-Visual Speech Enhancement: Architectural Design and Deployment   Strategies](http://arxiv.org/abs/2508.08468v1)|总结：  <br/>提出AI驱动的多模态语音增强系统，对比分析云、边缘辅助与独立设备三种部署架构，揭示其在延迟、计算开销与语音质量的平衡特性，为实际应用提供部署优化指南。  <br/><br/>贡献点：  <br/>1. **提出新型AVSE系统架构**：结合CNN（谱特征提取）与LSTM（时序建模），实现音频与视觉信息的多模态融合，提升语音增强效果。  <br/>2. **系统化部署场景对比**：分析云、边缘辅助和独立设备三种部署方式在语音质量、延迟、计算开销等方面的性能差异。  <br/>3. **实测跨网络环境分析**：在Ethernet、Wi-Fi 4/5、5G等实际网络条件下验证系统表现，量化处理延迟与通信延迟的权衡关系。  <br/>4. **提出实用部署指导**：明确边缘辅助架构在5G/Wi-Fi 6下可兼顾低延迟与语音可懂度，为助听设备、远程通信等场景提供选型依据。|
|2508.07563v1|[Exploring Efficient Directional and Distance Cues for Regional Speech   Separation](http://arxiv.org/abs/2508.07563v1)|贡献点总结（100字以内）:  <br/>本研究提出基于麦克风阵列的神经网络区域语音分离方法，创新性融合空间线索与直接-混响比率特征，改进延迟-求和技术提升方向识别，实现目标方向与距离的精准分离，在CHiME-8数据集上取得SOTA性能，并验证了其在真实场景的应用价值。<br/><br/>分点贡献:  <br/>1. **方法创新**：提出结合神经网络与麦克风阵列的区域语音分离框架，引入新型空间线索提取机制。  <br/>2. **技术改进**：改进传统延迟-求和技术，增强目标方向信号分离能力，同时融合直接-混响比率（D/R ratio）作为输入特征。  <br/>3. **距离判别**：通过D/R比率区分声源的远近，提升模型对距离信息的感知与分离效果。  <br/>4. **实验验证**：在真实场景数据集（CHiME-8）上实现最先进性能，验证方法在实际应用的有效性。|
|2508.07558v1|[UniFlow: Unifying Speech Front-End Tasks via Continuous Generative   Modeling](http://arxiv.org/abs/2508.07558v1)|**贡献点总结（100字以内）**  <br/>提出UniFlow统一框架，整合VAE与DiT处理多语音前端任务，通过条件嵌入实现参数共享与任务适配，比较三种生成目标优化性能，开源代码促进研究。<br/><br/>**详细贡献点**  <br/>1. **统一框架设计**：首次构建基于连续生成模型的统一框架UniFlow，整合多语音前端任务（如语音增强、目标说话人提取等）于共享潜在空间，减少冗余工程。  <br/>2. **混合生成模型架构**：将波形变分自编码器（VAE）与扩散变压器（DiT）结合，通过VAE学习紧凑潜在表示，DiT预测潜在更新，提升生成效率与质量。  <br/>3. **条件嵌入机制**：引入可学习条件嵌入（通过任务ID索引），实现模型参数最大化共享的同时保留任务特定的适应性，增强灵活性。  <br/>4. **生成目标对比研究**：系统性比较噪声消除扩散、流匹配和潜在域平均流三种生成目标，探索其在计算效率与性能间的平衡，提出优化策略。  <br/>5. **广泛基准验证**：在多个公开语音数据集上验证UniFlow，证明其在性能上优于现有最佳方法（SOTA基线），并展示良好的可扩展性。  <br/>6. **开源促进研究**：公开代码库，推动生成式语音处理技术的进一步发展与应用。|
|2508.07219v1|[ParaNoise-SV: Integrated Approach for Noise-Robust Speaker Verification   with Parallel Joint Learning of Speech Enhancement and Noise Extraction](http://arxiv.org/abs/2508.07219v1)|总结：  <br/>提出ParaNoise-SV模型，通过显式建模噪声和双U-Net结构设计，有效提升说话人验证的抗噪性能，实验显示EER降低8.4%。<br/><br/>贡献点：  <br/>1. **显式建模噪声**：首次将噪声提取（NE）与语音增强（SE）结合，替代传统隐式噪声抑制方法，更精确分离噪音与说话人特征。  <br/>2. **双U-Net架构**：引入噪声提取U-Net与语音增强U-Net的协同机制，通过并行连接实现噪声与语音的联合优化。  <br/>3. **特征保留策略**：在语音增强过程中利用噪声提取结果进行引导，有效保留学说话人相关特征，避免信息损失。  <br/>4. **性能提升验证**：在基准数据集上实现显著效果提升，相较现有联合SE-SV方法将EER降低8.4%。|
|2508.06842v3|[Speech Enhancement based on cascaded two flows](http://arxiv.org/abs/2508.06842v3)|贡献点总结（100字以内）:  <br/>提出通过单一flow matching模型实现语音增强及初始值生成，减少NFE需求并保持性能优势，有效简化流程，提升计算效率。|
|2508.06842v1|[Speech Enhancement based on cascaded two flow](http://arxiv.org/abs/2508.06842v1)|总结：  <br/>该论文提出基于流匹配的语音增强方法，通过统一模型结构降低计算需求，实现与基线相当或更优性能，无需额外预测模型。<br/><br/>贡献点：<br/>1. **方法创新**：首次将流匹配模型用于语音增强（SE）的两个关键步骤（均值漂移生成与初始值采样），实现统一模型架构。<br/>2. **计算效率提升**：在保持或超越传统基线性能的前提下，降低函数评估次数（NFE），减少计算开销。<br/>3. **消融额外模型**：无需依赖独立的预测模型进行初始语音增强，简化系统架构并避免额外训练成本。<br/>4. **实验验证**：通过对比实验证明，所提方法在单个或两个级联生成模型中均可实现高效性能，验证了其有效性。|
|2508.06840v1|[FlowSE: Flow Matching-based Speech Enhancement](http://arxiv.org/abs/2508.06840v1)|总结：  <br/>本文提出基于条件流匹配的语音增强方法，显著减少推理所需的函数评估次数（NFE），在性能上媲美传统扩散模型，且无需额外微调或优化传输条件向量场即可实现高效处理。<br/><br/>贡献点：  <br/>1. **引入条件流匹配框架**：首次将流匹配技术应用于语音增强任务，通过建模条件概率路径实现高效的生成过程。  <br/>2. **降低计算复杂度**：在保持性能的前提下，将NFE从60降至5，有效减少计算开销。  <br/>3. **无需额外微调**：相比于传统扩散模型需微调修正反向过程，所提方法无需额外细调即可实现相似性能。  <br/>4. **优化扩散模型结构**：基于修改后的最优传输条件向量场，构建扩散模型在NFE=5时仍保持良好效果。|
|2508.06393v1|[Robust Target Speaker Diarization and Separation via Augmented Speaker   Embedding Sampling](http://arxiv.org/abs/2508.06393v1)|总结：  <br/>本研究提出无需注册的语音分离与说话人辨识方法，通过双阶段训练和重叠频谱损失函数提升鲁棒性和准确性，实验验证在DER和cpWER指标上均优于现有SOTA基线。<br/><br/>贡献点：  <br/>1. **提出无需注册的方法**：首次无需依赖目标说话人先验信息或固定参与人数，直接通过自动识别目标说话人嵌入实现语音分离与说话人辨识。  <br/>2. **设计双阶段训练框架**：引入分阶段的训练流程，学习对背景噪声具有鲁棒性的说话人表征特征。  <br/>3. **开发重叠频谱损失函数**：针对重叠语音帧场景，优化损失函数以提升说话人辨识的准确性。  <br/>4. **实验证明性能提升**：在混合语音数据上验证，实现71% DER和69% cpWER的相对性能提升，超越当前SOTA基线。|
|2508.06310v1|[Egonoise Resilient Source Localization and Speech Enhancement for Drones   Using a Hybrid Model and Learning-Based Approach](http://arxiv.org/abs/2508.06310v1)|贡献点总结：  <br/>1. 提出混合方法：结合阵列信号处理（ASP）与深度神经网络（DNN），解决无人机麦克风低信噪比（SNR）下的自噪声抑制问题。  <br/>2. 设计麦克风阵列：采用六麦克风均匀圆形阵列与波束成形技术，提升目标语音的定位与增强效果。  <br/>3. 优化语音增强模型：引入改进型GSC-DF2系统，实现更高效的噪声消除与语音质量提升。  <br/>4. 验证效果显著：通过DREGON数据集与实测数据，证明该方法在-30 dB极端SNR条件下优于四种基线技术。|
|2508.05293v1|[Investigation of Speech and Noise Latent Representations in   Single-channel VAE-based Speech Enhancement](http://arxiv.org/abs/2508.05293v1)|总结：  <br/>本研究提出基于贝叶斯排列训练的VAE框架，通过分离语音和噪声的潜在表示提升单通道语音增强性能，验证了显式区分潜在空间对增强效果的显著优势。<br/><br/>贡献点：  <br/>1. **提出贝叶斯排列训练与VAE结合的新框架**：首次将贝叶斯排列训练应用于单通道语音增强，利用两个预训练VAE分别建模语音和噪声的潜在表示。  <br/>2. **显式分离潜在空间**：通过修改预训练VAE损失函数，实现语音和噪声潜在表示的分离，解决传统VAE中潜在空间重叠的问题。  <br/>3. **验证分离表示的有效性**：在DNS3、WSJ0-QUT和VoiceBank-DEMAND数据集上，实验证明分离潜在空间显著优于标准VAE的语音增强效果。|
|2508.03065v2|[Fast Algorithm for Moving Sound Source](http://arxiv.org/abs/2508.03065v2)|**贡献点分点总结：**  <br/>1. **提出Yang运动时空采样重建理论**：首次针对运动场景的声学建模问题，设计符合物理规律的运动连续时变混响仿真方法。  <br/>2. **突破传统ISM局限**：将运动声源脉冲响应分解为线性时不变调制与离散时变分数延迟，解决传统静态图像源法无法处理动态系统的缺陷。  <br/>3. **分层采样策略**：基于运动位移的带限特性，采用高、低采样率分层处理低次与高次声像，兼顾细节保留与计算效率。  <br/>4. **快速实时合成架构**：设计高效算法实现动态混响的实时仿真，提升系统训练效率与实用性。  <br/>5. **实验验证有效性**：通过对比开源模型，证明该理论能更精准还原运动场景的幅度与相位变化，为行业提供高质量的动态训练数据。  <br/><br/>**总结（100字以内）：**  <br/>针对运动场景语音增强数据不足问题，提出Yang运动时空采样理论，突破传统方法限制，实现高效物理合规仿真，提供高质量动态训练数据，提升系统性能与应用价值。|
|2508.03065v1|[Fast Algorithm for Moving Sound Source](http://arxiv.org/abs/2508.03065v1)|总结：  <br/>该论文提出了一种新的运动时空采样理论，通过物理合规模型提升运动声场模拟精度，解决语音增强模型在动态场景中的训练数据不足问题，显著提高语音追踪算法的鲁棒性。<br/><br/>贡献点：  <br/>1. **提出Yang运动时空采样重建理论**：首次构建可高效模拟运动引起的连续时变混响的理论框架，突破传统静态声学模型的局限。  <br/>2. **分解运动脉冲响应**：将移动声源的时变脉冲响应拆分为线性时不变调制与离散时变分数延迟，建立符合物理规律的动态声场模型。  <br/>3. **分层采样策略**：基于运动位移的带限特性，采用高低阶图像差异化的采样率设计，兼顾细节保留与计算复杂度控制。  <br/>4. **快速合成架构**：设计高效实时模拟系统，结合带限性质与分层策略，实现高保真动态数据生成。  <br/>5. **实验验证与应用价值**：在运动声源数据模拟任务中超越GSound开源模型，为语音增强提供高质量动态训练数据，提升多通道端到端语音追踪性能。|
|2508.03047v1|[TF-MLPNet: Tiny Real-Time Neural Speech Separation](http://arxiv.org/abs/2508.03047v1)|总结：  <br/>本文提出TF-MLPNet，首次实现低功耗神经加速器上的实时语音分离，结合时频域处理结构与混合精度量化训练，显著提升效率并超越现有盲分离与目标语音提取模型。<br/><br/>贡献点：  <br/>1. **首个实时语音分离模型**：TF-MLPNet是首个能在低功耗神经加速器（如GAP9）上实现实时语音分离的网络。  <br/>2. **时频域混合处理架构**：采用分离的时频域处理方式，通过全连接层交替处理通道与频率维度，结合卷积层独立处理时间序列。  <br/>3. **混合精度量化感知训练**：提出混合精度量化感知训练（QAT）方法，优化模型在低功耗硬件上的运行效率。  <br/>4. **性能突破**：在盲语音分离和目标语音提取任务中，性能优于现有流式模型，且实测在GAP9处理器上实现6ms音频块的实时处理，速度提升3.5-4倍。|
|2508.02974v1|[Real-time speech enhancement in noise for throat microphone using neural   audio codec as foundation model](http://arxiv.org/abs/2508.02974v1)|**总结（100字以内）**  <br/>本文提出基于喉咙麦克风的实时语音增强系统，利用神经音频编解码器和新数据集提升抗噪性能，开发交互界面支持可视化和延迟监控。<br/><br/>**贡献点**  <br/>1. **集成完整流程**：构建从录音到深度学习后处理的实时语音增强演示框架，适用于身体传导麦克风在噪声环境下的应用。  <br/>2. **数据集与模型优化**：使用Vibravox（空气传导与喉咙麦克风配对）数据集对神经音频编解码器Kyutai的Mimi进行微调，解决喉麦克风带宽不足的问题。  <br/>3. **性能对比验证**：通过实验验证该方法在噪声环境下的优越性，超越现有主流模型的增强效果。  <br/>4. **交互式功能设计**：开发用户友好的界面，支持增强功能切换、频谱图可视化和实时延迟监控，提升系统实用性。|
|2508.01847v1|[Test-Time Training for Speech Enhancement](http://arxiv.org/abs/2508.01847v1)|**贡献点总结**（100字以内）:  <br/>本论文提出一种基于TTT的语音增强新方法，结合主任务与自监督辅助任务的Y型架构，利用推理时优化实现噪声环境下的动态适应，无需标注数据。通过多策略设计平衡效率与适应性，在合成与真实数据集上均优于基线，为语音处理提供了适应性与鲁棒性改进的范式。<br/><br/>**分点贡献**:<br/>1. **创新性TTT应用**：首次将Test-Time Training（TTT）方法引入语音增强领域，解决噪声条件和领域偏移的动态适应问题。<br/>2. **Y型架构设计**：提出主任务（语音增强）与自监督辅助任务（如噪声增强信号重建、掩码频谱预测）的整合架构，提升模型泛化能力。<br/>3. **无监督动态优化**：在推理阶段通过优化自监督任务实现模型自适应，无需离线标注数据，降低实际部署成本。<br/>4. **多策略平衡机制**：设计多种TTT策略，综合考虑适应性与计算效率，提供灵活的模型应用方案。<br/>5. **实验验证效果**：在合成与真实数据集上均验证了方法的有效性，提升语音质量指标并超越基线模型。<br/>6. **理论与实践意义**：阐明TTT在语音增强中的潜力，为未来适应性语音处理技术提供新思路。|
|2507.21448v2|[Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations](http://arxiv.org/abs/2507.21448v2)|**贡献点（分点）**  <br/>1. 提出首个实时音频-视觉语音增强系统RAVEN，实现目标说话人隔离与噪声抑制。  <br/>2. 系统性分析视觉嵌入（来自AVSR和ASD）在不同SNR及多说话人场景下的增强效果。  <br/>3. 开发基于CPU的实时流处理系统，并提供开源代码与视频演示。  <br/>4. 验证拼接AVSR与ASD嵌入在低SNR多说话人环境的显著优势，及AVSR嵌入在纯噪声场景的最优效果。  <br/><br/>**总结（100字以内）**  <br/>本文提出实时多模态语音增强系统RAVEN，结合AVSR与ASD的视觉嵌入提升低SNR下多说话人场景的增强效果，并实现纯噪声场景的最优表现。开发了CPU部署的实时流系统，首次开源该领域实时方案，推动实际应用。|
|2507.21448v1|[Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations](http://arxiv.org/abs/2507.21448v1)|**总结**：  <br/>本文提出RAVEN，首个开源实时音频-视觉语音增强系统，通过融合AVSR和ASD的视觉嵌入，在低SNR和多说话者场景中显著提升增强效果，同时提供代码和视频演示。<br/><br/>**贡献点**：  <br/>1. **提出RAVEN系统**：设计了一种简单且高效的实时音频-视觉语音增强系统，能够有效分离目标说话者并抑制干扰声音和背景噪声。  <br/>2. **探索视觉嵌入的贡献**：分析了音频-视觉语音识别（AVSR）和主动说话人检测（ASD）中提取的视觉嵌入对AVSE的提升作用，发现融合两者的嵌入在低信噪比和多说话者环境中效果最佳，而单独使用AVSR嵌入在纯噪声场景中性能最优。  <br/>3. **实现开源实时系统**：开发了基于CPU的实时流处理系统，提供视频演示和代码库，是首个公开的实时AVSE开源实现。|
|2507.20027v1|[Binaural Localization Model for Speech in Noise](http://arxiv.org/abs/2507.20027v1)|**贡献点总结（100字以内）：**  <br/>提出了一种轻量级端到端双耳定位模型，结合内部耳噪声模拟频率依赖性听阈；通过与经典算法对比和听力测试验证模型有效性；为双耳语音增强方法提供中间耳线索保留度量新途径。  <br/><br/>**分点贡献：**  <br/>1. **模型创新**：设计了一种轻量级卷积循环神经网络（CRN），专门用于嘈杂回声环境下的双耳声源定位，聚焦于前方位移平面。  <br/>2. **生理机制建模**：引入加性内部耳噪声模块，更真实地反映人类听觉系统的频率依赖性听阈特性。  <br/>3. **性能验证**：通过实验对比该模型与Steered Response Power算法的定位精度，并通过听觉测试验证其与人类定位能力的匹配度。  <br/>4. **应用拓展**：将模型作为评估双耳语音增强方法中中间耳线索保留程度的新指标，拓展其在语音处理领域的应用价值。|
|2507.20023v1|[Binaural Speech Enhancement Using Complex Convolutional Recurrent   Networks](http://arxiv.org/abs/2507.20023v1)|贡献点总结：  <br/>1. 提出端到端双耳语音增强方法，采用复数循环卷积网络（Complex RNN）结合编码器-解码器结构，引入复数LSTM模块提升性能。  <br/>2. 设计新型损失函数，同时优化语音清晰度、降噪与空间信息保留。  <br/>3. 在时频域估计左右耳通道的独立复数比例掩码，适配双耳助听设备。  <br/>4. 在单目标说话人及各向同性噪声场景下，验证方法显著提升语音可懂度并保留空间信息。|
|2507.19208v1|[Comparison of Knowledge Distillation Methods for Low-complexity   Multi-microphone Speech Enhancement using the FT-JNF Architecture](http://arxiv.org/abs/2507.19208v1)|总结（100字以内）:  <br/>本研究结合FT-JNF架构与知识蒸馏方法，提出高效多麦克风语音增强模型压缩方案。实验表明，在保持PESQ得分的同时，可将模型参数减少至25%甚至缩小96%，显著提升轻量化部署可行性。<br/><br/>贡献点列表:  <br/>1. **提出结合FT-JNF架构的知识蒸馏方法**：首次将频率-时间联合非线性滤波（FT-JNF）架构与知识蒸馏技术结合，实现语音增强模型的高效压缩。  <br/>2. **设计多维度KD评估框架**：系统性评估五种KD方法，创新性引入直接输出匹配、中间层自相似性分析及多层损失融合三类评估指标。  <br/>3. **验证轻量化模型的有效性**：在模拟数据集（五麦克风紧凑阵列）上证明，学生模型仅需25%教师参数即可达到0 dB SNR下与原模型相当的PESQ性能。  <br/>4. **实现超大规模模型压缩**：通过优化KD策略，使模型体积缩减达96%，仅导致PESQ得分微小下降（验证了压缩与性能的平衡）。  <br/>5. **提供实际部署指导**：为资源受限设备的语音增强应用提供理论支持和实验依据，推动DNN模型的边缘计算落地。|
|2507.19062v1|[From Continuous to Discrete: Cross-Domain Collaborative General Speech   Enhancement via Hierarchical Language Models](http://arxiv.org/abs/2507.19062v1)|总结: <br/>OmniGSE提出了一种两阶段框架，通过融合判别式与生成式方法，有效应对多种复合语音失真问题，实现了更鲁棒的语音增强性能。<br/><br/>贡献点:<br/>1. 提出OmniGSE框架：解决现有语音增强方法无法同时处理多种复杂失真（如噪声、回声、带宽限制等）的局限性。<br/>2. 两阶段架构设计：整合轻量级NAC-RoFormer（处理连续特征）与语言模型（生成离散token重构语音）的协同优化机制。<br/>3. 层次化语言模型结构：创新性地构建包含RootLM（跨编码层建模）和BranchLMs（捕捉编码层渐进关系）的分层架构。<br/>4. 多任务泛化能力：在多个基准测试中验证框架有效性，尤其在复合失真场景表现显著优于现有方法。|