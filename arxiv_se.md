|Source|Title|Summary|
|---|---|---|
|2511.02454v1|[Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech   Enhancement on Discrete Codec Token](http://arxiv.org/abs/2511.02454v1)|总结（100字以内）:  <br/>本文提出一种基于双向结构状态空间模型的DF-Conformer，解决自注意力二次复杂度，保持线性效率，并在Genhancer实验中验证性能优势。<br/><br/>贡献点:  <br/>1. 提出将双向选择结构状态空间模型（DR2S）替代FAVOR+，用于语音增强任务，消除原有方法的近似误差。  <br/>2. 通过Hydra（Mamba的双向扩展）实现线性时间复杂度，避免传统自注意力的序列长度平方复杂度。  <br/>3. 结合结构矩阵混合框架，增强模型的全局时序建模能力，提升语音增强效果。  <br/>4. 在生成式语音增强模型Genhancer上验证方法有效性，展示优于DF-Conformer的性能表现。  <br/>5. 推动语音增强领域在复杂度优化与序列建模能力上的技术进展，提供可扩展的高效架构设计。|
|2510.27198v1|[Reference Microphone Selection for Guided Source Separation based on the   Normalized L-p Norm](http://arxiv.org/abs/2510.27198v1)|总结：  <br/>本研究提出两种基于归一化ℓ_p范数的参考麦克风选择方法，有效结合SNR与ELR差异，显著提升远场语音识别系统的词错误率表现。<br/><br/>贡献点：  <br/>1. **提出新方法**：设计两种参考麦克风选择策略，基于归一化ℓ_p范数（单用或结合SNR），解决传统单凭SNR选择的局限性。  <br/>2. **优化声学特性**：综合考虑麦克风间的SNR和ELR差异，提升语音增强的鲁棒性和下游ASR性能。  <br/>3. **实验验证效果**：在CHiME-8远场ASR系统中验证方法有效性，证明ℓ_p范数方法可显著降低宏观平均词错误率。|
|2510.26825v1|[Audio-Visual Speech Enhancement In Complex Scenarios With Separation And   Dereverberation Joint Modeling](http://arxiv.org/abs/2510.26825v1)|**贡献点：**<br/><br/>1. 提出一种适用于复杂声学环境的高效音视频语音增强系统。  <br/>2. 设计“分离先去混响”的新型处理流程，可扩展至其他AVSE网络。  <br/>3. 在AVSEC-4挑战赛中取得优异成绩，三项客观指标领先，并在主观测试中排名第一。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种高效的音视频语音增强系统，采用“分离先去混响”流程，显著提升了复杂环境下的语音质量，在AVSEC-4挑战赛中取得领先成绩。|
|2510.26372v1|[UniTok-Audio: A Unified Audio Generation Framework via Generative   Modeling on Discrete Codec Tokens](http://arxiv.org/abs/2510.26372v1)|**贡献点：**<br/><br/>1. 提出UniTok-Audio框架，实现音频生成任务的统一建模，提升模型的可扩展性和复用性。  <br/>2. 引入连续条件特征提取与自回归生成机制，生成离散目标音频token。  <br/>3. 设计特殊任务标识符token，统一处理多任务学习模式。  <br/>4. 开发双流音频编解码器，包含声学与语义分支，提升波形重建的保真度。  <br/>5. 在五个时间对齐音频任务上取得与SOTA模型相竞争的性能，并开源代码以促进研究。  <br/><br/>**总结：**  <br/>UniTok-Audio是一种统一音频生成框架，通过自回归生成、任务标识符和双流编解码器提升性能与通用性，并公开代码支持研究。|
|2510.26299v1|[Modeling strategies for speech enhancement in the latent space of a   neural audio codec](http://arxiv.org/abs/2510.26299v1)|**贡献点分点列出：**<br/><br/>1. 对比了连续向量与离散token两种NAC编码方式在监督语音增强任务中的表现。  <br/>2. 分析了自回归与非自回归模型在语音增强中的质量、可懂度与效率差异。  <br/>3. 提出并验证了通过微调NAC编码器进行语音增强的方法，效果最佳但牺牲了编码器重建质量。  <br/>4. 提供了代码和音频样本供进一步研究和验证。  <br/><br/>**总结（100字以内）：**  <br/>本文比较了连续与离散NAC编码在语音增强中的效果，分析了自回归与非自回归模型的优劣，并验证了编码器微调方法的有效性，为实际应用提供了参考。|
|2510.23312v2|[Low-Resource Audio Codec (LRAC): 2025 Challenge Description](http://arxiv.org/abs/2510.23312v2)|总结：  <br/>2025 Low-Resource Audio Codec Challenge推动低资源音频编解码器研究，提供标准化数据集与评估框架，促进高效、鲁棒的语音编解码技术发展。<br/><br/>贡献点：  <br/>1. 提出2025 Low-Resource Audio Codec Challenge，聚焦资源受限场景下的语音编解码器设计。  <br/>2. 提供标准化训练数据集，确保研究的可比性和可复现性。  <br/>3. 提供两个基线系统，作为参与者的参考和对比基准。  <br/>4. 构建全面的评估框架，涵盖低资源运算、低延迟、低码率及抗噪抗混响能力。  <br/>5. 促进神经编解码器与语音增强技术的整合研究。|
|2510.23141v1|[Treble10: A high-quality dataset for far-field speech recognition,   dereverberation, and enhancement](http://arxiv.org/abs/2510.23141v1)|**贡献点：**<br/><br/>1. 提出Treble10，首个大规模、物理精确的远场语音数据集。  <br/>2. 结合波基与几何声学求解器，实现更真实的声学模拟。  <br/>3. 包含多种子集（单声道、八阶Ambisonics、六通道设备RIRs），提升数据多样性。  <br/>4. 提供预混响语音场景与LibriSpeech的配对数据，便于模型训练与评估。  <br/>5. 支持32kHz采样，精确建模低频波效应与高频反射。  <br/>6. 数据集开源，可用于基准测试与下一代模拟驱动音频研究的模板。  <br/><br/>**总结（100字以内）：**  <br/>Treble10是首个物理精准、大规模的远场语音数据集，结合波基与几何声学模拟，提供多种子集与真实混响语音场景，填补了测量与模拟之间的现实差距，支持可复现评估和数据增强。|
|2510.22637v1|[HyBeam: Hybrid Microphone-Beamforming Array-Agnostic Speech Enhancement   for Wearables](http://arxiv.org/abs/2510.22637v1)|总结：  <br/>HyBeam提出一种混合框架，结合低频麦克风信号与高频波束形成器信号，解决语音增强在多麦克风配置下的局限性。通过频段分析验证其有效，实验结果显示在多种指标上优于传统方法。<br/><br/>贡献点：  <br/>1. **提出HyBeam混合框架**：首次将低频原始麦克风信号与高频波束形成器信号结合，利用其互补特性提升语音增强性能。  <br/>2. **增强阵列无关性**：通过混合信号处理，降低对固定麦克风阵列几何结构的依赖，适用于动态变化的声学环境和设备形态。  <br/>3. **跨频段性能优化**：频段分析表明，高频利用波束形成器的方向性，低频依赖麦克风空间线索，实现全频段统一增强效果。  <br/>4. **实验验证有效性**：在多样化的房间环境和可穿戴麦克风配置中，HyBeam在PESQ、STOI和SI-SDR指标上显著优于单信号源方法。|
|2510.21485v1|[FlexIO: Flexible Single- and Multi-Channel Speech Separation and   Enhancement](http://arxiv.org/abs/2510.21485v1)|总结：  <br/>本文提出FlexIO系统，整合单、多通道语音分离增强，通过条件分离和提示向量处理变量输入输出，引入无阵列感知的通道通信机制，实验证明其在多种麦克风和说话者配置下的有效性与鲁棒性。<br/><br/>贡献点：  <br/>1. **提出统一输入输出框架**：构建首个支持灵活麦克风阵列（1-5麦克风）和说话者数量（1-3说话者）的语音分离增强系统（FlexIO）。  <br/>2. **条件分离机制**：引入基于提示向量的条件分离方法，每个说话者对应独立条件，实现对任意数量说话者的分离能力。  <br/>3. **无阵列感知通道通信**：设计一种独立于麦克风阵列结构的通道通信机制，增强多通道混合信号处理的通用性。  <br/>4. **实验证明有效性**：在真实数据集CHiME-4上验证系统鲁棒性，展示对复杂场景的适应能力。|
|2510.21014v2|[ReFESS-QI: Reference-Free Evaluation For Speech Separation With Joint   Quality And Intelligibility Scoring](http://arxiv.org/abs/2510.21014v2)|总结：  <br/>提出一种无需参考音频和文本的语音分离评估框架，结合自监督学习表示预测音频质量与可懂度，实验证明其在真实混合场景中的有效性与鲁棒性。<br/><br/>贡献点：  <br/>1. **提出无参考评估框架**：首次构建基于自监督学习（SSL）的文本与参考均无需的语音分离评估方法，解决传统方法依赖参考音频和转录的局限性。  <br/>2. **联合预测多维度指标**：同时预测音频质量（SI-SNR）和语音可懂度（WER），而非单独评估，提升综合评估能力。  <br/>3. **实验证明有效性**：在WHAMR!数据集上取得优异结果（SI-SNR MAE=1.38, PCC=0.95；WER MAE=17%, PCC=0.77），验证框架的可行性。  <br/>4. **鲁棒性验证**：通过多种SSL表示策略测试，证明框架在不同模型下的稳定性和通用性。|
|2510.20441v1|[UniSE: A Unified Framework for Decoder-only Autoregressive LM-based   Speech Enhancement](http://arxiv.org/abs/2510.20441v1)|**贡献点总结（100字以内）：**  <br/>提出UniSE框架，统一处理多种语音增强任务，利用自回归语言模型生成目标语音离散标记，实验证明其性能优于传统方法，展示LM在语音增强任务统一中的潜力。<br/><br/>---<br/><br/>**贡献点分点列出：**<br/><br/>1. **提出统一框架**：UniSE是首个基于自回归语言模型（AR LM）的统一解码器框架，用于处理多种语音增强子任务。<br/>2. **兼容多任务模式**：通过将不同任务的学习模式统一为AR LM生成目标语音离散标记，促进多任务间的兼容性与协同。<br/>3. **实验验证有效性**：在多个基准数据集上进行实验，证明UniSE在多种SE任务中表现优异，优于传统判别式和生成式基线模型。<br/>4. **展示LM潜力**：实验证明语言模型在语音增强任务统一建模中的强大能力，拓展其在语音处理领域的应用。|
|2510.19439v1|[Relative Transfer Matrix Estimator using Covariance Subtraction](http://arxiv.org/abs/2510.19439v1)|**贡献点：**<br/><br/>1. 提出基于协方差差分的ReTM估计方法，增强在噪声环境下的灵活性和实用性；  <br/>2. 首次将该方法应用于说话人分离任务，验证其在混响条件下的有效性；  <br/>3. 在低信噪比环境下，与现有ReTM和相对传输函数方法相比，表现出更好的分离性能；  <br/>4. 方法适用于多通道录音，为实际应用提供有力支持。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种基于协方差差分的ReTM估计方法，用于说话人分离，并在低信噪比和混响环境下验证其有效性，展示了其在语音增强和分离任务中的优越性能。|
|2510.18744v1|[Diffusion Buffer for Online Generative Speech Enhancement](http://arxiv.org/abs/2510.18744v1)|**贡献点总结：**<br/><br/>1. 提出Diffusion Buffer，实现在线语音增强的生成模型，仅需一次神经网络调用。  <br/>2. 设计2D卷积UNet架构，与Diffusion Buffer的预览机制相匹配。  <br/>3. 引入数据预测损失，实现算法延迟与质量之间的灵活平衡。  <br/>4. 显著降低算法延迟（320-960ms → 32-176ms），同时提升性能。  <br/>5. 证明在线Diffusion Buffer在未见过的噪声语音数据上优于预测模型。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出Diffusion Buffer，实现低延迟的在线语音增强，结合2D UNet和数据预测损失，提升性能并降低延迟至32-176ms，验证其在未见噪声数据上的优越性。|
|2510.16997v1|[Towards Real-Time Generative Speech Restoration with Flow-Matching](http://arxiv.org/abs/2510.16997v1)|总结：  <br/>本文提出一种基于流匹配的低延迟实时语音恢复系统，通过因果架构和优化采样策略，在少量函数评估下实现高质量语音增强，同时与对抗损失训练方法进行对比验证。<br/><br/>贡献点：  <br/>1. **低延迟实时架构**：设计无时间下采样的因果模型，总延迟仅20ms，适用于实时通信场景。  <br/>2. **多任务处理能力**：支持降噪、去混响和生成性语音恢复等多样化实时任务。  <br/>3. **高效采样策略**：仅需5次函数评估（NFEs）即可达到与20次NFEs相当的增强质量。  <br/>4. **架构优化研究**：系统性探索不同架构变体和采样方法以提升训练效率与推理性能。  <br/>5. **对比实验分析**：对比流匹配与对抗损失训练方法的效果，验证其在相同模型下的优势。|
|2510.16995v1|[Adaptive Deterministic Flow Matching for Target Speaker Extraction](http://arxiv.org/abs/2510.16995v1)|总结：  <br/>本文提出AD-FlowTSE方法，通过自适应步长和基于混合比例的流路径设计，实现高效准确的目标说话人提取，并验证了其在单步处理中的优越性能。<br/><br/>贡献点：  <br/>1. **提出自适应步长机制**：首次在流匹配框架中引入动态调整的步长，取代传统固定步长的逆步骤流程。  <br/>2. **重新定义流路径**：将源-背景混合比例（MR）作为流匹配的核心，建立从背景到源的动态运输路径，而非混合/干净语音之间的固定路径。  <br/>3. **MR感知初始化**：通过自适应初始化策略，使模型根据噪声条件在背景-源轨迹的不同点启动，提升初始化效率。  <br/>4. **辅助MR估计优化**：结合额外的MR估计模块，进一步提高目标语音的提取精度，验证了多阶段协同的有效性。  <br/>5. **实验验证高效性**：证明AD-FlowTSE在单步处理下即可达到高性能，表明其对噪声条件的自适应能力显著优于现有方法。|
|2510.16834v1|[Schrödinger Bridge Mamba for One-Step Speech Enhancement](http://arxiv.org/abs/2510.16834v1)|总结：  <br/>提出基于Schrödinger Bridge范式与Mamba架构的新型训练-推理框架SBM，实现高效语音增强，并在单步推理下超越现有方法，具有广泛的应用潜力。<br/><br/>贡献点：  <br/>1. **提出SBM框架**：首次结合Schrödinger Bridge训练范式与选择性状态空间模型Mamba，构建训练-推理一体化新范式。  <br/>2. **生成性语音增强应用**：以语音去噪与去混响任务为例，实现SBM方法，验证其在实际场景中的有效性。  <br/>3. **实验性能突破**：在四个基准数据集上证明，SBM仅需1步推理即可超越现有单步或迭代方法，且达到最优实时因子（RTF）。  <br/>4. **理论整合与扩展**：阐述SB范式与选择性状态空间模型的潜在兼容性，为更广泛的生成模型设计提供新思路。|
|2510.16437v1|[Audio-Visual Speech Enhancement for Spatial Audio - Spatial-VisualVoice   and the MAVE Database](http://arxiv.org/abs/2510.16437v1)|**贡献点总结（100字以内）:**  <br/>该论文提出多通道AVSE框架结合空间音频与视觉信息，引入新数据库并验证在低SNR条件下显著提升语音质量。<br/><br/>**分点贡献:**  <br/>1. **提出多通道AVSE框架**：基于VisualVoice，融合麦克风阵列的空间线索与视觉信息，实现对目标说话人在噪声环境中的增强，填补了空间音频提升的空白。  <br/>2. **构建MAVe数据库**：创建首个包含多通道音频-视觉信号的数据库，提供可控、可重复的房间环境与广泛SNR范围，为研究提供标准化数据支持。  <br/>3. **实验验证有效性**：在低SNR场景下，方法在SI-SDR、STOI和PESQ指标上表现优异，证明了跨模态融合在语音增强中的显著优势。  <br/>4. **保持空间线索与可懂度**：通过双耳信号分析确认增强后的语音保留空间信息及高可懂度，适用于增强现实等空间音频需求场景。|
|2510.12485v1|[I-DCCRN-VAE: An Improved Deep Representation Learning Framework for   Complex VAE-based Single-channel Speech Enhancement](http://arxiv.org/abs/2510.12485v1)|总结：  <br/>该论文提出改进的DCCRN-VAE系统，通过移除skip connection、采用β-VAE预训练及同时生成语音与噪声潜在表征，提升模型在不匹配数据集上的泛化能力，并简化训练流程。<br/><br/>贡献点：  <br/>1. **移除Skip Connections**：在预训练VAE中删除跳连结构，促使模型提取更具信息量的语音与噪声潜在表征。  <br/>2. **β-VAE预训练方法**：用β-VAE替代传统VAE，优化重建精度与潜在空间正则化之间的平衡。  <br/>3. **联合生成潜在表征**：NSVAE同时生成语音与噪声潜在表示，增强模型对噪声的建模能力。  <br/>4. **提升泛化性能**：实验表明，改进系统在不匹配数据集（如WSJ0-QUT、Voicebank-DEMEND）上优于基线，证明更强的泛化能力。  <br/>5. **简化训练流程**：消融实验显示，传统微调可替代对抗训练，降低训练复杂度。|
|2510.11395v1|[Dynamically Slimmable Speech Enhancement Network with Metric-Guided   Training](http://arxiv.org/abs/2510.11395v1)|总结（100字以内）:  <br/>本文提出基于门控的动态可剪枝网络DSN，通过架构无关的动态结构和策略模块优化计算负载，结合度量引导训练提升资源分配效率，实验表明在保持性能的同时节省约27%的计算资源。<br/><br/>贡献点:<br/>1. 提出Dynamically Slimmable Network (DSN)结构，融合静态与动态组件以降低轻量级语音增强模型的复杂度  <br/>2. 设计针对常用模块(循环神经网络、注意力机制、卷积层等)的架构无关动态结构  <br/>3. 开发帧级策略模块，根据输入信号质量自适应控制动态组件的计算负载  <br/>4. 引入Metric-Guided Training (MGT)方法，显式指导策略模块评估输入语音质量  <br/>5. 在保持与SOTA轻量基线相当的增强性能前提下，使模型计算负载降低至73%（平均节省27%） | 动态组件使用比例验证资源分配有效性|
|2510.10687v1|[LSZone: A Lightweight Spatial Information Modeling Architecture for   Real-time In-car Multi-zone Speech Separation](http://arxiv.org/abs/2510.10687v1)|总结：  <br/>LSZone 是一种轻量级架构，通过 SpaIEC 和 CNP 模块实现高效实时车内多区域语音分离，具有低复杂度和高性能。<br/><br/>贡献点：  <br/>1. 提出 LSZone 架构，用于车内多区域语音分离，具有低计算开销。  <br/>2. 设计 SpaIEC 模块，融合 Mel �光谱图与双耳相位差以降低计算负担。  <br/>3. 引入 CNP 模块，采用极轻量的 Conv-GRU 实现高效的空间信息建模。  <br/>4. 在复杂噪声和多说话人场景下表现出色，复杂度为 0.56G MACs，RTF 为 0.37。|
|2510.09974v1|[Universal Discrete-Domain Speech Enhancement](http://arxiv.org/abs/2510.09974v1)|**贡献点总结（100字以内）:**  <br/>提出UDSE，将语音增强转化为离散域分类任务，利用预训练神经语音编解码器的残差向量量化器预测干净标记，有效应对多种同时存在的干扰，优于传统回归方法。<br/><br/>**分点贡献:**  <br/>1. **提出通用离散域SE模型UDSE**：首次将语音增强任务重新定义为离散域分类，而非传统回归预测连续特征或波形，提升模型对多干扰场景的泛化能力。  <br/>2. **多干扰联合处理机制**：通过残差向量量化器（RVQ）的序列依赖预测规则，实现对多种同时存在的干扰（如噪声、混响、压缩等）的协同增强。  <br/>3. **端到端训练策略**：采用教师强制策略和交叉熵损失优化，简化模型训练流程并提高稳定性，适用于复杂真实环境中的语音增强。  <br/>4. **广泛验证性能**：在多种传统与非传统干扰（包括组合干扰）下验证模型有效性，展示其在实际应用中的优越性和通用性。|
|2510.08914v1|[VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR   Virtual Microphone Arrays](http://arxiv.org/abs/2510.08914v1)|**总结**（100字以内）:  <br/>VM-UNSSOR通过引入虚拟麦克风信号提升未监督语音分离性能，解决了麦克风数量减少导致的分离效果下降和频率排列问题。<br/><br/>**贡献点**：  <br/>1. 提出VM-UNSSOR方法，通过虚拟麦克风信号增强未监督语音分离性能。  <br/>2. 利用线性空间解混技术（如IVA、空间聚类）生成高SNR的虚拟信号。  <br/>3. 增加额外的混合一致性（MC）损失，提升模型训练效果。  <br/>4. 有效缓解频率排列问题，提升分离鲁棒性。  <br/>5. 在两麦克风情况下，显著优于传统UNSSOR方法，验证了方法的有效性。|
|2510.05295v1|[AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture   with Cross-Attention and Squeezeformer for Speech Enhancement](http://arxiv.org/abs/2510.05295v1)|**贡献点总结（100字以内）:**  <br/>提出AUREXA-SE框架，融合音频-视觉模态；采用U-Net和Swin Transformer V2进行特征编码；引入双向交叉注意机制实现深层上下文融合；结合Squeezeformer块建模时间依赖性；实验验证性能提升，代码开源。<br/><br/>**分点贡献:**  <br/>1. **双模态框架设计**：提出AUREXA-SE，结合音频波形与视觉线索进行语音增强，实现跨模态信息交互。  <br/>2. **高效编码结构**：采用U-Net-based 1D卷积编码器提取音频特征，配合Swin Transformer V2进行视觉特征高效提取。  <br/>3. **双向交叉注意机制**：设计新型双向跨模态注意力模块，提升多模态表示学习的互补性与上下文融合深度。  <br/>4. **轻量时间建模**：引入堆叠的Squeezeformer块（融合卷积与注意力模块），有效建模融合嵌入的时间依赖性。  <br/>5. **端到端语音增强**：通过U-Net风格解码器直接重建语音波形，确保感知一致性与可理解性输出，并在噪声基线中取得显著性能提升（STOI 0.516、PESQ 1.323、SI-SDR -4.322 dB）。  <br/>6. **开源实现**：提供完整源代码供研究复现，推动语音增强领域技术共享（GitHub链接）。|
|2510.04157v1|[GDiffuSE: Diffusion-based speech enhancement with noise model guidance](http://arxiv.org/abs/2510.04157v1)|**贡献点分点列出：**<br/><br/>1. 提出一种基于去噪扩散概率模型（DDPM）的新型语音增强方法 GDiffuSE。  <br/>2. 引入轻量辅助模型估计噪声分布，并通过引导机制融入扩散去噪过程。  <br/>3. 提高了模型对未见过噪声类型的鲁棒性与适应能力。  <br/>4. 利用原本为语音生成训练的大型 DDPM 模型，应用于语音增强任务。  <br/>5. 在 BBC 声音效果数据库与 LibriSpeech 混合噪声数据上验证，效果优于现有方法。  <br/><br/>**总结（100字以内）：**  <br/>本文提出 GDiffuSE，结合轻量辅助模型与扩散模型，提升语音增强对未知噪声的鲁棒性，并在混合噪声数据上取得优于现有方法的性能。|
|2510.02187v1|[High-Fidelity Speech Enhancement via Discrete Audio Tokens](http://arxiv.org/abs/2510.02187v1)|**贡献点：**<br/><br/>1. 提出DAC-SE1框架，简化了传统多阶段语音增强流程。  <br/>2. 利用离散的高分辨率音频表示，保留细粒度声学细节。  <br/>3. 在客观指标和MUSHRA主观评价中超越现有SOTA自回归SE方法。  <br/>4. 开源代码与模型权重，促进语音增强研究的统一与扩展。  <br/><br/>**总结（100字以内）：**  <br/>DAC-SE1提出了一种简化且高效的语音增强框架，结合高分辨率离散音频表示，提升语义连贯性与声学细节保留，在客观和主观评估中均优于现有方法，并开源支持进一步研究。|
|2510.01958v1|[Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for   Improved Cross-Corpus Speech Enhancement](http://arxiv.org/abs/2510.01958v1)|**贡献点：**<br/><br/>1. 提出RWSA-MambaUNet，结合Mamba与多头注意力机制，提升跨语料泛化性能。  <br/>2. 引入分辨率级共享注意力（RWSA），实现时间与频率分辨率的注意力共享。  <br/>3. 模型性能优于现有基线，在两个跨语料测试集上取得SOTA结果。  <br/>4. 在参数量和计算量上显著优化，使用不到一半参数及部分FLOPs即超越基线。  <br/><br/>**总结（100字以内）：**  <br/>本文提出RWSA-MambaUNet，结合Mamba与注意力机制，实现高效跨语料语音增强，在DNS 2020和EARS-WHAM_v2测试集上表现优异，参数与计算量大幅减少。|
|2510.01130v2|[Learning Time-Graph Frequency Representation for Monaural Speech   Enhancement](http://arxiv.org/abs/2510.01130v2)|**贡献点：**  <br/>1. 提出可学习的图拓扑，替代固定结构，提升表示适应性与灵活性。  <br/>2. 利用1-D卷积神经层定义可学习的图傅里叶基，无需矩阵求逆。  <br/>3. 解决GFT-SVD和GFT-EVD中因求逆导致的数值误差和稳定性问题。  <br/><br/>**总结：**  <br/>该论文提出一种无需矩阵求逆的可学习GFT-SVD框架，通过构建灵活图拓扑和卷积基提升语音增强的鲁棒性与效率。|
|2509.25982v1|[An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction](http://arxiv.org/abs/2509.25982v1)|**总结（100字以内）:**  <br/>该论文提出非线性深度神经网络可有效解决传统线性空间滤波器在空间混叠下的局限，通过联合空间与时频谱处理提升多通道语音增强的鲁棒性，尤其在大间距麦克风阵列中表现更优，为深度学习方法在语音处理中的应用提供了新动力。<br/><br/>**贡献点分点列出:**  <br/>1. **揭示传统方法的局限**：指出线性空间滤波器在大麦克风间距和高频场景下易受空间混叠影响，导致非目标信号增强。  <br/>2. **非线性网络的适应性**：提出深度神经网络能更高效处理空间混叠问题，替代传统线性波束成形器。  <br/>3. **联合处理的优势**：证明联合空间与时频谱处理比单独空间或分离式处理更鲁棒，提升抗混叠能力。  <br/>4. **场景适配性验证**：强调深度网络在大间距麦克风阵列中的有效性，扩展其在实际语音增强中的应用场景。|
|2509.24708v1|[SenSE: Semantic-Aware High-Fidelity Universal Speech Enhancement](http://arxiv.org/abs/2509.24708v1)|**贡献点：**  <br/>1. 提出SenSE方法，结合语言模型提取语义信息，提升语音增强的语义理解能力。  <br/>2. 引入语义感知的语音语言模型，生成语义标记以增强增强语音的语义一致性。  <br/>3. 设计语义引导机制，有效减少增强语音中的语义歧义和声学不连续。  <br/>4. 提出提示引导机制，利用参考语音缓解严重失真下的说话人相似性丢失问题。  <br/>5. 在多个基准数据集上验证，证明SenSE在感知质量、语音保真度和鲁棒性方面均有显著提升。<br/><br/>**总结：**  <br/>SenSE通过融合语言模型与流匹配框架，提升语音增强的语义理解和鲁棒性。|
|2509.23299v2|[MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow](http://arxiv.org/abs/2509.23299v2)|**贡献点：**  <br/>1. 提出MeanFlowSE，首个单步生成语音增强框架，解决传统多步采样（如扩散/流匹配）的实时性瓶颈。  <br/>2. 采用MeanFlow预测平均速度场，通过单步潜在空间优化提升推理效率。  <br/>3. 利用自监督学习（SSL）表示替代VAE潜变量，增强声学-语义指导能力并简化模型结构。  <br/>4. 在Interspeech 2020 DNS Challenge测试集上实现SOTA感知质量与竞争性可懂度。  <br/>5. 显著降低实时因子（RTF）和模型体积，提升实际部署可行性。  <br/>6. 开源代码促进技术复现与验证。  <br/><br/>**总结：**  <br/>MeanFlowSE通过单步生成框架和SSL表示，有效提升语音增强效率与性能，降低资源消耗，推动实用化应用。|
|2509.22425v2|[From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation](http://arxiv.org/abs/2509.22425v2)|贡献点（分点）：<br/>1. 提出CSFNet网络结构：引入递归语义增强范式，实现从粗分离到细分离的两阶段处理流程<br/>2. 开发新型递归机制：将粗音频反馈至音频-视觉语音识别模型，结合动态视觉流进行精细分离<br/>3. 设计说话者感知融合模块：通过跨模态编码实现说话者身份的感知融合<br/>4. 创新多范围频时分离网络：同时捕捉局部/全局时频模式，增强分离鲁棒性<br/>5. 实验证明有效性：在3个基准数据集和2个噪声数据集上实现SOTA性能，验证粗到细改进效果<br/><br/>总结：本研究提出CSFNet框架，通过递归语义增强和双模态融合技术显著提升音频-视觉语音分离性能，实现跨模态语义引导与多尺度特征建模的有机结合。|
|2509.21087v2|[Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](http://arxiv.org/abs/2509.21087v2)|**贡献点：**  <br/>1. 首次指出语音增强模型的高表达性可能带来对抗攻击的脆弱性。  <br/>2. 实验证明现代预测性语音增强模型可被对抗噪声操纵，改变输出语义。  <br/>3. 指出扩散模型因设计具有内在的对抗攻击鲁棒性。  <br/><br/>**总结：**  <br/>本文揭示了语音增强模型的对抗脆弱性，并验证了扩散模型的鲁棒性。|
|2509.15666v3|[TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation](http://arxiv.org/abs/2509.15666v3)|总结：  <br/>提出TISDiSS框架，实现训练和推理的可扩展性，通过多损失监督、共享参数设计和动态推理重复，平衡速度与性能，降低参数量并提升低延迟应用效果，实验验证其在语音分离任务中的优越性。<br/><br/>贡献点：  <br/>1. **提出统一框架**：整合early-split多损失监督、共享参数设计与动态推理重复，实现训练与推理的可扩展性。  <br/>2. **灵活速度-性能权衡**：通过调整推理深度实现灵活的速度与性能平衡，无需额外模型训练。  <br/>3. **系统分析设计选择**：揭示训练中增加推理重复可提升浅层推理性能，优化低延迟应用场景。  <br/>4. **实验验证有效性**：在标准基准上实现SOTA性能，同时减少参数量，证明框架的实用性和可扩展性。  <br/>5. **开源代码支持**：提供实现代码，便于复现与进一步研究。|