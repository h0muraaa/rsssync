|Source|Title|Summary|
|---|---|---|
|2509.20072v2|[From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](https://arxiv.org/abs/2509.20072v2)|**论文贡献（分点列出）**  <br/><br/>1. **统一的音文框架 Text‑to‑Talk (TtT)**  <br/>   - 在同一 Transformer 中同时实现自回归 (AR) 文本生成和非自回归 (NAR) 音频扩散，解决了文本依赖目标‑目标关系、音频依赖源‑目标关系的不同建模需求。<br/><br/>2. **基于离散扩散的任意序列自回归特性**  <br/>   - 通过吸收式离散扩散 (absorbing discrete diffusion)，构建统一的训练目标，使文本和音频可以共享同一损失函数进行端到端优化。<br/><br/>3. **模态感知注意力机制**  <br/>   - 设计了能够对文本执行因果解码、对音频块执行双向建模的注意力结构，兼顾两种模态的不同解码约束。<br/><br/>4. **三项训练策略以削减训练‑推理差距**  <br/>   - 包括（a）噪声调度对齐、（b）块级教师强制、（c）可变长输出对齐，提升模型在实际推理时的稳健性。<br/><br/>5. **块式扩散并行生成**  <br/>   - 在推理阶段采用块级扩散，实现音频的并行合成，同时支持可变长度的输出，显著加速生成过程。<br/><br/>6. **广泛实验与细致消融**  <br/>   - 在 Audio‑QA 与 ASR 任务上进行全面评估，实验结果验证了框架及各组件的有效性。<br/><br/>7. **开源资源**  <br/>   - 将公开模型、数据及代码，为后续多模态语音研究提供基准与复现平台。<br/><br/>---<br/><br/>**100字以内的中文总结**  <br/>提出 Text‑to‑Talk，统一 AR 文本生成与 NAR 音频扩散，创新模态感知注意力和三种训练策略，块式并行扩散提升生成效率，并在 Audio‑QA 与 ASR 上验证效果，代码、模型将开源。|
|2509.14684v1|[DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis](https://arxiv.org/abs/2509.14684v1)|**主要贡献**  <br/>1. **提出 DAIEN‑TTS 零-shot TTS 框架**：实现了在合成语音时可独立控制说话人音色和背景环境，实现环境感知的语音合成。  <br/>2. **引入预训练的语音‑环境分离 (SES) 模块**：将混合音频分解为干净语音和环境音的 mel‑spectrogram，为后续的 disentangled audio infilling 提供基础。  <br/>3. **采用双随机跨度掩码并与文本嵌入共同条件**：在干净语音和环境音的 mel‑spectrogram 上同时施加不同长度的掩码，实现对两类音频的同步填充与延续。  <br/>4. **提出双类无监督引导 (DCFG)**：在推理阶段分别对语音和环境两个分支施加类‑free guidance，提升可控性与合成质量。  <br/>5. **设计信噪比 (SNR) 自适应策略**：根据环境提示动态调节合成语音的 SNR，使生成的语音与目标环境更匹配。  <br/>6. **实验验证**：在自然度、说话人相似度和环境保真度方面均显著优于基线，证明了该框架在环境个性化语音合成上的有效性。<br/><br/>**100字以内摘要**  <br/>DAIEN‑TTS 基于 F5‑TTS，融合预训练语音‑环境分离、双随机掩码填充与双类无监督引导，实现说话人与环境的独立、零-shot 控制，并通过 SNR 自适应提升匹配度，实验表明其在自然度、说话人相似度和环境保真度上均表现优异。|