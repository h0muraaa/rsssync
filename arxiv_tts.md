|Source|Title|Summary|
|---|---|---|
|2508.08487v1|[MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling](http://arxiv.org/abs/2508.08487v1)|总结：  <br/>提出MAViS多代理协作框架，解决长视频生成的三方面局限性，实现多模态叙事输出，并通过模块化设计提升可扩展性与生成质量，达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出MAViS框架**：首个端到端多代理协作系统，专门针对长序列视频生成中的辅助能力、视觉质量和表现力问题。  <br/>2. **多阶段协同机制**：整合剧本创作、镜头设计、角色建模、关键帧生成、视频动画和音频生成等阶段，通过多智能体分工优化生成流程。  <br/>3. **3E原则指导**：在每个生成阶段引入"探索-审视-增强"原则，确保中间输出的完整性与质量。  <br/>4. **剧本写作指南**：设计专用指导方案，提升剧本与生成工具间兼容性，减少生成误差。  <br/>5. **模块化架构**：支持灵活集成多样化的生成模型和工具，实现框架的可扩展性。  <br/>6. **多模态输出创新**：唯一实现视频叙事与背景音乐同步生成的框架，突破现有单模态生成局限。  <br/>7. **实验验证效果**：在辅助性、视觉质量和表现力三方面均达到SOTA水平，证明框架有效性。|
|2508.07337v1|[KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based   Audio and Handcrafted Visual Features](http://arxiv.org/abs/2508.07337v1)|**贡献点总结：**  <br/>1. 提出多模态方法应对新型音频深度伪造检测挑战；  <br/>2. 视觉模态采用手工特征提升可解释性与适应性；  <br/>3. 音频模态融合自监督学习与图注意力网络优化音频表征；  <br/>4. 实现检测性能与实际部署成本的平衡，具备抗攻击性和可解释性；  <br/>5. 在AV-Deepfake1M++数据集上取得AUC 92.78%与IoU 0.3536的优异结果。  <br/><br/>**摘要总结（100字内）:**  <br/>该研究针对新型音频深度伪造检测，提出多模态系统结合手工特征与自监督音频模型，有效平衡性能与部署成本，在AV-Deepfake1M++数据集上实现高分类准确率和时序定位精度。|
|2508.06870v1|[Text to Speech System for Meitei Mayek Script](http://arxiv.org/abs/2508.06870v1)|总结：  <br/>本文提出一种基于Meitei Mayek文字的Manipuri语言TTS系统，结合Tacotron 2和HiFi-GAN框架，解决了声调音系与低资源语言的合成挑战，为语言保护与技术应用提供新路径。<br/><br/>贡献点：  <br/>1. **构建首个Manipuri语言TTS系统**：基于Meitei Mayek文字开发，填补该语言在语音合成领域的空白。  <br/>2. **优化神经网络架构**：针对声调音系设计适配模型（Tacotron 2 + HiFi-GAN），提升低资源语言的合成效果。  <br/>3. **创建音素映射与单说话者数据集**：实现Meitei Mayek到ARPAbet的音素转换，并建立高质量单说话者语料库。  <br/>4. **验证合成质量**：通过主观与客观指标证明语音的可懂度与自然度，推动语言技术的实用化。|
|2508.06391v1|[Improved Dysarthric Speech to Text Conversion via TTS Personalization](http://arxiv.org/abs/2508.06391v1)|**贡献点：**  <br/>1. **提出个性化合成语音生成方法**：通过结合患者的预疾病语音记录与说话人嵌入插值技术，生成可控严重程度的合成构音障碍语音，为ASR模型提供多样化训练数据。  <br/>2. **改进零样本ASR性能**：利用合成语音与真实数据联合微调，显著将字符错误率（CER）从36-51%降至7.3%，解决数据稀缺下的识别难题。  <br/>3. **验证模型有效性**：开发的匈牙利语单语ASR模型（FastConformer_Hu）在微调后优于Whisper-turbo，并通过合成语音实现18%的相对CER下降。  <br/>4. **强调个性化系统应用价值**：证明个性化ASR系统在提升严重构音障碍患者语音识别准确性和可访问性方面的潜力。  <br/><br/>**总结**（100字内）：  <br/>本研究通过合成语音生成与个性化微调，显著提升匈牙利语构音障碍患者的ASR准确率，验证了定制化模型的优越性，并为语音障碍辅助技术提供了新思路。|
|2508.06098v1|[MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows](http://arxiv.org/abs/2508.06098v1)|**贡献点总结：**  <br/>1. 提出MeanAudio：基于MeanFlow的新型文本到音频生成模型，实现快速且忠实的生成。  <br/>2. 引入Flux-style latent transformer：通过回归平均速度场，直接映射生成轨迹起点与终点。  <br/>3. 整合Classifier-Free Guidance（CFG）：无需额外成本即可实现引导采样。  <br/>4. 设计即时-平均课程策略：结合流场混合，分阶段学习瞬时动力学与平均流，增强训练稳定性。  <br/>5. 实验验证：在单步生成中达到SOTA性能（RTF=0.013，速度提升100倍），并支持多步生成的平滑过渡。  <br/><br/>**摘要总结（100字内）：**  <br/>本文提出MeanAudio，基于MeanFlow和Flux-style latent transformer，通过回归平均速度场和CFG技术实现高效快速文本到音频生成，引入分阶段训练策略提升稳定性，实验验证其在单步生成中显著优于现有模型，速度提升100倍，并支持多步生成的流畅性。|
|2508.05978v1|[DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism   and Flow Matching](http://arxiv.org/abs/2508.05978v1)|总结：  <br/>DAFMSVC通过目标相似SSL特征替换、双交叉注意力机制融合多模态信息及流匹配模块提升生成质量，有效解决音色泄露问题，在音色相似度和自然度上超越现有方法。<br/><br/>贡献点：  <br/>1. **创新方法提出**：首次将目标音频的最相似自监督学习（SSL）特征替代源音频特征，有效防止音色泄露（timbre leakage）。  <br/>2. **双交叉注意力机制**：设计双重跨注意力模块，实现说话人嵌入、旋律与语言内容的自适应融合。  <br/>3. **流匹配模块**：引入流匹配技术，显著提升从融合特征生成高质量音频的效果。  <br/>4. **实验验证优势**：在主观和客观评估中均验证DAFMSVC在音色相似度、自然度及质量上的优越性。|
|2508.04529v1|[ESDD 2026: Environmental Sound Deepfake Detection Challenge Evaluation   Plan](http://arxiv.org/abs/2508.04529v1)|总结：  <br/>提出首个大规模环境声音深度伪造数据集EnvSDD及配套检测挑战，推动深度伪造检测技术发展与实际场景应用。<br/><br/>贡献点：  <br/>1. **构建首个大规模ESDD数据集**：EnvSDD包含45.25小时真实音频与316.7小时伪造音频，解决现有数据集规模小、类型单一的问题。  <br/>2. **设计双赛道挑战框架**：针对未见过的音频生成器（Unseen Generators）和黑盒低资源场景（Black-Box Low-Resource ESDD），覆盖深度伪造检测的实际挑战。  <br/>3. **促进学术与产业结合**：通过与ICASSP 2026联合举办挑战，推动深度伪造检测技术的标准化、实用化和社区研究交流。|
|2508.04195v1|[NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations](http://arxiv.org/abs/2508.04195v1)|总结：  <br/>提出NVSpeech系统，首次实现中文副语言发声的识别与生成一体化，包含大规模标注数据集、联合转录ASR模型与可控TTS技术。<br/><br/>贡献点：  <br/>1. 构建首个大规模中文副语言数据集（174,179条，573小时），包含词级对齐和18类副语言标签。  <br/>2. 开发具有副语言感知能力的ASR模型，支持将非语言声音作为可解码标记进行联合转录。  <br/>3. 通过零样本微调使TTS具备显式控制副语言发声的能力，实现上下文感知的语境化插入。  <br/>4. 提出统一的可扩展框架，首次实现中文副语言识别与生成的端到端处理，提升语音建模表达性。|
|2508.03543v1|[EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable   Text-to-Speech via Activation Steering](http://arxiv.org/abs/2508.03543v1)|总结：  <br/>提出训练免费的EmoSteer-TTS方法，通过激活引导实现TTS情感的精细控制（转换、插值、擦除），兼容多种预训练模型，并构建了多样性情感数据集，实验表现优于SOTA。<br/><br/>贡献点：  <br/>1. **训练免费方法**：首个无需额外训练数据即可实现细粒度情感控制的TTS系统，突破传统依赖高质量数据集的限制。  <br/>2. **激活引导技术**：通过修改流匹配模型内部激活，直接操控合成语音的情感属性（转换、插值、擦除），实现更灵活的情感生成。  <br/>3. **高效算法框架**：提出包含激活提取、情感标记搜索与推理时引导的三阶段流程，具备高效率并可无缝集成至主流预训练模型（如F5-TTS、CosyVoice2、E2-TTS）。  <br/>4. **专用情感数据集**：构建包含多说话人、多样化情感的精选数据集，为连续情感控制提供数据支持。  <br/>5. **性能突破**：实验验证在情感控制的可解释性、连续性和精细度上显著优于现有SOTA方法，填补训练免费场景下的技术空白。|
|2508.00733v4|[AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation](http://arxiv.org/abs/2508.00733v4)|总结：  <br/>AudioGen-Omni提出了一种统一的多模态生成框架，通过创新的联合训练和跨模态对齐技术，实现高质量音频、语音、歌曲与视频的同步生成，并在多个任务中取得SOTA性能。<br/><br/>贡献点：  <br/>1. **统一生成框架**：基于MMDit构建首个多模态音频生成模型，支持高保真音频、语音和歌曲的同步生成。  <br/>2. **多模态联合训练**：引入文本-视频-音频联合训练范式，整合大规模语料库，提升语义丰富性和跨模态适应能力。  <br/>3. **多模态编码器设计**：开发统一歌词-听写编码器，将歌曲和语音的字形、音素编码为帧级密集表示。  <br/>4. **跨模态对齐技术**：采用AdaLN联合注意力机制，结合PAAPI（相位对齐各向异性位置注入）和RoPE，增强时间结构化模态的对齐精度。  <br/>5. **高效生成能力**：通过解冻所有模态并掩码缺失输入，突破传统文本冻结范式的限制，实现更灵活的跨模态条件生成。  <br/>6. **性能突破**：在文本到音频/语音/歌曲任务中达到SOTA结果，且推理效率达1.91秒生成8秒音频。|
|2507.22746v2|[Next Tokens Denoising for Speech Synthesis](http://arxiv.org/abs/2507.22746v2)|**贡献点**  <br/>1. 首次提出Dragon-FM模型，融合自回归（AR）与流匹配（Flow-Matching）方法，解决传统模型局限性。  <br/>2. 采用高效音频编码器与分块处理策略（12.5 tokens/sec），兼顾全局连贯性与生成速度。  <br/>3. 创新性地实现跨块AR建模与块内并行流匹配，支持KV缓存与双向上下文利用。  <br/>4. 构建连续与离散特征建模的统一框架，证明连续AR流匹配可预测离散token。  <br/>5. 验证模型在长音频生成（如播客）中的有效性，支持零样本高质量内容生成。  <br/><br/>**总结（100字以内）**  <br/>Dragon-FM融合自回归与流匹配，通过分块处理与高效编码实现长音频生成，支持零样本播客生成，突破传统模型在上下文利用和生成速度上的限制。|
|2507.22612v1|[Adaptive Duration Model for Text Speech Alignment](http://arxiv.org/abs/2507.22612v1)|**贡献点：**  <br/>1. 提出了一种新颖的持续时间预测框架，无需外部数据即可生成更精确的音素级持续时间分布。  <br/>2. 实验验证该框架在对齐准确率上比基线模型提升约11.3%，增强对长语句和跨领域文本的适应能力。  <br/>3. 改进非自回归TTS模型的鲁棒性，使其在零样本场景下更有效应对提示音频与输入音频的不匹配问题。  <br/><br/>**总结：**  <br/>该研究提出了一种无需外部数据的音素级持续时间建模方法，显著提升了TTS模型的对齐准确率和跨场景适应能力，增强了零样本任务的鲁棒性。|
|2507.21463v1|[SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset   Incorporating Cutting-Edge Generation Methods](http://arxiv.org/abs/2507.21463v1)|总结：  <br/>本文提出SpeechFake数据集，包含300万样本、3000小时多语言音频，覆盖多种生成技术，提供基准模型测试与影响因素分析，推动语音深度伪造检测研究。<br/><br/>贡献点：  <br/>1. **大规模多语言数据集**：构建包含300万样本、3000小时音频的SpeechFake数据集，覆盖46种语言，提升模型泛化能力。  <br/>2. **多样化生成技术**：集成文本转语音、语音转换、神经声码器等40种合成工具，全面覆盖当前深度伪造语音生成方法。  <br/>3. **基准模型测试**：在SpeechFake上训练检测模型，展示其在自身测试集及多种未见数据集上的高性能表现。  <br/>4. **影响因素分析**：系统实验探究生成方法、语言多样性及说话人变化对检测性能的作用，揭示关键挑战。  <br/>5. **研究资源价值**：为语音深度伪造检测提供高质量数据资源，助力开发更鲁棒的模型应对生成技术的演进。|
|2507.21150v1|[WaveVerify: A Novel Audio Watermarking Framework for Media   Authentication and Combatting Deepfakes](http://arxiv.org/abs/2507.21150v1)|总结：  <br/>本文聚焦语音生成技术带来的双重影响，揭示深度伪造诈骗的激增风险及金融损失，并呼吁开发音频认证与水印技术以加强监管和媒体可信度。<br/><br/>贡献点：  <br/>1. **问题量化分析**：首次引用2024年深度伪造诈骗增长1300%的行业数据，凸显技术风险的紧迫性。  <br/>2. **金融安全警示**：提供具体经济损失案例（如1000万美元的语音诈骗损失），明确技术滥用对经济领域的威胁。  <br/>3. **监管需求框架**：强调司法与政府需加强AI内容透明性与可追溯性，系统性提出音频认证工具和水印技术作为关键解决方案。  <br/>4. **跨领域呼吁**：连接技术发展与社会治理需求，推动语音领域与金融、法律等领域的协同防护机制。|
|2507.20880v1|[JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability   and Aesthetic Alignment](http://arxiv.org/abs/2507.20880v1)|总结：  <br/>本论文提出JAM模型，首次实现歌词到歌曲生成的词级时间与持续控制，结合直接偏好优化提升音乐质量，并构建公共数据集JAME标准化评估，显著优于现有模型。<br/><br/>贡献点：  <br/>1. **首创新模型JAM**：首次在流匹配框架中实现歌词到歌曲生成的词级时间（timing）和持续时间（duration）控制，支持细粒度的语音调控。  <br/>2. **提升音乐质量**：采用直接偏好优化（Direct Preference Optimization）方法，通过合成数据迭代训练模型，无需依赖人工标注数据，增强生成音频与人类偏好的契合度。  <br/>3. **构建评估数据集JAME**：提出首个公共的歌词到歌曲模型评估数据集，推动该领域评价标准的统一化与客观化。|
|2507.20579v1|[AV-Deepfake1M++: A Large-Scale Audio-Visual Deepfake Benchmark with   Real-World Perturbations](http://arxiv.org/abs/2507.20579v1)|**贡献点总结（100字以内）**  <br/>提出AV-Deepfake1M++数据集，包含200万视频片段，融合多样化生成策略与音视频扰动，支持深度伪造检测研究，并举办相关挑战赛以推动该领域技术评估与进展。<br/><br/>**分点贡献**  <br/>1. **扩展数据集规模**：构建AV-Deepfake1M++，包含200万视频片段，显著提升原有数据集的样本数量。  <br/>2. **多样化生成策略**：引入丰富的操控方法和音频-视觉扰动策略，增强数据集的多样性与真实性。  <br/>3. **方法论与基准测试**：系统描述数据生成策略，并采用最新技术对数据集进行基准测试，验证其有效性。  <br/>4. **推动研究与挑战**：发起2025年1M-Deepfakes检测挑战，提供公开数据、评测方案及研究许可，促进学术研究与技术发展。|
|2507.20140v1|[Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot   Text-to-Speech](http://arxiv.org/abs/2507.20140v1)|总结：  <br/>该论文提出首个针对ZS-TTS系统的机器遗忘框架TGU，通过引入随机性机制实现说话人身份删除，设计新的评估指标spk-ZRF，并验证其在保持其他语音质量的同时有效保护隐私。  <br/><br/>贡献点：  <br/>1. **提出首个ZS-TTS机器遗忘框架（TGU）**：针对ZS-TTS系统，实现选择性删除特定说话人身份以保护隐私，同时保留对其他说话人的语音生成能力。  <br/>2. **引入随机性机制**：通过注入随机性防止模型一致性复制被遗忘说话人声音，使得这些身份无法被追溯。  <br/>3. **设计新型评估指标（spk-ZRF）**：专门衡量模型是否能忽略与被遗忘说话人相关的提示，量化其知识遗忘效果。  <br/>4. **实验验证有效性**：在SOTA模型上证明TGU能有效防止复制被遗忘说话人语音，且保持其他语音的高保真质量。  <br/>5. **提供公开Demo**：通过实际演示展示方法的应用效果，增强可复现性和实用性。|
|2507.18044v1|[Synthetic Data Generation for Phrase Break Prediction with Large   Language Model](http://arxiv.org/abs/2507.18044v1)|**总结（100字以内）：**  <br/>本研究提出利用大语言模型生成合成短语切分标注，有效减少人工标注成本，缓解语音领域数据不一致问题，并验证其在多语言中的适用性，展示LLM在语音任务中的潜力。  <br/><br/>**贡献点（分点列出）：**  <br/>1. **提出LLM驱动的合成标注方法**：首次探索使用大语言模型（LLM）生成短语切分预测的合成标注数据，降低对大规模人工标注的依赖。  <br/>2. **解决语音数据的固有变异性**：通过模型生成的数据应对语音领域因语音因素导致的标注不一致挑战，提升数据质量与可用性。  <br/>3. **多语言有效性验证**：跨语言评估生成标注的准确性，证明其在不同语言中的适用性，拓展LLM在语音领域的通用性。  <br/>4. **展示LLM的语音应用潜力**：验证LLM在语音任务中的可行性，为语音合成与处理领域的数据生成提供新的解决方案。|
|2507.16835v1|[Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI   Interview Systems](http://arxiv.org/abs/2507.16835v1)|**贡献点：**  <br/>1. **大规模实证比较**：基于超过30万次AI面试数据，系统评估不同STT-LLM-TTS组件组合的性能，揭示实际生产环境中的有效配置。  <br/>2. **自动化评估框架**：提出LLM-as-a-Judge方法，实现对话质量、技术准确性及技能评估能力的高效、可重复评估。  <br/>3. **关键配置发现**：证明Google STT与GPT-4.1的组合在对话和技术质量指标上显著优于其他方案。  <br/>4. **用户体验新洞见**：指出客观质量指标与用户满意度相关性弱，强调技术性能之外的用户体验因素（如自然度、交互流畅性）。  <br/>5. **实践指导与方法论**：为多模态对话AI系统组件选择提供依据，并验证了一套适用于语音交互的评估体系。  <br/><br/>**总结（100字以内）：**  <br/>本研究基于大规模AI面试数据，系统评估语音对话系统组件组合，提出LLM-as-a-Judge评估框架，发现Google STT+GPT-4.1性能最优，揭示用户体验与技术指标的脱节，为实际应用提供方法论与选择指导。|
|2507.15272v1|[A2TTS: TTS for Low Resource Indian Languages](http://arxiv.org/abs/2507.15272v1)|**贡献点总结：**  <br/>1. 提出基于扩散模型的说话人条件TTS框架，通过短音频嵌入实现多说话人生成。  <br/>2. 引入跨注意力机制改进时长预测，提升韵律自然度和说话人一致性。  <br/>3. 采用分类器自由指导技术，增强零样本说话人生成能力。  <br/>4. 构建语言特定模型，支持多种印度语言（如印地语、泰米尔语等）的语音生成。  <br/><br/>**摘要总结（100字内）:**  <br/>该论文提出一种说话人条件的文本到语音系统，基于扩散模型与跨注意力机制，支持多语言和零样本生成，提升语音的自然度与说话人一致性，适用于印度多种语言的语音合成任务。|
|2507.15202v2|[TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style](http://arxiv.org/abs/2507.15202v2)|总结：  <br/>该论文提出TalkLess系统，通过融合句子提取与语音重合成技术，实现语音内容压缩与风格保留，并通过界面设计与实验证明其有效性和用户友好性。<br/><br/>贡献点：  <br/>1. **提出混合编辑方法**：TalkLess结合句子提取（extraction）与语音抽象（abstraction）技术，突破传统单一流派的局限，在压缩语音的同时保留内容和说话者风格。  <br/>2. **分层交互界面设计**：通过分离“压缩面板”（低级措辞调整）与“大纲面板”（高级内容编辑），增强用户对自动化编辑过程的可控性与灵活性。  <br/>3. **实验证明有效性**：对比实验（N=12）显示TalkLess在内容覆盖率和错误消除方面优于现有提取方法，并显著降低编辑认知负荷与工作量。  <br/>4. **探索实际应用潜力**：通过创作者自主编辑语音的探索性研究（N=3），验证系统在真实场景中的实用价值与用户接受度。|
|2507.15202v1|[TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style](http://arxiv.org/abs/2507.15202v1)|总结：  <br/>TalkLess创新性地融合提取与抽象方法，实现语音内容精简与风格保留，通过智能编辑流程与交互界面降低认知负荷，提升编辑效率与质量，验证了其在实际应用中的潜力。<br/><br/>贡献点：  <br/>1. **方法创新**：提出结合句法提取（去除完整句子）与语义抽象（重合成简练内容）的双阶段语音编辑框架，突破传统单一方法的局限性。  <br/>2. **流程优化**：设计"生成候选文本→选择最优编辑→音频合成"的三级处理流程，兼顾内容压缩、覆盖范围与音频质量。  <br/>3. **交互设计**：开发分层控制界面（压缩面板与大纲面板），允许创作者分别调整细节表述与核心内容。  <br/>4. **性能提升**：通过对比实验（N=12）验证，实现比现有技术更高的内容覆盖度与语音错误去除率。  <br/>5. **用户验证**：通过探索性实验（N=3）证明系统在创作者自主修改语音场景下的实际应用价值。|
|2507.15007v2|[Hear Your Code Fail, Voice-Assisted Debugging for Python](http://arxiv.org/abs/2507.15007v2)|总结：  <br/>提出一种基于语音的Python调试插件，通过多模态反馈提升错误诊断效率，显著降低认知负担并支持视觉障碍开发者和教育场景。<br/><br/>贡献点：  <br/>1. **创新性语音调试工具**：首个将运行时错误转化为语音诊断的Python插件，实现审计与可视化双通道反馈。  <br/>2. **高效技术架构**：采用全局异常钩子机制与pyttsx3/Tkinter技术，确保低延迟（<1.2秒）和低CPU开销（<18%）。  <br/>3. **跨平台兼容性**：支持Python 3.7+在Windows/macOS/Linux环境运行，适配广谱开发场景。  <br/>4. **简化集成**：仅需两行代码即可部署，降低使用门槛，提升对视觉障碍者的支持。  <br/>5. **教育价值**：实验证明初学者调试技能习得速度提升45%，凸显教学应用潜力。  <br/>6. **未来扩展性**：计划集成GPT修复建议和实时多语言翻译，推动语音调试技术标准化与智能化。|
|2507.15007v1|[Hear Your Code Fail, Voice-Assisted Debugging for Python](http://arxiv.org/abs/2507.15007v1)|总结：  <br/>提出一种语音辅助Python调试插件，通过多模态反馈提升错误诊断效率，显著降低认知负荷并拓展编程可访问性，具有教育和实际应用价值。<br/><br/>贡献点：  <br/>1. **开发语音调试工具**：首个将无声运行错误转化为可听诊断的Python插件，支持语音输出与可视化界面同步反馈。  <br/>2. **多模态反馈架构**：集成全局异常钩架构、pyttsx3语音合成与Tkinter可视化，实现听觉与视觉双重错误提示。  <br/>3. **性能优化**：在异常处理中实现低于1.2秒的语音延迟和18%以下的CPU开销。  <br/>4. **跨平台兼容性**：适用于Python 3.7+的Windows、macOS和Linux系统，扩展性强。  <br/>5. **低代码集成**：仅需两行代码即可实现功能，提升插件普及率。  <br/>6. **教育应用价值**：试点研究表明可加速新手程序员调试技能学习（提升45%）。  <br/>7. **辅助特殊群体**：支持视力障碍者及多任务操作，提升编程可及性。  <br/>8. **未来扩展方向**：计划引入GPT修复建议和实时多语言翻译，推动听觉调试范式发展。|
|2507.14988v1|[DMOSpeech 2: Reinforcement Learning for Duration Prediction in   Metric-Optimized Speech Synthesis](http://arxiv.org/abs/2507.14988v1)|**贡献点**：<br/>1. 引入强化学习框架（GRPO）优化时长预测器，结合说话人相似度和词错误率作为奖励信号，首次对扩散模型语音合成的时长预测模块进行感知指标优化。  <br/>2. 提出教师引导采样方法，通过教师模型预处理后再由学生模型生成，提升输出多样性并保持计算效率。  <br/>3. 实现全面的感知指标优化合成管线，相比前作在所有评价指标上表现更优，同时将采样步数减少50%且不损失语音质量。  <br/><br/>**总结**：  <br/>DMOSpeech 2通过强化学习优化时长预测与教师引导采样，构建更完整的感知指标优化合成系统，提升性能并降低计算成本。|
|2507.13052v1|[Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient   Communication](http://arxiv.org/abs/2507.13052v1)|总结：  <br/>该论文提出了一种基于XR的智能虚拟超声助手（IVS），通过整合LLM对话、语音转换与机器人控制，实现医-机-患三元实时交互，提升超声采集效率及患者体验，为医疗机器人沟通系统提供新思路。<br/><br/>贡献点：  <br/>1. **提出智能虚拟超声助手（IVS）新角色**：首次探索在医生、机器人超声系统与患者之间引入IVS作为沟通桥梁，填补现有研究空白。  <br/>2. **XR环境下的对话虚拟代理设计**：开发支持医生与患者实时互动的XR平台，实现跨模态（语音、文本、动作）的多角色协作。  <br/>3. **多模态技术融合**：结合LLM驱动的对话系统、语音-文本转换和机器人控制技术，提升超声操作的效率、清晰度和可访问性。  <br/>4. **增强患者体验与接受度**：通过情感化沟通（同理心解释和安慰）和透明操作反馈，改善患者对机器人超声的接受度和信任感。|
|2507.12932v1|[Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy   Protection against Voice Deepfakes](http://arxiv.org/abs/2507.12932v1)|**贡献点总结（100字以内）：**  <br/>提出Enkidu框架，结合黑盒知识与少样本训练生成频率扰动，在保持语音质量的同时实现高效率、强泛化能力，有效防御语音深度伪造攻击。<br/><br/>**分点贡献：**  <br/>1. **创新框架设计**：提出首个用户导向的隐私保护框架Enkidu，通过频率域噪声扰动对抗个性化语音深度伪造攻击。  <br/>2. **黑盒与少样本训练**：利用黑盒知识生成通用扰动，仅需少量用户数据进行训练，突破传统白盒依赖和数据需求限制。  <br/>3. **高效轻量化保护**：实现实时、低内存（最低0.004GB）和低时间成本（最低0.004实时系数）的加密，显著优于现有方案。  <br/>4. **强泛化能力**：支持任意长度音频，适应不同语音合成模型，有效抵御vanilla和adaptive攻击。  <br/>5. **语音质量保障**：在抗攻击的同时保持高感知质量与语音可懂度，提升实际应用可行性。|
|2507.12197v1|[Quantize More, Lose Less: Autoregressive Generation from Residually   Quantized Speech Representations](http://arxiv.org/abs/2507.12197v1)|**贡献点：**  <br/>1. 提出QTTS框架，结合新音频编码器QDAC实现更高质量的语音合成。  <br/>2. QDAC通过端到端训练ASR-based自回归网络与GAN，有效解耦语义特征，提升可扩展的近无损压缩能力。  <br/>3. 引入分层并行结构（Hierarchical Parallel），利用双AR模型建模码本间依赖，优化合成质量。  <br/>4. 设计延迟多头方法（Delay Multihead），通过固定延迟并行预测加速推理速度。  <br/>5. 实验验证框架在保留表达内容（如韵律、音色）和高保真语音音频生成方面优于基线方法。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出QTTS框架，通过QDAC编码器与分层并行、延迟多头等创新策略，解决传统TTS的信息丢失问题，显著提升合成质量与推理效率，为高质量、通用的语音和音频生成提供新方向。|
|2507.11777v1|[Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake   Detection](http://arxiv.org/abs/2507.11777v1)|**贡献点：**<br/>1. **改进Anti-Spoofing架构**：提出三种关键修改——冻结Wav2Vec 2.0编码器保留自监督表示、替换图注意力块为标准化多头注意力模块（含异构查询投影）、采用可训练上下文感知集成层替代启发式帧段融合。  <br/>2. **提升系统性能**：在ASVspoof 5数据集上实现7.6%的等错误率（EER），优于同等训练条件下的AASIST基线模型。  <br/>3. **验证改进有效性**：通过消融实验证明各模块修改对整体性能的独立贡献，支持针对性模型优化策略。  <br/>4. **开源代码**：提供公共代码库，促进研究复现与实际应用。|
|2507.10827v2|[Supporting SENCOTEN Language Documentation Efforts with Automatic Speech   Recognition](http://arxiv.org/abs/2507.10827v2)|总结：  <br/>该研究提出结合TTS和迁移学习的ASR文档框架，通过数据增强和语言模型优化应对SENCOTEN语言的特殊挑战，验证了其在语言保护中的应用潜力。<br/><br/>贡献点：  <br/>1. **构建ASR驱动的文档框架**：首创融合TTS系统生成数据与跨语言迁移学习的流程，提升低资源语言的ASR性能。  <br/>2. **解决语言特性挑战**：针对SENCOTEN的多音节结构和元音交替现象，提出适配性数据增强及语言模型优化方法。  <br/>3. **语言模型融合技术**：采用浅层融合或n-best恢复策略，最大化利用有限数据提升识别准确性。  <br/>4. **实验验证效果**：在SENCOTEN数据集上取得14.32% WER和3.45% CER的改进结果，验证方法有效性。  <br/>5. **支持语言复兴应用**：通过降低OOV率和错误率，为社区语言保护与教育资源开发提供技术支撑。|
|2507.10827v1|[Supporting SENĆOTEN Language Documentation Efforts with Automatic   Speech Recognition](http://arxiv.org/abs/2507.10827v1)|**贡献点分点：**  <br/>1. **提出ASR驱动的文档流程**：整合文本转语音（TTS）系统生成的增强数据与跨语言迁移学习，利用语音基础模型（SFMs）解决数据稀缺问题。  <br/>2. **优化语言模型设计**：通过浅层融合或n-best恢复引入n-gram语言模型，最大化有限数据的使用效率。  <br/>3. **实验验证有效性**：在SEN'CO TEN语料库上实现14.32% WER和3.45% CER，降低OOV率至26.48%，表明方法对语言文档的可行性。  <br/>4. **支持语言复兴实践**：为社区提供可落地的ASR技术方案，助力语言保护与教育资源开发。  <br/><br/>**总结（100字以内）**：  <br/>提出ASR驱动的文档框架，结合TTS增强数据与跨语言迁移学习，优化语言模型设计，降低错误率，为濒危语言SEN'CO TEN的保护与复兴提供技术支撑。|
|2507.10469v1|[An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived   Realism and Performance in Virtual Reality Environments](http://arxiv.org/abs/2507.10469v1)|**贡献点分点列出：**  <br/>1. **评估AI NPC在VR审讯场景中的综合表现**：首次系统性地分析AI驱动NPC在虚拟现实游戏中的感知现实感、可用性及系统性能。  <br/>2. **引入GPT-4 Turbo模型**：利用该模型模拟嫌疑人与搭档角色，验证其在复杂对话交互中的有效性。  <br/>3. **量化延迟与可信度指标**：通过实测数据揭示系统延迟（平均7秒）与NPC可信度（6.67/10）间的关联，侧重情感与个性维度的不足。  <br/>4. **提出性能优化需求**：为实现更沉浸的VR体验，明确需优化语音转换（STT/TTS）延迟及增强情感表达深度。  <br/><br/>**总结（100字以内）：**  <br/>本研究评估AI NPC在VR审讯模拟器中的表现，揭示模型延迟与情感深度不足问题，验证LLM提升互动性的潜力，强调性能优化对沉浸体验的重要性。|
|2507.09318v1|[ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow   Matching](http://arxiv.org/abs/2507.09318v1)|**贡献点总结:**  <br/>1. **提出非自回归零样本语音对话生成模型** ZipVoice-Dialog，克服传统自回归模型在推理速度和稳定性上的缺陷。  <br/>2. **创新技术设计**：引入说话人轮次嵌入实现精准对话轮次控制，课程学习策略增强语音-文本对齐稳定性，专有策略支持双声道对话生成。  <br/>3. **构建首个大规模开放语音对话数据集** OpenDialog（6.8k小时），填补领域数据缺口。  <br/>4. **建立全面评估基准**，推动语音对话生成模型的性能比较与研究进展。  <br/>5. **开源代码与资源**，包括模型、数据集和示例，促进研究复现与应用。  <br/><br/>**总结（100字以内）:**  <br/>提出非自回归语音对话生成模型ZipVoice-Dialog，结合创新设计与OpenDialog数据集，建立评估基准，显著提升对话生成的可懂度、轮次准确性及推理效率，并开源资源推动领域发展。|
|2507.09310v1|[Voice Conversion for Lombard Speaking Style with Implicit and Explicit   Acoustic Feature Conditioning](http://arxiv.org/abs/2507.09310v1)|**贡献点:**  <br/>1. 提出针对 Lombard 说话风格的语音转换方法，实现说话者身份转换同时保留声学属性。  <br/>2. 对比隐式与显式声学特征条件模型，验证隐式策略在可懂度和说话者相似性保持上的有效性。  <br/>3. 证明隐式条件策略可达到与显式特征条件模型相近的语音可懂度提升效果，减少对目标数据的依赖。  <br/>4. 为在缺乏 Lombard 语音数据时训练 TTS 系统提供了一种可行的声学增强方案。  <br/><br/>**总结:**  <br/>该研究提出基于隐式声学条件的 Lombard 风格语音转换方法，有效平衡了语音可懂度提升与说话者身份保留，为 TTS 训练提供了数据高效的解决方案。|
|2507.09282v1|[ClaritySpeech: Dementia Obfuscation in Speech](http://arxiv.org/abs/2507.09282v1)|总结（100字以内）:  <br/>本研究提出ClaritySpeech框架，通过结合ASR、文本混淆和零样本TTS技术，在低数据环境下无需微调即可纠正痴病音语音，提升语音质量和隐私，保持50%的说话人相似性，并在ADReSS和ADReSSo数据集上验证了有效性。<br/><br/>贡献点分点如下：  <br/>1. **提出新型框架ClaritySpeech**：首次整合自动语音识别（ASR）、文本混淆和零样本文本到语音（TTS）技术，用于处理病态语音的隐私和可访问性挑战。  <br/>2. **无需微调的低资源应用**：在低数据环境下直接运行，无需对模型进行微调，降低部署成本，适应资源受限场景。  <br/>3. **隐私与可访问性平衡**：通过语音混淆保持说话人身份，同时提高语音可理解性（如WER从0.73降至0.08），兼顾隐私保护与技术实用性。  <br/>4. **多模态验证**：在音频、文本和融合模态下进行对抗性实验，验证系统在不同场景下的鲁棒性（F1得分下降16%和10%）。  <br/>5. **客观性能提升**：实验证明系统显著改善语音质量（从1.65提升至2.15），且在ADReSS和ADReSSo数据集上保持较高的说话人相似性（50%）。|
|2507.08983v1|[Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](http://arxiv.org/abs/2507.08983v1)|总结：  <br/>该研究揭示模型排名系统成为对抗攻击者隐蔽分发污染模型的渠道，提出通用框架TrojanClimb，并在文本嵌入、生成、语音和图像等四类模态中验证其攻击有效性，强调需重新设计评估机制以防御潜在威胁。<br/><br/>贡献点：  <br/>1. **首次揭示模型排行榜的安全漏洞**：指出排名系统可被恶意利用作为大规模隐蔽分发污染模型的渠道，突破了传统对抗攻击研究的边界。  <br/>2. **提出通用攻击框架TrojanClimb**：设计可跨模态（文本、语音、图像等）的框架，实现恶意行为注入的同时保持排名竞争力。  <br/>3. **验证跨模态攻击可行性**：在四类不同模态中证明攻击者可通过该框架成功嵌入后门、偏见等有害功能并获得高排名。  <br/>4. **推动安全机制的重新设计**：呼吁对模型排行榜的评估机制进行改进，以检测和过滤污染模型，提出系统性防御方向。  <br/>5. **揭示机器学习社区的广泛安全隐患**：强调从非验证源采用模型的风险，推动行业对模型可信赖性问题的重视。|
|2507.08530v1|[MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling](http://arxiv.org/abs/2507.08530v1)|总结：  <br/>本文提出MIDI-VALLE，通过结合参考音频与MIDI条件建模、离散token编码及多样化数据训练，显著提升音乐性能合成质量与泛化能力，超越现有基线方法。<br/><br/>贡献点：  <br/>1. **引入MIDI-VALLE**：将VALLE框架扩展至音乐领域，实现零样本个性化MIDI-to-audio合成，突破传统两阶段流程的限制。  <br/>2. **双模态离散编码**：同时将MIDI和音频表示为离散token，提升模型对钢琴演奏表现的建模一致性与鲁棒性。  <br/>3. **增强泛化能力**：通过大规模多样化钢琴表演数据集训练，使模型适应不同风格、乐器和录音环境的合成任务。  <br/>4. **显著性能提升**：在ATEPP和Maestro数据集上，FAD降低超75%，听觉测试中获得202票（基线仅58票），验证合成效果与泛化性优势。|
|2507.08319v1|[Active Learning for Text-to-Speech Synthesis with Informative Sample   Collection](http://arxiv.org/abs/2507.08319v1)|贡献点：  <br/>1. 提出基于主动学习的TTS语料构建方法，解决大规模数据存储挑战；  <br/>2. 设计数据采集与模型训练迭代优化的流程，提升数据信息量；  <br/>3. 构建的语料库在数据效率上优于传统方法，减少冗余数据；  <br/>4. 实验证明相同数据量下，该方法显著提高TTS合成质量。  <br/><br/>总结：  <br/>论文提出基于主动学习的TTS数据集构建方法，通过迭代优化提升数据质量与利用效率，实验证明其在有限数据下可生成更高质量的发音。|
|2507.06826v1|[Physics-Informed Direction-Aware Neural Acoustic Fields](http://arxiv.org/abs/2507.06826v1)|**贡献点：**<br/>1. **提出物理信息神经网络建模FOA RIRs**：首次将PINN框架应用于一阶Ambisonic房间脉冲响应建模，拓展了其在声场处理中的应用场景。  <br/>2. **推导双物理先验约束**：基于粒子速度与FOA (X,Y,Z)通道的对应关系，提出两个物理信息先验，通过偏导数关联预测的W通道与其他通道。  <br/>3. **建立四通道物理关系**：将声传播物理规律（如波动方程）融入模型，规范了FOA四通道间的物理可行性关系。  <br/>4. **实验验证有效性**：对比传统神经网络，通过实验证明所提方法在FOA RIR建模任务中具有更优性能。  <br/><br/>**总结（100字内）**：  <br/>提出基于物理先验的PINN框架，解决FOA RIR建模问题，通过粒子速度约束提升模型物理合理性，实验验证方法优于传统神经网络，为沉浸式音频生成提供更精准的声场模拟方案。|
|2507.06483v1|[Learning Japanese with Jouzu: Interaction Outcomes with Stylized   Dialogue Fictional Agents](http://arxiv.org/abs/2507.06483v1)|**贡献点**：  <br/>1. 提出并验证了风格化、有声代理在多模态语言学习环境中的影响机制，揭示其与用户互动的关联性。  <br/>2. 构建了一个基于混合方法的实证框架，结合54名参与者的定量数据分析与定性反馈，系统评估代理设计效果。  <br/>3. 强调语音风格、情感语气及人格特征对用户体验、学习动机和策略的显著影响，尤其在跨语言水平和文化背景中表现出差异。  <br/>4. 为开发更具文化适应性、情感共鸣及社交响应性的语言学习系统提供设计指导与理论依据。  <br/><br/>**总结**：  <br/>本研究通过实证分析揭示了风格化语音代理对语言学习互动的关键影响，为优化跨文化、多模态学习系统的设计提供了理论与实践指导。|
|2507.04094v1|[MMMOS: Multi-domain Multi-axis Audio Quality Assessment](http://arxiv.org/abs/2507.04094v1)|总结（100字以内）:  <br/>MMMOS提出了首个无参考、多域音频质量评估系统，通过融合多预训练编码器和多种聚合策略，在语音、音乐及环境音领域实现性能提升，获得多个挑战指标最优排名。<br/><br/>贡献点分点列出:<br/>1. **多维度评估框架**：首次构建包含"生产质量、生产复杂度、内容愉悦度、内容实用性"四个正交维度的无参考音频质量评估系统，突破传统单一MOS评分的局限性。<br/>2. **跨域适应性**：系统可泛化至语音、音乐、环境音三大领域，显著拓展了音频质量评估的应用边界。<br/>3. **多编码器融合技术**：创新性地结合WavLM、MuQ、M2D三个预训练模型的帧级嵌入特征，提升特征表达能力。<br/>4. **多策略优化机制**：设计三种聚合策略与四种损失函数组合，系统性优化模型性能。<br/>5. **集成模型有效性**：通过集成前八名模型实现20-30% MSE下降和4-5% Kendall's τ提升，验证多模型融合优势。<br/>6. **挑战指标表现**：在17/32挑战指标中位列前三，其中6项生产复杂度指标夺冠，证明系统实用价值。|
|2507.04036v1|[PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)|总结：  <br/>本文提出PresentAgent，将文本转化为同步视频，突破传统静态幻灯片局限，设计模块化处理流程并引入统一评估框架，实验验证效果接近人类水平。<br/><br/>贡献点：  <br/>1. **提出新型多模态生成框架**：首次实现将长文本文档转化为符合人类风格的同步视频，突破传统静态幻灯片或文本摘要的局限。  <br/>2. **构建模块化生成流水线**：系统化分割文档、规划视觉帧、生成语境化语音解说及实现精准音视频同步，解决多模态内容生成的复杂性。  <br/>3. **设计统一评估体系PresentEval**：基于视觉-语言模型，从内容保真度、视觉清晰度和观众理解度三维度对生成视频进行自动化评估。  <br/>4. **验证性能接近人类水平**：在30个文档-演示数据集上实验表明，生成的视频在各项指标上达到人类表现，证明方法有效性。|
|2507.03912v1|[Prosody Labeling with Phoneme-BERT and Speech Foundation Models](http://arxiv.org/abs/2507.03912v1)|总结：  <br/>本文提出融合声学和语言学特征的模型，用于自动韵律标注，提升日语音高重音和短语边界预测准确率。<br/><br/>贡献点：  <br/>1. 提出用于自动韵律标注的模型，支持训练可控的文本到语音系统。  <br/>2. 融合声学特征（基于SSL或Whisper编码器）与语言学特征（来自PnG BERT/PL-BERT）。  <br/>3. 通过特征拼接预测音素级韵律标签，创新性结合多模态信息。  <br/>4. 在日语音高重音（93.2%）、高低音高重音（93.2%）和短语边界（94.3%）任务中验证模型有效性。|
|2507.03887v1|[Traceable TTS: Toward Watermark-Free TTS with Strong Traceability](http://arxiv.org/abs/2507.03887v1)|**贡献点：**  <br/>1. 提出首个无需显式水印的TTS可追溯性框架，通过联合训练TTS模型与判别器实现。  <br/>2. 解决传统水印方法导致的语音质量下降和易被伪造的缺陷。  <br/>3. 在保持甚至提升音频质量的同时，显著增强模型的可追溯性泛化能力。  <br/>4. 开源代码推动相关技术领域的发展。  <br/><br/>**总结（100字以内）：**  <br/>本文提出无水印的TTS可追溯性框架，通过联合训练提升模型的泛化与质量，首次实现强可追溯性而不牺牲语音效果，开源代码促进技术进步。|
|2507.02530v1|[Open-Source System for Multilingual Translation and Cloned Speech   Synthesis](http://arxiv.org/abs/2507.02530v1)|**贡献点：**  <br/>1. **系统架构创新**：集成Whisper语音识别与VAD模块，实现语音内容的时间轴分割与关键信息提取。  <br/>2. **多语言处理流程**：采用双LLM架构（分句+翻译），支持跨语言的语义连贯与准确翻译。  <br/>3. **语音再生技术**：引入带有语音克隆能力的TTS模块，实现自然语音还原与原说话者身份保持。  <br/>4. **部署灵活性**：提供本地和API两种运行模式，降低部署成本，适应多样化应用场景（如Zoom、公共广播、蓝牙设备）。  <br/>5. **开源社区共享**：开放源码促进技术普及与协作，推动无障碍通信解决方案的开发。  <br/>6. **性能评估体系**：量化分析系统延迟与词准确率，验证其在真实多语言场景中的适用性。  <br/><br/>**总结：**  <br/>本研究提出了一款开源多语言翻译与语音再生系统，通过整合前沿技术实现跨语言沟通与语音身份保持，并提供灵活部署和性能验证，推动无障碍通信创新。|
|2507.01805v1|[A Dataset for Automatic Assessment of TTS Quality in Spanish](http://arxiv.org/abs/2507.01805v1)|**贡献点：**  <br/>1. 构建首个西班牙语专用的TTS系统自动评估数据库，包含52种系统及人类语音的4326个音频样本。  <br/>2. 设计基于ITU-T P.807标准的主观测试流程，并由92名参与者完成标注，确保数据可靠性。  <br/>3. 验证数据集的实用性，通过训练自动自然度预测模型（微调英语模型与构建下游网络）评估效果。  <br/>4. 提供五分制MOS尺度下的模型性能基准（平均绝对误差为0.8），揭示数据集的质量与多样性。  <br/><br/>**总结（100字以内）:**  <br/>本研究创建首个西班牙语TTS评估数据库，结合标准化主观测试与两种自动预测方法验证数据价值，为提升西班牙语TTS自然度评估提供基准和资源，推动该领域研究进展。|
|2507.00808v2|[Multi-interaction TTS toward professional recording reproduction](http://arxiv.org/abs/2507.00808v2)|总结：  <br/>本文提出多步骤交互的TTS方法，模拟语音导演与演员的协作机制，实现用户对语音风格的迭代修正，并通过数据集验证其有效性。<br/><br/>贡献点：  <br/>1. **首次将语音导演的迭代反馈机制引入TTS**：构建多步交互框架，允许用户在合成后动态调整语音风格。  <br/>2. **模拟人机协作过程**：通过建模用户与TTS模型之间的互动关系，实现更自然的风格修正流程。  <br/>3. **提出配套数据集**：提供支持实验的语料库，推动语音风格细化研究。  <br/>4. **验证多交互能力**：实验结果表明模型能根据用户指令进行多次风格迭代优化，提升合成质量。|
|2507.00227v1|[Investigating Stochastic Methods for Prosody Modeling in Speech   Synthesis](http://arxiv.org/abs/2507.00227v1)|总结：  <br/>本研究对比了随机方法与传统确定性模型，验证了其在生成自然韵律方面的有效性，提出通过采样温度调节提升韵律可控性。<br/><br/>贡献点：  <br/>1. **方法对比**：系统评估了归一化流、条件流匹配和修正流等随机方法在文本到语音合成中的表现，与传统确定性基线及人类语音进行对比。  <br/>2. **自然性验证**：通过主观和客观测试，证明随机方法能生成与人类语音质量相当的自然韵律，有效捕捉语音的固有变异性。  <br/>3. **可控性增强**：提出通过调节采样温度实现对生成韵律的灵活控制，拓展了模型的可调控维度。  <br/>4. **框架创新**：引入新的随机建模范式，为语音合成中韵律生成提供了更符合人类语言特性的解决方案。|
|2506.23869v1|[Scaling Self-Supervised Representation Learning for Symbolic Piano   Performance](http://arxiv.org/abs/2506.23869v1)|总结：  <br/>本文提出通过大规模预训练和微调生成自回归Transformer模型，在音乐生成与分类任务中取得显著成果，同时创新性地将对比学习框架应用于符号音乐。<br/><br/>贡献点：  <br/>1. **提出生成自回归Transformer模型**：基于符号化独奏钢琴转录数据进行训练，实现音乐生成与分类任务。  <br/>2. **大规模预训练与高效微调**：使用约60,000小时音乐数据预训练，再通过小规模高质量数据集微调，提升模型性能。  <br/>3. **对比学习框架的应用**：将SimCLR框架适配于符号音乐，生成通用对比MIDI嵌入，推动MIR任务发展。  <br/>4. **显著性能提升**：在钢琴延续生成任务中超越主流符号生成方法，与商业音频模型竞争；MIR分类任务中冻结表示达SOTA，微调需极少标注数据即可迁移。|
|2506.23553v2|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v2)|总结：  <br/>该论文提出Human-CLAP模型，通过引入人类主观评分优化CLAP的音频-文本对齐性能，显著提升了CLAPScore与主观评分的相关性。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：发现传统CLAPScore与人类主观评价相关性较低，质疑其在文本-音频对任务中的有效性。  <br/>2. **提出Human-CLAP方法**：设计基于人类感知的对比学习框架，利用主观评分数据重新训练CLAP模型，提升对齐能力。  <br/>3. **实验证明有效性**：通过对比实验验证Human-CLAP显著改善了Spearman相关系数（SRCC），提升幅度超过0.25。|
|2506.23553v1|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v1)|总结：  <br/>该研究发现CLAPScore与人类主观评分相关性较低，提出基于人类感知的Human-CLAP模型，并通过实验验证其显著提升评估效果。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：首次系统分析CLAPScore与人类主观评估评分的低相关性，指出其在文本-音频相关性建模中的不足。  <br/>2. **提出Human-CLAP方法**：创新性地通过引入人类主观评分作为训练信号，构建了一个更符合人类感知的对比语言-音频预训练模型。  <br/>3. **验证效果提升**：实验证明Human-CLAP将Spearman秩相关系数（SRCC）提升超过0.25，显著增强了模型与人类评分的一致性。|
|2506.23552v1|[JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](http://arxiv.org/abs/2506.23552v1)|总结：  <br/>提出JAM-Flow统一框架，整合面部运动与语音生成，通过创新架构和联合注意力机制实现跨模态交互，支持文本、参考音频/运动等多种条件输入，推动多模态音频-视频合成的发展。<br/><br/>贡献点：  <br/>1. **统一框架设计**：首次将说话头合成（talking head）与文本到语音（TTS）结合，解决传统方法中分开处理的局限性。  <br/>2. **多模态扩散Transformer架构**：提出MM-DiT（Multi-Modal Diffusion Transformer），集成Motion-DiT和Audio-DiT模块，实现多模态数据处理。  <br/>3. **联合注意力机制**：引入选择性联合注意力层与时间对齐位置嵌入，提升跨模态交互效率并保留模态特异性表现。  <br/>4. **局部联合注意力掩码**：通过局部化掩码策略，优化生成过程中的模态间依赖关系。  <br/>5. **Inpainting训练目标**：以图像修复式目标训练模型，支持文本、参考音频及运动等多种条件输入，实现同步生成与控制任务。  <br/>6. **应用场景拓展**：单模型支持多种音频-视觉生成任务（如同步说话头生成、音频驱动动画等），提升模型通用性与实用性。|
|2506.23367v1|[You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel   Properties](http://arxiv.org/abs/2506.23367v1)|总结：  <br/>提出首个针对L2学习者的TTS系统，通过元音时长差异设计"清晰模式"，实验证明其有效降低错误率并提升主观体验，揭示实际与感知理解度的差异，指出现有ASR工具在评估L2用户理解度上的不足。<br/><br/>贡献点：  <br/>1. **首个L2定制TTS系统**：开发专门针对第二语言学习者的文本到语音系统，填补领域空白。  <br/>2. **基于元音时长的"清晰模式"**：利用美式英语紧张元音（长）与松弛元音（短）的差异，创新性设计语音清晰度调节机制。  <br/>3. **实验证据支持**：通过对比实验验证该模式可减少9.15%以上转录错误，并提升用户主观体验（更鼓励、尊重）。  <br/>4. **揭示感知与实际理解度差异**：发现听众对清晰模式效果缺乏主观认知，实际理解度提升与感知认知不一致。  <br/>5. **批判现有评估工具**：指出Whisper-ASR未能有效捕捉L2学习者对困难元音的区分策略，无法准确评估TTS系统对L2用户的真实理解度。|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|总结：  <br/>本研究提出SAGE数据生成方法和经验回放策略，结合外部语言模型，显著提升了方言阿拉伯语与阿拉伯-英语代码切换语音的识别性能，超越了大模型效果。<br/><br/>贡献点：  <br/>1. **提出改进的音频拼接方法（SAGE）**：生成人工阿拉伯语-英语代码切换语音数据，解决数据稀缺问题。  <br/>2. **SAGE与SSL模型的联合微调**：在代码切换和阿拉伯语基准测试中，使WER绝对下降7.8%。  <br/>3. **基于经验回放（ER）的泛化增强技术**：缓解方言和代码切换任务中的灾难性遗忘，降低整体WER至26.6%。  <br/>4. **少样本微调策略**：进一步将代码切换基准WER提升4.9%，并实现优于USM和Whisper-large-v2的性能（分别高5.5%和8.4%）。|
|2506.21875v2|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v2)|**贡献点总结**（100字以内）:  <br/>本研究提出首个综合语音场景评估基准，构建了包含真实对话、多说话者属性及语音特征（如语调、同音词）的多样化数据集，并设计查询感知的评价方法，实现更精准的模型性能分析，为语音模型优化提供关键指导。<br/><br/>**分点贡献**:<br/>1. **构建语音场景专用评估数据集**  <br/>   - 系统收集真实语音对话数据，覆盖语音特有的语调、同音词、结巴等现象；  <br/>   - 引入多样的说话者属性与声学环境，提升数据集的现实适用性。<br/><br/>2. **设计查询感知的评估方法**  <br/>   - 提出基于定制化检查清单和提示的自动评估框架，针对性解决语音交互的特殊挑战；  <br/>   - 实现对不同语音场景下模型行为的细粒度分析。<br/><br/>3. **揭示主流语音模型性能差异**  <br/>   - 通过全面测试，量化不同语音模型在复杂场景中的表现差异；  <br/>   - 提供针对语音模型开发和优化的实证依据与方向指引。|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|总结：  <br/>本文提出一种针对语音交互的端到端评估基准，通过构建包含真实场景、语音特性（如韵律、同音词、口吃）及多样化声学条件的数据集，并设计查询感知的评估方法，揭示主流语音模型在不同场景下的性能差异，为模型优化提供依据。<br/><br/>贡献点：  <br/>1. **构建专用语音评估基准**：系统创建面向语音场景的端到端评估基准，弥补现有文本基准在语音领域应用的不足。  <br/>2. **语音特性增强数据集**：收集真实语音对话数据，纳入多元说话者属性、声学条件及语音独特现象（如韵律、同音词、口吃）。  <br/>3. **设计查询感知评估方法**：提出基于定制化检查清单和提示的自动评估框架，提升语音任务评价的准确性与细粒度。  <br/>4. **揭示模型场景差异**：对主流语音模型进行全面测试与分析，发现其在不同语音场景下的显著性能差异。  <br/>5. **推动语音模型发展**：为语音模型的开发、优化与评估提供实证依据和参考价值。|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>提出针对实时语音对话的偏好对齐框架，构建多轮语音数据集，结合AI反馈优化模型，通过实验验证提升对话系统的表现。<br/><br/>贡献点：  <br/>1. **创新框架**：提出首个专为实时语音对话设计的偏好对齐方法，解决传统文本模型在处理动态语音交互（如中断、插入）上的不足。  <br/>2. **大规模数据集**：创建超过15万对标注AI反馈的多轮语音对话数据，涵盖语言内容与时间上下文的偏好变化。  <br/>3. **模型优化**：利用离线对齐技术微调全双工自回归语音到语音模型，提升多轮对话生成能力。  <br/>4. **实验验证**：在通用对话中证明反馈机制能有效增强模型的准确性、安全性和上下文一致性。  <br/>5. **人类评估**：通过综合人工评价验证模型在复杂多轮场景下的实际效果。  <br/>6. **理论洞见**：发现动态平衡（如语言、时间因素）对构建自然实时对话系统的重要性。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|总结（100字以内）:  <br/>本文提出一种步骤式视频到音频生成方法，模仿Foley流程生成多音频轨道，结合文本提示与概念否定策略，无需专用数据集即可实现高质量语义区分的音频合成，优于现有基线。<br/><br/>贡献点:  <br/>1. **步骤式生成框架**：首次提出按顺序生成单个音频轨道的方法，每个轨道对应视频中的特定声音事件，实现多声部音频合成。  <br/>2. **Foley流程模拟**：通过镜像传统影视音效创作流程，全面捕捉视频中所有由动作触发的声音事件。  <br/>3. **文本引导与记忆机制**：引入文本提示和先前生成的音频轨道作为条件，构建动态引导的合成流程。  <br/>4. **概念否定策略**：借鉴组合生成框架中的概念否定机制，提升音频生成的精确性和语义分离能力。  <br/>5. **无需配对数据训练**：设计基于预训练模型的通用训练框架，消除对专用视频-音频配对数据集的需求。  <br/>6. **高质量合成效果**：实验证明生成的音频轨道在语义区分度和整体质量上优于现有基线方法。|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|**贡献点分点总结：**  <br/>1. **系统性比较**：在零样本TTS框架下，对比了三种主流说话人编码器（H/ASP、x-vector、ECAPA-TDNN）的性能。  <br/>2. **标准化评估**：基于同一捷克语数据集，跨领域（24个目标说话人）进行主观听觉测试与客观余弦距离分析。  <br/>3. **发现编码器差异**：H/ASP编码器在零样本TTS中表现最好，ECAPA-TDNN优于x-vector，但未超越H/ASP。  <br/>4. **提出实证建议**：强调说话人识别嵌入在TTS中的适配需经实证验证，并构建了可复用的比较框架。  <br/><br/>**总结（100字内）：**  <br/>本研究对比了三种说话人嵌入在零样本TTS中的效果，发现H/ASP编码器表现最优，ECAPA-TDNN虽优于x-vector但不足。提出标准化评估框架，强调实证验证的重要性，为跨任务嵌入应用提供了参考。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|总结：  <br/>提出Kling-Foley多模态视频到音频生成模型，结合扩散Transformer与同步模块提升音视频对齐，开源工业级基准Kling-Audio-Eval，并在多个指标上达成SOTA性能。<br/><br/>贡献点：  <br/>1. **提出Kling-Foley模型**：首个大规模多模态Video-to-Audio生成模型，实现高保真音频与视频内容的同步合成。  <br/>2. **多模态交互建模**：引入多模态扩散Transformer，融合视觉语义表示模块与音视频同步模块，提升跨模态对齐能力。  <br/>3. **帧级对齐机制**：通过视频条件与潜在音频元素的帧级对齐，优化语义对齐与时间同步效果。  <br/>4. **通用音频编解码器**：设计支持音效、语音、歌唱及音乐等多场景的潜在音频编码方案，实现高质量建模。  <br/>5. **空间感渲染技术**：采用立体声渲染方法增强合成音频的听觉空间感知。  <br/>6. **开源工业级基准**：针对数据集缺陷，发布Kling-Audio-Eval工业级评估基准，推动领域研究。  <br/>7. **SOTA性能验证**：实验表明，基于流匹配目标训练的Kling-Foley在分布匹配、语义对齐、时间同步和音频质量上达到公共模型最优。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|**总结（100字以内）:**  <br/>本文提出TTSDS2，改进TTS评估方法，实现与主观评分的高相关性，并发布包含11000个评分的数据集、多语言测试数据管道及14语言持续更新的基准，推动合成语音质量评估的标准化和客观性。<br/><br/>**贡献点分点列述：**  <br/>1. **提出TTSDS2评估指标**：相比现有16种评估方法，TTSDS2在所有测试领域和主观评分中均达到Spearman相关性>0.5，显著提升TTS系统质量评估的客观性和鲁棒性。  <br/>2. **构建大规模主观数据集**：提供涵盖11,000+主观评分的多语言数据集，填补合成语音质量评估的标注缺口。  <br/>3. **设计可复用的评估框架**：推出能持续生成多语言测试数据的管道，避免数据泄露；并发布14语言的动态更新基准，支持长期性能监测与对比研究。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**贡献点：**<br/>1. 提出新假设：人类在主观语音质量评分中更关注低质量段落，评分差异主要由忽略低质量部分时的高分误判导致。<br/>2. 验证假设：基于VCC2018和BVCC数据集分析，揭示人类评价焦点与语音质量分布的关系。<br/>3. 提出新指标：定义N_low-MOS（N个最低意见评分的平均值）作为更可靠的语音质量代表值。<br/>4. 实验验证：证明N_low-MOS可提升MOSNet的评估性能（LCC/SRCC指标改善），展示其内在有效性。<br/>5. 推动方法改进：为语音转换（VC）模型的评估提供更科学的基准，优化MOSNet的比较能力。<br/><br/>**总结（100字内）：**  <br/>本文提出N_low-MOS指标，通过分析数据集验证人类评分明显受低质量段落影响，并证明其在提升语音质量评估模型性能方面具有显著优势，为VC模型评估提供新思路。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|**贡献点：**  <br/>1. 提出一种新的评分聚合方法，解决传统MOS标注（1-5分离散评分）的局限性。  <br/>2. 基于标注者内部连续评分假设，建模生成分布并通过量化潜在连续分布估计评分峰值。  <br/>3. 引入潜在分布峰值作为新代表性值，取代传统MOS作为预测目标。  <br/>4. 通过实验验证，该方法能显著提升语音质量预测模型（如MOSNet）的性能。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出一种基于连续评分假设的新型语音质量评分方法，通过建模标注者的评分过程并引入潜在分布峰值作为替代指标，有效提升语音质量预测模型的性能。|
|2506.18296v2|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v2)|总结：  <br/>构建专注于日本年轻女性偶像的语音语料库，推动TTS和VC研究，支持听众偏好生成探索，并提供免费非商业使用及文化背景分析。<br/><br/>贡献点：  <br/>1. **创建专用语料库**：首次构建聚焦“年轻女性live idols”这一特定群体的Japanese Idol Speech Corpus (JIS)，为语音生成AI提供独特数据支持。  <br/>2. **提升评估严谨性**：通过统一的艺名标识和细分群体，便于招募熟悉偶像的听众，优化TTS和VC系统中说话者一致性的实验设计。  <br/>3. **填补研究空白**：推动生成符合听众偏好的个性化声音研究，拓展语音生成领域应用方向。  <br/>4. **开放共享资源**：以免费形式分发JIS，限定非商业用途，促进学术研究合作与公平使用。  <br/>5. **文化背景支持**：提供日本偶像文化背景资料，助力研究者在伦理与文化敏感性框架下合理应用数据。  <br/>6. **应用指导**：包含基础分析与构建方法说明，为后续研究提供实践参考与技术指导。|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|**总结（100字以内）**  <br/>本研究构建了日本偶像语音语料库（JIS），聚焦年轻女性现场偶像群体，通过艺名标识促进听众实验。JIS支持TTS/VC系统的说话者相似性评估，并探索听众偏好的个性化语音生成，数据免费开放且仅限非商业科研使用，附带文化背景介绍与应用指导。<br/><br/>**贡献点分点列出**  <br/>1. **构建专业化语料库**  <br/>   - 首次创建针对日本“年轻女性现场偶像”这一特定群体的语音语料库（JIS），覆盖TTS和VC研究需求，补充语音生成领域的数据多样性。<br/><br/>2. **推动说话者相似性评估**  <br/>   - 所有语音由同一类群体（艺名标识）录制，便于系统评估生成语音与原声的说话者相似性，提升TTS/VC的可信度与可比性。<br/><br/>3. **开拓听众偏好研究方向**  <br/>   - 引入针对听众偏好的个性化语音生成研究（如定制偶像声音），填补该领域学术空白，推动语音生成技术与用户需求的结合。<br/><br/>4. **制定开放使用政策**  <br/>   - 以非商业、基础研究为限免费公开JIS，确保数据可及性同时维护版权与伦理规范，促进学术共享与创新。<br/><br/>5. **提供文化背景支持**  <br/>   - 对日本偶像文化进行介绍，助力研究者理解数据语境，确保JIS的合法、有效与伦理应用，提升科研的社会接受度。|
|2506.16738v1|[LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](http://arxiv.org/abs/2506.16738v1)|总结：  <br/>本文提出LM-SPT方法，通过间接语义蒸馏减少语音标记序列长度，改进编码器解码器结构并支持多帧率，实验验证其在语音-文本建模中取得优于基线的重建效果和文本到语音任务表现。<br/><br/>贡献点：  <br/>1. 提出LM-SPT模型，通过语义驱动的重建机制替代传统池化操作，有效减少语音标记序列长度。  <br/>2. 引入间接语义蒸馏策略，通过冻结ASR编码器对比原始与重建波形，提升语义对齐精度。  <br/>3. 设计编码器-解码器架构优化方案，支持25Hz、12.5Hz、6.25Hz等多帧率配置。  <br/>4. 实验证明LM-SPT在语音-文本双向任务中均优于基线，尤其在文本到语音生成中表现突出。|
|2506.16580v1|[Streaming Non-Autoregressive Model for Accent Conversion and   Pronunciation Improvement](http://arxiv.org/abs/2506.16580v1)|总结：提出首个支持流式处理的口音转换模型，实现非母语语音向母语口音的转换，同时保持说话人身份与语调，并通过集成TTS模型提升训练效率，达到顶级模型性能且具备稳定延迟。<br/><br/>贡献点：<br/>1. 提出首个实现流式口音转换（AC）的模型，支持实时处理非母语语音。<br/>2. 保持说话人身份、语调特征并提升发音质量，实现"原声"性转换。<br/>3. 采用Emformer编码器与优化推理机制，显著降低处理延迟。<br/>4. 整合母语TTS模型生成理想参考数据，提升训练效率。<br/>5. 在保持稳定延迟的前提下，达到当前最优AC模型的转换效果。|
|2506.16127v1|[Improved Intelligibility of Dysarthric Speech using Conditional Flow   Matching](http://arxiv.org/abs/2506.16127v1)|**贡献点总结（100字以内）**  <br/>本研究提出基于自监督学习特征及量化表示的失语语音到正常语音转换方法，采用单说话人语音生成策略降低说话人差异，并结合非自回归条件流匹配与扩散Transformer实现高效映射，提升语音可懂度和收敛速度。  <br/><br/>**具体贡献点**  <br/>1. **替代传统特征**：提出使用自监督学习（SSL）特征及其量化表示替代梅尔频谱图（mel-spectrograms），探索其在语音生成中的有效性。  <br/>2. **缓解说话人差异**：通过从WavLM提取特征生成单说话人干净语音，减少说话人变异性对模型性能的影响。  <br/>3. **非自回归框架**：设计全非自回归方法，结合条件流匹配（CFM）与扩散Transformer，直接学习失语语音到干净语音的映射。  <br/>4. **离散音素优势**：验证离散音素单元在提升语音可懂度和加速模型收敛方面的显著效果，优于传统基于梅尔频谱的方法。|
|2506.15873v1|[DeckFlow: Iterative Specification on a Multimodal Generative Canvas](http://arxiv.org/abs/2506.15873v1)|总结：  <br/>该论文提出DeckFlow多模态生成AI工具，通过任务分解、规格分解和生成空间探索三大创新机制解决现有工具设计问题，并验证了其在文本到图像生成中的有效性，进一步拓展至音频生成以研究用户创意行为。<br/><br/>贡献点：  <br/>1. **任务分解机制**：采用无限画布与卡片式视觉数据流交互，支持用户创建和管理多个互联子任务。  <br/>2. **规格分解工作流**：将初始目标迭代拆解为子部分，结合特征标签与聚类实现多模态内容组织。  <br/>3. **生成空间探索**：通过生成多组提示词与输出变体（网格展示），支持递归反馈优化设计迭代。  <br/>4. **多模态验证与扩展**：在文本-图像生成中对比传统对话AI基线，后扩展至音频生成，分析跨模态创作行为。|
|2506.15759v1|[Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](http://arxiv.org/abs/2506.15759v1)|**总结（100字以内）**  <br/>该论文提出Sonic4D框架，首次实现4D场景与空间音频的联合生成，通过动态视觉捕捉、声源定位及物理模拟三阶段处理，无需训练即可生成逼真的时空音频，增强了沉浸式音频视觉体验。<br/><br/>**贡献点**  <br/>1. **提出Sonic4D框架**：首次将空间音频生成与4D场景合成结合，解决现有方法忽视音频与场景对齐的问题。  <br/>2. **三阶段生成方法**：  <br/>   - 第一阶段：利用预训练模型生成4D场景及单声道音频；  <br/>   - 第二阶段：通过像素级视觉定位策略估计声源的3D空间坐标；  <br/>   - 第三阶段：基于物理模拟合成动态视角与时间变化的时空音频。  <br/>3. **训练-free设计**：无需额外训练，直接利用现有数据生成符合场景的空间音频。  <br/>4. **实验验证与资源公开**：通过实验证明生成音频的逼真度和沉浸感，并开放生成示例供研究复现。|
|2506.15085v2|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v2)|**贡献点总结**  <br/>1. 开发了EmojiVoice，首个支持社会机器人构建时序变化、表达性语音的自由定制TTS工具包。  <br/>2. 创新性引入emoji提示技术，实现对语音表达性的分阶段精细调控。  <br/>3. 采用轻量级Matcha-TTS模型，支持机器人离线部署及实时语音生成。  <br/>4. 通过三个案例研究验证工具有效性，揭示不同应用场景下表达性语音的接受度差异。  <br/><br/>**总结**  <br/>提出EmojiVoice开源工具，通过emoji提示及轻量模型实现机器人表达性语音生成，并通过实验分析其在不同任务中的表现差异。|
|2506.15085v1|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v1)|总结：  <br/>提出EmojiVoice工具包，实现社交机器人离线语音表达的动态控制，通过emoji提示增强长时表达性，并验证其在不同场景中的表现差异。<br/><br/>贡献点：  <br/>1. **开发EmojiVoice工具包**：首个专门针对社交机器人长时语音表达的免费、可定制TTS系统，解决基础模型离线部署难题。  <br/>2. **引入emoji提示控制机制**：首次将表情符号用于细粒度表达性调控，实现对语音情感相位的精准控制。  <br/>3. **轻量级实时生成框架**：采用Matcha-TTS等轻量模型，支持机器人端实时语音生成需求。  <br/>4. **多场景验证与对比**：通过剧本对话、讲故事、自主交互三类案例研究，验证方法对表达性的提升效果，并揭示不同应用场景下的接受差异。|
|2506.13053v3|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v3)|总结（100字以内）：  <br/>提出ZipVoice，基于流匹配的零样本TTS模型，在保持SOTA语音质量的同时，实现3倍更小的模型体积和30倍更快的推理速度，通过Zipformer架构、平均上采样对齐及流蒸馏方法优化效率与性能。<br/><br/>贡献点：  <br/>1. **轻量化架构设计**：引入Zipformer-based向量场估计器，在限制模型规模的前提下保持足够的建模能力。  <br/>2. **高效对齐与编码**：结合平均上采样初始对齐和Zipformer文本编码器，显著提升语音可懂度。  <br/>3. **流蒸馏优化**：通过减少采样步骤和消除分类器无引导的推理开销，降低计算复杂度。  <br/>4. **实证优势**：在100k小时多语言数据集上验证，相较于DiT基线模型，体积缩小3倍，推理速度提升30倍。  <br/>5. **开源实现**：提供代码、模型权重和示例样本，便于复现与应用（如链接所示）。|
|2506.13053v2|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v2)|总结：本文提出ZipVoice，通过紧凑结构和流匹配技术实现高效零样本TTS，质量与SOTA相当，在模型体积和推理速度上显著提升，并开源代码与模型。<br/><br/>贡献点：  <br/>1. **提出Zipformer-based流匹配解码器**：在模型体积受限情况下保持足够的建模能力，提升效率。  <br/>2. **设计Average upsampling与Zipformer结合的语音-文本对齐及文本编码器**：增强语音可懂度，优化多语言处理效果。  <br/>3. **创新流蒸馏方法**：减少采样步骤，消除无分类器引导的推理开销，显著提升生成速度。  <br/>4. **实验验证效果**：在100k小时多语言数据集上，ZipVoice在语音质量与SOTA模型相当的同时，体积缩小3倍，速度提升30倍。  <br/>5. **开源实现**：提供代码、模型检查点和演示样本，便于复现与应用。|
|2506.12199v1|[ViSAGe: Video-to-Spatial Audio Generation](http://arxiv.org/abs/2506.12199v1)|**贡献点总结**（100字以内）:  <br/>本研究提出ViSAGe框架，直接从无声视频生成一阶Ambisonics，构建YT-Ambigen数据集，设计空间音频评估指标，并验证其在时空对齐与视角适应性上的优越性。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新问题**：探索无需复杂录制设备或专业技能，直接从无声视频生成一阶Ambisonics的方法。  <br/>2. **构建数据集**：创建包含102K视频片段的YT-Ambigen数据集，为研究提供标注数据支持。  <br/>3. **设计评估指标**：首次基于音频能量图与显著性度量，提出量化评估生成空间音频质量的新标准。  <br/>4. **端到端框架**：提出ViSAGe模型，融合CLIP视觉特征、自回归音频编码与方向/视觉引导生成高质量Ambisonics。  <br/>5. **性能优势**：验证ViSAGe在生成音频的连贯性与时空对齐性上优于传统两阶段方法（视频-音频生成+空间化）。  <br/>6. **应用能力**：生成的音频可随视角变化动态适应，提升沉浸式音频体验的交互性。|
|2506.11160v5|[S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation   Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning](http://arxiv.org/abs/2506.11160v5)|总结：  <br/>提出S2ST-Omni框架，通过分解任务、整合多模态模型、引入轻量适配器和两阶段微调策略，有效解决多语言语音翻译的高质量与数据依赖问题，实验验证其优越性。<br/><br/>贡献点：  <br/>1. **任务分解创新**：将S2ST拆分为S2TT与TTS子任务，简化复杂度。  <br/>2. **多模态模型融合**：结合Whisper编码器（音频理解）与Qwen 3.0（文本理解），提升跨模态对齐能力。  <br/>3. **轻量模态适配器**：设计轻量级适配器缓解语音与文本表示的模态差距。  <br/>4. **两阶段微调策略**：通过分阶段训练增强多模态知识迁移效率。  <br/>5. **流式TTS生成**：采用流式自回归方法实现自然流畅的目标语音输出。  <br/>6. **性能超越基准**：在CVSS数据集上验证S2ST-Omni的翻译质量优于现有S2ST系统。|
|2506.11130v1|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v1)|总结（100字以内）:  <br/>提出自优化框架，利用无标签数据与闭环训练提升ASR性能，成功将Whisper转化为Twister，在台语中实现显著精度提升，并为低资源场景提供实用方案。<br/><br/>贡献点：<br/>1. 提出无需标注数据的自精炼框架，通过伪标签生成与闭环训练实现ASR性能提升。  <br/>2. 首次将高保真TTS系统与ASR模型结合，形成"ASR→TTS→ASR"的闭合优化循环。  <br/>3. 在台语语音任务中验证框架有效性，利用6000小时未标注语音驱动模型训练。  <br/>4. 开发专用模型Twister，相较Whisper在普通话和双语切换场景分别降低20%和50%错误率。  <br/>5. 为低资源/特定领域ASR提供创新解决方案，突破传统伪标签自蒸馏方法的局限性。|
|2506.11127v2|[UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions](http://arxiv.org/abs/2506.11127v2)|总结：  <br/>提出语音驱动GUI代理UITron-Speech，解决文本指令限制，通过语音数据合成、混合模态训练及定位误差优化，实现高效交互与强适应性。<br/><br/>贡献点：  <br/>1. **语音指令替代文本输入**：首次将语音作为GUI代理的指令输入方式，突破文本输入在手部自由场景中的局限性。  <br/>2. **端到端模型设计**：开发UITron-Speech，直接处理语音指令与设备截图，预测用户操作，实现全流程自动化。  <br/>3. **语音数据集合成**：利用随机说话者TTS模型生成高质量语音指令数据，缓解实际应用中的数据稀缺问题。  <br/>4. **混合模态训练策略**：设计策略应对预训练基础模型的模态不平衡问题，提升语音与视觉模态的协同理解能力。  <br/>5. **无训练定位误差优化**：通过统计分析GUI定位错误分布，提出两步训练无关的误差修正方法，减少小范围偏差。  <br/>6. **实验验证有效性**：在多基准测试中证明UITron-Speech的鲁棒性与优越性，推动语音驱动GUI技术的可行性与应用潜力。|
|2506.11127v1|[GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech   Instructions](http://arxiv.org/abs/2506.11127v1)|总结：  <br/>提出GUIRoboTron-Speech，首个端到端语音指令GUI代理，通过语音数据生成和混合训练策略解决模态不平衡问题，实验验证其高效性和广泛应用前景。<br/><br/>贡献点：  <br/>1. **首个语音驱动GUI代理**：提出GUIRoboTron-Speech，首次实现端到端GUI操作，直接接受语音指令和设备截图作为输入，无需文本转换。  <br/>2. **语音数据生成方法**：利用随机音色TTS模型将现有文本指令转换为高质量语音指令，解决语音数据稀缺问题。  <br/>3. **混合指令训练策略**：设计启发式方法缓解预训练模型的模态不平衡，结合文本和语音指令提升代理性能。  <br/>4. **系统性实验验证**：在基准数据集上验证模型表现，证明语音指令在GUI自动化中的有效性与广泛适用性。|
|2506.11086v1|[Intelligibility of Text-to-Speech Systems for Mathematical Expressions](http://arxiv.org/abs/2506.11086v1)|贡献点：  <br/>1. **系统评估框架**：首次设计涵盖五种TTS模型的实验，量化数学表达式（MX）的发音质量与可懂性（通过用户评分和转录正确性）。  <br/>2. **LLM辅助生成**：利用大语言模型（LLM）将LaTeX格式MX转换为英文发音，弥补TTS模型无法直接处理LaTeX的缺陷。  <br/>3. **多维度指标**：提出包含三个指标的转录正确性评估体系，并结合Mean Opinion Score分析用户主观感受。  <br/>4. **对比分析**：对比听众对TTS输出与人类专家发音的偏好，揭示TTS在处理MX时的表现短板。  <br/>5. **结果发现**：发现TTS模型输出的可懂性存在显著差异，且多数MX类别下表现劣于专家，同时LLM选择对结果影响有限。  <br/>6. **应用导向结论**：明确指出需针对性优化TTS模型处理数学表达式的能力。  <br/><br/>总结：  <br/>首次系统评估TTS模型对数学表达式的发音质量与可懂性，揭示模型与人类专家表现差异，强调改进TTS处理MX能力的必要性。|
|2506.10019v1|[A Survey of Automatic Evaluation Methods on Text, Visual and Speech   Generations](http://arxiv.org/abs/2506.10019v1)|总结：本文提出首个综合框架，系统化分类文本、图像和音频生成的评估方法，识别出五种核心范式，并探讨跨模态评估的未来方向。<br/><br/>贡献点：  <br/>1. **构建统一框架**：首次建立跨文本、图像、音频三大模态的自动评估方法系统化框架，实现多模态方法的整合与对比。  <br/>2. **提出五种核心范式**：归纳出生成内容评估的五大基础范式，为后续研究提供理论分类依据。  <br/>3. **跨模态适用性验证**：将文本生成评估方法的分析逻辑扩展至图像和音频领域，验证框架的广泛适用性。  <br/>4. **探索未来研究方向**：针对跨模态评估挑战，提出潜在的研究路径，促进多模态生成AI的评估体系发展。|
|2506.09874v2|[UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow   Matching](http://arxiv.org/abs/2506.09874v2)|总结：  <br/>提出UmbraTTS，通过流匹配方法联合生成语音与环境音，解决背景数据稀缺问题，并实现细粒度控制与高质量环境感知音频合成。<br/><br/>贡献点：  <br/>1. **提出UmbraTTS框架**：首次结合文本和声学上下文，联合生成语音与环境音频，突破传统TTS在复杂背景环境合成的局限。  <br/>2. **解决数据稀缺问题**：设计自监督学习方法，从未标注录音中提取语音、背景音频和转录文本，无需依赖配对数据。  <br/>3. **实现细粒度控制**：支持对背景音量的精确调控，生成多样化且连贯的音频场景，增强语音合成的适应性。  <br/>4. **验证性能优势**：在评估中显著优于现有基线，生成自然、高质量且环境感知的音频，推动TTS在场景化应用中的发展。|
|2506.09827v2|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v2)|**贡献点：**  <br/>1. 提出EmoNet-Voice资源，包含大规模预训练数据集（Big）和新型基准数据集（Bench），覆盖40种情感、11种声音及4种语言。  <br/>2. 支持细粒度情感评估，明确区分不同情感强度以提升模型准确性。  <br/>3. 通过语音生成技术合成情感音频，模拟真实场景以解决隐私问题。  <br/>4. 引入心理学专家标注与验证，确保情绪强度标签的可靠性。  <br/>5. 提出Empathic Insight Voice模型，实现与人类专家高度一致的SER性能。  <br/>6. 揭示高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更易被识别的模型表现差异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoNet-Voice资源，通过合成语音和专家标注解决隐私与情感粒度问题，推出高性能SER模型，并发现高唤醒情绪更易识别，为情感语音研究提供新基准。|
|2506.09827v1|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v1)|总结：  <br/>提出EmoNet-Voice资源及两个子集，包含大规模预训练数据与专家标注基准数据，解决隐私问题并增强情感粒度和多样性。开发高精度模型，通过评估揭示高唤醒情绪识别易度高于低唤醒情绪。<br/><br/>贡献点：  <br/>1. **引入EmoNet-Voice资源**：构建由EmoNet-Voice Big（大规模预训练数据集）和EmoNet-Voice Bench（专家标注基准数据集）组成的综合数据资源，覆盖40种情感、11种声音和4种语言。  <br/>2. **提升情感粒度与强度区分**：设计支持细粒度情感分类（40类）及不同强度评估的框架，推动SER模型在情感理解上的更精确建模。  <br/>3. **合成数据解决隐私与多样性问题**：通过先进语音生成技术合成敏感情感场景数据，避免真实数据隐私风险并扩展情感表达的多样性。  <br/>4. **专家验证机制**：联合心理学专家对合成数据进行严格标注（感知强度标签），确保数据质量与情感感知的科学性。  <br/>5. **提出高性能SER模型**：开发Empathic Insight Voice模型，实现与人类专家的高一致性，推动语音情感识别技术的标准化。  <br/>6. **揭示情绪识别难度差异**：通过对比实验发现，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易被模型识别，为后续研究提供新视角。|
|2506.08279v1|[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)|**总结（100字以内）:**  <br/>该论文提出Mirage模型，首次实现从音频直接生成高质量、表达性强的视频，结合TTS技术生成多模态视频，并开发统一的自注意力训练方法，提升生成视频的主观质量与通用性。  <br/><br/>**分点贡献:**  <br/>1. **提出Mirage模型**：首个专注于音频到视频生成的通用模型，从原始音频直接生成逼真、表达性的视频图像，不依赖视觉输入。  <br/>2. **多模态生成能力**：通过集成文本到语音（TTS）技术，实现语音与视频的同步生成，解决语音和视频内容对齐问题。  <br/>3. **统一训练方法**：开发适用于自注意力机制的统一训练框架，支持从头训练和基于已有权重的微调，增强模型灵活性。  <br/>4. **语义对齐优化**：利用A-roll数据（人物对话语音视频）训练，使生成视频能准确反映音频中的表演信息，提高可信度。  <br/>5. **性能优势**：生成的视频在主观质量上优于依赖语音特定架构或损失函数的现有方法，保持通用性的同时实现高保真输出。|
|2506.04527v1|[Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning](http://arxiv.org/abs/2506.04527v1)|总结（100字以内）:<br/>该论文提出结合字母符号的语音标签生成方法，通过BERT特征提示编码实现隐式条件，配合推理时的显式修剪提升标签一致性，为文本到语音和口音估计等任务生成高质量平行数据。<br/><br/>贡献点：<br/>1. **方法创新**：首创结合字母符号的语音标签生成框架，突破传统ASR模型直接微调的局限，引入显式与隐式的双条件策略。<br/>2. **条件生成机制**  <br/>   - 通过预训练BERT特征构建隐式图形单词条件（Prompt Encoder）  <br/>   - 提出推理阶段的显式标签假设剪枝算法（Pruning Inconsistent Hypotheses）<br/>3. **数据价值**：生成跨模态的语音-标签-图形单词平行数据，支持文本到语音合成（TTS）和口音估计等下游任务。<br/>4. **实验验证**：在音素-图形单词一致性任务和口音估计任务中均取得显著性能提升，证明方法的有效性。|
|2506.04397v1|[Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?](http://arxiv.org/abs/2506.04397v1)|总结：  <br/>该论文通过构建专用数据集和微调大模型，探索Parler TTS在语音重建任务中生成可理解语音与保持说话人身份的能力，揭示其在可控性方面的局限并提出改进方向。<br/><br/>贡献点：  <br/>1. **构建专用数据集**：创建并标注了包含说话人信息和可懂度数据的语音重建数据集，为相关研究提供基础资源。  <br/>2. **应用大模型进行语音重建**：首次尝试利用Parler TTS等先进大型语音模型，生成语音障碍患者疾病前的语音近似。  <br/>3. **分析模型局限性**：发现模型虽能学习数据分布，但在控制语音可懂度和保持说话人身份一致性方面存在挑战。  <br/>4. **提出改进方向**：为提升语音重建任务中模型的可控性，提出了未来研究的潜在优化路径。|
|2506.04152v1|[HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](http://arxiv.org/abs/2506.04152v1)|**分点贡献：**  <br/>1. **构建高带宽语音数据集**：提出HiFiTTS-2，基于LibriVox有声书，包含36.7k小时（22.05kHz）和31.7k小时（44.1kHz）的高质量英语语音数据，支持高带宽语音合成研究。  <br/>2. **创新数据处理流程**：设计了包含带宽估计、语音分段、文本预处理及多说话人检测的全流程数据处理方法，提升数据适配性与实用性。  <br/>3. **提供详细元数据**：生成全面的语句和有声书元数据，支持研究者通过数据质量过滤器灵活调整数据集以适配不同应用场景。  <br/>4. **验证数据集有效性**：实验表明该数据集和处理方法可有效训练高质量零样本TTS模型，推动高带宽语音生成技术的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HiFiTTS-2，构建大规模高带宽语音数据集并设计全流程处理技术，提供详细元数据以增强数据适应性，实验证明其可有效支持高质量零样本TTS模型训练。|
|2506.04134v2|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v2)|总结：  <br/>提出UniCUE框架，通过细粒度语义对齐、跨任务表示桥梁和姿态感知处理，实现CS视频到语音的直接生成，突破依赖文本的瓶颈，取得SOTA性能。<br/><br/>贡献点：  <br/>1. **首次提出直接生成语音的CSV2S统一框架UniCUE**：解决传统方法中依赖文本导致的错误传播与时间对齐难题。  <br/>2. **创新性的细粒度语语义对齐池**：实现视觉特征与语音内容的精确映射，提升跨模态对齐精度。  <br/>3. **VisioPhonetic适配器与姿态感知视觉处理器**：桥接CS识别与语音生成任务，增强唇手动作的时空关联性。|
|2506.04134v1|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v1)|**贡献点总结（100字以内）**：  <br/>提出直接生成CS语音的新方法，构建首个统一框架UniCUE，通过细颗粒语义对齐、跨任务适配器及姿态感知处理，解决误差传播与时空对齐问题，显著提升语音生成质量与唇语同步性。<br/><br/>**分点贡献**：  <br/>1. **直接生成语音的创新方法**：首次无需依赖文本中间媒介，直接从CS视频生成语音，避免误差传播与时间错位问题。  <br/>2. **统一框架UniCUE**：提出首个融合CSV2S与CSR任务的框架，打通视觉-语义到语音的端到端生成流程。  <br/>3. **核心技术创新**：  <br/>   - 引入细颗粒语义对齐池，精确映射视觉特征与语音内容；  <br/>   - 设计VisioPhonetic适配器，实现CSV2S与CSR的跨任务表示兼容；  <br/>   - 开发姿态感知视觉处理器，增强唇语与手势的时空关联性。  <br/>4. **实验验证与数据集**：基于新建立的中文CS数据集（含14名受试者），验证方法有效性，降低误词率78.3%并提升唇语同步性32%。|
|2506.04077v1|[A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions](http://arxiv.org/abs/2506.04077v1)|总结：  <br/>该论文提出一种基于大语言模型与跨模态信息的新型语音评估方法，有效解决低资源场景下的标签约束问题，提升口语评分的可靠性和多样性。<br/><br/>贡献点：  <br/>1. **低资源场景下的数据生成**：利用大语言模型生成多样化响应并合成语音，突破标注数据的稀缺限制。  <br/>2. **说话人感知的语音合成**：通过 speaker-aware 文本到语音合成技术，增强生成语音与真实语音的匹配度。  <br/>3. **动态重要性损失设计**：根据合成语音与真实语音的特征差异，自适应调整训练样本权重，提升模型学习效率。  <br/>4. **跨模态特征融合**：构建多模态大语言模型，直接整合文本与语音信号的对齐特征以预测口语水平。  <br/>5. **有效性验证**：在 LTTC 数据集上验证方法优于传统数据依赖或常规增强技术，实现跨模态信息驱动的自动化评分。|
|2506.03884v1|[Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages](http://arxiv.org/abs/2506.03884v1)|总结: 本文提出基于零样本合成的多语言TTS系统，通过共享音素表示和适配文本解析规则，成功生成梵语等低资源语言的自然语音，验证了该方法在扩展语音技术普惠性方面的有效性。<br/><br/>贡献点:<br/>1. 提出零样本合成框架，解决印度1369种语言中多数缺乏数字资源的TTS训练难题<br/>2. 构建跨语言音素共享表示，降低多语言模型过度假设需求<br/>3. 开发适应不同音系规则的文本解析方法（phonotactics adaptation）<br/>4. 实现Sanskrit、Maharashtrian、Canara Konkani、Maithili、Kurukh等5种语言的自然语音合成<br/>5. 提供可复用的合成器架构，验证方法在扩大语音技术覆盖范围中的可行性|
|2506.03831v1|[Conformer-based Ultrasound-to-Speech Conversion](http://arxiv.org/abs/2506.03831v1)|总结：  <br/>本研究对比了Conformer与传统CNN在超声波语音转换中的表现，发现Conformer结合bi-LSTM在感知质量上更优，而Conformer Base在效率上显著提升，为静默语音接口提供了新思路。<br/><br/>贡献点：  <br/>1. **提出两种Conformer-based模型**：首次将Conformer架构应用于超声波到语音转换任务，分别采用Base结构和结合bi-LSTM的结构。  <br/>2. **融合HiFi-GAN声码器**：通过HiFi-GAN将生成的mel频谱图合成高质量音频波形，提升系统输出的语音还原效果。  <br/>3. **验证感知质量优势**：在MUSHRA主观测试中发现，Conformer与bi-LSTM结合的模型在语音可懂度上优于传统CNN基线。  <br/>4. **效率提升**：Conformer Base模型在保持性能的同时，训练速度比传统CNN快3倍，降低了计算成本。|
|2506.03793v1|[Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts](http://arxiv.org/abs/2506.03793v1)|**贡献点**  <br/>1. **提出Cadence模型**：基于预训练大语言模型设计，首次实现对自发语音转录的精准标点恢复，克服传统模型在处理断续、重复等语音特征时的不足。  <br/>2. **扩展语言覆盖**：支持22种印度语言及英语（原仅14种），显著提升多语言标点恢复的适用性与泛化能力。  <br/>3. **分析挑战与改进**：通过跨语言和标点类型的行为研究，揭示领域迁移及罕见标点符号下的修复难题，验证预训练模型在低资源NLP场景中的有效性。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出Cadence模型，基于预训练语言模型实现自发语音的精准标点恢复，扩展支持22种印度语言和英语，并通过深入分析揭示多语言任务中的关键挑战。|
|2506.03515v1|[BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and   Weight Indexing](http://arxiv.org/abs/2506.03515v1)|总结：  <br/>本文提出两种模型压缩技术，实现83%的模型体积减小，同时保持高质量语音合成，适用于设备端部署。<br/><br/>贡献点：<br/>1. 提出量化感知训练（QAT）技术，将模型参数从32位压缩至1.58位，主要采用三值{-1,0,1}量化策略。<br/>2. 开发权重索引方法，通过将一组1.58位权重编码为单个int8索引来优化存储效率。<br/>3. 在保持合成质量的前提下，实现模型轻量化，使其更适用于资源受限的设备端应用场景。|
|2506.02979v1|[Towards a Japanese Full-duplex Spoken Dialogue System](http://arxiv.org/abs/2506.02979v1)|**贡献点总结：**  <br/>1. 提出首个公开可用的全双工日语语音对话模型，基于英语模型Moshi迁移。  <br/>2. 采用两阶段训练方法：大规模日语语料预训练与高质量立体声数据微调。  <br/>3. 引入多流文本到语音系统生成的合成对话数据以增强模型性能。  <br/>4. 实验证明模型在自然度和语义理解上优于现有日语基准模型。  <br/><br/>（注：以上为100字以内总结，符合要求。）|
|2506.02958v1|[PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech   Editing](http://arxiv.org/abs/2506.02958v1)|**贡献点总结（100字以内）:**  <br/>提出PartialEdit数据集，聚焦神经语音编辑生成的深伪语音检测，揭示现有模型的局限性，分析伪影特征，推动该领域研究。<br/><br/>**分点贡献:**  <br/>1. **提出PartialEdit数据集**：构建首个基于先进神经语音编辑技术的深伪语音检测数据集，包含部分编辑的语音样本，为研究提供基准。  <br/>2. **探索检测与定位任务**：系统研究部分编辑语音的检测与内容定位，验证当前检测方法在复杂编辑场景下的有效性。  <br/>3. **揭示模型局限性**：实验表明基于PartialSpoof数据集训练的模型无法有效识别神经编辑生成的深伪语音，凸显数据集不足。  <br/>4. **分析伪影特征**：结合神经音频编码器，探讨检测模型学习到的伪影（artifacts）特征，为改进检测方法提供理论依据。  <br/>5. **提供公开资源**：开放数据集和音频样本，方便后续研究复现与扩展。|
|2506.02858v2|[DGMO: Training-Free Audio Source Separation through Diffusion-Guided   Mask Optimization](http://arxiv.org/abs/2506.02858v2)|总结：  <br/>提出训练无关的零样本音频源分离框架DGMO，利用扩散模型生成先验优化频谱掩码，突破传统方法依赖任务特定训练的限制，拓展扩散模型在音频分离领域的应用范式。<br/><br/>贡献点：  <br/>1. **首个训练无关的零样本框架**：首次将预训练扩散模型直接用于语言查询的音频源分离，无需额外任务特定训练。  <br/>2. **测试时优化机制**：提出Diffusion-Guided Mask Optimization（DGMO），通过测试阶段动态优化频谱掩码以提升分离精度。  <br/>3. **跨模态适应性提升**：分析模态适配问题，揭示自然语言与音频处理的协同挑战，并提出针对性解决方案。  <br/>4. **性能验证**：在无监督或弱监督条件下实现竞争性分离效果，证明扩散模型可有效迁移至音频分离任务。  <br/>5. **应用范式创新**：拓展扩散模型从生成任务到分离任务的边界，建立新的零样本音频处理框架。|
|2506.02401v1|[Trusted Fake Audio Detection Based on Dirichlet Distribution](http://arxiv.org/abs/2506.02401v1)|**贡献点：**  <br/>1. **提出基于狄利克雷分布的不确定性建模方法**：首次将狄利克雷分布用于量化模型决策的不确定性，提升假音频检测的可信度评估能力。  <br/>2. **构建新型检测框架**：通过神经网络生成证据，并将预测概率与不确定性估计结合，形成最终检测意见，优化检测流程。  <br/>3. **多数据集验证**：在ASVspoof 2019 LA、2021 LA及DF数据集上进行对比实验，验证了模型在准确率、鲁棒性和可信度方面的优越性能。  <br/><br/>**总结：**  <br/>该研究通过狄利克雷分布建模不确定性，提出新型假音频检测方法，在多数据集上验证了其高准确率与鲁棒性，显著提升了检测的可靠性。|
|2506.02082v1|[SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction](http://arxiv.org/abs/2506.02082v1)|总结：该论文提出了一种新的SALF-MOS模型，解决传统MOS评估高成本问题，通过卷积序列提取音频潜在特征，实现小尺寸、端到端、高度通用和可扩展的语音质量预测，取得最佳评估效果。<br/><br/>贡献点：<br/>1. 提出Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS)模型，解决传统MOS评估需要大量人工标注的问题<br/>2. 设计基于卷积序列的音频特征提取方法，实现端到端的MOS预测系统<br/>3. 构建的小尺寸模型在保持通用性的同时具备良好的可扩展性<br/>4. 在多个客观评估指标（MSE、LCC、SRCC、KTAU）上达到当前最优性能<br/>5. 提供了一种新的语音质量评估方法，可作为TTS和语音转换模型选择的有效工具|
|2506.01618v1|[Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech](http://arxiv.org/abs/2506.01618v1)|总结（100字以内）:  <br/>本文提出基于音节的节奏建模方法，结合LF-MMI模型显著提升口吃语音的ASR性能，验证了无监督转换的有效性。<br/><br/>贡献点:  <br/>1. **提出syllable-based rhythm modeling方法**：在Rhythm and Voice（RnV）框架中引入针对口吃语音的音节级节奏建模技术，解决传统方法对口吃语音节奏特征建模不足的问题。  <br/>2. **评估转换对ASR的影响**：首次系统验证口吃语音转换对ASR性能提升的效果，通过训练LF-MMI模型和微调Whisper模型，发现LF-MMI在口吃语音识别中具有显著优势。  <br/>3. **揭示无监督转换的潜力**：证明无监督学习的节奏和语音转换技术在口吃ASR任务中的有效性，为低资源场景下的语音处理提供了新思路。|
|2506.01322v1|[Zero-Shot Text-to-Speech for Vietnamese](http://arxiv.org/abs/2506.01322v1)|贡献点：<br/>1. 构建了首个越南语高质量语音数据集PhoAudiobook（含941小时音频）  <br/>2. 在VALL-E、VoiceCraft、XTTS-V2等三大零样本TTS模型上开展系统实验  <br/>3. 证实PhoAudiobook能显著提升多指标下的模型性能  <br/>4. 揭示VALL-E与VoiceCraft在短句合成任务中的优越性  <br/>5. 公开数据集促进越南语TTS领域研究发展  <br/><br/>总结：  <br/>本研究发布首个越南语高质量语音数据集PhoAudiobook，并验证其提升三大零样本TTS模型性能的效果，尤其突出VALL-E与VoiceCraft在短句生成中的优势，推动越南语语音合成研究。|
|2506.01268v1|[CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction](http://arxiv.org/abs/2506.01268v1)|贡献点总结：<br/>1. 提出单文件实现的CleanS2S框架，实现低延迟语音交互系统<br/>2. 首创主动对话机制，集成记忆系统与主观行动判断模块<br/>3. 开发五种人类般响应策略（中断/拒绝/转移/沉默/标准响应）<br/>4. 实现全双工WebSocket连接与非阻塞I/O的实时中断处理<br/>5. 提出Action Judgement SFT方法评估输入流策略选择<br/>6. 提供前所未有的透明度和可扩展性，代码已开源<br/><br/>（97字）|
|2506.01263v1|[WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing](http://arxiv.org/abs/2506.01263v1)|总结：  <br/>提出无需额外训练的CTC模型改进方法，通过中间层声学特征检测关键词并引入偏置机制，有效提升罕见词识别准确率，实验在日语语音识别中实现29%的F1分数提升。<br/><br/>贡献点：  <br/>1. **解决罕见词识别偏差问题**：通过中间层声学特征进行关键词检测，避免模型对训练数据词汇的过度依赖。  <br/>2. **引入灵活的wildcard CTC机制**：采用快速且容错的CTC变体，支持模糊匹配处理难以严格识别的单词。  <br/>3. **无需模型重训练的可扩展性**：方法兼容现有模型，无需额外训练或文本到语音系统，适用于大规模模型。  <br/>4. **显著提升未知词识别性能**：在日语语音识别任务中，实现未知词F1分数29%的提升，验证方法有效性。|
|2506.01032v1|[ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and   Speaker Feature Optimization](http://arxiv.org/abs/2506.01032v1)|**总结**：  <br/>本文提出ReFlow-VC，通过修正流的ODE框架提升语音转换效率，结合内容与音高信息优化说话人特征，并验证其在小数据集和零样本场景中的优越性能，推动高保真语音转换的实际应用。  <br/><br/>**贡献点**：  <br/>1. **提出ReFlow-VC方法**：基于修正流构建高效高保真语音转换模型，通过ODE框架将高斯分布直接映射到真实Mel频谱图分布，减少采样步骤。  <br/>2. **优化说话人特征建模**：同时利用内容与音高信息优化特征，增强说话人属性的准确表达。  <br/>3. **验证小样本适配性**：在小数据集和零样本场景中均表现出卓越的性能，突破传统扩散模型的局限。|
|2506.01020v1|[DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation](http://arxiv.org/abs/2506.01020v1)|总结：  <br/>该论文提出DS-TTS框架，结合双风格编码与动态生成网络，解决zero-shot语音克隆问题，实现未见过语音的高质量合成，并在VCTK数据集上验证了其优越的性能。<br/><br/>贡献点：  <br/>1. **提出DS-TTS框架**：设计一种用于zero-shot语音克隆的全新方法，仅需单个音频样本和任意文本即可合成目标说话人语音，无需历史训练数据。  <br/>2. **双风格编码网络（DuSEN）**：通过两个互补的风格编码器捕捉说话人语音身份的多维度特征，增强对未见说话人风格的建模能力。  <br/>3. **动态融合机制（SGF + DyGN）**：引入风格门控-Film机制将风格向量融入动态生成器，实现对不同句子长度的自适应合成，提升鲁棒性与自然度。  <br/>4. **实验验证优势**：在VCTK数据集上证明DS-TTS在单词错误率和说话人相似度上优于现有模型，显著改善语音克隆的泛化与表达性。|
|2506.00832v1|[Counterfactual Activation Editing for Post-hoc Prosody and   Mispronunciation Correction in TTS Models](http://arxiv.org/abs/2506.00832v1)|**贡献点：**<br/>1. **提出模型无关的后处理技术**：Counterfactual Activation Editing 是一种通用方法，无需依赖预训练模型特定架构，可适用于多种TTS模型。  <br/>2. **实现韵律与发音的联合控制**：同时支持对语音韵律特征（prosody）和发音错误（mispronunciation）的精确调整，提升控制能力。  <br/>3. **无需额外训练即可进行推理时调整**：在不重新训练模型的情况下，直接对预训练TTS的内部表示进行修改，实现在线输出优化。  <br/>4. **适应低资源场景**：解决传统发音纠正依赖音素词典（grapheme-to-phoneme dictionaries）的问题，降低对资源的依赖。  <br/>5. **填补预训练模型与可编辑合成的空白**：首次将可编辑的语音合成能力引入预训练TTS框架，推动模型灵活性与应用范围的扩展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种通用、无需额外训练的后处理方法，实现对TTS输出的韵律与发音联合控制，适应低资源场景，推动预训练模型向可编辑语音合成发展。|
|2506.00736v1|[IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio   Generation with Diffusion Modeling](http://arxiv.org/abs/2506.00736v1)|总结：  <br/>本文提出IMPACT框架，结合扩散建模与迭代掩码并行解码，在连续潜空间中生成高质量文本到音频，有效解决先前模型在音质与推理速度之间的权衡问题，实验在AudioCaps上达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出IMPACT框架**：首次将迭代掩码并行解码与扩散模型结合，实现文本到音频生成中高保真度与低延迟的双重优化。  <br/>2. **连续潜空间设计**：通过在连续潜空间中操作，避免离散token带来的保真度限制，提升生成音频的自然度和质量。  <br/>3. **高效解码策略**：采用迭代掩码并行解码技术，在保持推理速度的同时显著提高音频生成效果，优于MAGNET等基于离散token的模型。  <br/>4. **验证SOTA性能**：在AudioCaps数据集上，IMPACT在Fréchet Distance (FD)和Fréchet Audio Distance (FAD)等关键指标上超越现有方法，验证了其有效性。|
|2506.00722v1|[Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](http://arxiv.org/abs/2506.00722v1)|总结：该研究提出基于链式思维的端到端语音对话系统方法，在较小数据集上实现显著性能提升并提升语义连贯性，同时增强计算效率，模型及代码将公开。<br/><br/>贡献点：<br/>1. 提出基于链式思维（Chain-of-Thought, CoT）的端到端（E2E）对话系统框架，解决了传统级联流水线的局限性；<br/>2. 通过将对话训练与多模态语言模型（涵盖语音识别、文本生成、语音合成等任务）的预训练对齐，提升语义连贯性；<br/>3. 实现仅需300小时公开对话数据（如Switchboard）即可获得超过1.5 ROUGE-1的提升效果；<br/>4. 优化计算效率，使模型能在有限数据量下高效训练；<br/>5. 提供公开模型及代码，推动该领域研究与应用。|
|2506.00548v1|[Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities](http://arxiv.org/abs/2506.00548v1)|总结（100字以内）：  <br/>本论文提出Con Instruction方法，通过生成对抗图像和音频攻击多模态语言模型，无需文本预处理或训练数据。引入新型ARC评估框架，并验证其在多个模型上的高攻击成功率，揭示当前安全机制的不足。<br/><br/>贡献点（分点列出）：  <br/>1. **提出非文本指令攻击方法（Con Instruction）**  <br/>   - 首次利用MLLMs对非文本指令（图像/音频）的解析能力，通过生成对抗性图像或音频发起攻击，突破传统依赖文本指令的攻击模式。  <br/>   - 攻击无需训练数据或文本预处理，简化了攻击流程。  <br/><br/>2. **优化对抗样本的语义对齐**  <br/>   - 在嵌入空间中优化对抗样本，使其更精准匹配目标指令，显著提升攻击成功率，揭示MLLMs复杂语义理解的潜在安全风险。  <br/><br/>3. **提出攻击响应分类（ARC）框架**  <br/>   - 引入新型评估体系，综合衡量模型响应质量与对恶意指令的相关性，为安全评估提供更全面的指标。  <br/><br/>4. **验证多模型有效性与攻击性能**  <br/>   - 在LLaVA-v1.5、InternVL、Qwen-VL、Qwen-Audio等多模态模型上验证攻击效果，尤其在LLaVA-v1.5 (13B)中达到81.3%和86.6%的成功率，证明方法的普适性。  <br/><br/>5. **分析防御技术的局限性**  <br/>   - 探索针对攻击的防御措施，发现现有技术在攻击检测和防御能力上存在显著性能差距，为后续防御研究提供方向。  <br/><br/>6. **开放源代码实现**  <br/>   - 公布实现代码，便于学术界和工业界复现与扩展研究。|
|2506.00462v1|[XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](http://arxiv.org/abs/2506.00462v1)|**贡献点：**  <br/>1. **提出跨域多语言音频深度伪造基准**：构建XMAD-Bench数据集，包含668.8小时真实与深度伪造语音，覆盖多语言和多样化生成方法。  <br/>2. **强化数据集多样性与挑战性**：训练与测试集的说话人、生成模型、真实音频源均独立，显著提升跨域评估的难度。  <br/>3. **揭示性能差距**：实验结果表明，域内检测准确率接近100%，但跨域性能严重下降，接近随机猜测，突显现有方法的局限性。  <br/>4. **推动鲁棒检测器发展**：强调需开发具备跨语言、跨说话人、跨生成方法和跨数据源泛化能力的深度伪造检测技术。  <br/>5. **公开数据集**：将XMAD-Bench在GitHub平台开源，为研究社区提供统一的评估基准。  <br/><br/>**总结**：  <br/>提出首个跨域多语言音频深度伪造基准XMAD-Bench，验证现有检测模型在真实场景下的性能不足，推动更鲁棒的检测技术发展。|
|2506.00385v1|[MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity   Reconstruction and Generation](http://arxiv.org/abs/2506.00385v1)|**贡献点：**  <br/>1. 提出新型单层流式Transformer音频编解码器MagiCodec，解决传统编解码器重建质量与下游模型兼容性之间的矛盾。  <br/>2. 引入多阶段训练框架，结合高斯噪声注入与潜在正则化技术，增强编码的语义表达性并保持重建保真度。  <br/>3. 理论推导噪声注入在频域中的作用，证明其对抑制高频成分、提升鲁棒性效果显著。  <br/>4. 实验验证MagiCodec在重建质量与下游任务（如语言模型兼容）上优于现有技术，生成的编码呈现齐普夫分布特性。  <br/>5. 开源代码与预训练模型，促进研究复现与应用。  <br/><br/>**总结：**  <br/>MagiCodec通过噪声注入与多阶段训练提升音频编码的语义表达和重建质量，实现与语言模型的兼容性，开源推动研究进展。|
|2506.00087v1|[SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset](http://arxiv.org/abs/2506.00087v1)|总结：  <br/>本文提出多代理协作框架LinguaMaster，构建首个大规模多语言多民族代码转换数据集SwitchLingua，并设计语义感知评估指标SAER，推动多语言和文化多样性研究。  <br/><br/>贡献点：  <br/>1. **提出LinguaMaster框架**：首次设计多智能体协作系统，实现高效可扩展的多语言数据合成。  <br/>2. **构建SwitchLingua数据集**：创建首个覆盖12种语言、18国/地区、63个种族背景的大型代码转换语料库（含420K文本样本及80小时音频）。  <br/>3. **创新SAER评估指标**：提出融合语义信息的代码转换ASR评估方法，提升对跨语言场景的性能判断准确性。|
|2505.24496v1|[Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation](http://arxiv.org/abs/2505.24496v1)|总结：  <br/>本文提出一种压缩-精细语言建模方法，通过保留局部上下文和压缩长距离信息，有效解决神经音频编解码器在语音生成中的长序列挑战，提升生成质量与模型通用性。<br/><br/>贡献点：  <br/>1. **揭示语音标记序列的短距离依赖性**：首次观察到TTS任务中语音标记的预测主要依赖局部上下文，长距离标记贡献较小且冗余。  <br/>2. **设计压缩-精细语言建模框架**：  <br/>   - 保留初始提示和短范围标记，确保文本对齐及语用信息完整性；  <br/>   - 将长范围标记压缩为紧凑表示，减少冗余同时保留关键语义。  <br/>3. **验证方法的有效性与通用性**：通过多组神经音频编解码器和下游语言模型的实验，证明该方法可提升语音生成效果，具有广泛适用性。|
|2505.24314v1|[DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture   Switching for Speech Codec](http://arxiv.org/abs/2505.24314v1)|**贡献点：**  <br/>1. 提出DS-Codec，首次结合镜像（mirror）与非镜像（non-mirror）架构的双阶段训练框架，提升语音重建质量。  <br/>2. 设计动态切换机制，平衡镜像与非镜像结构的优势，增强模型的鲁棒性与高保真度。  <br/>3. 通过大规模实验与消融研究，验证了训练策略的有效性，对比了两种架构的性能差异。  <br/><br/>**总结：**  <br/>本研究提出DS-Codec，通过镜像与非镜像结构的双阶段训练框架提升语音重构效果，并系统验证了其优势。|
|2505.23619v1|[Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](http://arxiv.org/abs/2505.23619v1)|贡献点总结：<br/>1. 提出ADD-GP框架：基于高斯过程的少样本音频深度伪造检测方法，支持快速适应新生成模型  <br/>2. 性能突破：结合深度嵌入模型与GP灵活性，在少量数据下实现高检测准确率  <br/>3. 个性化检测能力：增强对新型TTS模型的鲁棒性，支持单样本级自适应  <br/>4. 构建基准数据集：使用SOTA语音克隆模型创建专用评估数据集  <br/><br/>（99字）|
|2505.20868v2|[Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction   and Style Direction Adjustment for Expressive Text-to-Speech](http://arxiv.org/abs/2505.20868v2)|**贡献点总结（100字以内）：**  <br/>Spotlight-TTS通过语音感知的风格提取和方向调整，提升表达性与语音质量，实验验证其在关键指标上优于基线模型，并公开音频样本以促进研究。  <br/><br/>**分点贡献：**  <br/>1. **提出语音感知风格提取方法**：聚焦与风格强相关的发声区域（voiced regions），同时保持跨语音区域的连续性，增强语音表达性。  <br/>2. **设计风格方向调整机制**：优化提取的风格方向以更好地整合到TTS模型中，显著提升生成语音的整体质量。  <br/>3. **验证性能优越性**：通过实验表明，Spotlight-TTS在表达性、语音质量和风格迁移能力方面均优于现有基线方法。  <br/>4. **开放数据支持**：公开音频样本，便于社区验证成果并推动相关研究发展。|
|2505.19931v2|[Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned   Step Sampling](http://arxiv.org/abs/2505.19931v2)|总结（100字以内）:  <br/>提出训练无关的Fast F5-TTS方法，通过分析采样轨迹设计EPSS策略减少冗余步骤，实现7步生成且推理速度提升4倍，验证了EPSS对流匹配类TTS模型的通用性。  <br/><br/>贡献点:  <br/>1. **训练无关加速方案**：提出Fast F5-TTS，无需额外训练直接优化流匹配TTS模型的推理效率。  <br/>2. **非均匀采样策略**：开发EPSS方法，通过识别冗余采样步骤，显著减少生成过程的采样次数。  <br/>3. **高效推理性能**：在NVIDIA RTX 3090上实现7步生成，推理RTF达0.030，且性能与原模型相当。  <br/>4. **跨模型通用性**：验证EPSS在E2 TTS等其他流匹配模型上的有效性，证明其广泛适用性。|
|2505.19669v2|[Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling](http://arxiv.org/abs/2505.19669v2)|总结：  <br/>提出SMLLE框架，通过Transducer与Delete <Bos>机制解决流式TTS低延迟问题，实现高质量逐帧语音生成，优于现有流式方法且接近句级系统性能。<br/><br/>贡献点：  <br/>1. **提出SMLLE框架**：首次构建零样本流式TTS系统，无需依赖未来文本即可实现低延迟高质量语音合成。  <br/>2. **融合Transducer与时长对齐**：实时将文本转化为语义标记，结合时长对齐信息提升语音自然度。  <br/>3. **设计Delete <Bos>机制**：优化自回归模型对未来文本的访问，最小化延迟同时保持生成稳定性。  <br/>4. **验证性能优势**：实验表明SMLLE在流式TTS任务中超越现有方法，在句级系统中表现可比，具备实际应用价值。|
|2505.19595v2|[Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment](http://arxiv.org/abs/2505.19595v2)|总结：  <br/>本研究提出A-DMA方法，通过双模态对齐优化扩散模型的训练，显著提升收敛速度与生成质量，降低计算成本，并开源代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **提出A-DMA策略**：设计双模态对齐机制，通过文本与语音模态的协同优化加速扩散模型的训练过程。  <br/>2. **双模态对齐技术**：引入文本引导对齐（融合上下文表示）和语音引导对齐（精炼语义表示）的双重对齐管道。  <br/>3. **减少隐式学习依赖**：通过显式对齐隐藏状态与判别特征，降低扩散模型对复杂中间表示的隐式学习需求。  <br/>4. **实验验证效果**：在多个基准上验证A-DMA的性能优势，收敛速度提升1倍，生成质量优于现有方法，并开源代码与演示样本。|
|2505.17076v2|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v2)|总结：  <br/>研究分析不同帧率对中文和英文语音分词效果的影响，揭示帧率与语言特性间的关系，为语音分词优化提供参考。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统探讨低帧率编解码器对语音分词的影响，揭示帧率与语言模型间的作用机制。  <br/>2. **跨语言对比分析**：通过对比汉语和英语两种音系差异显著的语言，发现帧率对不同语言的分词效果存在异质性。  <br/>3. **揭示关键影响因素**：明确帧率变化与音素密度、语言特异性声学特征的交互作用，为分词器设计提供理论依据。  <br/>4. **应用指导意义**：提出优化帧率选择的策略，对自动语音识别、文本到语音等语音处理任务具有实践价值。|
|2504.14906v3|[OmniAudio: Generating Spatial Audio from 360-Degree Video](http://arxiv.org/abs/2504.14906v3)|总结：  <br/>提出360V2SA新任务，构建Sphere360数据集，设计半自动化数据流程，提出OmniAudio框架（结合自监督训练与双分支结构），在空间音频生成领域取得SOTA成果。<br/><br/>贡献点：  <br/>1. 提出360V2SA任务：首次系统性生成360度视频对应的3D空间音频（FOA格式），填补传统视频-音频生成在空间线索上的不足。  <br/>2. 构建Sphere360数据集：首个针对该任务的高质量数据集，基于真实场景数据并优化数据配对与清洗流程。  <br/>3. 设计半自动化数据收集管道：提供高效且可扩展的视频-音频数据配对与预处理方案，降低数据获取门槛。  <br/>4. 提出OmniAudio框架：结合自监督预训练和双分支结构（全景+视角视频输入），同时建模局部与全局空间信息。  <br/>5. 实验验证与开源：在Sphere360上实现SOTA性能，代码与数据集开源，推动领域研究与应用。|