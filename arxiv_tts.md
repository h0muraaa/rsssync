|Source|Title|Summary|
|---|---|---|
|2507.10827v1|[Supporting SENĆOTEN Language Documentation Efforts with Automatic   Speech Recognition](http://arxiv.org/abs/2507.10827v1)|总结（100字以内）:  <br/>提出ASR驱动的文档流水线，结合TTS数据增强与跨语言迁移学习，有效应对SEN'COOTEN语言数据稀缺和词汇变异问题，实验验证了方法在语言记录和教育资源创建中的潜力。<br/><br/>贡献点:  <br/>1. **提出ASR驱动的文档流程**：构建新型语音技术框架，整合TTS系统生成的增强语音数据，辅助原住民语言复兴工作。  <br/>2. **跨语言迁移学习技术**：利用语音基础模型（SFMs）实现跨语言知识迁移，解决小语种数据不足的挑战。  <br/>3. **n-gram语言模型优化**：通过浅层融合或n-best恢复策略，最大化有限数据的利用效率以提升ASR性能。  <br/>4. **实验证明有效性**：在SEN'COOTEN数据集上取得14.32% WER和3.45% CER的改进，展示方法对多合成语言和元音交替语言的适应性。|
|2507.10469v1|[An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived   Realism and Performance in Virtual Reality Environments](http://arxiv.org/abs/2507.10469v1)|总结：  <br/>该研究评估AI驱动的VR审讯模拟器NPC性能，发现其在行为、社交和智力表现优异，但情感深度不足，系统延迟较高，为提升NPC沉浸感提出了性能优化需求。  <br/><br/>贡献点：  <br/>1. **首次系统评估AI增强的VR审讯NPC**：通过量化分析（SUS、GEQ、虚拟代理可信度问卷）与延迟测量，全面评估AI驱动NPC的现实感与交互体验。  <br/>2. **揭示系统性能瓶颈**：发现平均循环延迟达7秒（受对话上下文影响），为AI在实时语音交互中的优化提供数据依据。  <br/>3. **量化情感与个性表现短板**：尽管在行为、社交关系和智力方面得分较高（6.67/10），但情感与个性维度得分中等，指出AI情感建模的不足。  <br/>4. **推动沉浸式NPC开发方向**：强调通过性能优化和情感深度增强，可进一步提升虚拟环境的沉浸感与交互质量。  <br/>5. **验证LLM在NPC领域的应用潜力**：证明大语言模型（如GPT-4 Turbo）在提升NPC现实感和交互性方面的有效性，拓展AI在语音交互场景的应用边界。|
|2507.09318v1|[ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow   Matching](http://arxiv.org/abs/2507.09318v1)|总结：  <br/>提出非自回归流匹配模型ZipVoice-Dialog，设计说话者回合嵌入与课程学习策略，开发OpenDialog数据集，建立多指标评估基准，实现对话生成效率与质量的突破。<br/><br/>贡献点：  <br/>1. **模型创新**：提出首个基于流匹配的非自回归zero-shot语音对话生成模型，解决传统自回归模型推理速度慢、不稳定问题。  <br/>2. **说话者建模**：引入说话者回合嵌入（speaker-turn embeddings）实现精准对话轮次切换，增强多说话者区分度。  <br/>3. **对齐优化**：采用课程学习策略（curriculum learning）提升语音-文本对齐稳定性，增强生成一致性。  <br/>4. **数据集构建**：开源OpenDialog数据集（6.8k小时），填补领域内大规模真实对话数据的空白。  <br/>5. **评估体系**：建立全面基准，涵盖可懂度、轮次准确率、说话者相似性等关键指标。  <br/>6. **资源开放**：提供代码、模型、演示样本及数据集，促进技术复现与研究进展。|
|2507.09310v1|[Voice Conversion for Lombard Speaking Style with Implicit and Explicit   Acoustic Feature Conditioning](http://arxiv.org/abs/2507.09310v1)|总结（100字以内）:  <br/>本研究提出隐式声学特征条件的语音转换方法，有效实现Lombard风格的说话人转换，实验表明该方法在保持说话人相似性的同时达到与显式条件同等的可懂度提升，为TTS系统提供数据增强方案。<br/><br/>贡献点:  <br/>1. 提出基于隐式声学特征条件的Lombard风格语音转换策略，解决目标说话人数据不足的训练问题。  <br/>2. 对比隐式与显式声学特征条件的VC模型，验证隐式策略在保持说话人相似性的同时可实现同等语言可懂度提升。  <br/>3. 探索Lombard效应的风格迁移框架，为TTS系统在噪声环境下的语音增强提供新思路。  <br/>4. 通过实验证明隐式条件方法在数据采集效率与风格保持性之间的平衡优势。|
|2507.09282v1|[ClaritySpeech: Dementia Obfuscation in Speech](http://arxiv.org/abs/2507.09282v1)|**贡献点总结（100字以内）**  <br/>本文提出ClaritySpeech框架，通过整合ASR、文本模糊与零样本TTS，在无需微调的低数据环境下修正痴呆症患者语音，提升隐私保护与可访问性，显著降低WER并改善语音质量，保持说话人身份识别。<br/><br/>---<br/><br/>**分点贡献点**  <br/>1. **提出首个融合ASR与隐私保护的框架**：ClaritySpeech首次结合自动语音识别（ASR）、文本模糊和零样本TTS技术，解决痴呆症患者语音障碍问题。  <br/>2. **无需微调的低数据环境适应性**：框架在低数据场景下运行，无需额外微调，降低部署成本与技术门槛。  <br/>3. **隐私与可访问性双重优化**：通过隐藏敏感信息（如语音特征）和提升语音质量，兼顾隐私保护与无障碍通信需求。  <br/>4. **显著提升语音质量与识别准确率**：在ADReSS和ADReSSo数据集上，语音质量从1.65提升至2.15，WER分别降低至0.08和0.15。  <br/>5. **保持说话人身份识别能力**：在修正语音的同时，保留50%的说话人相似度，确保个体特征不被完全破坏。|
|2507.08983v1|[Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](http://arxiv.org/abs/2507.08983v1)|总结：  <br/>提出TrojanClimb框架，揭示模型排行榜在语音领域中可被利用为隐蔽分发中毒模型的渠道，验证其在多模态任务中的有效性，强调需改进排行榜评估机制以保障模型安全。<br/><br/>贡献点：  <br/>1. **提出TrojanClimb框架**：首个可在保持模型排行榜竞争力的同时注入恶意行为（如后门、偏见）的通用攻击框架。  <br/>2. **验证多模态攻击有效性**：实验证明框架适用于文本嵌入、生成、语音合成及图像生成等四个关键模态，展示对手可实现隐蔽的高排名传播。  <br/>3. **揭示系统性漏洞**：系统性发现模型排行榜作为模型分发渠道的潜在安全风险，指出当前机器学习生态系统的关键脆弱点。  <br/>4. **呼吁机制重构**：强调需重新设计排行榜评估方法以检测和过滤恶意模型，同时警示社区关于依赖未经验证模型的风险。|
|2507.08530v1|[MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling](http://arxiv.org/abs/2507.08530v1)|总结：  <br/>提出MIDI-VALLE模型，通过整合零样本个性化TTS框架和离散token编码技术，显著提升音乐表演合成的泛化能力和质量，优于现有基线方法。<br/><br/>贡献点：  <br/>1. **零样本个性化合成**：首次将VALLE框架应用于音乐表演合成，实现基于参考音频和MIDI的零样本生成，突破传统两阶段方法的限制。  <br/>2. **双模态离散编码**：创新性地将MIDI和音频都编码为离散token，提升模型对音乐表现的建模一致性与鲁棒性。  <br/>3. **参考条件优化**：通过引入参考音频及对应MIDI作为条件，改善合成结果与目标性能的匹配度。  <br/>4. **多样化数据训练**：基于大规模且多样的钢琴表演数据集训练模型，显著增强跨风格和环境的泛化能力。  <br/>5. **性能指标领先**：在ATEPP和Maestro数据集上，以75%以上的FAD降低量和监听投票数超越基线，验证了模型有效性。|
|2507.08319v1|[Active Learning for Text-to-Speech Synthesis with Informative Sample   Collection](http://arxiv.org/abs/2507.08319v1)|总结（100字以内）:  <br/>提出基于主动学习的语音合成语料库构建方法，通过迭代优化提升数据效率与合成质量，验证在同等数据规模下性能优于传统方法。<br/><br/>贡献点：<br/>1. **创新方法框架**：首次将主动学习引入TTS语料库构建，突破传统前馈式和模型无关方法的局限，实现数据收集与模型训练的动态协同。<br/>2. **数据效率提升**：通过迭代机制优先选择对模型优化更关键的数据样本，降低存储与计算成本，构建更紧凑高效的数据集。<br/>3. **性能验证**：实验证明，所构建的语料库在相同数据量下显著提高语音合成质量，优于现有方法。<br/>4. **应用价值拓展**：为应对大规模数据挑战提供可扩展的解决方案，推动资源受限场景下的TTS系统优化。|
|2507.06826v1|[Physics-Informed Direction-Aware Neural Acoustic Fields](http://arxiv.org/abs/2507.06826v1)|总结：  <br/>提出物理引导神经网络模型，引入两个基于粒子速度的物理先验，通过通道导数关联增强空间建模准确性，实验验证优于传统方法。<br/><br/>贡献点：  <br/>1. **扩展PINN应用领域**：首次将物理引导神经网络（PINN）框架应用于建模一阶Ambisonic（FOA）房间脉冲响应（RIRs），而非传统声压数据建模。  <br/>2. **提出两组物理先验**：基于粒子速度与FOA（X,Y,Z）通道的对应关系，推导出两个物理引导先验，关联W通道与其他通道的偏导数。  <br/>3. **建立物理约束关系**：通过强制施加四通道间的物理可行性关系，提升模型对声场空间特性的建模能力。  <br/>4. **验证方法有效性**：实验结果表明，该方法在FOA RIR建模任务中优于无物理先验的神经网络模型。|
|2507.06483v1|[Learning Japanese with Jouzu: Interaction Outcomes with Stylized   Dialogue Fictional Agents](http://arxiv.org/abs/2507.06483v1)|**贡献点分点列表：**  <br/>1. **提出文化与情感化代理的设计影响机制**：系统分析了风格化语音（包括语音、个性和语言风格）对多模态语言学习环境中用户交互的深层影响，揭示了其在跨文化背景和语言水平中的差异化作用。  <br/>2. **构建混合方法评估框架**：首次结合定量与定性方法，研究54名用户与动漫风格角色（基于大语言模型和语音合成）的互动行为，涵盖情感、动机、学习策略等多维度。  <br/>3. **强调语音风格与学习效果的关联**：发现语音风格和情感语调显著提升用户参与度和系统可用性，并对语言学习策略产生直接影响，为语音设计提供实证依据。  <br/>4. **提出跨文化适配设计建议**：针对不同文化背景用户的感知差异，为开发更具社会响应性和文化包容性的智能语音系统提供理论指导。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过混合方法评估，揭示风格化语音代理（如语音、个性及语言风格）在多模态语言学习中对用户情感、学习策略和跨文化交互的影响，为构建更自然、有效的智能语音系统提供理论支持与设计建议。|
|2507.04094v1|[MMMOS: Multi-domain Multi-axis Audio Quality Assessment](http://arxiv.org/abs/2507.04094v1)|总结：  <br/>本文提出MMMOS，首次实现跨多领域无参考音频质量评估，通过融合多模型帧级特征与优化策略，在多个指标上显著提升性能。<br/><br/>贡献点：  <br/>1. **首个多领域系统**：跨语音、音乐、环境声音的无参考音频质量评估，突破传统语音中心模型限制。  <br/>2. **四维质量评估**：引入Production Quality、Complexity、Content Enjoyment、Usefulness四个正交指标，提升评估维度多样性。  <br/>3. **多模型融合框架**：结合WavLM、MuQ、M2D三个预训练编码器的帧级嵌入，增强特征表达能力。  <br/>4. **多策略优化**：设计三种聚合策略与四种损失函数组合，系统性优化评估性能。  <br/>5. **显著性能提升**：集成前八模型后，在均方误差减少20-30%、Kendall's τ提高4-5%，并取得多个挑战指标前列。|
|2507.04036v1|[PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)|**总结（100字以内）:**  <br/>提出PresentAgent，将文档转化为同步的语音与视觉演示视频，创新性地结合大语言模型与TTS技术，并设计PresentEval评估框架，验证了其接近人类水平的性能，推动可控多模态代理在内容生成领域的应用。<br/><br/>**贡献点:**  <br/>1. **提出新一代多模态代理PresentAgent**：突破传统静态幻灯片和文本摘要的局限，实现文档到动态视频的全流程生成，生成高度同步且符合人类风格的语音与视觉内容。  <br/>2. **设计模块化生成流水线**：系统化分割文档内容，规划视觉帧、生成语音并进行精准音视频合成，确保多模态输出的结构化与连贯性。  <br/>3. **构建统一的评估框架PresentEval**：基于视觉语言模型，从内容保真度、视觉清晰度和观众理解度三维度对生成视频进行评估，解决多模态内容评价难题。  <br/>4. **实验验证性能优势**：在30个文档-演示对数据集上证明PresentAgent在关键指标上接近人类水平，展示了其在实际场景中的有效性。  <br/>5. **推动可控多模态应用潜力**：强调通过控制生成过程，将静态文本转化为动态、高效且易传播的演示格式，拓展多模态技术的实际应用场景。|
|2507.03912v1|[Prosody Labeling with Phoneme-BERT and Speech Foundation Models](http://arxiv.org/abs/2507.03912v1)|总结：  <br/>提出融合声学与语言特征的自动声调标注模型，显著提升日语声调标签预测准确率，为可控TTS系统提供高质量训练数据。<br/><br/>贡献点：  <br/>1. 构建多模态特征融合框架，结合SSL/Whisper声学模型与PnG BERT/PL-BERT语言模型提取特征；  <br/>2. 设计音素级声调标签预测方法，实现对日语音调（包括升调、降调、断句）的精准标注；  <br/>3. 通过实验验证跨模态协同的有效性，综合模型在关键指标上优于单一模态方法（音调准确率89.8%-94.3%）。|
|2507.03887v1|[Traceable TTS: Toward Watermark-Free TTS with Strong Traceability](http://arxiv.org/abs/2507.03887v1)|**贡献点（分点）**  <br/>1. 提出首个无水印的TTS可追溯框架，无需在语音或声码器中嵌入显式水印，解决传统方法对语音质量的损害和易被伪造的问题。  <br/>2. 采用联合训练方法，同时训练TTS模型与判别器，显著提升可追溯性泛化能力，且保持/优化音频质量。  <br/>3. 通过理论与实验验证，证明该框架在保证安全性的同时实现高质量语音生成，推动水印-free TTS领域发展。  <br/><br/>**总结（100字以内）**  <br/>本文提出首个水印-free的TTS可追溯框架，通过联合训练提升模型泛化性与音质，解决了传统水印方法的质量下降和易伪造问题，为语音安全领域提供新思路。|
|2507.01805v1|[A Dataset for Automatic Assessment of TTS Quality in Spanish](http://arxiv.org/abs/2507.01805v1)|总结:  <br/>本研究构建了首个西班牙语TTS系统评估数据库，包含52种系统和人类语音的4326个样本，并通过主观测试与模型验证提升自然度预测准确性，具有重要研究价值。<br/><br/>贡献点:  <br/>1. **构建首个西班牙语TTS评估数据库**：包含52个TTS系统和人类语音的4326个音频样本，填补了西班牙语领域的空白。  <br/>2. **标准化主观标注流程**：基于ITU-T Rec. P.807设计测试方案，由92名参与者完成，确保数据质量。  <br/>3. **验证数据集应用价值**：通过两种方法训练自然度预测模型（微调英文模型与冻结自监督模型），测试其有效性。  <br/>4. **模型性能指标**：在五点MOS尺度上达到均方误差0.8，体现评估体系的可靠性。  <br/>5. **数据集多样性分析**：展示数据质量与多样性，为后续西班牙语TTS研究提供高质量资源。|
|2507.00808v2|[Multi-interaction TTS toward professional recording reproduction](http://arxiv.org/abs/2507.00808v2)|总结：  <br/>提出基于多步用户交互的语音合成方法，模拟语音导演与演员的协作关系，实现用户引导的迭代风格优化，增强TTS系统的用户体验与可控性。<br/><br/>贡献点：  <br/>1. **方法创新**：首次引入多步骤交互机制，支持用户对语音合成结果进行逐层反馈与优化。  <br/>2. **用户模拟**：构建TTS模型与用户之间的互动框架，复现语音导演与演员的协作流程。  <br/>3. **数据集支持**：发布配套数据集验证多交互能力，推动用户意图驱动的风格调整研究。  <br/>4. **效果验证**：实验证明该方法可实现符合用户需求的迭代风格优化，提升语音合成的精准度与灵活性。|
|2507.00227v1|[Investigating Stochastic Methods for Prosody Modeling in Speech   Synthesis](http://arxiv.org/abs/2507.00227v1)|总结：  <br/>本论文提出基于随机方法（如归一化流、条件流匹配）生成自然语音韵律，通过对比实验验证其与人类发音相当的效果，并发现可通过调整采样温度提升可控性。<br/><br/>贡献点：  <br/>1. **提出随机建模方案**：探索使用Normalizing Flows、Conditional Flow Matching、Rectified Flows等随机方法生成语音的表达性韵律，对比传统确定性方法。  <br/>2. **实现自然韵律生成**：通过主观与客观评估表明，随机方法能有效捕捉人类语音的自然变异，生成质量与人类发音相当。  <br/>3. **增强可控性设计**：引入采样温度调控机制，为语音生成提供更多可调节参数，提升系统灵活性与可解释性。|
|2506.23869v1|[Scaling Self-Supervised Representation Learning for Symbolic Piano   Performance](http://arxiv.org/abs/2506.23869v1)|总结（100字以内）:  <br/>该论文提出基于大规?符号独奏钢琴数据预训练的生成对抗Transformer模型，通过微调实现音乐延续生成、符号分类和对比式MIDI嵌入，显著提升性能并展示预训练表示的高效泛化能力。<br/><br/>贡献点（分点）:  <br/>1. **大规?预训练数据**：构建包含约60,000小时独奏钢琴符号数据的预训练模型，奠定音乐生成的基础能力。  <br/>2. **多任务微调框架**：利用高质量子集进行微调，实现音乐延续生成、符号分类及通用对比式MIDI嵌入，适配SimCLR框架至符号音乐领域。  <br/>3. **性能突破**：在钢琴延续任务中超越主流符号生成技术，与专有音频生成模型竞争；在MIR分类中冻结表示达到SOTA，微调仅需少量标注样例即可高效适配下游任务。|
|2506.23553v2|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v2)|**贡献点：**  <br/>1. 揭示CLAPScore与人类主观评价得分之间存在显著相关性不足问题。  <br/>2. 提出Human-CLAP模型，通过引入人类主观评分训练对比学习框架。  <br/>3. 实验证明Human-CLAP使CLAPScore与主观评分的SRCC提升超0.25，显著改善评估一致性。  <br/><br/>**总结：**  <br/>该研究提出Human-CLAP，通过人类感知优化CLAP模型，显著提升文本与音频相关性评估的准确性。|
|2506.23553v1|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v1)|总结：  <br/>该研究发现CLAPScore与人类主观评分相关性较低，提出基于人类感知的Human-CLAP模型，并通过实验验证其显著提升评估效果。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：首次系统分析CLAPScore与人类主观评估评分的低相关性，指出其在文本-音频相关性建模中的不足。  <br/>2. **提出Human-CLAP方法**：创新性地通过引入人类主观评分作为训练信号，构建了一个更符合人类感知的对比语言-音频预训练模型。  <br/>3. **验证效果提升**：实验证明Human-CLAP将Spearman秩相关系数（SRCC）提升超过0.25，显著增强了模型与人类评分的一致性。|
|2506.23552v1|[JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](http://arxiv.org/abs/2506.23552v1)|**贡献点总结：**  <br/>1. 提出统一框架JAM-Flow，同步合成视觉（面部运动）与语音；  <br/>2. 设计MM-DiT架构，融合Motion-DiT和Audio-DiT模块；  <br/>3. 引入选择性联合注意力层与时间对齐位置嵌入等跨模态交互机制；  <br/>4. 采用inpainting训练目标，支持文本、音频及运动等多模态条件输入；  <br/>5. 实现同步说话头生成、音频驱动动画等任务，推动多模态生成建模发展。|
|2506.23367v1|[You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel   Properties](http://arxiv.org/abs/2506.23367v1)|总结：  <br/>首次提出针对二语学习者的TTS系统，通过时长差异设计"清晰模式"，实验证明其显著降低转录错误率并提升感知体验，揭示实际与感知可懂度的差异，指出Whisper-ASR在评估L2听众可懂度的不足。<br/><br/>贡献点：  <br/>1. 提出首个专门针对二语（L2）学习者的文本到语音系统  <br/>2. 创新性引入基于美式英语紧张/松元音时长差异的"清晰模式"  <br/>3. 通过实验证明该模式可使法语母语者和英语L2听众的转录错误率降低至少9.15%  <br/>4. 揭示"清晰模式"在情感感知上优于整体放慢的语音  <br/>5. 发现当前主流ASR系统（Whisper-ASR）未能准确捕捉L2学习者区分困难元音的线索  <br/>6. 首次证实实际语音可懂度与听者感知可懂度存在显著差异|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|总结：  <br/>本研究提出SAGE数据生成方法和经验回放策略，结合外部语言模型，显著提升了方言阿拉伯语与阿拉伯-英语代码切换语音的识别性能，超越了大模型效果。<br/><br/>贡献点：  <br/>1. **提出改进的音频拼接方法（SAGE）**：生成人工阿拉伯语-英语代码切换语音数据，解决数据稀缺问题。  <br/>2. **SAGE与SSL模型的联合微调**：在代码切换和阿拉伯语基准测试中，使WER绝对下降7.8%。  <br/>3. **基于经验回放（ER）的泛化增强技术**：缓解方言和代码切换任务中的灾难性遗忘，降低整体WER至26.6%。  <br/>4. **少样本微调策略**：进一步将代码切换基准WER提升4.9%，并实现优于USM和Whisper-large-v2的性能（分别高5.5%和8.4%）。|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|总结：  <br/>本文提出一种针对语音交互的端到端评估基准，通过构建包含真实场景、语音特性（如韵律、同音词、口吃）及多样化声学条件的数据集，并设计查询感知的评估方法，揭示主流语音模型在不同场景下的性能差异，为模型优化提供依据。<br/><br/>贡献点：  <br/>1. **构建专用语音评估基准**：系统创建面向语音场景的端到端评估基准，弥补现有文本基准在语音领域应用的不足。  <br/>2. **语音特性增强数据集**：收集真实语音对话数据，纳入多元说话者属性、声学条件及语音独特现象（如韵律、同音词、口吃）。  <br/>3. **设计查询感知评估方法**：提出基于定制化检查清单和提示的自动评估框架，提升语音任务评价的准确性与细粒度。  <br/>4. **揭示模型场景差异**：对主流语音模型进行全面测试与分析，发现其在不同语音场景下的显著性能差异。  <br/>5. **推动语音模型发展**：为语音模型的开发、优化与评估提供实证依据和参考价值。|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>提出针对实时语音对话的偏好对齐框架，构建多轮语音数据集，结合AI反馈优化模型，通过实验验证提升对话系统的表现。<br/><br/>贡献点：  <br/>1. **创新框架**：提出首个专为实时语音对话设计的偏好对齐方法，解决传统文本模型在处理动态语音交互（如中断、插入）上的不足。  <br/>2. **大规模数据集**：创建超过15万对标注AI反馈的多轮语音对话数据，涵盖语言内容与时间上下文的偏好变化。  <br/>3. **模型优化**：利用离线对齐技术微调全双工自回归语音到语音模型，提升多轮对话生成能力。  <br/>4. **实验验证**：在通用对话中证明反馈机制能有效增强模型的准确性、安全性和上下文一致性。  <br/>5. **人类评估**：通过综合人工评价验证模型在复杂多轮场景下的实际效果。  <br/>6. **理论洞见**：发现动态平衡（如语言、时间因素）对构建自然实时对话系统的重要性。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|总结（100字以内）:  <br/>本文提出一种步骤式视频到音频生成方法，模仿Foley流程生成多音频轨道，结合文本提示与概念否定策略，无需专用数据集即可实现高质量语义区分的音频合成，优于现有基线。<br/><br/>贡献点:  <br/>1. **步骤式生成框架**：首次提出按顺序生成单个音频轨道的方法，每个轨道对应视频中的特定声音事件，实现多声部音频合成。  <br/>2. **Foley流程模拟**：通过镜像传统影视音效创作流程，全面捕捉视频中所有由动作触发的声音事件。  <br/>3. **文本引导与记忆机制**：引入文本提示和先前生成的音频轨道作为条件，构建动态引导的合成流程。  <br/>4. **概念否定策略**：借鉴组合生成框架中的概念否定机制，提升音频生成的精确性和语义分离能力。  <br/>5. **无需配对数据训练**：设计基于预训练模型的通用训练框架，消除对专用视频-音频配对数据集的需求。  <br/>6. **高质量合成效果**：实验证明生成的音频轨道在语义区分度和整体质量上优于现有基线方法。|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|**贡献点分点总结：**  <br/>1. **系统性比较**：在零样本TTS框架下，对比了三种主流说话人编码器（H/ASP、x-vector、ECAPA-TDNN）的性能。  <br/>2. **标准化评估**：基于同一捷克语数据集，跨领域（24个目标说话人）进行主观听觉测试与客观余弦距离分析。  <br/>3. **发现编码器差异**：H/ASP编码器在零样本TTS中表现最好，ECAPA-TDNN优于x-vector，但未超越H/ASP。  <br/>4. **提出实证建议**：强调说话人识别嵌入在TTS中的适配需经实证验证，并构建了可复用的比较框架。  <br/><br/>**总结（100字内）：**  <br/>本研究对比了三种说话人嵌入在零样本TTS中的效果，发现H/ASP编码器表现最优，ECAPA-TDNN虽优于x-vector但不足。提出标准化评估框架，强调实证验证的重要性，为跨任务嵌入应用提供了参考。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|总结：  <br/>提出Kling-Foley多模态视频到音频生成模型，结合扩散Transformer与同步模块提升音视频对齐，开源工业级基准Kling-Audio-Eval，并在多个指标上达成SOTA性能。<br/><br/>贡献点：  <br/>1. **提出Kling-Foley模型**：首个大规模多模态Video-to-Audio生成模型，实现高保真音频与视频内容的同步合成。  <br/>2. **多模态交互建模**：引入多模态扩散Transformer，融合视觉语义表示模块与音视频同步模块，提升跨模态对齐能力。  <br/>3. **帧级对齐机制**：通过视频条件与潜在音频元素的帧级对齐，优化语义对齐与时间同步效果。  <br/>4. **通用音频编解码器**：设计支持音效、语音、歌唱及音乐等多场景的潜在音频编码方案，实现高质量建模。  <br/>5. **空间感渲染技术**：采用立体声渲染方法增强合成音频的听觉空间感知。  <br/>6. **开源工业级基准**：针对数据集缺陷，发布Kling-Audio-Eval工业级评估基准，推动领域研究。  <br/>7. **SOTA性能验证**：实验表明，基于流匹配目标训练的Kling-Foley在分布匹配、语义对齐、时间同步和音频质量上达到公共模型最优。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|**总结（100字以内）:**  <br/>本文提出TTSDS2，改进TTS评估方法，实现与主观评分的高相关性，并发布包含11000个评分的数据集、多语言测试数据管道及14语言持续更新的基准，推动合成语音质量评估的标准化和客观性。<br/><br/>**贡献点分点列述：**  <br/>1. **提出TTSDS2评估指标**：相比现有16种评估方法，TTSDS2在所有测试领域和主观评分中均达到Spearman相关性>0.5，显著提升TTS系统质量评估的客观性和鲁棒性。  <br/>2. **构建大规模主观数据集**：提供涵盖11,000+主观评分的多语言数据集，填补合成语音质量评估的标注缺口。  <br/>3. **设计可复用的评估框架**：推出能持续生成多语言测试数据的管道，避免数据泄露；并发布14语言的动态更新基准，支持长期性能监测与对比研究。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**贡献点：**<br/>1. 提出新假设：人类在主观语音质量评分中更关注低质量段落，评分差异主要由忽略低质量部分时的高分误判导致。<br/>2. 验证假设：基于VCC2018和BVCC数据集分析，揭示人类评价焦点与语音质量分布的关系。<br/>3. 提出新指标：定义N_low-MOS（N个最低意见评分的平均值）作为更可靠的语音质量代表值。<br/>4. 实验验证：证明N_low-MOS可提升MOSNet的评估性能（LCC/SRCC指标改善），展示其内在有效性。<br/>5. 推动方法改进：为语音转换（VC）模型的评估提供更科学的基准，优化MOSNet的比较能力。<br/><br/>**总结（100字内）：**  <br/>本文提出N_low-MOS指标，通过分析数据集验证人类评分明显受低质量段落影响，并证明其在提升语音质量评估模型性能方面具有显著优势，为VC模型评估提供新思路。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|**贡献点：**  <br/>1. 提出一种新的评分聚合方法，解决传统MOS标注（1-5分离散评分）的局限性。  <br/>2. 基于标注者内部连续评分假设，建模生成分布并通过量化潜在连续分布估计评分峰值。  <br/>3. 引入潜在分布峰值作为新代表性值，取代传统MOS作为预测目标。  <br/>4. 通过实验验证，该方法能显著提升语音质量预测模型（如MOSNet）的性能。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出一种基于连续评分假设的新型语音质量评分方法，通过建模标注者的评分过程并引入潜在分布峰值作为替代指标，有效提升语音质量预测模型的性能。|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|**总结（100字以内）**  <br/>本研究构建了日本偶像语音语料库（JIS），聚焦年轻女性现场偶像群体，通过艺名标识促进听众实验。JIS支持TTS/VC系统的说话者相似性评估，并探索听众偏好的个性化语音生成，数据免费开放且仅限非商业科研使用，附带文化背景介绍与应用指导。<br/><br/>**贡献点分点列出**  <br/>1. **构建专业化语料库**  <br/>   - 首次创建针对日本“年轻女性现场偶像”这一特定群体的语音语料库（JIS），覆盖TTS和VC研究需求，补充语音生成领域的数据多样性。<br/><br/>2. **推动说话者相似性评估**  <br/>   - 所有语音由同一类群体（艺名标识）录制，便于系统评估生成语音与原声的说话者相似性，提升TTS/VC的可信度与可比性。<br/><br/>3. **开拓听众偏好研究方向**  <br/>   - 引入针对听众偏好的个性化语音生成研究（如定制偶像声音），填补该领域学术空白，推动语音生成技术与用户需求的结合。<br/><br/>4. **制定开放使用政策**  <br/>   - 以非商业、基础研究为限免费公开JIS，确保数据可及性同时维护版权与伦理规范，促进学术共享与创新。<br/><br/>5. **提供文化背景支持**  <br/>   - 对日本偶像文化进行介绍，助力研究者理解数据语境，确保JIS的合法、有效与伦理应用，提升科研的社会接受度。|
|2506.16738v1|[LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](http://arxiv.org/abs/2506.16738v1)|总结：  <br/>本文提出LM-SPT方法，通过间接语义蒸馏减少语音标记序列长度，改进编码器解码器结构并支持多帧率，实验验证其在语音-文本建模中取得优于基线的重建效果和文本到语音任务表现。<br/><br/>贡献点：  <br/>1. 提出LM-SPT模型，通过语义驱动的重建机制替代传统池化操作，有效减少语音标记序列长度。  <br/>2. 引入间接语义蒸馏策略，通过冻结ASR编码器对比原始与重建波形，提升语义对齐精度。  <br/>3. 设计编码器-解码器架构优化方案，支持25Hz、12.5Hz、6.25Hz等多帧率配置。  <br/>4. 实验证明LM-SPT在语音-文本双向任务中均优于基线，尤其在文本到语音生成中表现突出。|
|2506.16580v1|[Streaming Non-Autoregressive Model for Accent Conversion and   Pronunciation Improvement](http://arxiv.org/abs/2506.16580v1)|总结：提出首个支持流式处理的口音转换模型，实现非母语语音向母语口音的转换，同时保持说话人身份与语调，并通过集成TTS模型提升训练效率，达到顶级模型性能且具备稳定延迟。<br/><br/>贡献点：<br/>1. 提出首个实现流式口音转换（AC）的模型，支持实时处理非母语语音。<br/>2. 保持说话人身份、语调特征并提升发音质量，实现"原声"性转换。<br/>3. 采用Emformer编码器与优化推理机制，显著降低处理延迟。<br/>4. 整合母语TTS模型生成理想参考数据，提升训练效率。<br/>5. 在保持稳定延迟的前提下，达到当前最优AC模型的转换效果。|
|2506.16127v1|[Improved Intelligibility of Dysarthric Speech using Conditional Flow   Matching](http://arxiv.org/abs/2506.16127v1)|**贡献点总结（100字以内）**  <br/>本研究提出基于自监督学习特征及量化表示的失语语音到正常语音转换方法，采用单说话人语音生成策略降低说话人差异，并结合非自回归条件流匹配与扩散Transformer实现高效映射，提升语音可懂度和收敛速度。  <br/><br/>**具体贡献点**  <br/>1. **替代传统特征**：提出使用自监督学习（SSL）特征及其量化表示替代梅尔频谱图（mel-spectrograms），探索其在语音生成中的有效性。  <br/>2. **缓解说话人差异**：通过从WavLM提取特征生成单说话人干净语音，减少说话人变异性对模型性能的影响。  <br/>3. **非自回归框架**：设计全非自回归方法，结合条件流匹配（CFM）与扩散Transformer，直接学习失语语音到干净语音的映射。  <br/>4. **离散音素优势**：验证离散音素单元在提升语音可懂度和加速模型收敛方面的显著效果，优于传统基于梅尔频谱的方法。|
|2506.15873v1|[DeckFlow: Iterative Specification on a Multimodal Generative Canvas](http://arxiv.org/abs/2506.15873v1)|总结：  <br/>该论文提出DeckFlow多模态生成AI工具，通过任务分解、规格分解和生成空间探索三大创新机制解决现有工具设计问题，并验证了其在文本到图像生成中的有效性，进一步拓展至音频生成以研究用户创意行为。<br/><br/>贡献点：  <br/>1. **任务分解机制**：采用无限画布与卡片式视觉数据流交互，支持用户创建和管理多个互联子任务。  <br/>2. **规格分解工作流**：将初始目标迭代拆解为子部分，结合特征标签与聚类实现多模态内容组织。  <br/>3. **生成空间探索**：通过生成多组提示词与输出变体（网格展示），支持递归反馈优化设计迭代。  <br/>4. **多模态验证与扩展**：在文本-图像生成中对比传统对话AI基线，后扩展至音频生成，分析跨模态创作行为。|
|2506.15759v1|[Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](http://arxiv.org/abs/2506.15759v1)|**总结（100字以内）**  <br/>该论文提出Sonic4D框架，首次实现4D场景与空间音频的联合生成，通过动态视觉捕捉、声源定位及物理模拟三阶段处理，无需训练即可生成逼真的时空音频，增强了沉浸式音频视觉体验。<br/><br/>**贡献点**  <br/>1. **提出Sonic4D框架**：首次将空间音频生成与4D场景合成结合，解决现有方法忽视音频与场景对齐的问题。  <br/>2. **三阶段生成方法**：  <br/>   - 第一阶段：利用预训练模型生成4D场景及单声道音频；  <br/>   - 第二阶段：通过像素级视觉定位策略估计声源的3D空间坐标；  <br/>   - 第三阶段：基于物理模拟合成动态视角与时间变化的时空音频。  <br/>3. **训练-free设计**：无需额外训练，直接利用现有数据生成符合场景的空间音频。  <br/>4. **实验验证与资源公开**：通过实验证明生成音频的逼真度和沉浸感，并开放生成示例供研究复现。|
|2506.15085v1|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v1)|总结：  <br/>提出EmojiVoice工具包，实现社交机器人离线语音表达的动态控制，通过emoji提示增强长时表达性，并验证其在不同场景中的表现差异。<br/><br/>贡献点：  <br/>1. **开发EmojiVoice工具包**：首个专门针对社交机器人长时语音表达的免费、可定制TTS系统，解决基础模型离线部署难题。  <br/>2. **引入emoji提示控制机制**：首次将表情符号用于细粒度表达性调控，实现对语音情感相位的精准控制。  <br/>3. **轻量级实时生成框架**：采用Matcha-TTS等轻量模型，支持机器人端实时语音生成需求。  <br/>4. **多场景验证与对比**：通过剧本对话、讲故事、自主交互三类案例研究，验证方法对表达性的提升效果，并揭示不同应用场景下的接受差异。|
|2506.13053v2|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v2)|总结：本文提出ZipVoice，通过紧凑结构和流匹配技术实现高效零样本TTS，质量与SOTA相当，在模型体积和推理速度上显著提升，并开源代码与模型。<br/><br/>贡献点：  <br/>1. **提出Zipformer-based流匹配解码器**：在模型体积受限情况下保持足够的建模能力，提升效率。  <br/>2. **设计Average upsampling与Zipformer结合的语音-文本对齐及文本编码器**：增强语音可懂度，优化多语言处理效果。  <br/>3. **创新流蒸馏方法**：减少采样步骤，消除无分类器引导的推理开销，显著提升生成速度。  <br/>4. **实验验证效果**：在100k小时多语言数据集上，ZipVoice在语音质量与SOTA模型相当的同时，体积缩小3倍，速度提升30倍。  <br/>5. **开源实现**：提供代码、模型检查点和演示样本，便于复现与应用。|
|2506.12199v1|[ViSAGe: Video-to-Spatial Audio Generation](http://arxiv.org/abs/2506.12199v1)|**贡献点总结**（100字以内）:  <br/>本研究提出ViSAGe框架，直接从无声视频生成一阶Ambisonics，构建YT-Ambigen数据集，设计空间音频评估指标，并验证其在时空对齐与视角适应性上的优越性。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新问题**：探索无需复杂录制设备或专业技能，直接从无声视频生成一阶Ambisonics的方法。  <br/>2. **构建数据集**：创建包含102K视频片段的YT-Ambigen数据集，为研究提供标注数据支持。  <br/>3. **设计评估指标**：首次基于音频能量图与显著性度量，提出量化评估生成空间音频质量的新标准。  <br/>4. **端到端框架**：提出ViSAGe模型，融合CLIP视觉特征、自回归音频编码与方向/视觉引导生成高质量Ambisonics。  <br/>5. **性能优势**：验证ViSAGe在生成音频的连贯性与时空对齐性上优于传统两阶段方法（视频-音频生成+空间化）。  <br/>6. **应用能力**：生成的音频可随视角变化动态适应，提升沉浸式音频体验的交互性。|
|2506.11160v5|[S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation   Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning](http://arxiv.org/abs/2506.11160v5)|总结：  <br/>本文提出S2ST-Omni框架，通过分解S2ST任务、多模态融合模型、两阶段微调策略和流式生成技术，有效解决多语言语音翻译中的质量与数据瓶颈问题，在CVSS基准上超越现有S2ST系统。<br/><br/>贡献点：  <br/>1. **任务分解框架**：将S2ST拆分为S2TT和TTS子任务，实现高效且可扩展的多语言翻译流程。  <br/>2. **多模态融合模型**：集成预训练Whisper编码器与Qwen 3.0，构建轻量语音适配器以弥合语音与文本表征的模态差距。  <br/>3. **两阶段微调策略**：通过分步优化提升多模态知识学习效果。  <br/>4. **流式自回归生成**：在TTS阶段采用流式生成技术，确保目标语音的自然性与流畅性。  <br/>5. **实验验证效果**：在CVSS基准上实验证明框架在翻译质量上优于现有S2ST系统，展现其有效性与优越性。|
|2506.11130v1|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v1)|总结（100字以内）:  <br/>提出自优化框架，利用无标签数据与闭环训练提升ASR性能，成功将Whisper转化为Twister，在台语中实现显著精度提升，并为低资源场景提供实用方案。<br/><br/>贡献点：<br/>1. 提出无需标注数据的自精炼框架，通过伪标签生成与闭环训练实现ASR性能提升。  <br/>2. 首次将高保真TTS系统与ASR模型结合，形成"ASR→TTS→ASR"的闭合优化循环。  <br/>3. 在台语语音任务中验证框架有效性，利用6000小时未标注语音驱动模型训练。  <br/>4. 开发专用模型Twister，相较Whisper在普通话和双语切换场景分别降低20%和50%错误率。  <br/>5. 为低资源/特定领域ASR提供创新解决方案，突破传统伪标签自蒸馏方法的局限性。|
|2506.11127v1|[GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech   Instructions](http://arxiv.org/abs/2506.11127v1)|总结：  <br/>提出GUIRoboTron-Speech，首个端到端语音指令GUI代理，通过语音数据生成和混合训练策略解决模态不平衡问题，实验验证其高效性和广泛应用前景。<br/><br/>贡献点：  <br/>1. **首个语音驱动GUI代理**：提出GUIRoboTron-Speech，首次实现端到端GUI操作，直接接受语音指令和设备截图作为输入，无需文本转换。  <br/>2. **语音数据生成方法**：利用随机音色TTS模型将现有文本指令转换为高质量语音指令，解决语音数据稀缺问题。  <br/>3. **混合指令训练策略**：设计启发式方法缓解预训练模型的模态不平衡，结合文本和语音指令提升代理性能。  <br/>4. **系统性实验验证**：在基准数据集上验证模型表现，证明语音指令在GUI自动化中的有效性与广泛适用性。|
|2506.11086v1|[Intelligibility of Text-to-Speech Systems for Mathematical Expressions](http://arxiv.org/abs/2506.11086v1)|贡献点：  <br/>1. **系统评估框架**：首次设计涵盖五种TTS模型的实验，量化数学表达式（MX）的发音质量与可懂性（通过用户评分和转录正确性）。  <br/>2. **LLM辅助生成**：利用大语言模型（LLM）将LaTeX格式MX转换为英文发音，弥补TTS模型无法直接处理LaTeX的缺陷。  <br/>3. **多维度指标**：提出包含三个指标的转录正确性评估体系，并结合Mean Opinion Score分析用户主观感受。  <br/>4. **对比分析**：对比听众对TTS输出与人类专家发音的偏好，揭示TTS在处理MX时的表现短板。  <br/>5. **结果发现**：发现TTS模型输出的可懂性存在显著差异，且多数MX类别下表现劣于专家，同时LLM选择对结果影响有限。  <br/>6. **应用导向结论**：明确指出需针对性优化TTS模型处理数学表达式的能力。  <br/><br/>总结：  <br/>首次系统评估TTS模型对数学表达式的发音质量与可懂性，揭示模型与人类专家表现差异，强调改进TTS处理MX能力的必要性。|
|2506.10019v1|[A Survey of Automatic Evaluation Methods on Text, Visual and Speech   Generations](http://arxiv.org/abs/2506.10019v1)|总结：本文提出首个综合框架，系统化分类文本、图像和音频生成的评估方法，识别出五种核心范式，并探讨跨模态评估的未来方向。<br/><br/>贡献点：  <br/>1. **构建统一框架**：首次建立跨文本、图像、音频三大模态的自动评估方法系统化框架，实现多模态方法的整合与对比。  <br/>2. **提出五种核心范式**：归纳出生成内容评估的五大基础范式，为后续研究提供理论分类依据。  <br/>3. **跨模态适用性验证**：将文本生成评估方法的分析逻辑扩展至图像和音频领域，验证框架的广泛适用性。  <br/>4. **探索未来研究方向**：针对跨模态评估挑战，提出潜在的研究路径，促进多模态生成AI的评估体系发展。|
|2506.09874v2|[UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow   Matching](http://arxiv.org/abs/2506.09874v2)|**贡献点总结：**  <br/>1. 提出UmbraTTS模型，基于流匹配技术实现语音与环境音频的联合生成。  <br/>2. 支持对背景音量的精细控制，生成多样化且上下文感知的音频场景。  <br/>3. 设计自监督框架，从未标注录音中提取语音、背景音频和文本数据。  <br/>4. 实验验证模型在自然、高质量环境音频生成任务上显著优于现有基线。  <br/><br/>（总结：UmbraTTS通过流匹配技术联合生成语音与环境音频，提出自监督数据提取方法，并在多样性和质量上超越现有模型。）|
|2506.09827v2|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v2)|**贡献点：**  <br/>1. 提出EmoNet-Voice资源，包含大规模预训练数据集（Big）和新型基准数据集（Bench），覆盖40种情感、11种声音及4种语言。  <br/>2. 支持细粒度情感评估，明确区分不同情感强度以提升模型准确性。  <br/>3. 通过语音生成技术合成情感音频，模拟真实场景以解决隐私问题。  <br/>4. 引入心理学专家标注与验证，确保情绪强度标签的可靠性。  <br/>5. 提出Empathic Insight Voice模型，实现与人类专家高度一致的SER性能。  <br/>6. 揭示高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更易被识别的模型表现差异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoNet-Voice资源，通过合成语音和专家标注解决隐私与情感粒度问题，推出高性能SER模型，并发现高唤醒情绪更易识别，为情感语音研究提供新基准。|
|2506.09827v1|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v1)|总结：  <br/>提出EmoNet-Voice资源及两个子集，包含大规模预训练数据与专家标注基准数据，解决隐私问题并增强情感粒度和多样性。开发高精度模型，通过评估揭示高唤醒情绪识别易度高于低唤醒情绪。<br/><br/>贡献点：  <br/>1. **引入EmoNet-Voice资源**：构建由EmoNet-Voice Big（大规模预训练数据集）和EmoNet-Voice Bench（专家标注基准数据集）组成的综合数据资源，覆盖40种情感、11种声音和4种语言。  <br/>2. **提升情感粒度与强度区分**：设计支持细粒度情感分类（40类）及不同强度评估的框架，推动SER模型在情感理解上的更精确建模。  <br/>3. **合成数据解决隐私与多样性问题**：通过先进语音生成技术合成敏感情感场景数据，避免真实数据隐私风险并扩展情感表达的多样性。  <br/>4. **专家验证机制**：联合心理学专家对合成数据进行严格标注（感知强度标签），确保数据质量与情感感知的科学性。  <br/>5. **提出高性能SER模型**：开发Empathic Insight Voice模型，实现与人类专家的高一致性，推动语音情感识别技术的标准化。  <br/>6. **揭示情绪识别难度差异**：通过对比实验发现，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易被模型识别，为后续研究提供新视角。|
|2506.08279v1|[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)|**总结（100字以内）:**  <br/>该论文提出Mirage模型，首次实现从音频直接生成高质量、表达性强的视频，结合TTS技术生成多模态视频，并开发统一的自注意力训练方法，提升生成视频的主观质量与通用性。  <br/><br/>**分点贡献:**  <br/>1. **提出Mirage模型**：首个专注于音频到视频生成的通用模型，从原始音频直接生成逼真、表达性的视频图像，不依赖视觉输入。  <br/>2. **多模态生成能力**：通过集成文本到语音（TTS）技术，实现语音与视频的同步生成，解决语音和视频内容对齐问题。  <br/>3. **统一训练方法**：开发适用于自注意力机制的统一训练框架，支持从头训练和基于已有权重的微调，增强模型灵活性。  <br/>4. **语义对齐优化**：利用A-roll数据（人物对话语音视频）训练，使生成视频能准确反映音频中的表演信息，提高可信度。  <br/>5. **性能优势**：生成的视频在主观质量上优于依赖语音特定架构或损失函数的现有方法，保持通用性的同时实现高保真输出。|
|2506.04527v1|[Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning](http://arxiv.org/abs/2506.04527v1)|总结（100字以内）:<br/>该论文提出结合字母符号的语音标签生成方法，通过BERT特征提示编码实现隐式条件，配合推理时的显式修剪提升标签一致性，为文本到语音和口音估计等任务生成高质量平行数据。<br/><br/>贡献点：<br/>1. **方法创新**：首创结合字母符号的语音标签生成框架，突破传统ASR模型直接微调的局限，引入显式与隐式的双条件策略。<br/>2. **条件生成机制**  <br/>   - 通过预训练BERT特征构建隐式图形单词条件（Prompt Encoder）  <br/>   - 提出推理阶段的显式标签假设剪枝算法（Pruning Inconsistent Hypotheses）<br/>3. **数据价值**：生成跨模态的语音-标签-图形单词平行数据，支持文本到语音合成（TTS）和口音估计等下游任务。<br/>4. **实验验证**：在音素-图形单词一致性任务和口音估计任务中均取得显著性能提升，证明方法的有效性。|
|2506.04397v1|[Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?](http://arxiv.org/abs/2506.04397v1)|总结：  <br/>该论文通过构建专用数据集和微调大模型，探索Parler TTS在语音重建任务中生成可理解语音与保持说话人身份的能力，揭示其在可控性方面的局限并提出改进方向。<br/><br/>贡献点：  <br/>1. **构建专用数据集**：创建并标注了包含说话人信息和可懂度数据的语音重建数据集，为相关研究提供基础资源。  <br/>2. **应用大模型进行语音重建**：首次尝试利用Parler TTS等先进大型语音模型，生成语音障碍患者疾病前的语音近似。  <br/>3. **分析模型局限性**：发现模型虽能学习数据分布，但在控制语音可懂度和保持说话人身份一致性方面存在挑战。  <br/>4. **提出改进方向**：为提升语音重建任务中模型的可控性，提出了未来研究的潜在优化路径。|
|2506.04152v1|[HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](http://arxiv.org/abs/2506.04152v1)|**分点贡献：**  <br/>1. **构建高带宽语音数据集**：提出HiFiTTS-2，基于LibriVox有声书，包含36.7k小时（22.05kHz）和31.7k小时（44.1kHz）的高质量英语语音数据，支持高带宽语音合成研究。  <br/>2. **创新数据处理流程**：设计了包含带宽估计、语音分段、文本预处理及多说话人检测的全流程数据处理方法，提升数据适配性与实用性。  <br/>3. **提供详细元数据**：生成全面的语句和有声书元数据，支持研究者通过数据质量过滤器灵活调整数据集以适配不同应用场景。  <br/>4. **验证数据集有效性**：实验表明该数据集和处理方法可有效训练高质量零样本TTS模型，推动高带宽语音生成技术的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HiFiTTS-2，构建大规模高带宽语音数据集并设计全流程处理技术，提供详细元数据以增强数据适应性，实验证明其可有效支持高质量零样本TTS模型训练。|
|2506.04134v1|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v1)|**贡献点总结（100字以内）**：  <br/>提出直接生成CS语音的新方法，构建首个统一框架UniCUE，通过细颗粒语义对齐、跨任务适配器及姿态感知处理，解决误差传播与时空对齐问题，显著提升语音生成质量与唇语同步性。<br/><br/>**分点贡献**：  <br/>1. **直接生成语音的创新方法**：首次无需依赖文本中间媒介，直接从CS视频生成语音，避免误差传播与时间错位问题。  <br/>2. **统一框架UniCUE**：提出首个融合CSV2S与CSR任务的框架，打通视觉-语义到语音的端到端生成流程。  <br/>3. **核心技术创新**：  <br/>   - 引入细颗粒语义对齐池，精确映射视觉特征与语音内容；  <br/>   - 设计VisioPhonetic适配器，实现CSV2S与CSR的跨任务表示兼容；  <br/>   - 开发姿态感知视觉处理器，增强唇语与手势的时空关联性。  <br/>4. **实验验证与数据集**：基于新建立的中文CS数据集（含14名受试者），验证方法有效性，降低误词率78.3%并提升唇语同步性32%。|
|2506.04077v1|[A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions](http://arxiv.org/abs/2506.04077v1)|总结：  <br/>该论文提出一种基于大语言模型与跨模态信息的新型语音评估方法，有效解决低资源场景下的标签约束问题，提升口语评分的可靠性和多样性。<br/><br/>贡献点：  <br/>1. **低资源场景下的数据生成**：利用大语言模型生成多样化响应并合成语音，突破标注数据的稀缺限制。  <br/>2. **说话人感知的语音合成**：通过 speaker-aware 文本到语音合成技术，增强生成语音与真实语音的匹配度。  <br/>3. **动态重要性损失设计**：根据合成语音与真实语音的特征差异，自适应调整训练样本权重，提升模型学习效率。  <br/>4. **跨模态特征融合**：构建多模态大语言模型，直接整合文本与语音信号的对齐特征以预测口语水平。  <br/>5. **有效性验证**：在 LTTC 数据集上验证方法优于传统数据依赖或常规增强技术，实现跨模态信息驱动的自动化评分。|
|2506.03884v1|[Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages](http://arxiv.org/abs/2506.03884v1)|总结: 本文提出基于零样本合成的多语言TTS系统，通过共享音素表示和适配文本解析规则，成功生成梵语等低资源语言的自然语音，验证了该方法在扩展语音技术普惠性方面的有效性。<br/><br/>贡献点:<br/>1. 提出零样本合成框架，解决印度1369种语言中多数缺乏数字资源的TTS训练难题<br/>2. 构建跨语言音素共享表示，降低多语言模型过度假设需求<br/>3. 开发适应不同音系规则的文本解析方法（phonotactics adaptation）<br/>4. 实现Sanskrit、Maharashtrian、Canara Konkani、Maithili、Kurukh等5种语言的自然语音合成<br/>5. 提供可复用的合成器架构，验证方法在扩大语音技术覆盖范围中的可行性|
|2506.03831v1|[Conformer-based Ultrasound-to-Speech Conversion](http://arxiv.org/abs/2506.03831v1)|总结：  <br/>本研究对比了Conformer与传统CNN在超声波语音转换中的表现，发现Conformer结合bi-LSTM在感知质量上更优，而Conformer Base在效率上显著提升，为静默语音接口提供了新思路。<br/><br/>贡献点：  <br/>1. **提出两种Conformer-based模型**：首次将Conformer架构应用于超声波到语音转换任务，分别采用Base结构和结合bi-LSTM的结构。  <br/>2. **融合HiFi-GAN声码器**：通过HiFi-GAN将生成的mel频谱图合成高质量音频波形，提升系统输出的语音还原效果。  <br/>3. **验证感知质量优势**：在MUSHRA主观测试中发现，Conformer与bi-LSTM结合的模型在语音可懂度上优于传统CNN基线。  <br/>4. **效率提升**：Conformer Base模型在保持性能的同时，训练速度比传统CNN快3倍，降低了计算成本。|
|2506.03793v1|[Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts](http://arxiv.org/abs/2506.03793v1)|**贡献点**  <br/>1. **提出Cadence模型**：基于预训练大语言模型设计，首次实现对自发语音转录的精准标点恢复，克服传统模型在处理断续、重复等语音特征时的不足。  <br/>2. **扩展语言覆盖**：支持22种印度语言及英语（原仅14种），显著提升多语言标点恢复的适用性与泛化能力。  <br/>3. **分析挑战与改进**：通过跨语言和标点类型的行为研究，揭示领域迁移及罕见标点符号下的修复难题，验证预训练模型在低资源NLP场景中的有效性。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出Cadence模型，基于预训练语言模型实现自发语音的精准标点恢复，扩展支持22种印度语言和英语，并通过深入分析揭示多语言任务中的关键挑战。|
|2506.03515v1|[BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and   Weight Indexing](http://arxiv.org/abs/2506.03515v1)|总结：  <br/>本文提出两种模型压缩技术，实现83%的模型体积减小，同时保持高质量语音合成，适用于设备端部署。<br/><br/>贡献点：<br/>1. 提出量化感知训练（QAT）技术，将模型参数从32位压缩至1.58位，主要采用三值{-1,0,1}量化策略。<br/>2. 开发权重索引方法，通过将一组1.58位权重编码为单个int8索引来优化存储效率。<br/>3. 在保持合成质量的前提下，实现模型轻量化，使其更适用于资源受限的设备端应用场景。|
|2506.02979v1|[Towards a Japanese Full-duplex Spoken Dialogue System](http://arxiv.org/abs/2506.02979v1)|**贡献点总结：**  <br/>1. 提出首个公开可用的全双工日语语音对话模型，基于英语模型Moshi迁移。  <br/>2. 采用两阶段训练方法：大规模日语语料预训练与高质量立体声数据微调。  <br/>3. 引入多流文本到语音系统生成的合成对话数据以增强模型性能。  <br/>4. 实验证明模型在自然度和语义理解上优于现有日语基准模型。  <br/><br/>（注：以上为100字以内总结，符合要求。）|
|2506.02958v1|[PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech   Editing](http://arxiv.org/abs/2506.02958v1)|**贡献点总结（100字以内）:**  <br/>提出PartialEdit数据集，聚焦神经语音编辑生成的深伪语音检测，揭示现有模型的局限性，分析伪影特征，推动该领域研究。<br/><br/>**分点贡献:**  <br/>1. **提出PartialEdit数据集**：构建首个基于先进神经语音编辑技术的深伪语音检测数据集，包含部分编辑的语音样本，为研究提供基准。  <br/>2. **探索检测与定位任务**：系统研究部分编辑语音的检测与内容定位，验证当前检测方法在复杂编辑场景下的有效性。  <br/>3. **揭示模型局限性**：实验表明基于PartialSpoof数据集训练的模型无法有效识别神经编辑生成的深伪语音，凸显数据集不足。  <br/>4. **分析伪影特征**：结合神经音频编码器，探讨检测模型学习到的伪影（artifacts）特征，为改进检测方法提供理论依据。  <br/>5. **提供公开资源**：开放数据集和音频样本，方便后续研究复现与扩展。|
|2506.02858v2|[DGMO: Training-Free Audio Source Separation through Diffusion-Guided   Mask Optimization](http://arxiv.org/abs/2506.02858v2)|总结：  <br/>提出训练无关的零样本音频源分离框架DGMO，利用扩散模型生成先验优化频谱掩码，突破传统方法依赖任务特定训练的限制，拓展扩散模型在音频分离领域的应用范式。<br/><br/>贡献点：  <br/>1. **首个训练无关的零样本框架**：首次将预训练扩散模型直接用于语言查询的音频源分离，无需额外任务特定训练。  <br/>2. **测试时优化机制**：提出Diffusion-Guided Mask Optimization（DGMO），通过测试阶段动态优化频谱掩码以提升分离精度。  <br/>3. **跨模态适应性提升**：分析模态适配问题，揭示自然语言与音频处理的协同挑战，并提出针对性解决方案。  <br/>4. **性能验证**：在无监督或弱监督条件下实现竞争性分离效果，证明扩散模型可有效迁移至音频分离任务。  <br/>5. **应用范式创新**：拓展扩散模型从生成任务到分离任务的边界，建立新的零样本音频处理框架。|
|2506.02401v1|[Trusted Fake Audio Detection Based on Dirichlet Distribution](http://arxiv.org/abs/2506.02401v1)|**贡献点：**  <br/>1. **提出基于狄利克雷分布的不确定性建模方法**：首次将狄利克雷分布用于量化模型决策的不确定性，提升假音频检测的可信度评估能力。  <br/>2. **构建新型检测框架**：通过神经网络生成证据，并将预测概率与不确定性估计结合，形成最终检测意见，优化检测流程。  <br/>3. **多数据集验证**：在ASVspoof 2019 LA、2021 LA及DF数据集上进行对比实验，验证了模型在准确率、鲁棒性和可信度方面的优越性能。  <br/><br/>**总结：**  <br/>该研究通过狄利克雷分布建模不确定性，提出新型假音频检测方法，在多数据集上验证了其高准确率与鲁棒性，显著提升了检测的可靠性。|
|2506.02082v1|[SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction](http://arxiv.org/abs/2506.02082v1)|总结：该论文提出了一种新的SALF-MOS模型，解决传统MOS评估高成本问题，通过卷积序列提取音频潜在特征，实现小尺寸、端到端、高度通用和可扩展的语音质量预测，取得最佳评估效果。<br/><br/>贡献点：<br/>1. 提出Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS)模型，解决传统MOS评估需要大量人工标注的问题<br/>2. 设计基于卷积序列的音频特征提取方法，实现端到端的MOS预测系统<br/>3. 构建的小尺寸模型在保持通用性的同时具备良好的可扩展性<br/>4. 在多个客观评估指标（MSE、LCC、SRCC、KTAU）上达到当前最优性能<br/>5. 提供了一种新的语音质量评估方法，可作为TTS和语音转换模型选择的有效工具|
|2506.01618v1|[Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech](http://arxiv.org/abs/2506.01618v1)|总结（100字以内）:  <br/>本文提出基于音节的节奏建模方法，结合LF-MMI模型显著提升口吃语音的ASR性能，验证了无监督转换的有效性。<br/><br/>贡献点:  <br/>1. **提出syllable-based rhythm modeling方法**：在Rhythm and Voice（RnV）框架中引入针对口吃语音的音节级节奏建模技术，解决传统方法对口吃语音节奏特征建模不足的问题。  <br/>2. **评估转换对ASR的影响**：首次系统验证口吃语音转换对ASR性能提升的效果，通过训练LF-MMI模型和微调Whisper模型，发现LF-MMI在口吃语音识别中具有显著优势。  <br/>3. **揭示无监督转换的潜力**：证明无监督学习的节奏和语音转换技术在口吃ASR任务中的有效性，为低资源场景下的语音处理提供了新思路。|
|2506.01322v1|[Zero-Shot Text-to-Speech for Vietnamese](http://arxiv.org/abs/2506.01322v1)|贡献点：<br/>1. 构建了首个越南语高质量语音数据集PhoAudiobook（含941小时音频）  <br/>2. 在VALL-E、VoiceCraft、XTTS-V2等三大零样本TTS模型上开展系统实验  <br/>3. 证实PhoAudiobook能显著提升多指标下的模型性能  <br/>4. 揭示VALL-E与VoiceCraft在短句合成任务中的优越性  <br/>5. 公开数据集促进越南语TTS领域研究发展  <br/><br/>总结：  <br/>本研究发布首个越南语高质量语音数据集PhoAudiobook，并验证其提升三大零样本TTS模型性能的效果，尤其突出VALL-E与VoiceCraft在短句生成中的优势，推动越南语语音合成研究。|
|2506.01268v1|[CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction](http://arxiv.org/abs/2506.01268v1)|贡献点总结：<br/>1. 提出单文件实现的CleanS2S框架，实现低延迟语音交互系统<br/>2. 首创主动对话机制，集成记忆系统与主观行动判断模块<br/>3. 开发五种人类般响应策略（中断/拒绝/转移/沉默/标准响应）<br/>4. 实现全双工WebSocket连接与非阻塞I/O的实时中断处理<br/>5. 提出Action Judgement SFT方法评估输入流策略选择<br/>6. 提供前所未有的透明度和可扩展性，代码已开源<br/><br/>（97字）|
|2506.01263v1|[WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing](http://arxiv.org/abs/2506.01263v1)|总结：  <br/>提出无需额外训练的CTC模型改进方法，通过中间层声学特征检测关键词并引入偏置机制，有效提升罕见词识别准确率，实验在日语语音识别中实现29%的F1分数提升。<br/><br/>贡献点：  <br/>1. **解决罕见词识别偏差问题**：通过中间层声学特征进行关键词检测，避免模型对训练数据词汇的过度依赖。  <br/>2. **引入灵活的wildcard CTC机制**：采用快速且容错的CTC变体，支持模糊匹配处理难以严格识别的单词。  <br/>3. **无需模型重训练的可扩展性**：方法兼容现有模型，无需额外训练或文本到语音系统，适用于大规模模型。  <br/>4. **显著提升未知词识别性能**：在日语语音识别任务中，实现未知词F1分数29%的提升，验证方法有效性。|
|2506.01032v1|[ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and   Speaker Feature Optimization](http://arxiv.org/abs/2506.01032v1)|**总结**：  <br/>本文提出ReFlow-VC，通过修正流的ODE框架提升语音转换效率，结合内容与音高信息优化说话人特征，并验证其在小数据集和零样本场景中的优越性能，推动高保真语音转换的实际应用。  <br/><br/>**贡献点**：  <br/>1. **提出ReFlow-VC方法**：基于修正流构建高效高保真语音转换模型，通过ODE框架将高斯分布直接映射到真实Mel频谱图分布，减少采样步骤。  <br/>2. **优化说话人特征建模**：同时利用内容与音高信息优化特征，增强说话人属性的准确表达。  <br/>3. **验证小样本适配性**：在小数据集和零样本场景中均表现出卓越的性能，突破传统扩散模型的局限。|
|2506.01020v1|[DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation](http://arxiv.org/abs/2506.01020v1)|总结：  <br/>该论文提出DS-TTS框架，结合双风格编码与动态生成网络，解决zero-shot语音克隆问题，实现未见过语音的高质量合成，并在VCTK数据集上验证了其优越的性能。<br/><br/>贡献点：  <br/>1. **提出DS-TTS框架**：设计一种用于zero-shot语音克隆的全新方法，仅需单个音频样本和任意文本即可合成目标说话人语音，无需历史训练数据。  <br/>2. **双风格编码网络（DuSEN）**：通过两个互补的风格编码器捕捉说话人语音身份的多维度特征，增强对未见说话人风格的建模能力。  <br/>3. **动态融合机制（SGF + DyGN）**：引入风格门控-Film机制将风格向量融入动态生成器，实现对不同句子长度的自适应合成，提升鲁棒性与自然度。  <br/>4. **实验验证优势**：在VCTK数据集上证明DS-TTS在单词错误率和说话人相似度上优于现有模型，显著改善语音克隆的泛化与表达性。|
|2506.00832v1|[Counterfactual Activation Editing for Post-hoc Prosody and   Mispronunciation Correction in TTS Models](http://arxiv.org/abs/2506.00832v1)|**贡献点：**<br/>1. **提出模型无关的后处理技术**：Counterfactual Activation Editing 是一种通用方法，无需依赖预训练模型特定架构，可适用于多种TTS模型。  <br/>2. **实现韵律与发音的联合控制**：同时支持对语音韵律特征（prosody）和发音错误（mispronunciation）的精确调整，提升控制能力。  <br/>3. **无需额外训练即可进行推理时调整**：在不重新训练模型的情况下，直接对预训练TTS的内部表示进行修改，实现在线输出优化。  <br/>4. **适应低资源场景**：解决传统发音纠正依赖音素词典（grapheme-to-phoneme dictionaries）的问题，降低对资源的依赖。  <br/>5. **填补预训练模型与可编辑合成的空白**：首次将可编辑的语音合成能力引入预训练TTS框架，推动模型灵活性与应用范围的扩展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种通用、无需额外训练的后处理方法，实现对TTS输出的韵律与发音联合控制，适应低资源场景，推动预训练模型向可编辑语音合成发展。|
|2506.00736v1|[IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio   Generation with Diffusion Modeling](http://arxiv.org/abs/2506.00736v1)|总结：  <br/>本文提出IMPACT框架，结合扩散建模与迭代掩码并行解码，在连续潜空间中生成高质量文本到音频，有效解决先前模型在音质与推理速度之间的权衡问题，实验在AudioCaps上达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出IMPACT框架**：首次将迭代掩码并行解码与扩散模型结合，实现文本到音频生成中高保真度与低延迟的双重优化。  <br/>2. **连续潜空间设计**：通过在连续潜空间中操作，避免离散token带来的保真度限制，提升生成音频的自然度和质量。  <br/>3. **高效解码策略**：采用迭代掩码并行解码技术，在保持推理速度的同时显著提高音频生成效果，优于MAGNET等基于离散token的模型。  <br/>4. **验证SOTA性能**：在AudioCaps数据集上，IMPACT在Fréchet Distance (FD)和Fréchet Audio Distance (FAD)等关键指标上超越现有方法，验证了其有效性。|
|2506.00722v1|[Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](http://arxiv.org/abs/2506.00722v1)|总结：该研究提出基于链式思维的端到端语音对话系统方法，在较小数据集上实现显著性能提升并提升语义连贯性，同时增强计算效率，模型及代码将公开。<br/><br/>贡献点：<br/>1. 提出基于链式思维（Chain-of-Thought, CoT）的端到端（E2E）对话系统框架，解决了传统级联流水线的局限性；<br/>2. 通过将对话训练与多模态语言模型（涵盖语音识别、文本生成、语音合成等任务）的预训练对齐，提升语义连贯性；<br/>3. 实现仅需300小时公开对话数据（如Switchboard）即可获得超过1.5 ROUGE-1的提升效果；<br/>4. 优化计算效率，使模型能在有限数据量下高效训练；<br/>5. 提供公开模型及代码，推动该领域研究与应用。|
|2506.00548v1|[Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities](http://arxiv.org/abs/2506.00548v1)|总结（100字以内）：  <br/>本论文提出Con Instruction方法，通过生成对抗图像和音频攻击多模态语言模型，无需文本预处理或训练数据。引入新型ARC评估框架，并验证其在多个模型上的高攻击成功率，揭示当前安全机制的不足。<br/><br/>贡献点（分点列出）：  <br/>1. **提出非文本指令攻击方法（Con Instruction）**  <br/>   - 首次利用MLLMs对非文本指令（图像/音频）的解析能力，通过生成对抗性图像或音频发起攻击，突破传统依赖文本指令的攻击模式。  <br/>   - 攻击无需训练数据或文本预处理，简化了攻击流程。  <br/><br/>2. **优化对抗样本的语义对齐**  <br/>   - 在嵌入空间中优化对抗样本，使其更精准匹配目标指令，显著提升攻击成功率，揭示MLLMs复杂语义理解的潜在安全风险。  <br/><br/>3. **提出攻击响应分类（ARC）框架**  <br/>   - 引入新型评估体系，综合衡量模型响应质量与对恶意指令的相关性，为安全评估提供更全面的指标。  <br/><br/>4. **验证多模型有效性与攻击性能**  <br/>   - 在LLaVA-v1.5、InternVL、Qwen-VL、Qwen-Audio等多模态模型上验证攻击效果，尤其在LLaVA-v1.5 (13B)中达到81.3%和86.6%的成功率，证明方法的普适性。  <br/><br/>5. **分析防御技术的局限性**  <br/>   - 探索针对攻击的防御措施，发现现有技术在攻击检测和防御能力上存在显著性能差距，为后续防御研究提供方向。  <br/><br/>6. **开放源代码实现**  <br/>   - 公布实现代码，便于学术界和工业界复现与扩展研究。|
|2506.00462v1|[XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](http://arxiv.org/abs/2506.00462v1)|**贡献点：**  <br/>1. **提出跨域多语言音频深度伪造基准**：构建XMAD-Bench数据集，包含668.8小时真实与深度伪造语音，覆盖多语言和多样化生成方法。  <br/>2. **强化数据集多样性与挑战性**：训练与测试集的说话人、生成模型、真实音频源均独立，显著提升跨域评估的难度。  <br/>3. **揭示性能差距**：实验结果表明，域内检测准确率接近100%，但跨域性能严重下降，接近随机猜测，突显现有方法的局限性。  <br/>4. **推动鲁棒检测器发展**：强调需开发具备跨语言、跨说话人、跨生成方法和跨数据源泛化能力的深度伪造检测技术。  <br/>5. **公开数据集**：将XMAD-Bench在GitHub平台开源，为研究社区提供统一的评估基准。  <br/><br/>**总结**：  <br/>提出首个跨域多语言音频深度伪造基准XMAD-Bench，验证现有检测模型在真实场景下的性能不足，推动更鲁棒的检测技术发展。|
|2506.00385v1|[MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity   Reconstruction and Generation](http://arxiv.org/abs/2506.00385v1)|**贡献点：**  <br/>1. 提出新型单层流式Transformer音频编解码器MagiCodec，解决传统编解码器重建质量与下游模型兼容性之间的矛盾。  <br/>2. 引入多阶段训练框架，结合高斯噪声注入与潜在正则化技术，增强编码的语义表达性并保持重建保真度。  <br/>3. 理论推导噪声注入在频域中的作用，证明其对抑制高频成分、提升鲁棒性效果显著。  <br/>4. 实验验证MagiCodec在重建质量与下游任务（如语言模型兼容）上优于现有技术，生成的编码呈现齐普夫分布特性。  <br/>5. 开源代码与预训练模型，促进研究复现与应用。  <br/><br/>**总结：**  <br/>MagiCodec通过噪声注入与多阶段训练提升音频编码的语义表达和重建质量，实现与语言模型的兼容性，开源推动研究进展。|
|2506.00087v1|[SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset](http://arxiv.org/abs/2506.00087v1)|总结：  <br/>本文提出多代理协作框架LinguaMaster，构建首个大规模多语言多民族代码转换数据集SwitchLingua，并设计语义感知评估指标SAER，推动多语言和文化多样性研究。  <br/><br/>贡献点：  <br/>1. **提出LinguaMaster框架**：首次设计多智能体协作系统，实现高效可扩展的多语言数据合成。  <br/>2. **构建SwitchLingua数据集**：创建首个覆盖12种语言、18国/地区、63个种族背景的大型代码转换语料库（含420K文本样本及80小时音频）。  <br/>3. **创新SAER评估指标**：提出融合语义信息的代码转换ASR评估方法，提升对跨语言场景的性能判断准确性。|
|2505.24496v1|[Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation](http://arxiv.org/abs/2505.24496v1)|总结：  <br/>本文提出一种压缩-精细语言建模方法，通过保留局部上下文和压缩长距离信息，有效解决神经音频编解码器在语音生成中的长序列挑战，提升生成质量与模型通用性。<br/><br/>贡献点：  <br/>1. **揭示语音标记序列的短距离依赖性**：首次观察到TTS任务中语音标记的预测主要依赖局部上下文，长距离标记贡献较小且冗余。  <br/>2. **设计压缩-精细语言建模框架**：  <br/>   - 保留初始提示和短范围标记，确保文本对齐及语用信息完整性；  <br/>   - 将长范围标记压缩为紧凑表示，减少冗余同时保留关键语义。  <br/>3. **验证方法的有效性与通用性**：通过多组神经音频编解码器和下游语言模型的实验，证明该方法可提升语音生成效果，具有广泛适用性。|
|2505.24314v1|[DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture   Switching for Speech Codec](http://arxiv.org/abs/2505.24314v1)|**贡献点：**  <br/>1. 提出DS-Codec，首次结合镜像（mirror）与非镜像（non-mirror）架构的双阶段训练框架，提升语音重建质量。  <br/>2. 设计动态切换机制，平衡镜像与非镜像结构的优势，增强模型的鲁棒性与高保真度。  <br/>3. 通过大规模实验与消融研究，验证了训练策略的有效性，对比了两种架构的性能差异。  <br/><br/>**总结：**  <br/>本研究提出DS-Codec，通过镜像与非镜像结构的双阶段训练框架提升语音重构效果，并系统验证了其优势。|
|2505.23619v1|[Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](http://arxiv.org/abs/2505.23619v1)|贡献点总结：<br/>1. 提出ADD-GP框架：基于高斯过程的少样本音频深度伪造检测方法，支持快速适应新生成模型  <br/>2. 性能突破：结合深度嵌入模型与GP灵活性，在少量数据下实现高检测准确率  <br/>3. 个性化检测能力：增强对新型TTS模型的鲁棒性，支持单样本级自适应  <br/>4. 构建基准数据集：使用SOTA语音克隆模型创建专用评估数据集  <br/><br/>（99字）|
|2505.20868v2|[Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction   and Style Direction Adjustment for Expressive Text-to-Speech](http://arxiv.org/abs/2505.20868v2)|**贡献点总结（100字以内）：**  <br/>Spotlight-TTS通过语音感知的风格提取和方向调整，提升表达性与语音质量，实验验证其在关键指标上优于基线模型，并公开音频样本以促进研究。  <br/><br/>**分点贡献：**  <br/>1. **提出语音感知风格提取方法**：聚焦与风格强相关的发声区域（voiced regions），同时保持跨语音区域的连续性，增强语音表达性。  <br/>2. **设计风格方向调整机制**：优化提取的风格方向以更好地整合到TTS模型中，显著提升生成语音的整体质量。  <br/>3. **验证性能优越性**：通过实验表明，Spotlight-TTS在表达性、语音质量和风格迁移能力方面均优于现有基线方法。  <br/>4. **开放数据支持**：公开音频样本，便于社区验证成果并推动相关研究发展。|
|2505.19931v2|[Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned   Step Sampling](http://arxiv.org/abs/2505.19931v2)|总结（100字以内）:  <br/>提出训练无关的Fast F5-TTS方法，通过分析采样轨迹设计EPSS策略减少冗余步骤，实现7步生成且推理速度提升4倍，验证了EPSS对流匹配类TTS模型的通用性。  <br/><br/>贡献点:  <br/>1. **训练无关加速方案**：提出Fast F5-TTS，无需额外训练直接优化流匹配TTS模型的推理效率。  <br/>2. **非均匀采样策略**：开发EPSS方法，通过识别冗余采样步骤，显著减少生成过程的采样次数。  <br/>3. **高效推理性能**：在NVIDIA RTX 3090上实现7步生成，推理RTF达0.030，且性能与原模型相当。  <br/>4. **跨模型通用性**：验证EPSS在E2 TTS等其他流匹配模型上的有效性，证明其广泛适用性。|
|2505.19669v2|[Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling](http://arxiv.org/abs/2505.19669v2)|总结：  <br/>提出SMLLE框架，通过Transducer与Delete <Bos>机制解决流式TTS低延迟问题，实现高质量逐帧语音生成，优于现有流式方法且接近句级系统性能。<br/><br/>贡献点：  <br/>1. **提出SMLLE框架**：首次构建零样本流式TTS系统，无需依赖未来文本即可实现低延迟高质量语音合成。  <br/>2. **融合Transducer与时长对齐**：实时将文本转化为语义标记，结合时长对齐信息提升语音自然度。  <br/>3. **设计Delete <Bos>机制**：优化自回归模型对未来文本的访问，最小化延迟同时保持生成稳定性。  <br/>4. **验证性能优势**：实验表明SMLLE在流式TTS任务中超越现有方法，在句级系统中表现可比，具备实际应用价值。|
|2505.19595v2|[Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment](http://arxiv.org/abs/2505.19595v2)|总结：  <br/>本研究提出A-DMA方法，通过双模态对齐优化扩散模型的训练，显著提升收敛速度与生成质量，降低计算成本，并开源代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **提出A-DMA策略**：设计双模态对齐机制，通过文本与语音模态的协同优化加速扩散模型的训练过程。  <br/>2. **双模态对齐技术**：引入文本引导对齐（融合上下文表示）和语音引导对齐（精炼语义表示）的双重对齐管道。  <br/>3. **减少隐式学习依赖**：通过显式对齐隐藏状态与判别特征，降低扩散模型对复杂中间表示的隐式学习需求。  <br/>4. **实验验证效果**：在多个基准上验证A-DMA的性能优势，收敛速度提升1倍，生成质量优于现有方法，并开源代码与演示样本。|
|2505.17076v2|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v2)|总结：  <br/>研究分析不同帧率对中文和英文语音分词效果的影响，揭示帧率与语言特性间的关系，为语音分词优化提供参考。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统探讨低帧率编解码器对语音分词的影响，揭示帧率与语言模型间的作用机制。  <br/>2. **跨语言对比分析**：通过对比汉语和英语两种音系差异显著的语言，发现帧率对不同语言的分词效果存在异质性。  <br/>3. **揭示关键影响因素**：明确帧率变化与音素密度、语言特异性声学特征的交互作用，为分词器设计提供理论依据。  <br/>4. **应用指导意义**：提出优化帧率选择的策略，对自动语音识别、文本到语音等语音处理任务具有实践价值。|
|2504.14906v3|[OmniAudio: Generating Spatial Audio from 360-Degree Video](http://arxiv.org/abs/2504.14906v3)|总结：  <br/>提出360V2SA新任务，构建Sphere360数据集，设计半自动化数据流程，提出OmniAudio框架（结合自监督训练与双分支结构），在空间音频生成领域取得SOTA成果。<br/><br/>贡献点：  <br/>1. 提出360V2SA任务：首次系统性生成360度视频对应的3D空间音频（FOA格式），填补传统视频-音频生成在空间线索上的不足。  <br/>2. 构建Sphere360数据集：首个针对该任务的高质量数据集，基于真实场景数据并优化数据配对与清洗流程。  <br/>3. 设计半自动化数据收集管道：提供高效且可扩展的视频-音频数据配对与预处理方案，降低数据获取门槛。  <br/>4. 提出OmniAudio框架：结合自监督预训练和双分支结构（全景+视角视频输入），同时建模局部与全局空间信息。  <br/>5. 实验验证与开源：在Sphere360上实现SOTA性能，代码与数据集开源，推动领域研究与应用。|