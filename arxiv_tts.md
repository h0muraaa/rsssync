|Source|Title|Summary|
|---|---|---|
|2510.21671v1|[A Data-Centric Approach to Multilingual E-Commerce Product Search: Case   Study on Query-Category and Query-Item Relevance](http://arxiv.org/abs/2510.21671v1)|总结：  <br/>该论文提出一种数据为中心的框架，通过翻译增强、语义负采样和自验证过滤解决多语言电商搜索中的数据不平衡与标签噪声问题，显著提升模型性能并适用于实际场景。<br/><br/>贡献点：  <br/>1. **提出架构无关的数据为中心框架**：针对多语言电商搜索的挑战，设计无需修改模型结构的训练数据优化方案。  <br/>2. **三重数据增强策略**：  <br/>   - 基于翻译的增强，合成缺失语言的训练样本；  <br/>   - 语义负采样生成硬负样本，缓解类别不平衡；  <br/>   - 自验证过滤机制，移除误标数据。  <br/>3. **实验证明有效性**：在CIKM AnalytiCup 2025数据集上超越LLM基线，实现高F1分数和竞赛竞争力。  <br/>4. **强调数据工程价值**：表明系统性数据优化可替代复杂模型修改，提升实际部署可行性与模型鲁棒性。|
|2510.21603v1|[Doc-Researcher: A Unified System for Multimodal Document Parsing and   Deep Research](http://arxiv.org/abs/2510.21603v1)||
|2510.21447v1|[PhysWorld: From Real Videos to World Models of Deformable Objects via   Physics-Aware Demonstration Synthesis](http://arxiv.org/abs/2510.21447v1)||
|2510.21432v1|[ArtiLatent: Realistic Articulated 3D Object Generation via Structured   Latents](http://arxiv.org/abs/2510.21432v1)||
|2510.21401v1|[FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract   Security](http://arxiv.org/abs/2510.21401v1)||
|2510.21396v1|[Depth-Supervised Fusion Network for Seamless-Free Image Stitching](http://arxiv.org/abs/2510.21396v1)||
|2510.21324v1|[CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray   Interpretation](http://arxiv.org/abs/2510.21324v1)||
|2510.20812v1|[Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via   Speculation](http://arxiv.org/abs/2510.20812v1)|总结：提出一种无需训练的视觉语言模型框架，通过结合轻量模型与大模型实现高效推理，引入共识机制提升准确性与效率，在多个高密度视觉问答任务中表现优异。<br/><br/>贡献点：<br/>1. **训练无关框架**：基于speculative decoding理念，首次提出无需额外训练的VLM推理方法，降低计算成本。<br/>2. **多模型协同推理**：创新性地融合轻量级draft专家（生成多样定位候选）与强模型verdict（综合路径生成答案）。<br/>3. **共识专家选择**：引入高共识路径筛选机制，仅传递高一致性推理结果，提升准确率并优化资源利用。<br/>4. **高密度场景有效性**：在InfographicVQA、ChartMuseum等密集图文问答基准上验证效果，展现对复杂视觉信息的处理优势。|
|2510.20766v1|[DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion](http://arxiv.org/abs/2510.20766v1)||
|2510.18355v1|[KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory   Call Center for Bengali Farmers](http://arxiv.org/abs/2510.18355v1)|**贡献点**  <br/>1. **构建多语言语音农业咨询系统**：开发KrishokBondhu平台，支持Bengali语音交互，实现远程农民的专家级农业指导。  <br/>2. **RAG框架应用**：结合检索增强生成技术，通过权威农业资料库提供精准、上下文相关的实时回答。  <br/>3. **数字化内容处理**：采用OCR与文档解析技术，将农业手册、手册和NGO资料结构化并存储于向量数据库。  <br/>4. **性能提升验证**：在试点测试中，系统对农业问题的响应质量达72.7%，综合评分比基准提升44.7%，显著增强上下文丰富性和完整性。  <br/>5. **可扩展性示范**：证明AI驱动的农业咨询生态系统的可行性，为低资源地区的语音技术应用提供参考。  <br/><br/>**总结（100字以内）**  <br/>KrishokBondhu通过RAG框架与语音交互技术，为孟加拉国农民提供实时农业指导，克服语言和地域障碍，提升咨询质量并验证AI在农业领域的可行性。|
|2510.16497v1|[Edge-Based Speech Transcription and Synthesis for Kinyarwanda and   Swahili Languages](http://arxiv.org/abs/2510.16497v1)|**贡献点分点总结：**  <br/>1. 提出边缘-云并行框架，提升卢旺达语和斯瓦希里语的语音转录与合成效率。  <br/>2. 填补东非低技术基础设施地区缺乏高效语言处理工具的空白。  <br/>3. 集成Whisper与SpeechT5预训练模型，实现双向语音-文本转换与翻译。  <br/>4. 设计级联机制，动态分配推理任务以降低延迟和资源占用。  <br/>5. 边缘设备上实现SpeechT5和Whisper模型的内存压缩（分别9.5%和14%）。  <br/>6. 实验证明框架在低配置边缘设备（1.7GHz CPU、1MB/s网络）下可高效处理文本（270字符<1分钟）。  <br/>7. 基于肯尼亚实测数据，验证框架的实际应用价值与准确性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出面向东非低资源语言的边缘-云并行框架，通过模型压缩与任务分配降低计算需求，实验表明其在低配置设备上可实现高效语音转录与合成，具备实际应用潜力。|
|2510.13747v1|[InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn   Dialogue](http://arxiv.org/abs/2510.13747v1)|**总结（100字以内）**:  <br/>本文提出InteractiveOmni，一种轻量级多模态大语言模型，整合视觉、音频、语言和语音生成模块，通过多阶段训练策略提升跨模态交互能力，并构建专用评估基准，验证其在长期记忆和多任务理解上的优越性能。  <br/><br/>**贡献点分点列出**:  <br/>1. **统一架构设计**：首次实现音视频多模态与语言模型的整合，构建支持多轮交互的统一模型框架。  <br/>2. **多阶段训练策略**：提出预训练（多模态理解）+后训练（语音对话与音频-视觉交互）的分阶段训练方法。  <br/>3. **多轮对话数据集优化**：精心构建专项多轮训练数据集，增强复杂交互场景下的对话能力。  <br/>4. **专用评估基准**：创建多模态多轮记忆与多轮语音交互两大基准，系统化评估模型性能。  <br/>5. **参数效率与性能优势**：InteractiveOmni-4B在通用基准上媲美7B级模型，性能保留率高达97%，显著提升轻量化模型的竞争力。  <br/>6. **开源与可扩展性**：提供开放源代码，为下一代智能交互系统构建可访问的基础框架。  <br/>7. **跨模态能力验证**：通过实验验证模型在多模态理解与语音生成任务上达到SOTA，尤其在长期记忆表现突出。|
|2510.13293v1|[Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models](http://arxiv.org/abs/2510.13293v1)|总结：  <br/>本论文提出了一种自适应 Classifier-Free Guidance 方法，解决自回归 TTS 模型中风格与语义冲突问题，提升情感表达的同时保持音频质量与可懂性。<br/><br/>贡献点：  <br/>1. **提出自适应 CFG 框架**：针对自回归 TTS 模型中风格提示与文本语义冲突的问题，设计可动态调整的 CFG 方案。  <br/>2. **引入检测机制**：利用大规模语言模型或自然语言推理模型量化风格与内容的不匹配程度。  <br/>3. **理论分析支撑**：系统研究 CFG 对情感表达的影响，为方案设计提供理论依据。  <br/>4. **有效性能验证**：实验证明方案在提升情感表达度的同时，保持音频质量和语音可懂性。|
|2510.09592v1|[Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in   Spoken Language Models](http://arxiv.org/abs/2510.09592v1)|总结：本论文提出MPS框架，通过双脑架构实现实时推理与语音生成的协同，显著提升实时语音模型的推理效率与准确性，填补了高质量推理与实时交互之间的技术鸿沟。<br/><br/>贡献点：<br/>1. 首创脑启发的"双脑"机制，首次实现SLMs边思考边说话的实时推理框架<br/>2. 设计"Formulation Brain"与"Articulation Brain"分工协作架构，消除模式切换带来的推理中断<br/>3. 在零延迟配置下达到92.8%数学推理准确率和82.5对话得分，超越现有实时推理方法<br/>4. 实现推理性能与预计算CoT模型相当的突破性成果，同时降低延迟60%以上<br/>5. 为实时语音交互系统提供新的理论范式，显著提升自然语言处理与语音生成的协同效率|
|2510.02327v1|[KAME: Tandem Architecture for Enhancing Knowledge in Real-Time   Speech-to-Speech Conversational AI](http://arxiv.org/abs/2510.02327v1)|总结：<br/>提出混合架构结合实时S2S模型与LLM，在保持低延迟的同时显著提升响应知识性与正确性，接近级联系统性能。<br/><br/>贡献点：<br/>1. 提出混合架构：融合实时S2S生成器与文本LLM的知识表示，解决纯实时模型知识不足与级联系统延迟过高的矛盾<br/>2. 实时知识注入机制：通过并行处理将LLM的文本响应实时注入S2S生成过程，增强语音输出的语义理解能力<br/>3. 低延迟保持：在提升知识性的同时维持与纯S2S模型相当的实时性，避免级联系统的交互阻断问题<br/>4. 新型评估基准：开发基于MT-Bench的语音合成测试集，系统性验证多轮对话场景下的性能提升<br/>5. 性能验证：实验表明该方法在响应正确性上超越基线S2S模型，接近级联系统效果，证明架构有效性|
|2510.02322v1|[SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for   Voice-Native Multimodal CT Analysis](http://arxiv.org/abs/2510.02322v1)|总结：本研究提出Speech-RATE数据集与SpeechCT-CLIP模型，通过知识蒸馏实现语音与CT图像的语义对齐，显著提升医疗AI在零样本分类和语音检索任务中的性能，为临床语音驱动诊断工具提供新思路。<br/><br/>贡献点：<br/>1. 构建首个大规模口语放射学报告数据集Speech-RATE，填补医学领域语音-影像配对数据空白<br/>2. 提出SpeechCT-CLIP对比学习框架，实现语音与3D CT体积的跨模态对齐<br/>3. 首次将CLIP模型的知识蒸馏技术迁移至语音领域，建立文本语义到语音表示的迁移机制<br/>4. 通过对比实验验证语音建模可达到与文本建模相当的性能（零样本分类F1提升88%）<br/>5. 展示语音驱动的多模态预训练模型在无需文本输入的临床检索任务中的有效性<br/>6. 为发展基于语音的临床诊断辅助工具提供理论依据和实践方案|
|2510.00485v1|[PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation](http://arxiv.org/abs/2510.00485v1)|总结：本文提出PodEval框架，构建真实播客数据集，设计多模态评估策略与方法，验证生成系统的有效性，并开源以促进研究应用。<br/><br/>贡献点：<br/>1. 构建首个涵盖多主题的真实播客数据集，作为人类级创意质量的基准参考<br/>2. 提出多模态评估策略，将复杂任务分解为文本、语音、音频三维度<br/>3. 设计兼顾内容（客观指标）与格式（主观测试）的评估体系<br/>4. 整合开源/闭源/人工生成系统进行实验验证，实现跨系统对比<br/>5. 开发完整评估流程，包含量化指标与人工评测双重验证<br/>6. 发布开源框架促进语音生成领域研究与应用（GitHub地址）|
|2510.00050v1|[Object-AVEdit: An Object-level Audio-Visual Editing Model](http://arxiv.org/abs/2510.00050v1)|总结：  <br/>本文提出Object-AVEdit框架，通过反转再生范式实现对象级音视频编辑，解决了跨模态对象控制与结构信息保持问题，实验验证了其在音视频语义对齐和生成性能上的优势。<br/><br/>贡献点：  <br/>1. **提出Object-AVEdit框架**：首次基于反转再生范式实现对象级音视频编辑（添加/替换/删除），同时保持源实例结构信息。  <br/>2. **开发词-声音对象对齐音频生成模型**：构建音频生成模型，弥合音频与视频生成模型在对象级控制能力上的差距。  <br/>3. **设计反转再生整体优化算法**：通过联合优化反转与再生过程，提升结构信息保持与编辑效果的连贯性。  <br/>4. **实验验证先进性**：在音视频对象级编辑任务中实现细粒度语义对齐，展示模型性能优势。  <br/>5. **提供项目页面资源**：公开实验结果与工具，便于复现与进一步研究。|
|2509.26604v1|[Video Object Segmentation-Aware Audio Generation](http://arxiv.org/abs/2509.26604v1)|总结：  <br/>本研究提出视频物体分割感知音频生成新任务，开发SAGANet模型，结合分割图、视频与文本实现精细音频控制，并构建Segmented Music Solos数据集，推动高保真Foley合成技术发展。<br/><br/>贡献点：  <br/>1. **提出新任务**：定义视频物体分割感知的音频生成任务，通过物体级分割图实现对音频生成的显式控制。  <br/>2. **创新模型SAGANet**：开发基于视觉分割掩码、视频及文本线索的新型可控音频生成模型，支持细粒度与视觉定位的音频控制。  <br/>3. **构建基准数据集**：发布Segmented Music Solos数据集，包含带分割信息的音乐表演视频，为相关研究提供资源。  <br/>4. **性能提升**：在分割感知Foley领域显著超越现有SOTA方法，建立新的高保真可控音频生成标准。  <br/>5. **开源实现**：公开代码、音频样本及数据集，促进技术复现与进一步研究。|
|2509.22808v1|[ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech   Detection](http://arxiv.org/abs/2509.22808v1)|总结：  <br/>本文提出首个多方言阿拉伯语欺骗语音数据集，并通过多种评估方法（嵌入+分类器、传统机器学习、RawNet2、MOS与WER）验证FishSpeech在阿拉伯语语音克隆中的优越性，同时指出单一模型数据集可能限制泛化性。<br/><br/>贡献点：  <br/>1. **首个多方言阿拉伯语欺骗语音数据集**：填补了阿拉伯语方言在语音合成检测领域研究的空白。  <br/>2. **多维度评估框架**：结合现代嵌入方法、传统机器学习、RawNet2架构，以及MOS与WER指标，全面分析合成语音的挑战性。  <br/>3. **FishSpeech性能验证**：发现FishSpeech在阿拉伯语音克隆任务中生成的合成语音质量最高，更具真实性。  <br/>4. **泛化性讨论**：指出依赖单一TTS模型构建数据集的局限性，为未来研究提供方向。|
|2509.22728v1|[Prompt-aware classifier free guidance for diffusion models](http://arxiv.org/abs/2509.22728v1)|总结：  <br/>提出prompt-aware指导尺度选择框架，通过合成数据集和轻量预测器提升扩散模型在语音生成中的质量与对齐效果。<br/><br/>贡献点：  <br/>1. **提出Prompt-Aware框架**：动态根据提示内容选择最优指导尺度，解决固定尺度无法适配复杂提示的问题。  <br/>2. **构建合成数据集**：生成多尺度样本并结合可靠评估指标，为尺度选择提供训练与验证数据。  <br/>3. **设计轻量预测器**：基于语义嵌入和语言复杂性预测多指标质量曲线，优化尺度选择决策。  <br/>4. **引入效用函数与正则化**：通过数学建模结合正则化项，实现对指导尺度的高效、准确选择。  <br/>5. **实验证明有效性**：在MSCOCO和AudioCaps数据集上验证，提升生成质量、对齐度和感知偏好，且无需额外训练。|
|2509.22727v1|[DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with   Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation](http://arxiv.org/abs/2509.22727v1)|总结（100字以内）:  <br/>DiaMoE-TTS提出基于IPA的统一语音合成框架，解决方言数据稀缺和音素歧义问题，通过MoE和参数高效适配技术实现跨方言迁移，并验证了在零样本场景下的有效性。<br/><br/>贡献点:<br/>1. **提出统一IPA框架**：标准化方言语音的音素表示，解决因拼写不统一和音素歧义导致的建模困难。<br/>2. **方言感知的MoE建模**：在F5-TTS基础上引入方言意识的专家混合模型，有效捕捉不同方言的音系差异。<br/>3. **参数高效适配方法**：结合LoRA与Conditioning Adapters，实现对新方言的快速参数迁移，降低训练成本。<br/>4. **开放数据驱动设计**：无需依赖大规模或专有数据，仅需少量方言数据即可生成高质量语音，提升可扩展性。<br/>5. **零样本跨领域验证**：在未见过的方言及专业领域（如京剧）实现高表现，证明框架的泛化能力与适应性。|
|2509.20378v1|[Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with   Dynamic Word-Level Modulation](http://arxiv.org/abs/2509.20378v1)|**总结（100字以内）**：  <br/>提出Emo-FiLM框架，实现LLM-TTS的细粒度情感建模，通过FiLM层直接调控文本嵌入，构建FEDD数据集支持评估，实验验证其在全局和细粒度情感任务上的优越性，提升语音合成的表达能力。<br/><br/>**贡献点分点**：  <br/>1. **提出Emo-FiLM框架**：首次将细粒度情感建模引入基于大语言模型（LLM）的语音合成，通过将情感特征（emotion2vec）与词对齐并映射至文本嵌入，实现基于词级别的动态情感控制。  <br/>2. **构建FEDD数据集**：设计并发布细粒度情感动态数据集，包含详细的情感过渡注释，为评估和研究情感语音合成的动态特性提供基准。  <br/>3. **验证有效性与通用性**：实验表明Emo-FiLM在全局情感表达和细粒度情感动态任务上均优于现有方法，证明其在情感语音合成中的高效性和广泛适用性。|
|2509.18060v1|[TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for   Ü-Tsang, Amdo and Kham Speech Dataset Generation](http://arxiv.org/abs/2509.18060v1)|总结：本研究针对藏语低资源问题，提出TMD-TTS框架，通过方言融合模块和DSDR-Net实现多方言语音合成，在表达性和S2SDC任务中均优于基线方法。<br/><br/>贡献点：<br/>1. 提出首个统一的藏语多方言TTS框架TMD-TTS，解决方言间平行语料不足的挑战<br/>2. 开发方言融合模块与DSDR-Net动态路由网络，精准捕捉方言间的声学-语言差异<br/>3. 通过大规模客观/主观测试验证系统在方言表达性上的显著优势<br/>4. 在复杂S2SDC任务中证明合成语音的质量与实际应用价值|
|2509.16603v1|[An Octave-based Multi-Resolution CQT Architecture for Diffusion-based   Audio Generation](http://arxiv.org/abs/2509.16603v1)|**贡献点：**  <br/>1. **提出MR-CQTdiff架构**：首次将基于扩散模型的音频生成与多分辨率Constant-Q Transform（CQT）结合，优化时间-频率分辨率的动态调整机制。  <br/>2. **解决低频时间分辨率问题**：设计可逆CQT框架，按八度层级调整分辨率，提升低频段的时序表达能力，增强生成音频的灵活性和表现力。  <br/>3. **实验验证优越性**：通过Fréchet Audio Distance（FAD）在两个数据集上对比多架构，证明MR-CQTdiff达到SOTA性能，超越现有方法。  <br/><br/>**总结：**  <br/>本文提出MR-CQTdiff，结合扩散模型与多分辨率CQT，解决低频时间分辨率不足问题，并在两个数据集上通过FAD实验验证其SOTA性能。|
|2509.15845v1|[Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and   Context-Aware Instruct-TTS](http://arxiv.org/abs/2509.15845v1)|**贡献点：**<br/>1. **提出端到端自动化系统DeepDubbing**：整合脚本分析、音色选择与语音合成流程，首次实现多参与者有声书全自动化生产。  <br/>2. **创新Text-to-Timbre (TTT)模型**：基于文本描述生成角色专属音色嵌入，解决传统手动选择的低效问题。  <br/>3. **开发Context-Aware Instruct-TTS模型**：结合上下文对话分析与细粒度情感指令，提升TTS的情感表达与场景适配能力。  <br/>4. **实现音色与情感双重匹配**：系统首次兼顾角色音色一致性与情感表达，突破单一TTS的情感局限。  <br/><br/>**总结：**  <br/>DeepDubbing通过双模型协同，实现多参与者有声书的端到端自动化生成，解决音色匹配与情感表达两大难题。|
|2509.15626v1|[LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice   Impression Control](http://arxiv.org/abs/2509.15626v1)|总结（100字以内）:  <br/>该论文提出两项方法解决语音生成中的印象泄露问题，发布首个标注语音印象数据集LibriTTS-VI，并通过客观与主观评估验证了控制性提升，为可操控语音合成研究提供重要支持。<br/><br/>贡献点:  <br/>1. **提出抗印象泄露训练策略**：通过分离使用不同语句处理说话人身份与目标语音印象，降低参考音频对合成结果的干扰。  <br/>2. **设计参考无关生成模型**：仅从目标语音印象生成说话人嵌入，提升模型鲁棒性与生成灵活性。  <br/>3. **构建首个公开语音印象数据集LibriTTS-VI**：基于LibriTTS-R，建立标准化标注体系，推动领域研究可重复性。  <br/>4. **验证控制性提升效果**：客观指标（MSE降低0.20）与主观评价均显示语音印象可操控性显著改善，同时保持高保真度。|
|2509.15462v1|[A Novel Semantic Compression Approach for Ultra-low Bandwidth Voice   Communication](http://arxiv.org/abs/2509.15462v1)|**贡献点：**<br/>1. 提出基于生成语音模型的语义分解方法，实现语音信号的高效语义编码。<br/>2. 在降低2-4倍比特率的同时，保持与现有编解码器相当的感知质量。<br/>3. 显著提升转录、情感分析和说话人验证等下游任务的性能。<br/>4. 在低比特率下超越Encodec的感知质量和说话人验证效果，实现4倍比特率节省。<br/><br/>**总结（100字以内）：**  <br/>本文创新性地融合生成语音模型的语义分解能力与传统编解码技术，提出语义通信新框架。在2-4倍低比特率下，实现高质量语音压缩，且在多个下游任务中性能优于现有编解码器，尤其显著超越Encodec，为高效语音传输提供新方案。|
|2509.15373v1|[Frustratingly Easy Data Augmentation for Low-Resource ASR](http://arxiv.org/abs/2509.15373v1)|总结：  <br/>提出三种低资源ASR的数据增强方法，通过生成文本并合成语音提升模型性能，验证其在多种语言（包括高资源语言）中的广泛适用性。<br/><br/>贡献点：  <br/>1. **提出三种自包含的文本生成方法**：基于词汇表替换、随机替换和LLM生成，无需外部数据源即可创建新文本。  <br/>2. **合成语音增强低资源语言数据**：通过TTS将生成文本转化为合成音频，解决低资源语言数据不足的问题。  <br/>3. **验证方法有效性**：在四种低资源语言（Vatlongos、Nashta、Shinekhen Buryat、Kakabe）上实现显著性能提升（如Nashta的WER降低14.3%）。  <br/>4. **展示跨语言适用性**：方法不仅适用于低资源语言，也对高资源语言（如英语）有效，具有广泛的适用价值。|
|2509.14678v1|[Stochastic Clock Attention for Aligning Continuous and Ordered Sequences](http://arxiv.org/abs/2509.14678v1)|总结：该论文提出了一种基于学习非负时钟机制的注意力模型，通过路径积分推导出闭式评分规则，提升语音生成中对齐稳定性与鲁棒性，支持平行和自回归解码，具备参数简化优势，并拓展至视频等连续目标建模。<br/><br/>贡献点：<br/>1. 提出连续有序序列的注意力机制，显式建模对齐关系，适配帧同步目标需求<br/>2. 引入学习的非负"时钟"参数，替代传统位置编码和掩码，强制连续性和单调性约束<br/>3. 通过路径积分推导得到具有因果偏倚的高斯类评分规则，无需外部位置正则化<br/>4. 构建支持两种解码模式的框架（归一化/非归一化时钟），实现参数极简且易替换的模型结构|
|2509.14579v2|[Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech   Synthesis](http://arxiv.org/abs/2509.14579v2)|**贡献点：**  <br/>1. 提出跨语言语音克隆框架Cross-Lingual F5-TTS，无需参考转录本即可实现多语言语音合成。  <br/>2. 利用强制对齐技术从音频提示中提取词边界，解决训练阶段对转录本的依赖问题。  <br/>3. 设计多粒度说话率预测器，根据说话者语速动态生成语音时长，提升推理阶段的时长建模能力。  <br/>4. 通过实验验证，该方法在跨语言场景下达到与F5-TTS相当的语音质量，填补了无转录本跨语言语音克隆的技术空白。  <br/><br/>**总结：**  <br/>提出Cross-Lingual F5-TTS框架，无需转录本实现跨语言语音克隆，结合强制对齐与多粒度说话率预测器，有效解决时长建模与词边界识别问题，实验性能与基准模型相当。|
|2509.14298v1|[SpeechOp: Inference-Time Task Composition for Generative Speech   Processing](http://arxiv.org/abs/2509.14298v1)|总结：  <br/>论文提出SpeechOp多任务扩散模型，通过迁移学习提升多任务处理效率和内容保留，实现语音增强的SOTA性能。<br/><br/>贡献点：  <br/>1. **提出SpeechOp多任务框架**：首次将预训练TTS模型转化为通用语音处理器，支持多种语音任务（如增强）及推理时的创造性组合。  <br/>2. **迁移学习加速训练与提升质量**：利用预训练TTS模型的语音理解能力，减少对额外数据的依赖，同时优化语音到语音任务（S2S）的表现及核心TTS性能。  <br/>3. **引入隐式任务组合（ITC）**：结合ASR转录文本与扩散模型生成能力，通过推理时的结构化任务组合实现更鲁棒的内容保真，达到SOTA效果。|
|2509.12171v2|[Preservation of Language Understanding Capabilities in Speech-aware   Large Language Models](http://arxiv.org/abs/2509.12171v2)|**贡献点：**  <br/>1. 提出C3T（跨模态能力保留测试）作为评估语音感知大语言模型性能的新基准。  <br/>2. 通过结合文本任务与语音克隆的文本到语音模型，量化语言理解能力在语音输入下的保持程度。  <br/>3. 测量模型对不同说话者类别的公平性，确保评估的包容性。  <br/>4. 验证模型在文本与语音模态间的稳健性，揭示跨模态一致性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出C3T基准，通过文本任务与语音克隆模型，评估语音感知大语言模型的语言理解能力保留、说话者公平性及跨模态鲁棒性，为语音交互系统提供多维度性能分析框架。|
|2509.12171v1|[Preservation of Language Understanding Capabilities in Speech-aware   Large Language Models](http://arxiv.org/abs/2509.12171v1)|总结：  <br/>提出C3T基准，评估语音感知大模型的语言理解能力保留、公平性及跨模态鲁棒性。<br/><br/>贡献点：  <br/>1. 提出C3T（跨模态能力保留测试）基准，首次系统量化语音输入下语言理解能力的保留程度。  <br/>2. 结合文本任务与语音克隆TTS模型，创新性地构建跨模态评估框架。  <br/>3. 评估模型对不同说话者类别的公平性，揭示潜在的语音偏见问题。  <br/>4. 测试模型在文本与语音模态间的跨模态鲁棒性，推动多模态模型研究。|
|2509.10452v1|[WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained   Speech Recognition Transformers](http://arxiv.org/abs/2509.10452v1)|总结：提出WhisTLE，一种文本-only深度监督ASR领域适配方法，结合VAE与TTS辅助，无需额外运行开销，显著提升性能。<br/><br/>贡献点：<br/>1. 提出文本-only领域自适应框架WhisTLE，解决语音数据获取困难问题<br/>2. 构建深度监督体系，通过VAE建模文本到编码器输出的潜在空间<br/>3. 引入TTS辅助适配机制，提升模型对未见词汇的识别能力<br/>4. 保持推理阶段原始编码器结构，实现零额外运行时成本<br/>5. 在4个数据集/4个模型的全面实验中验证有效性，27/32场景超越基线|
|2509.10086v1|[Towards Data Drift Monitoring for Speech Deepfake Detection in the   context of MLOps](http://arxiv.org/abs/2509.10086v1)|**贡献点总结：**  <br/>1. 提出基于分布距离的语音Deepfake分布漂移监测方法，解决静态检测器对新攻击的防御不足问题。  <br/>2. 探索通过微调检测器应对漂移的策略，利用新TTS攻击生成的数据降低检测错误率。  <br/>3. 在玩具数据集和MLAAD大规模数据集上验证方法有效性，证明其可应用于实际场景。  <br/>4. 强调MLOps视角下语音Deepfake检测的持续更新与动态适应性。  <br/><br/>**摘要总结（100字内）：**  <br/>论文从MLOps角度提出语音Deepfake分布漂移监测与微调策略，通过计算新旧数据分布距离识别攻击，利用漂移数据微调检测器以提升性能，验证了方法在玩具与大规模数据集上的有效性。|
|2509.09748v1|[DiTReducio: A Training-Free Acceleration for DiT-Based TTS via   Progressive Calibration](http://arxiv.org/abs/2509.09748v1)|**贡献点：**<br/>1. 提出训练无关的DiTReducio框架，无需训练即可压缩DiT-based TTS模型计算。<br/>2. 引入Temporal Skipping和Branch Skipping两种压缩方法，消除推理阶段冗余计算。<br/>3. 基于DiT层中发现的两种注意力模式设计模式引导策略，选择性应用压缩方法。<br/>4. 实现生成质量与计算效率的可调节平衡，通过动态压缩阈值控制。<br/>5. 在F5-TTS和MegaTTS 3上验证效果，实现75.4% FLOPs减少、37.1% RTF提升，且保持生成质量。<br/><br/>**总结：**  <br/>本研究提出训练无关的DiTReducio框架，通过两种压缩方法和注意力模式策略，显著降低DiT-based TTS的计算成本，同时保持生成质量。|
|2509.09631v2|[DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for   Low-Latency Zero-Shot Text-To-Speech](http://arxiv.org/abs/2509.09631v2)|**贡献点总结**（100字以内）:  <br/>提出DiFlow-TTS，全球首个纯离散流匹配语音合成模型，通过统一架构显式建模语音属性，结合上下文学习实现零样本属性克隆，采用分层预测机制提升任务特定分布学习，显著提高生成速度与质量，在多个指标上优于现有基线。<br/><br/>---<br/><br/>**分点贡献**:<br/>1. **首创纯离散流匹配框架**：首次将离散代码表示与流匹配结合，突破传统方法依赖连续空间的限制，解决重复性问题。<br/>2. **统一架构显式建模语音属性**：在紧凑结构中显式建模韵律、声学等多维属性，提升生成语音的自然性和控制精度。<br/>3. **上下文学习与属性克隆**：通过文本内容及参考语音提取的韵律和声学属性实现零样本场景下的属性迁移与风格保留。<br/>4. **分层流预测机制**：独立设计韵律和声学头部，分别学习任务相关的分布，增强生成的可控性与多样性。<br/>5. **高效性能表现**：在自然度、韵律、语气保持和能量控制等关键指标上优于现有方法，且模型轻量、推理延迟低（速度提升25.8倍）。|
|2509.09155v1|[HISPASpoof: A New Dataset For Spanish Speech Forensics](http://arxiv.org/abs/2509.09155v1)|**贡献点分点列出：**  <br/>1. **提出首个西班牙语大规模合成语音检测数据集HISPASpoof**，填补了西班牙语在语音取证领域的研究空白。  <br/>2. **构建具有代表性的多模态数据集**，包含6种口音的真实语音和6种零样本TTS系统生成的合成语音。  <br/>3. **验证现有英语语音检测模型对西班牙语的泛化能力不足**，通过HISPASpoof训练的模型显著提升检测性能。  <br/>4. **首次系统评估西班牙语合成语音归因性能**，探索识别合成语音生成方法的有效性。  <br/>5. **为西班牙语语音取证提供关键基准**，推动技术可靠性与语言包容性研究。  <br/><br/>**总结（100字以内）：**  <br/>该研究构建了首个西班牙语合成语音检测数据集HISPASpoof，验证了英语模型在西班牙语场景中的局限性，并首次评估了归因性能，为改进多语言语音取证技术提供了重要基准。|
|2509.08753v2|[Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](http://arxiv.org/abs/2509.08753v2)|总结：DSM提出了一种新型流式多模态序列到序列框架，通过预处理对齐与延迟策略实现高效推理，在ASR和TTS任务中达到SOTA性能并支持任意长序列。<br/><br/>贡献点：<br/>1. 提出Delayed Streams Modeling (DSM)框架，实现流式多模态序列到序列学习的灵活建模<br/>2. 通过预处理对齐和引入延迟机制，首次将时间对齐问题前置到流式处理流程<br/>3. 支持任意长度输入输出序列的流式推理，突破传统离线处理限制<br/>4. 在ASR和TTS两大语音任务中均取得SOTA性能，且推理延迟显著优于离线基线<br/>5. 提供完整的代码、样本和演示资源，具备良好的可复现性和应用延展性|
|2509.08753v1|[Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](http://arxiv.org/abs/2509.08753v1)|**Summary (100字以内):**  <br/>该论文提出Delayd Streams Modeling（DSM），通过预处理时间对齐和延迟机制，实现流式多模态序列到序列推理，支持任意长序列处理，在ASR和TTS任务中达到SOTA性能与延迟，并提供开源代码促进应用。<br/><br/>**贡献点:**  <br/>1. **提出DSM框架**：设计了一种灵活的流式多模态序列到序列学习方法，区别于传统离线模式。  <br/>2. **时间对齐预处理**：通过预处理步处理时间对齐，简化流式推理过程，支持任意输出序列生成。  <br/>3. **延迟机制创新**：引入适当输入输出流延迟，实现动态适应不同任务（如ASR/TTS）的灵活推理。  <br/>4. **端到端性能验证**：在ASR和TTS任务中达到SOTA性能与低延迟，甚至优于离线基线模型。  <br/>5. **开源实现支持**：提供代码、样本和演示，方便研究者复现与扩展模型应用。|
|2509.08696v1|[Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer   Layer Caching](http://arxiv.org/abs/2509.08696v1)|总结：  <br/>本文提出SmoothCache方法，通过选择性缓存Transformer层输出优化扩散模型TTS推理效率，在提升速度的同时保持合成质量，且无需架构调整或重新训练。<br/><br/>贡献点：  <br/>1. **提出Selective Caching机制**：将SmoothCache集成至F5-TTS，通过缓存自注意力和前馈网络层的输出减少冗余计算，加速推理。  <br/>2. **设计校准阶段**：分析时序间的L1相对误差，动态优化缓存调度策略以最小化质量下降。  <br/>3. **统一缓存调度**：解决层间依赖问题，将自注意力层的缓存模式扩展至前馈网络层，实现跨层高效缓存。  <br/>4. **实验证明有效性**：在LibriSpeech-PC和Seed-TTS数据集上验证，高步骤缓存可显著提速且质量无损，低步骤缓存则可能引入质量下降。  <br/>5. **无需架构修改**：优化方法仅依赖缓存策略调整，无需改变模型架构或进行额外训练。|
|2509.06926v2|[Continuous Audio Language Models](http://arxiv.org/abs/2509.06926v2)|**贡献点：**  <br/>1. 提出连续音频语言模型（CALM），通过替代离散token表示，解决有损压缩带来的音质与计算成本矛盾。  <br/>2. 构建基于Transformer的上下文嵌入机制，结合MLP和一致性建模技术，直接生成连续音频帧。  <br/>3. 实验证明在语音和音乐生成任务中，CALM在效率与音质上均优于现有离散模型，支持轻量级高质量生成。  <br/>4. 提供可访问的实验样本（hf.co/spaces/kyutai/calm-samples），验证方法的可行性与性能提升。  <br/><br/>**总结（100字以内）：**  <br/>本论文提出CALM框架，通过连续表示替代离散token，结合Transformer和一致性建模技术，实现高效高质量的语音与音乐生成，突破了传统模型的音质与计算成本权衡，具有重要应用价值。|
|2509.06027v1|[DreamAudio: Customized Text-to-Audio Generation with Diffusion Models](http://arxiv.org/abs/2509.06027v1)|总结：  <br/>本文提出DreamAudio框架，通过参考音频引导生成定制化文本到音频内容，开发两类数据集并构建真实场景基准，实现对细粒度声学特征的精确控制与语义对齐。  <br/><br/>贡献点：  <br/>1. **提出DreamAudio框架**：首次设计可识别用户提供的参考概念并生成特定声学事件的定制化文本到音频生成系统。  <br/>2. **开发定制化数据集**：创建两种类型的数据集（训练与测试）以支持定制化系统训练，提升生成效果可评估性。  <br/>3. **实现细粒度声学控制**：在生成过程中精准控制特定音频事件的声学特性，满足用户对个性化音频内容的需求。  <br/>4. **构建真实基准数据集**：提供包含真实世界CTTA案例的数据库，作为定制生成任务的标准化评估基准。|
|2509.05863v1|[LatinX: Aligning a Multilingual TTS Model with Direct Preference   Optimization](http://arxiv.org/abs/2509.05863v1)|总结：提出LatinX多语言TTS模型，解决级联语音翻译中说话人身份保持问题，通过分阶段训练提升语音质量与相似度。<br/><br/>贡献点：<br/>1. 提出LatinX模型：首个专为级联语音-语音翻译设计的多语言TTS系统，实现跨语言说话人身份保留。<br/>2. 三阶段训练框架：包含文本-音频映射预训练、零样本语音克隆监督微调、基于DPO的对齐优化。<br/>3. 自动标注机制：采用Word Error Rate与说话人相似度指标生成对齐数据，提升训练效率。<br/>4. 性能提升：DPO训练使模型在WER和客观相似度指标上优于基线，人类评估显示更强的感知说话人相似度。<br/>5. 未来研究方向：提出跨语言分析框架，探讨平衡偏好信号与低延迟架构的优化策略。|
|2509.04957v1|[Efficient Video-to-Audio Generation via Multiple Foundation Models   Mapper](http://arxiv.org/abs/2509.04957v1)|总结（100字以内）：  <br/>提出MFM-Mapper，融合双视觉编码器与GPT-2提升跨模态对齐，显著降低训练数据需求（16%），在保持语义-时间一致性的同时实现高效视频到音频生成。<br/><br/>贡献点分点列表：  <br/>1. **多模态特征融合**：通过双视觉编码器融合语义与时间信息，提升特征表示的丰富性。  <br/>2. **模型结构创新**：用GPT-2替代线性映射器，提高跨模态特征对齐效果，借鉴自回归翻译任务机制。  <br/>3. **高效训练方法**：仅需16%训练数据量即可达到与大规模模型相当的性能，降低计算资源消耗。  <br/>4. **性能验证**：实验证明在语义-时间一致性方面优于传统mapper方法，具备竞争力的生成效果。|
|2509.04871v1|[Cloning a Conversational Voice AI Agent from Call\,Recording Datasets   for Telesales](http://arxiv.org/abs/2509.04871v1)|总结：  <br/>提出一种通用方法克隆对话式语音AI代理，通过整合ASR、对话管理及TTS技术构建流式推理系统，并基于22项评估标准分析其在常规对话与说服能力上的表现差异，为语音AI应用提供设计框架与改进方向。<br/><br/>贡献点：  <br/>1. **通用方法论**：首次提出从通话记录语料库中克隆对话式语音AI代理的系统化框架，适用于多种领域（如客服、医疗）。  <br/>2. **流式推理集成**：将自动语音识别（ASR）、基于大语言模型的对话管理器和文本到语音合成（TTS）整合为端到端的流式推理管道。  <br/>3. **多维度评估体系**：设计22项评估标准，系统对比AI代理与人类代理在对话流程各环节（如引入、异议处理等）的表现差异。  <br/>4. **提示工程优化**：针对AI在说服和异议处理上的不足，提出通过改进提示设计提升性能的策略。  <br/>5. **设计经验与未来方向**：总结可复用的设计经验，并提出大规模模拟与自动化评估等未来研究方向。|
|2509.04685v1|[Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive   Clustering and Implicit Duration Coding](http://arxiv.org/abs/2509.04685v1)|**贡献点总结（100字以内）:**  <br/>提出VARSTok，一种基于局部特征相似度的动态可变帧率语音分词器，通过密度峰值聚类和隐式时长编码提升性能与效率，减少token使用量并改善零样本语音合成效果，首次实现动态分词器与下游语言模型的无缝集成。  <br/><br/>**分点贡献列出:**  <br/>1. **动态可变帧率分词机制**：根据语音信号的信息密度变化，自适应分配token数，解决固定帧率分词与语音结构不匹配的问题。  <br/>2. **创新算法设计**：提出时间感知的密度峰值聚类方法（T-DPC），实现语音的可变长度单位分割。  <br/>3. **隐式时长编码方案**：通过单个token索引同时编码内容与时间跨度，消除了对额外时长预测模块的需求。  <br/>4. **性能与效率突破**：在重建自然度、token使用量（减少23%）及零样本语音合成任务中显著优于固定帧率基线，首次验证动态分词器在下游模型中的可行性。|
|2509.04345v1|[AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds](http://arxiv.org/abs/2509.04345v1)|总结：  <br/>本文提出AUDETER，首个大规模、多样的深度伪造音频数据集，验证了现有方法在泛化能力上的不足，并显著提升了深度伪造检测性能。<br/><br/>贡献点：  <br/>1. **构建首个大规模深度伪造音频数据集**：包含超过4,500小时合成音频，涵盖11种TTS模型与10种声码器，总计300万音频片段，是目前规模最大的深度伪造音频数据集。  <br/>2. **解决现实场景泛化性不足问题**：通过引入真实语音与深度伪造音频的多样性挑战，填补了现有数据集在真实应用场景测试的空白。  <br/>3. **验证现存检测方法局限性**：实验表明，基于传统数据集的SOTA方法在面对新型深度伪造音频时泛化能力差，误报率高。  <br/>4. **显著提升检测性能**：使用AUDETER训练的模型在跨领域测试中将检测错误率降低44.1%-51.6%，达到仅4.17%的优异表现。  <br/>5. **开源共享促进研究**：数据集公开在GitHub，推动深度伪造音频检测模型的通用化与实际应用发展。|
|2509.03300v1|[LatPhon: Lightweight Multilingual G2P for Romance Languages and English](http://arxiv.org/abs/2509.03300v1)|**贡献点：**  <br/>1. **多语言支持**：首次提出针对六种拉丁文字语言（英语、西班牙语、法语、意大利语、葡萄牙语、罗马尼亚语）的联合训练多语言G2P模型LatPhon。  <br/>2. **参数效率**：模型仅含7.5M参数，显著低于现有方法，且内存占用仅30MB，便于设备端部署。  <br/>3. **性能突破**：在ipa-dict数据集上达到3.5%的PER，优于byte-level ByT5基线（5.4%），接近语言专用WFST模型（3.2%）。  <br/>4. **通用性验证**：证明紧凑多语言G2P模型可作为拉丁语系语音处理系统的通用前端，简化多语言语音任务的架构。  <br/><br/>**总结（100字以内）**：  <br/>本文提出LatPhon，一种高效的多语言G2P模型，支持六种拉丁文字语言。其7.5M参数和30MB内存占用显著优于现有基线，性能接近语言专用模型，为拉丁语系语音系统提供通用轻量前端解决方案。|
|2509.03292v1|[Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and   Self-Supervised Embeddings](http://arxiv.org/abs/2509.03292v1)|**贡献点:**<br/>1. 构建了多轴感知质量预测系统，支持TTS、TTA和TTM生成音频的Production Quality等四项指标评估。  <br/>2. 针对领域偏移问题，融合BEATs预训练音频模型与多分支LSTM预测器实现跨域适配。  <br/>3. 设计三元组损失结合缓冲采样策略，优化嵌入空间以增强感知相似性建模。  <br/>4. 实现无需合成训练数据的领域鲁棒音频质量评估，提升模型泛化能力。  <br/><br/>**总结:**  <br/>该研究提出一种结合预训练模型与自定义损失函数的系统，解决生成音频质量评估中的领域偏移问题，实现跨域鲁棒的多指标预测。|
|2509.02859v1|[Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models](http://arxiv.org/abs/2509.02859v1)|1. 提出首个全面的语音深度伪造检测基准Speech DeepFake Arena，整合14个多样化数据集与攻击场景。  <br/>2. 提供统一评估工具包，支持跨数据集的标准化测试与结果复现。  <br/>3. 设计可复现、透明的评估指标与协议，推动领域研究规范化。  <br/>4. 构建系统排名leaderboard，直观比较不同检测系统的性能与鲁棒性。  <br/>5. 汇总12个开源与3个专有检测系统，覆盖当前先进方法。  <br/>6. 强调跨域评估的重要性，揭示现有系统的性能瓶颈。  <br/>7. 搭建开放平台（Huggingface与GitHub），便于研究社区参与与复现。|
|2509.01391v1|[MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using   Speech Self-Supervised Learning and Language Model](http://arxiv.org/abs/2509.01391v1)|贡献点总结：  <br/>1. 提出无需G2P转换的语音生成方法，直接从语音生成离散标记；  <br/>2. 整合预训练语音SSL模型与T5编码器，实现混合文本的伪语言标签生成；  <br/>3. 消除人工音标需求，降低标注成本并提升大规模非转录音频数据的处理效率；  <br/>4. 保持自然语音特征（如口音、语调），性能与传统G2P系统相当。  <br/><br/>（99字）|
|2509.01336v1|[The AudioMOS Challenge 2025](http://arxiv.org/abs/2509.01336v1)|总结：  <br/>本文提出首个针对合成音频的自动主观质量评估挑战（AudioMOS Challenge 2025），设计了三个评测赛道，涵盖文本到音乐、文本到语音/音频、不同采样率的合成语音质量评估，并验证了方法改进的有效性，推动了音频生成系统的自动评估研究。<br/><br/>贡献点：  <br/>1. **开创性挑战**：首次组织针对合成音频的自动主观质量预测挑战，填补了语音领域研究空白。  <br/>2. **多任务评测体系**：设置三个创新赛道，覆盖文本到音乐（整体质量+文本对齐）、文本到语音/音频（Meta Audiobox Aesthetics四维度）以及多采样率合成语音评估。  <br/>3. **多样化数据集**：提供包含文本-to-语音、文本-to-音频、文本-to-音乐的统一测试集，促进模型泛化能力验证。  <br/>4. **实证验证**：通过24支跨学术与工业团队的参与，确认了算法在基线上的改进效果。  <br/>5. **领域推动**：为音频生成系统的自动评估方法发展提供基准和方向，加速技术落地。|
|2509.01246v1|[An AI-Based Shopping Assistant System to Support the Visually Impaired](http://arxiv.org/abs/2509.01246v1)|**贡献点：**  <br/>1. **开发AI购物助手原型**：首次整合计算机视觉、语音识别、文本-语音合成和室内导航技术，构建用户友好型平台，提升视觉障碍者超市购物的自主性和包容性。  <br/>2. **环境感知与导航功能**：通过摄像头实时扫描环境，结合ArUco标记检测，实现精准导航、产品定位及动态听觉引导。  <br/>3. **多模态交互设计**：采用语音指令与多模态反馈（如语音合成、触觉提示），增强用户与系统的互动体验，促进购物过程的动态性和参与感。  <br/>4. **实证评估与有效性验证**：通过实验验证系统在实际场景中的可行性，证明其对改善视觉障碍者购物体验的显著效果。  <br/>5. **推动包容性AI技术发展**：为无障碍AI应用提供可扩展的框架，强调技术在增强社会公平性与用户独立性中的价值。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出基于AI的购物助手，整合多项技术实现导航与产品识别，通过多模态交互提升用户体验，并经实验验证其有效性，推动了无障碍AI辅助系统的创新与应用。|
|2509.01200v1|[SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation](http://arxiv.org/abs/2509.01200v1)|**贡献点**：<br/>1. 提出SimulMEGA框架，通过混合专家（MoE）门控机制与前缀训练相结合，实现隐式读写决策优化，解决多语言多对多场景下的语义连贯性与延迟平衡问题。<br/>2. 首次引入无监督策略学习方法，无需额外推理开销，提升实时语音翻译效率。<br/>3. 仅需对标准Transformer架构进行微小调整，通用性强，可适配语音-文本（S2T）和文本-语音（TTS）流式任务。<br/>4. 在6种语言对上的实验表明，其500M参数模型在1.5秒延迟下仍保持超越基准模型（Seamless）的翻译质量（BLEU损失<7%），且延迟与质量的权衡更优。<br/>5. 扩展至流式TTS任务，采用单向骨干结构，进一步实现延迟性能的突破性提升。<br/><br/>**总结**（100字以内）：<br/>本文提出SimulMEGA框架，通过MoE门控与前缀训练的整合，在减少延迟的同时保持高质量实时语音翻译，适用于多语言任务，并扩展至流式TTS，显著优于现有方法。|
|2509.00685v1|[MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech](http://arxiv.org/abs/2509.00685v1)|**总结**：本文提出多维偏好优化（MPO）方法，通过引入偏好集简化多维数据构建，结合正则化训练缓解DPO方法中的性能退化问题，显著提升TTS系统的可懂度、说话人相似性和语调表现。<br/><br/>**贡献点**：  <br/>1. 提出Multidimensional Preference Optimization (MPO)框架，解决TTS系统在多维偏好数据优化中的对齐难题。  <br/>2. 引入偏好集机制，统一构建多维度（如可懂度、语调、相似性）的偏好数据，提升训练效率。  <br/>3. 通过正则化策略缓解DPO类方法因奖励过自信导致的性能退化问题。  <br/>4. 实验验证MPO在关键语音质量指标（可懂度、说话人相似性、语调）上优于基线系统，表现出更高的泛化能力。|
|2509.00683v1|[PicoAudio2: Temporal Controllable Text-to-Audio Generation with Natural   Language Description](http://arxiv.org/abs/2509.00683v1)|**贡献点：**<br/>1. **新型数据处理流程**：提出使用grounding模型对真实音频-文本数据集进行事件时间戳标注，构建高时序准确性的真实数据集。<br/>2. **混合数据训练**：将真实数据与模拟数据结合训练，突破以往依赖单一模拟数据的局限，提升模型泛化能力。<br/>3. **时间戳矩阵设计**：继承PicoAudio的timestamp matrix机制，叠加细粒度时间对齐信息至粗粒度文本描述，增强控制粒度。<br/>4. **性能提升**：实验验证PicoAudio2在时序控制能力和音频质量上均优于现有方法。<br/><br/>**总结（100字以内）**：PicoAudio2通过真实数据标注、混合数据训练及时间戳矩阵设计，显著提升文本到音频生成的时序控制能力和音频质量。|
|2509.00675v1|[Speaker-Conditioned Phrase Break Prediction for Text-to-Speech with   Phoneme-Level Pre-trained Language Model](http://arxiv.org/abs/2509.00675v1)|**贡献点总结（100字以内）**  <br/>1. 首次将说话人嵌入整合至多说话人TTS的phrasing模型，提升性能。  <br/>2. 证明说话人嵌入可独立捕捉说话人相关特征，无需额外数据。  <br/>3. 提出少样本适配方法，拓展预训练嵌入到未见过的说话人。  <br/>4. 引入音素级预训练语言模型，显著提升phrasing准确性。  <br/>5. 通过客观与主观评估，验证方法有效性。  <br/><br/>**详细贡献点**  <br/>1. **说话人特征整合**：利用说话人嵌入增强多说话人TTS系统的phrasing模型，提升生成自然语音的连贯性。  <br/>2. **特征独立性验证**：发现说话人嵌入仅通过phrasing任务即可有效捕捉说话人相关特性，无需依赖其他任务数据。  <br/>3. **少样本适应**：探索预训练说话人嵌入的少样本适配能力，支持对未见过说话人的泛化处理。  <br/>4. **语言模型创新应用**：首次将音素级预训练语言模型应用于TTS前端的phrasing任务，显著提高模型精度。  <br/>5. **全面评估方法**：通过客观指标（如BLEU、WER）和主观测试（听觉评估）双重验证模型效果，证明其有效性。|
|2509.00186v1|[Generalizable Audio Spoofing Detection using Non-Semantic   Representations](http://arxiv.org/abs/2509.00186v1)|总结：  <br/>本研究提出基于非语义通用音频表示的新型反欺骗方法，通过TRILL和TRILLsson模型有效提升跨领域检测性能，在公共数据集上显著优于传统特征和现有模型。<br/><br/>贡献点：  <br/>1. 提出首个利用非语义通用音频表示的spoofing检测框架，突破语义依赖限制；  <br/>2. 引入TRILL与TRILLsson模型系统挖掘适合跨领域检测的音频特征；  <br/>3. 在in-domain测试中达到SOTA水平，在out-of-domain测试中显著超越现有方法；  <br/>4. 首次验证该方法在公共数据集的泛化能力优于手工特征、语义嵌入及端到端模型。|
|2508.21631v1|[Towards Improved Speech Recognition through Optimized Synthetic Data   Generation](http://arxiv.org/abs/2508.21631v1)|总结：  <br/>提出利用文本转语音模型生成合成数据解决语音识别训练数据不足问题，通过优化生成过程提升ASR性能，并验证其在魁北克法语口语数据集上的有效性。<br/><br/>贡献点：  <br/>1. **解决隐私与数据获取难题**：提出基于文本-语音模型（含语音克隆）生成合成音频，绕过真实转录音频的保密限制。  <br/>2. **优化合成数据生成流程**：系统性探索微调、过滤和评估方法，显著提升合成数据质量。  <br/>3. **端到端ASR模型训练应用**：将优化后的合成数据用于训练编码器-解码器结构的端到端语音识别模型。  <br/>4. **实证效果验证**：在两组魁北克法语口语数据集上验证，证明合成数据训练可达到与真实数据相当的ASR性能。  <br/>5. **提升数据生成与模型性能关联性**：量化表明合成数据生成质量的改进对最终识别效果具有显著正向影响。|
|2508.21407v1|[DRASP: A Dual-Resolution Attentive Statistics Pooling Framework for   Automatic MOS Prediction](http://arxiv.org/abs/2508.21407v1)|总结：  <br/>本文提出DRASP框架，通过双分辨率注意力机制融合全局统计与局部感知分析，显著提升MOS预测的准确性和泛化能力，在多个数据集和模型上优于传统池化方法。<br/><br/>贡献点：  <br/>1. **提出双分辨率注意力池化方法**：首次结合粗粒度全局统计（如整体音频特征）与细粒度注意力机制（聚焦关键语音片段），突破单一粒度池化局限。  <br/>2. **双视角架构设计**：同时捕捉语音质量的全局结构上下文与局部显著细节，增强特征表示的全面性和鲁棒性。  <br/>3. **跨任务与跨模型验证**：在MusicEval、AES-Natural等多数据集及CLAP-based、AudioBox-Aesthetics等不同MOS预测模型上验证有效性。  <br/>4. **性能提升**：在系统级SRCC指标上，相比传统平均池化方法提升10.39%，展现更强的泛化能力。|