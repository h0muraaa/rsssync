|Source|Title|Summary|
|---|---|---|
|2507.10827v1|[Supporting SENĆOTEN Language Documentation Efforts with Automatic   Speech Recognition](http://arxiv.org/abs/2507.10827v1)|总结：  <br/>本文提出ASR驱动的文档流程，整合TTS增强数据与跨语言迁移学习，有效提升少量数据下多合成语言的语音识别准确率，助力语言保护与教育资源开发。<br/><br/>贡献点：  <br/>1. **提出ASR驱动的文档框架**：设计结合文本到语音（TTS）增强数据与语音基础模型（SFMs）的跨语言迁移学习方案，解决SEN'CO TEN数据稀缺问题。  <br/>2. **创新语言模型融合策略**：通过浅层融合或n-best恢复方法整合n-gram语言模型，最大化利用有限的词汇资源。  <br/>3. **实证性能优化效果**：在SEN'CO TEN测试集上，通过过滤辅助错误，显著降低词错误率（WER）与字符错误率（CER），验证方法有效性。|
|2507.10469v1|[An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived   Realism and Performance in Virtual Reality Environments](http://arxiv.org/abs/2507.10469v1)|**总结（100字以内）:**  <br/>本研究评估了基于GPT-4 Turbo的AI驱动NPC在VR审讯模拟器中的表现，发现其在行为、社交和智力方面表现出较高可信度，但情感和个性较弱，并存在显著延迟问题，强调需优化性能以提升沉浸感。<br/><br/>**贡献点分点列出:**  <br/>1. **提出AI驱动NPC在VR审讯场景中的评估框架**：结合SUS、GEQ和可信度问卷，系统分析NPC的现实感、可用性及性能表现。  <br/>2. **验证GPT-4 Turbo在模拟对话中的有效性**：通过构建嫌疑人和搭档角色，评估其在复杂交互场景中的表现。  <br/>3. **量化系统延迟对体验的影响**：通过STT、TTS和整体循环延迟测量，发现延迟与对话上下文复杂度相关。  <br/>4. **揭示情绪与个性表达的短板**：结果表明NPC在情感和个性维度存在不足，为未来优化方向提供依据。  <br/>5. **强调性能优化对沉浸式体验的关键性**：提出需进一步降低延迟、增强情感深度以提升VR交互质量。|
|2507.09318v1|[ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow   Matching](http://arxiv.org/abs/2507.09318v1)|**贡献点分点总结:**<br/>1. 提出非自回归零样本语音对话生成模型ZipVoice-Dialog，解决传统自回归模型推理速度慢、不稳定的问题。  <br/>2. 引入说话者回合嵌入（speaker-turn embeddings）实现精确的对话回合切换。  <br/>3. 设计课程学习策略（curriculum learning）增强语音-文本对齐的稳定性。  <br/>4. 开发专项策略支持立体对话生成（考虑多说话者音色差异）。  <br/>5. 构建首个开放的6.8k小时大规模语音对话数据集OpenDialog，填补领域数据空白。  <br/>6. 建立全面的对话生成模型评估基准，推动技术发展。  <br/>7. 开源代码、模型、示例及数据集，促进学术研究与应用。  <br/><br/>**总结（100字以内）:**  <br/>该论文提出零样本非自回归语音对话生成模型ZipVoice-Dialog，构建大规模开放数据集OpenDialog，并建立基准评估体系，显著提升生成性能与效率。|
|2507.09310v1|[Voice Conversion for Lombard Speaking Style with Implicit and Explicit   Acoustic Feature Conditioning](http://arxiv.org/abs/2507.09310v1)|1. **提出Lombard风格迁移方法**：通过语音转换（VC）解决缺乏目标说话者Lombard风格数据的问题，用于增强TTS系统的训练。  <br/>2. **对比隐式与显式条件策略**：分析两种音学特征条件方法，验证隐式条件策略在保持说话者相似性的同时实现与显式条件相当的可理解性提升。  <br/>3. **验证隐式条件策略的有效性**：证明隐式条件方法在无需显式标注情况下，能有效复制Lombard效应，为低资源场景提供可行方案。  <br/><br/>总结：论文提出基于隐式条件的Lombard风格语音转换方法，有效解决数据不足问题，实现与显式条件相当的TTS性能提升。|
|2507.09282v1|[ClaritySpeech: Dementia Obfuscation in Speech](http://arxiv.org/abs/2507.09282v1)|**贡献点：**  <br/>1. 提出ClaritySpeech框架，首次结合ASR、文本混淆与零样本TTS技术，同步解决痴呆症语音失真与隐私保护问题。  <br/>2. 系统无需微调即可在低数据环境中运行，降低了对标注数据的依赖，提升了实际部署的可行性。  <br/>3. 实验验证了框架在ADReSS和ADReSSo数据集上的有效性：显著降低WER（0.73→0.08/0.15），提高语音质量（1.65→2.15），同时保持50%的说话者相似度。  <br/>4. 通过对抗设置评估，平衡了语音修复效果与隐私保护，为相关研究提供了新的技术路径。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出ClaritySpeech框架，整合ASR、文本混淆与零样本TTS，解决痴呆症语音失真问题，提升隐私与可访问性。实验表明其在低数据环境下无需微调即可有效改善语音质量，并保持说话者身份，为医疗语音处理提供新思路。|
|2507.08983v1|[Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](http://arxiv.org/abs/2507.08983v1)|总结：  <br/>本研究揭示了机器学习模型排行榜可能成为对抗攻击者大规模分发中毒模型的渠道，提出TrojanClimb框架并验证其在文本、语音等多模态中的有效性，强调需重构排行榜评估机制以防范安全风险。<br/><br/>贡献点：  <br/>1. **首次揭示模型排行榜的潜在安全风险**：提出模型排行榜可能被恶意利用作为隐蔽分发中毒模型的渠道，填补了对抗攻击大规模传播机制的研究空白。  <br/>2. **提出通用框架TrojanClimb**：设计可注入恶意行为（如后门、偏见）且不影响排行榜表现的框架，实现对抗样本与正常模型的性能平衡。  <br/>3. **跨模态验证攻击有效性**：在文本嵌入、文本生成、文本到语音和文本到图像四种模态中实验证明，展示攻击者可成功通过排行榜获得高排名并部署有害功能。  <br/>4. **推动安全机制重构需求**：强调需重新设计排行榜评估方法以检测和过滤恶意模型，并警示社区在采纳未验证模型时的潜在风险。|
|2507.08530v1|[MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling](http://arxiv.org/abs/2507.08530v1)|**贡献点总结：**  <br/>1. 提出MIDI-VALLE模型，将VALLE框架首次应用于音乐表演音频合成任务。  <br/>2. 改进架构，通过参考音频性能与对应MIDI的联合条件建模提升生成质量。  <br/>3. 引入离散token编码方式，替代传统piano roll，实现更一致和稳健的钢琴表现建模。  <br/>4. 构建广泛多样化的钢琴表演数据集，增强模型跨风格与场景的泛化能力。  <br/>5. 在ATEPP和Maestro数据集上验证性能，显著优于基线（FAD降低75%以上），并通过听觉测试（202 vs. 58票）证明效果提升。|
|2507.08319v1|[Active Learning for Text-to-Speech Synthesis with Informative Sample   Collection](http://arxiv.org/abs/2507.08319v1)|**贡献点总结**（100字以内）:  <br/>提出基于主动学习的TTS语料构建方法，解决数据规模增长导致的存储瓶颈，提升数据效率并验证其在合成质量上的优势。<br/><br/>**分点贡献**:<br/>1. **方法创新**：首次将主动学习应用于TTS语料构建，提出迭代交替的数据收集与模型训练框架，突破传统前馈式方法的局限。  <br/>2. **数据效率提升**：通过聚焦高信息量数据，减少冗余，优化存储资源利用，构建更高效的TTS语料库。  <br/>3. **实验验证**：实验证明，与同等规模语料库相比，所提方法显著提升语音合成质量，验证了其有效性与优越性。|
|2507.06826v1|[Physics-Informed Direction-Aware Neural Acoustic Fields](http://arxiv.org/abs/2507.06826v1)|总结（100字以内）:  <br/>本文提出一种物理信息神经网络框架，用于建模一阶Ambisonic房间脉冲响应，通过引入粒子速度与空间通道的物理先验约束，提升模型的物理一致性与声场生成效果，并通过实验验证其有效性。<br/><br/>贡献点:  <br/>1. **物理信息神经网络的扩展应用**：首次将PINN引入一阶Ambisonic房间脉冲响应（FOA RIRs）建模，结合神经网络与声波传播物理原理。  <br/>2. **声场物理先验的提出**：基于粒子速度与(X,Y,Z)-通道的关联性，推导出两个物理先验约束，强化模型对声场的物理合理性建模。  <br/>3. **空间通道耦合关系构建**：通过W通道与其他通道的偏导数组合，建立四通道间的物理可行关系，提升多通道一致性。  <br/>4. **有效性验证**：实验表明该方法在FOA RIRs建模任务中优于无物理先验的神经网络，验证了物理约束的有效性。|
|2507.06483v1|[Learning Japanese with Jouzu: Interaction Outcomes with Stylized   Dialogue Fictional Agents](http://arxiv.org/abs/2507.06483v1)|总结（100字以内）:  <br/>本研究通过混合方法分析动漫风格语音代理在多模态语言学习中的影响，发现其声音、人设和语言风格显著提升用户参与度与学习策略，为构建文化敏感的社交响应系统提供设计指导。<br/><br/>贡献点:  <br/>1. **方法论创新**：首次采用混合方法（定量+定性）系统评估风格化语音代理在语言学习场景中的交互效果。  <br/>2. **技术结合**：创新性地整合大语言模型与情感化语音合成技术，构建具有日本角色语言特征的动漫风格代理。  <br/>3. **多维度分析**：深入探讨用户参与度、情感反应、学习行为及跨语言能力与文化背景的交互差异。  <br/>4. **设计指导**：提出语音、人设与语言风格对用户体验和学习动机的直接影响机制，为情感化人机交互系统设计提供理论依据。  <br/>5. **文化敏感性**：强调文化背景在代理设计影响用户策略中的关键作用，拓展跨文化人机交互研究范式。|
|2507.04094v1|[MMMOS: Multi-domain Multi-axis Audio Quality Assessment](http://arxiv.org/abs/2507.04094v1)|总结：  <br/>本研究提出多领域、多维度无参考音频质量评估系统MMMOS，通过多编码器融合与模型集成显著提升评估性能，并在多个指标中取得领先。<br/><br/>贡献点：  <br/>1. **提出多维度评估框架**：首次定义Production Quality、Production Complexity、Content Enjoyment和Content Usefulness四个正交评估维度，覆盖语音、音乐及环境声音。  <br/>2. **融合多预训练编码器**：结合WavLM、MuQ、M2D三个模型的帧级嵌入，增强跨模态特征表示能力。  <br/>3. **创新聚合与损失策略**：设计三种聚合方法与四种损失函数组合，优化模型训练与多任务学习效果。  <br/>4. **模型集成提升性能**：通过集成前八名模型，在MOS评估中实现20-30%的均方误差下降和4-5%的Kendall's τ提升，显著优于基准模型。|
|2507.04036v1|[PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)|总结：  <br/>本论文提出PresentAgent，通过多模态整合生成同步视频演示，引入PresentEval评估框架，实验验证其接近人类水平，展示了可控多模态代理在文本转动态演示中的潜力。<br/><br/>贡献点：  <br/>1. **首个端到端文档-演示视频转换系统**：突破传统静态幻灯片或文本摘要的限制，实现视觉与口语内容的同步生成。  <br/>2. **模块化处理流程**：系统化分段、规划幻灯片框架、生成上下文相关口语叙述，并实现精确的音频-视觉对齐。  <br/>3. **统一评估框架PresentEval**：基于视觉语言模型，从内容忠实度、视觉清晰度、观众理解度三维度评估生成视频。  <br/>4. **高性能验证**：在30对文档-演示视频数据集上证明，系统在所有评估指标中均接近人类水平。  <br/>5. **开源实现**：提供代码库供研究与复现，促进技术应用与迭代。|
|2507.03912v1|[Prosody Labeling with Phoneme-BERT and Speech Foundation Models](http://arxiv.org/abs/2507.03912v1)|总结：  <br/>该论文提出结合声学与语言学特征的模型，利用预训练基础模型提升日语韵律标签预测准确率，适用于TTS系统的韵律控制。<br/><br/>贡献点：  <br/>1. **多模态特征融合**：首次将自监督学习（SSL）提取的声学特征与预训练语言学模型（如PnG BERT、PL-BERT）的linguistic features结合，用于音素级韵律标注。  <br/>2. **预训练模型应用**：引入Whisper编码器和多种phoneme-input预训练语言模型作为基础特征提取器，提升模型对语音和语言结构的表征能力。  <br/>3. **实验验证有效性**：在日语语料中验证了多模态融合方法的效果，实现89.8%（音高重音）、93.2%（高-低音高重音）、94.3%（句子边界）的预测准确率，优于单一模态方案。|
|2507.03887v1|[Traceable TTS: Toward Watermark-Free TTS with Strong Traceability](http://arxiv.org/abs/2507.03887v1)|**贡献点总结（100字以内）**:  <br/>首次提出无水印TTS框架，通过联合训练提升可追溯性并保留学语音质量，解决传统水印技术的局限，开放代码促进研究。<br/><br/>**分点贡献**:<br/>1. **提出首个无水印TTS方案**：首次构建无需显式水印的TTS系统，突破现有依赖水印的局限性。  <br/>2. **联合训练增强可追溯性**：通过同时训练TTS模型和判别器，显著提升生成语音的可追溯性泛化能力。  <br/>3. **质量与安全双保障**：在保持语音质量的前提下（甚至略有提升），实现强安全属性的模型可追溯性。  <br/>4. **开源促进领域发展**：代码开源推动语音安全与模型归因相关技术的进一步研究与应用。|
|2507.01805v1|[A Dataset for Automatic Assessment of TTS Quality in Spanish](http://arxiv.org/abs/2507.01805v1)|**贡献点分点总结：**  <br/>1. **首个西班牙语TTS数据库**：构建了包含4,326个音频样本（52个TTS系统和人类语音）的多语言数据库，填补西班牙语领域空白。  <br/>2. **标准化主观标注**：基于ITU-T P.807标准设计主观测试，由92名参与者完成，确保标注可靠性。  <br/>3. **验证数据集实用性**：通过训练自动自然度预测模型（两种方法：英语模型微调、自监督模型下游网络），验证数据集的广泛适用性。  <br/>4. **量化评估结果**：模型在五分制MOS上的平均绝对误差达0.8，体现数据集对模型训练的有效性。  <br/>5. **数据质量与多样性分析**：通过进一步分析证明数据集的高质量和多样性，为西班牙语TTS研究提供重要资源。  <br/><br/>**总结**（100字以内）：  <br/>本文构建首个西班牙语TTS评估数据库，包含4326个样本及标准化主观标注，验证其在模型训练中的有效性，并通过分析证明其质量与多样性，推动西班牙语TTS研究发展。|
|2507.00808v2|[Multi-interaction TTS toward professional recording reproduction](http://arxiv.org/abs/2507.00808v2)|总结：  <br/>本文提出一种多步交互的TTS方法，模拟语音导演与演员的协作机制，实现用户对语音风格的迭代调整，并构建对应数据集验证方法的有效性，展示其多交互能力。<br/><br/>贡献点：  <br/>1. **提出多步交互TTS框架**：首次将语音导演与演员的迭代反馈机制引入TTS系统，实现用户对合成语音的实时精细调整。  <br/>2. **用户-模型协同建模**：通过建模用户与TTS模型的互动过程，增强系统对用户风格意图的理解与响应能力。  <br/>3. **构建专门数据集**：提供可迭代风格调整的语料数据集，支持实验验证与后续研究。  <br/>4. **实验验证有效性**：通过对比实验证明该方法能根据用户指令实现迭代风格优化，展示多交互能力的提升。|
|2507.00227v1|[Investigating Stochastic Methods for Prosody Modeling in Speech   Synthesis](http://arxiv.org/abs/2507.00227v1)|总结：  <br/>该论文提出将随机方法（Normalizing Flows、Conditional Flow Matching、Rectified Flows）应用于语音合成韵律生成，并通过对比实验验证其生成效果可媲美人类，同时提供更灵活的控制参数（采样温度）。  <br/><br/>贡献点：  <br/>1. **方法创新**：首次系统性探索随机模型（如Normalizing Flows）在语音韵律生成中的有效性，突破传统确定性方法的局限。  <br/>2. **效果验证**：通过大量主观与客观评估，证明随机方法能生成自然且符合人类语音变异规律的表达性韵律。  <br/>3. **可控性增强**：引入采样温度调整机制，为语音合成提供更多可调控参数，提升生成结果的灵活性与可解释性。  <br/>4. **跨方法对比**：对比传统基线与真实人类发音，明确随机方法在韵律生成任务中的竞争力与优势。|
|2506.23869v1|[Scaling Self-Supervised Representation Learning for Symbolic Piano   Performance](http://arxiv.org/abs/2506.23869v1)|**贡献点：**  <br/>1. 提出基于生成自回归Transformer模型的音乐生成框架，利用60,000小时符号独奏钢琴数据进行大规模预训练，提升模型对音乐结构的理解。  <br/>2. 将SimCLR框架适配至符号音乐领域，生成通用对比式MIDI嵌入，支持音乐延续生成、符号分类任务及跨任务迁移。  <br/>3. 验证模型在钢琴延续生成任务中优于主流符号生成方法，在少样本场景下（仅需数百标签）实现下游任务的高效迁移，表现接近专有音频生成模型。  <br/><br/>**总结：**  <br/>该研究通过大规模符号数据预训练与SimCLR框架适配，开发出高性能音乐生成模型，在连贯性和分类任务中展现显著优势，推动音乐信号处理的迁移学习应用。|
|2506.23553v2|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v2)|总结：  <br/>本研究提出基于人类感知的Human-CLAP模型，通过将主观评价分数融入对比学习，显著提升了CLAPScore与人类评分的相关性（SRCC提高>0.25），解决了传统CLAP在音频-文本相关性评估中的局限性。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：首次发现CLAPScore与人类主观评价存在低相关性，指出其在评估音频-文本相关性任务中的不足。  <br/>2. **提出Human-CLAP方法**：设计一种基于人类感知的对比学习框架，通过直接使用主观评价分数优化模型训练目标。  <br/>3. **实验证明效果提升**：通过对比实验验证Human-CLAP在提升Spearman相关系数（SRCC）方面的显著效果，优于传统CLAP模型。|
|2506.23553v1|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v1)|总结：  <br/>该研究发现CLAPScore与人类主观评分相关性较低，提出基于人类感知的Human-CLAP模型，并通过实验验证其显著提升评估效果。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：首次系统分析CLAPScore与人类主观评估评分的低相关性，指出其在文本-音频相关性建模中的不足。  <br/>2. **提出Human-CLAP方法**：创新性地通过引入人类主观评分作为训练信号，构建了一个更符合人类感知的对比语言-音频预训练模型。  <br/>3. **验证效果提升**：实验证明Human-CLAP将Spearman秩相关系数（SRCC）提升超过0.25，显著增强了模型与人类评分的一致性。|
|2506.23552v1|[JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](http://arxiv.org/abs/2506.23552v1)|总结：  <br/>JAM-Flow提出统一框架，通过Flow Matching与MM-DiT结合，实现音频-视觉同步生成，支持多条件输入，提升跨模态交互效果。<br/><br/>贡献点：  <br/>1. **统一多模态生成**：首次将面部运动与语音合成整合为单一任务，突破传统分开处理的范式。  <br/>2. **MM-DiT架构创新**：设计包含Motion-DiT（运动模块）与Audio-DiT（音频模块）的多模态扩散Transformer，实现模态特异性建模。  <br/>3. **联合注意力机制**：引入选择性联合注意力层与时间对齐位置嵌入，优化跨模态信息交互效率。  <br/>4. **灵活输入支持**：基于inpainting训练目标，兼容文本、参考音频及运动作为输入条件，扩展应用范围。  <br/>5. **实用化解决方案**：提供端到端音频-视觉合成方法，显著提升多模态生成建模的实际应用效果。|
|2506.23367v1|[You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel   Properties](http://arxiv.org/abs/2506.23367v1)|贡献点总结：  <br/>1. **首个面向L2者的TTS系统**：提出首个专门针对第二语言学习者（如法语L1-英语L2）的语音合成系统，提升其可理解性。  <br/>2. **清晰模式设计**：基于美式英语紧张/松弛元音的时长差异，开发“清晰模式”以优化语音清晰度。  <br/>3. **实验证据**：通过感知研究验证清晰模式显著降低转录错误（≥9.15%），并被听众认为更友好而非整体放慢语速。  <br/>4. **认知差异揭示**：发现听众对清晰模式的实际效果缺乏意识，表明真实可理解性与感知可理解性无直接关联。  <br/>5. **对现有工具的批判**：指出Whisper-ASR未有效捕捉L2语音的区分线索，无法准确评估TTS系统对非母语者的可理解性。  <br/><br/>总结（100字以内）：  <br/>提出首个面向L2者的语音合成系统，通过元音时长策略优化清晰度，验证其有效性并揭示感知与实际理解的差异，批评现有工具的不足，为跨语言语音辅助技术提供新方向。|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|总结：  <br/>本研究提出SAGE数据生成方法和经验回放策略，结合外部语言模型，显著提升了方言阿拉伯语与阿拉伯-英语代码切换语音的识别性能，超越了大模型效果。<br/><br/>贡献点：  <br/>1. **提出改进的音频拼接方法（SAGE）**：生成人工阿拉伯语-英语代码切换语音数据，解决数据稀缺问题。  <br/>2. **SAGE与SSL模型的联合微调**：在代码切换和阿拉伯语基准测试中，使WER绝对下降7.8%。  <br/>3. **基于经验回放（ER）的泛化增强技术**：缓解方言和代码切换任务中的灾难性遗忘，降低整体WER至26.6%。  <br/>4. **少样本微调策略**：进一步将代码切换基准WER提升4.9%，并实现优于USM和Whisper-large-v2的性能（分别高5.5%和8.4%）。|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|总结：  <br/>本文提出一种针对语音交互的端到端评估基准，通过构建包含真实场景、语音特性（如韵律、同音词、口吃）及多样化声学条件的数据集，并设计查询感知的评估方法，揭示主流语音模型在不同场景下的性能差异，为模型优化提供依据。<br/><br/>贡献点：  <br/>1. **构建专用语音评估基准**：系统创建面向语音场景的端到端评估基准，弥补现有文本基准在语音领域应用的不足。  <br/>2. **语音特性增强数据集**：收集真实语音对话数据，纳入多元说话者属性、声学条件及语音独特现象（如韵律、同音词、口吃）。  <br/>3. **设计查询感知评估方法**：提出基于定制化检查清单和提示的自动评估框架，提升语音任务评价的准确性与细粒度。  <br/>4. **揭示模型场景差异**：对主流语音模型进行全面测试与分析，发现其在不同语音场景下的显著性能差异。  <br/>5. **推动语音模型发展**：为语音模型的开发、优化与评估提供实证依据和参考价值。|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>提出针对实时语音对话的偏好对齐框架，构建多轮语音数据集，结合AI反馈优化模型，通过实验验证提升对话系统的表现。<br/><br/>贡献点：  <br/>1. **创新框架**：提出首个专为实时语音对话设计的偏好对齐方法，解决传统文本模型在处理动态语音交互（如中断、插入）上的不足。  <br/>2. **大规模数据集**：创建超过15万对标注AI反馈的多轮语音对话数据，涵盖语言内容与时间上下文的偏好变化。  <br/>3. **模型优化**：利用离线对齐技术微调全双工自回归语音到语音模型，提升多轮对话生成能力。  <br/>4. **实验验证**：在通用对话中证明反馈机制能有效增强模型的准确性、安全性和上下文一致性。  <br/>5. **人类评估**：通过综合人工评价验证模型在复杂多轮场景下的实际效果。  <br/>6. **理论洞见**：发现动态平衡（如语言、时间因素）对构建自然实时对话系统的重要性。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|总结（100字以内）:  <br/>本文提出一种步骤式视频到音频生成方法，模仿Foley流程生成多音频轨道，结合文本提示与概念否定策略，无需专用数据集即可实现高质量语义区分的音频合成，优于现有基线。<br/><br/>贡献点:  <br/>1. **步骤式生成框架**：首次提出按顺序生成单个音频轨道的方法，每个轨道对应视频中的特定声音事件，实现多声部音频合成。  <br/>2. **Foley流程模拟**：通过镜像传统影视音效创作流程，全面捕捉视频中所有由动作触发的声音事件。  <br/>3. **文本引导与记忆机制**：引入文本提示和先前生成的音频轨道作为条件，构建动态引导的合成流程。  <br/>4. **概念否定策略**：借鉴组合生成框架中的概念否定机制，提升音频生成的精确性和语义分离能力。  <br/>5. **无需配对数据训练**：设计基于预训练模型的通用训练框架，消除对专用视频-音频配对数据集的需求。  <br/>6. **高质量合成效果**：实验证明生成的音频轨道在语义区分度和整体质量上优于现有基线方法。|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|**贡献点分点总结：**  <br/>1. **系统性比较**：在零样本TTS框架下，对比了三种主流说话人编码器（H/ASP、x-vector、ECAPA-TDNN）的性能。  <br/>2. **标准化评估**：基于同一捷克语数据集，跨领域（24个目标说话人）进行主观听觉测试与客观余弦距离分析。  <br/>3. **发现编码器差异**：H/ASP编码器在零样本TTS中表现最好，ECAPA-TDNN优于x-vector，但未超越H/ASP。  <br/>4. **提出实证建议**：强调说话人识别嵌入在TTS中的适配需经实证验证，并构建了可复用的比较框架。  <br/><br/>**总结（100字内）：**  <br/>本研究对比了三种说话人嵌入在零样本TTS中的效果，发现H/ASP编码器表现最优，ECAPA-TDNN虽优于x-vector但不足。提出标准化评估框架，强调实证验证的重要性，为跨任务嵌入应用提供了参考。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|总结：  <br/>提出Kling-Foley多模态视频到音频生成模型，结合扩散Transformer与同步模块提升音视频对齐，开源工业级基准Kling-Audio-Eval，并在多个指标上达成SOTA性能。<br/><br/>贡献点：  <br/>1. **提出Kling-Foley模型**：首个大规模多模态Video-to-Audio生成模型，实现高保真音频与视频内容的同步合成。  <br/>2. **多模态交互建模**：引入多模态扩散Transformer，融合视觉语义表示模块与音视频同步模块，提升跨模态对齐能力。  <br/>3. **帧级对齐机制**：通过视频条件与潜在音频元素的帧级对齐，优化语义对齐与时间同步效果。  <br/>4. **通用音频编解码器**：设计支持音效、语音、歌唱及音乐等多场景的潜在音频编码方案，实现高质量建模。  <br/>5. **空间感渲染技术**：采用立体声渲染方法增强合成音频的听觉空间感知。  <br/>6. **开源工业级基准**：针对数据集缺陷，发布Kling-Audio-Eval工业级评估基准，推动领域研究。  <br/>7. **SOTA性能验证**：实验表明，基于流匹配目标训练的Kling-Foley在分布匹配、语义对齐、时间同步和音频质量上达到公共模型最优。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|**总结（100字以内）:**  <br/>本文提出TTSDS2，改进TTS评估方法，实现与主观评分的高相关性，并发布包含11000个评分的数据集、多语言测试数据管道及14语言持续更新的基准，推动合成语音质量评估的标准化和客观性。<br/><br/>**贡献点分点列述：**  <br/>1. **提出TTSDS2评估指标**：相比现有16种评估方法，TTSDS2在所有测试领域和主观评分中均达到Spearman相关性>0.5，显著提升TTS系统质量评估的客观性和鲁棒性。  <br/>2. **构建大规模主观数据集**：提供涵盖11,000+主观评分的多语言数据集，填补合成语音质量评估的标注缺口。  <br/>3. **设计可复用的评估框架**：推出能持续生成多语言测试数据的管道，避免数据泄露；并发布14语言的动态更新基准，支持长期性能监测与对比研究。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**贡献点：**<br/>1. 提出新假设：人类在主观语音质量评分中更关注低质量段落，评分差异主要由忽略低质量部分时的高分误判导致。<br/>2. 验证假设：基于VCC2018和BVCC数据集分析，揭示人类评价焦点与语音质量分布的关系。<br/>3. 提出新指标：定义N_low-MOS（N个最低意见评分的平均值）作为更可靠的语音质量代表值。<br/>4. 实验验证：证明N_low-MOS可提升MOSNet的评估性能（LCC/SRCC指标改善），展示其内在有效性。<br/>5. 推动方法改进：为语音转换（VC）模型的评估提供更科学的基准，优化MOSNet的比较能力。<br/><br/>**总结（100字内）：**  <br/>本文提出N_low-MOS指标，通过分析数据集验证人类评分明显受低质量段落影响，并证明其在提升语音质量评估模型性能方面具有显著优势，为VC模型评估提供新思路。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|**贡献点：**  <br/>1. 提出一种新的评分聚合方法，解决传统MOS标注（1-5分离散评分）的局限性。  <br/>2. 基于标注者内部连续评分假设，建模生成分布并通过量化潜在连续分布估计评分峰值。  <br/>3. 引入潜在分布峰值作为新代表性值，取代传统MOS作为预测目标。  <br/>4. 通过实验验证，该方法能显著提升语音质量预测模型（如MOSNet）的性能。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出一种基于连续评分假设的新型语音质量评分方法，通过建模标注者的评分过程并引入潜在分布峰值作为替代指标，有效提升语音质量预测模型的性能。|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|**总结（100字以内）**  <br/>本研究构建了日本偶像语音语料库（JIS），聚焦年轻女性现场偶像群体，通过艺名标识促进听众实验。JIS支持TTS/VC系统的说话者相似性评估，并探索听众偏好的个性化语音生成，数据免费开放且仅限非商业科研使用，附带文化背景介绍与应用指导。<br/><br/>**贡献点分点列出**  <br/>1. **构建专业化语料库**  <br/>   - 首次创建针对日本“年轻女性现场偶像”这一特定群体的语音语料库（JIS），覆盖TTS和VC研究需求，补充语音生成领域的数据多样性。<br/><br/>2. **推动说话者相似性评估**  <br/>   - 所有语音由同一类群体（艺名标识）录制，便于系统评估生成语音与原声的说话者相似性，提升TTS/VC的可信度与可比性。<br/><br/>3. **开拓听众偏好研究方向**  <br/>   - 引入针对听众偏好的个性化语音生成研究（如定制偶像声音），填补该领域学术空白，推动语音生成技术与用户需求的结合。<br/><br/>4. **制定开放使用政策**  <br/>   - 以非商业、基础研究为限免费公开JIS，确保数据可及性同时维护版权与伦理规范，促进学术共享与创新。<br/><br/>5. **提供文化背景支持**  <br/>   - 对日本偶像文化进行介绍，助力研究者理解数据语境，确保JIS的合法、有效与伦理应用，提升科研的社会接受度。|
|2506.16738v1|[LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](http://arxiv.org/abs/2506.16738v1)|总结：  <br/>本文提出LM-SPT方法，通过间接语义蒸馏减少语音标记序列长度，改进编码器解码器结构并支持多帧率，实验验证其在语音-文本建模中取得优于基线的重建效果和文本到语音任务表现。<br/><br/>贡献点：  <br/>1. 提出LM-SPT模型，通过语义驱动的重建机制替代传统池化操作，有效减少语音标记序列长度。  <br/>2. 引入间接语义蒸馏策略，通过冻结ASR编码器对比原始与重建波形，提升语义对齐精度。  <br/>3. 设计编码器-解码器架构优化方案，支持25Hz、12.5Hz、6.25Hz等多帧率配置。  <br/>4. 实验证明LM-SPT在语音-文本双向任务中均优于基线，尤其在文本到语音生成中表现突出。|
|2506.16580v1|[Streaming Non-Autoregressive Model for Accent Conversion and   Pronunciation Improvement](http://arxiv.org/abs/2506.16580v1)|总结：提出首个支持流式处理的口音转换模型，实现非母语语音向母语口音的转换，同时保持说话人身份与语调，并通过集成TTS模型提升训练效率，达到顶级模型性能且具备稳定延迟。<br/><br/>贡献点：<br/>1. 提出首个实现流式口音转换（AC）的模型，支持实时处理非母语语音。<br/>2. 保持说话人身份、语调特征并提升发音质量，实现"原声"性转换。<br/>3. 采用Emformer编码器与优化推理机制，显著降低处理延迟。<br/>4. 整合母语TTS模型生成理想参考数据，提升训练效率。<br/>5. 在保持稳定延迟的前提下，达到当前最优AC模型的转换效果。|
|2506.16127v1|[Improved Intelligibility of Dysarthric Speech using Conditional Flow   Matching](http://arxiv.org/abs/2506.16127v1)|**贡献点总结（100字以内）**  <br/>本研究提出基于自监督学习特征及量化表示的失语语音到正常语音转换方法，采用单说话人语音生成策略降低说话人差异，并结合非自回归条件流匹配与扩散Transformer实现高效映射，提升语音可懂度和收敛速度。  <br/><br/>**具体贡献点**  <br/>1. **替代传统特征**：提出使用自监督学习（SSL）特征及其量化表示替代梅尔频谱图（mel-spectrograms），探索其在语音生成中的有效性。  <br/>2. **缓解说话人差异**：通过从WavLM提取特征生成单说话人干净语音，减少说话人变异性对模型性能的影响。  <br/>3. **非自回归框架**：设计全非自回归方法，结合条件流匹配（CFM）与扩散Transformer，直接学习失语语音到干净语音的映射。  <br/>4. **离散音素优势**：验证离散音素单元在提升语音可懂度和加速模型收敛方面的显著效果，优于传统基于梅尔频谱的方法。|
|2506.15873v1|[DeckFlow: Iterative Specification on a Multimodal Generative Canvas](http://arxiv.org/abs/2506.15873v1)|总结：  <br/>该论文提出DeckFlow多模态生成AI工具，通过任务分解、规格分解和生成空间探索三大创新机制解决现有工具设计问题，并验证了其在文本到图像生成中的有效性，进一步拓展至音频生成以研究用户创意行为。<br/><br/>贡献点：  <br/>1. **任务分解机制**：采用无限画布与卡片式视觉数据流交互，支持用户创建和管理多个互联子任务。  <br/>2. **规格分解工作流**：将初始目标迭代拆解为子部分，结合特征标签与聚类实现多模态内容组织。  <br/>3. **生成空间探索**：通过生成多组提示词与输出变体（网格展示），支持递归反馈优化设计迭代。  <br/>4. **多模态验证与扩展**：在文本-图像生成中对比传统对话AI基线，后扩展至音频生成，分析跨模态创作行为。|
|2506.15759v1|[Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](http://arxiv.org/abs/2506.15759v1)|**总结（100字以内）**  <br/>该论文提出Sonic4D框架，首次实现4D场景与空间音频的联合生成，通过动态视觉捕捉、声源定位及物理模拟三阶段处理，无需训练即可生成逼真的时空音频，增强了沉浸式音频视觉体验。<br/><br/>**贡献点**  <br/>1. **提出Sonic4D框架**：首次将空间音频生成与4D场景合成结合，解决现有方法忽视音频与场景对齐的问题。  <br/>2. **三阶段生成方法**：  <br/>   - 第一阶段：利用预训练模型生成4D场景及单声道音频；  <br/>   - 第二阶段：通过像素级视觉定位策略估计声源的3D空间坐标；  <br/>   - 第三阶段：基于物理模拟合成动态视角与时间变化的时空音频。  <br/>3. **训练-free设计**：无需额外训练，直接利用现有数据生成符合场景的空间音频。  <br/>4. **实验验证与资源公开**：通过实验证明生成音频的逼真度和沉浸感，并开放生成示例供研究复现。|
|2506.15085v1|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v1)|总结：  <br/>提出EmojiVoice工具包，实现社交机器人离线语音表达的动态控制，通过emoji提示增强长时表达性，并验证其在不同场景中的表现差异。<br/><br/>贡献点：  <br/>1. **开发EmojiVoice工具包**：首个专门针对社交机器人长时语音表达的免费、可定制TTS系统，解决基础模型离线部署难题。  <br/>2. **引入emoji提示控制机制**：首次将表情符号用于细粒度表达性调控，实现对语音情感相位的精准控制。  <br/>3. **轻量级实时生成框架**：采用Matcha-TTS等轻量模型，支持机器人端实时语音生成需求。  <br/>4. **多场景验证与对比**：通过剧本对话、讲故事、自主交互三类案例研究，验证方法对表达性的提升效果，并揭示不同应用场景下的接受差异。|
|2506.13053v2|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v2)|总结：本文提出ZipVoice，通过紧凑结构和流匹配技术实现高效零样本TTS，质量与SOTA相当，在模型体积和推理速度上显著提升，并开源代码与模型。<br/><br/>贡献点：  <br/>1. **提出Zipformer-based流匹配解码器**：在模型体积受限情况下保持足够的建模能力，提升效率。  <br/>2. **设计Average upsampling与Zipformer结合的语音-文本对齐及文本编码器**：增强语音可懂度，优化多语言处理效果。  <br/>3. **创新流蒸馏方法**：减少采样步骤，消除无分类器引导的推理开销，显著提升生成速度。  <br/>4. **实验验证效果**：在100k小时多语言数据集上，ZipVoice在语音质量与SOTA模型相当的同时，体积缩小3倍，速度提升30倍。  <br/>5. **开源实现**：提供代码、模型检查点和演示样本，便于复现与应用。|
|2506.12199v1|[ViSAGe: Video-to-Spatial Audio Generation](http://arxiv.org/abs/2506.12199v1)|**贡献点总结**（100字以内）:  <br/>本研究提出ViSAGe框架，直接从无声视频生成一阶Ambisonics，构建YT-Ambigen数据集，设计空间音频评估指标，并验证其在时空对齐与视角适应性上的优越性。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新问题**：探索无需复杂录制设备或专业技能，直接从无声视频生成一阶Ambisonics的方法。  <br/>2. **构建数据集**：创建包含102K视频片段的YT-Ambigen数据集，为研究提供标注数据支持。  <br/>3. **设计评估指标**：首次基于音频能量图与显著性度量，提出量化评估生成空间音频质量的新标准。  <br/>4. **端到端框架**：提出ViSAGe模型，融合CLIP视觉特征、自回归音频编码与方向/视觉引导生成高质量Ambisonics。  <br/>5. **性能优势**：验证ViSAGe在生成音频的连贯性与时空对齐性上优于传统两阶段方法（视频-音频生成+空间化）。  <br/>6. **应用能力**：生成的音频可随视角变化动态适应，提升沉浸式音频体验的交互性。|
|2506.11160v5|[S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation   Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning](http://arxiv.org/abs/2506.11160v5)|**贡献点列表：**  <br/>1. 提出 **S2ST-Omni** 框架，将多语言 S2ST 任务拆分为 S2TT 和 TTS 两个子任务，提升效率和可扩展性。  <br/>2. 设计 **语音语言模型**，融合预训练 Whisper 编码器（增强音频理解）与 Qwen 3.0（增强文本理解）。  <br/>3. 引入 **轻量级语音适配器**，有效桥接语音与文本表示的模态差异。  <br/>4. 提出 **两阶段微调策略**，优化多模态知识学习过程。  <br/>5. 采用 **流式自回归生成方法**，实现自然流畅的实时目标语音合成。  <br/>6. 在 CVSS 数据集上验证，S2ST-Omni 显著超越现有 S2ST 系统的翻译质量。  <br/><br/>**总结（100字内）：**  <br/>提出 S2ST-Omni 框架，通过分解任务、整合预训练模型、轻量化适配器及两阶段微调策略，提升多语言语音翻译质量，降低对大规模平行语料的依赖，并实现流式生成的高效合成。|
|2506.11130v1|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v1)|总结（100字以内）:  <br/>提出自优化框架，利用无标签数据与闭环训练提升ASR性能，成功将Whisper转化为Twister，在台语中实现显著精度提升，并为低资源场景提供实用方案。<br/><br/>贡献点：<br/>1. 提出无需标注数据的自精炼框架，通过伪标签生成与闭环训练实现ASR性能提升。  <br/>2. 首次将高保真TTS系统与ASR模型结合，形成"ASR→TTS→ASR"的闭合优化循环。  <br/>3. 在台语语音任务中验证框架有效性，利用6000小时未标注语音驱动模型训练。  <br/>4. 开发专用模型Twister，相较Whisper在普通话和双语切换场景分别降低20%和50%错误率。  <br/>5. 为低资源/特定领域ASR提供创新解决方案，突破传统伪标签自蒸馏方法的局限性。|
|2506.11127v1|[GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech   Instructions](http://arxiv.org/abs/2506.11127v1)|总结：  <br/>提出GUIRoboTron-Speech，首个端到端语音指令GUI代理，通过语音数据生成和混合训练策略解决模态不平衡问题，实验验证其高效性和广泛应用前景。<br/><br/>贡献点：  <br/>1. **首个语音驱动GUI代理**：提出GUIRoboTron-Speech，首次实现端到端GUI操作，直接接受语音指令和设备截图作为输入，无需文本转换。  <br/>2. **语音数据生成方法**：利用随机音色TTS模型将现有文本指令转换为高质量语音指令，解决语音数据稀缺问题。  <br/>3. **混合指令训练策略**：设计启发式方法缓解预训练模型的模态不平衡，结合文本和语音指令提升代理性能。  <br/>4. **系统性实验验证**：在基准数据集上验证模型表现，证明语音指令在GUI自动化中的有效性与广泛适用性。|
|2506.11086v1|[Intelligibility of Text-to-Speech Systems for Mathematical Expressions](http://arxiv.org/abs/2506.11086v1)|贡献点：  <br/>1. **系统评估框架**：首次设计涵盖五种TTS模型的实验，量化数学表达式（MX）的发音质量与可懂性（通过用户评分和转录正确性）。  <br/>2. **LLM辅助生成**：利用大语言模型（LLM）将LaTeX格式MX转换为英文发音，弥补TTS模型无法直接处理LaTeX的缺陷。  <br/>3. **多维度指标**：提出包含三个指标的转录正确性评估体系，并结合Mean Opinion Score分析用户主观感受。  <br/>4. **对比分析**：对比听众对TTS输出与人类专家发音的偏好，揭示TTS在处理MX时的表现短板。  <br/>5. **结果发现**：发现TTS模型输出的可懂性存在显著差异，且多数MX类别下表现劣于专家，同时LLM选择对结果影响有限。  <br/>6. **应用导向结论**：明确指出需针对性优化TTS模型处理数学表达式的能力。  <br/><br/>总结：  <br/>首次系统评估TTS模型对数学表达式的发音质量与可懂性，揭示模型与人类专家表现差异，强调改进TTS处理MX能力的必要性。|
|2506.10019v1|[A Survey of Automatic Evaluation Methods on Text, Visual and Speech   Generations](http://arxiv.org/abs/2506.10019v1)|总结：本文提出首个综合框架，系统化分类文本、图像和音频生成的评估方法，识别出五种核心范式，并探讨跨模态评估的未来方向。<br/><br/>贡献点：  <br/>1. **构建统一框架**：首次建立跨文本、图像、音频三大模态的自动评估方法系统化框架，实现多模态方法的整合与对比。  <br/>2. **提出五种核心范式**：归纳出生成内容评估的五大基础范式，为后续研究提供理论分类依据。  <br/>3. **跨模态适用性验证**：将文本生成评估方法的分析逻辑扩展至图像和音频领域，验证框架的广泛适用性。  <br/>4. **探索未来研究方向**：针对跨模态评估挑战，提出潜在的研究路径，促进多模态生成AI的评估体系发展。|
|2506.09874v2|[UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow   Matching](http://arxiv.org/abs/2506.09874v2)|总结：  <br/>提出UmbraTTS，通过流匹配方法联合生成语音与环境音，解决背景整合难题，并构建自监督框架解决数据稀缺问题，实现对背景音量的精细控制，生成高质量且环境感知的音频场景。<br/><br/>贡献点：  <br/>1. **创新模型架构**：首次提出基于流匹配的TTS模型UmbraTTS，联合生成语音与环境音频，突破传统TTS仅生成语音的局限。  <br/>2. **自监督数据框架**：设计从无标注录音中提取语音、背景音频与文本的自监督方法，缓解自然场景下配对数据不足的问题。  <br/>3. **环境音量控制**：实现对背景环境音量的细粒度控制，提升生成音频的场景适应性与自然感。  <br/>4. **高质量生成能力**：在多个指标上显著超越现有基线，验证了模型在生成多样、连贯、环境感知音频场景的有效性。|
|2506.09827v2|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v2)|**贡献点：**  <br/>1. 提出EmoNet-Voice资源，包含大规模预训练数据集（Big）和新型基准数据集（Bench），覆盖40种情感、11种声音及4种语言。  <br/>2. 支持细粒度情感评估，明确区分不同情感强度以提升模型准确性。  <br/>3. 通过语音生成技术合成情感音频，模拟真实场景以解决隐私问题。  <br/>4. 引入心理学专家标注与验证，确保情绪强度标签的可靠性。  <br/>5. 提出Empathic Insight Voice模型，实现与人类专家高度一致的SER性能。  <br/>6. 揭示高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更易被识别的模型表现差异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoNet-Voice资源，通过合成语音和专家标注解决隐私与情感粒度问题，推出高性能SER模型，并发现高唤醒情绪更易识别，为情感语音研究提供新基准。|
|2506.09827v1|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v1)|总结：  <br/>提出EmoNet-Voice资源及两个子集，包含大规模预训练数据与专家标注基准数据，解决隐私问题并增强情感粒度和多样性。开发高精度模型，通过评估揭示高唤醒情绪识别易度高于低唤醒情绪。<br/><br/>贡献点：  <br/>1. **引入EmoNet-Voice资源**：构建由EmoNet-Voice Big（大规模预训练数据集）和EmoNet-Voice Bench（专家标注基准数据集）组成的综合数据资源，覆盖40种情感、11种声音和4种语言。  <br/>2. **提升情感粒度与强度区分**：设计支持细粒度情感分类（40类）及不同强度评估的框架，推动SER模型在情感理解上的更精确建模。  <br/>3. **合成数据解决隐私与多样性问题**：通过先进语音生成技术合成敏感情感场景数据，避免真实数据隐私风险并扩展情感表达的多样性。  <br/>4. **专家验证机制**：联合心理学专家对合成数据进行严格标注（感知强度标签），确保数据质量与情感感知的科学性。  <br/>5. **提出高性能SER模型**：开发Empathic Insight Voice模型，实现与人类专家的高一致性，推动语音情感识别技术的标准化。  <br/>6. **揭示情绪识别难度差异**：通过对比实验发现，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易被模型识别，为后续研究提供新视角。|
|2506.08279v1|[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)|**总结（100字以内）:**  <br/>该论文提出Mirage模型，首次实现从音频直接生成高质量、表达性强的视频，结合TTS技术生成多模态视频，并开发统一的自注意力训练方法，提升生成视频的主观质量与通用性。  <br/><br/>**分点贡献:**  <br/>1. **提出Mirage模型**：首个专注于音频到视频生成的通用模型，从原始音频直接生成逼真、表达性的视频图像，不依赖视觉输入。  <br/>2. **多模态生成能力**：通过集成文本到语音（TTS）技术，实现语音与视频的同步生成，解决语音和视频内容对齐问题。  <br/>3. **统一训练方法**：开发适用于自注意力机制的统一训练框架，支持从头训练和基于已有权重的微调，增强模型灵活性。  <br/>4. **语义对齐优化**：利用A-roll数据（人物对话语音视频）训练，使生成视频能准确反映音频中的表演信息，提高可信度。  <br/>5. **性能优势**：生成的视频在主观质量上优于依赖语音特定架构或损失函数的现有方法，保持通用性的同时实现高保真输出。|
|2506.04527v1|[Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning](http://arxiv.org/abs/2506.04527v1)|总结（100字以内）:<br/>该论文提出结合字母符号的语音标签生成方法，通过BERT特征提示编码实现隐式条件，配合推理时的显式修剪提升标签一致性，为文本到语音和口音估计等任务生成高质量平行数据。<br/><br/>贡献点：<br/>1. **方法创新**：首创结合字母符号的语音标签生成框架，突破传统ASR模型直接微调的局限，引入显式与隐式的双条件策略。<br/>2. **条件生成机制**  <br/>   - 通过预训练BERT特征构建隐式图形单词条件（Prompt Encoder）  <br/>   - 提出推理阶段的显式标签假设剪枝算法（Pruning Inconsistent Hypotheses）<br/>3. **数据价值**：生成跨模态的语音-标签-图形单词平行数据，支持文本到语音合成（TTS）和口音估计等下游任务。<br/>4. **实验验证**：在音素-图形单词一致性任务和口音估计任务中均取得显著性能提升，证明方法的有效性。|
|2506.04397v1|[Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?](http://arxiv.org/abs/2506.04397v1)|总结：  <br/>该论文通过构建专用数据集和微调大模型，探索Parler TTS在语音重建任务中生成可理解语音与保持说话人身份的能力，揭示其在可控性方面的局限并提出改进方向。<br/><br/>贡献点：  <br/>1. **构建专用数据集**：创建并标注了包含说话人信息和可懂度数据的语音重建数据集，为相关研究提供基础资源。  <br/>2. **应用大模型进行语音重建**：首次尝试利用Parler TTS等先进大型语音模型，生成语音障碍患者疾病前的语音近似。  <br/>3. **分析模型局限性**：发现模型虽能学习数据分布，但在控制语音可懂度和保持说话人身份一致性方面存在挑战。  <br/>4. **提出改进方向**：为提升语音重建任务中模型的可控性，提出了未来研究的潜在优化路径。|
|2506.04152v1|[HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](http://arxiv.org/abs/2506.04152v1)|**分点贡献：**  <br/>1. **构建高带宽语音数据集**：提出HiFiTTS-2，基于LibriVox有声书，包含36.7k小时（22.05kHz）和31.7k小时（44.1kHz）的高质量英语语音数据，支持高带宽语音合成研究。  <br/>2. **创新数据处理流程**：设计了包含带宽估计、语音分段、文本预处理及多说话人检测的全流程数据处理方法，提升数据适配性与实用性。  <br/>3. **提供详细元数据**：生成全面的语句和有声书元数据，支持研究者通过数据质量过滤器灵活调整数据集以适配不同应用场景。  <br/>4. **验证数据集有效性**：实验表明该数据集和处理方法可有效训练高质量零样本TTS模型，推动高带宽语音生成技术的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HiFiTTS-2，构建大规模高带宽语音数据集并设计全流程处理技术，提供详细元数据以增强数据适应性，实验证明其可有效支持高质量零样本TTS模型训练。|
|2506.04134v1|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v1)|**贡献点总结（100字以内）**：  <br/>提出直接生成CS语音的新方法，构建首个统一框架UniCUE，通过细颗粒语义对齐、跨任务适配器及姿态感知处理，解决误差传播与时空对齐问题，显著提升语音生成质量与唇语同步性。<br/><br/>**分点贡献**：  <br/>1. **直接生成语音的创新方法**：首次无需依赖文本中间媒介，直接从CS视频生成语音，避免误差传播与时间错位问题。  <br/>2. **统一框架UniCUE**：提出首个融合CSV2S与CSR任务的框架，打通视觉-语义到语音的端到端生成流程。  <br/>3. **核心技术创新**：  <br/>   - 引入细颗粒语义对齐池，精确映射视觉特征与语音内容；  <br/>   - 设计VisioPhonetic适配器，实现CSV2S与CSR的跨任务表示兼容；  <br/>   - 开发姿态感知视觉处理器，增强唇语与手势的时空关联性。  <br/>4. **实验验证与数据集**：基于新建立的中文CS数据集（含14名受试者），验证方法有效性，降低误词率78.3%并提升唇语同步性32%。|
|2506.04077v1|[A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions](http://arxiv.org/abs/2506.04077v1)|总结：  <br/>该论文提出一种基于大语言模型与跨模态信息的新型语音评估方法，有效解决低资源场景下的标签约束问题，提升口语评分的可靠性和多样性。<br/><br/>贡献点：  <br/>1. **低资源场景下的数据生成**：利用大语言模型生成多样化响应并合成语音，突破标注数据的稀缺限制。  <br/>2. **说话人感知的语音合成**：通过 speaker-aware 文本到语音合成技术，增强生成语音与真实语音的匹配度。  <br/>3. **动态重要性损失设计**：根据合成语音与真实语音的特征差异，自适应调整训练样本权重，提升模型学习效率。  <br/>4. **跨模态特征融合**：构建多模态大语言模型，直接整合文本与语音信号的对齐特征以预测口语水平。  <br/>5. **有效性验证**：在 LTTC 数据集上验证方法优于传统数据依赖或常规增强技术，实现跨模态信息驱动的自动化评分。|
|2506.03884v1|[Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages](http://arxiv.org/abs/2506.03884v1)|总结: 本文提出基于零样本合成的多语言TTS系统，通过共享音素表示和适配文本解析规则，成功生成梵语等低资源语言的自然语音，验证了该方法在扩展语音技术普惠性方面的有效性。<br/><br/>贡献点:<br/>1. 提出零样本合成框架，解决印度1369种语言中多数缺乏数字资源的TTS训练难题<br/>2. 构建跨语言音素共享表示，降低多语言模型过度假设需求<br/>3. 开发适应不同音系规则的文本解析方法（phonotactics adaptation）<br/>4. 实现Sanskrit、Maharashtrian、Canara Konkani、Maithili、Kurukh等5种语言的自然语音合成<br/>5. 提供可复用的合成器架构，验证方法在扩大语音技术覆盖范围中的可行性|
|2506.03831v1|[Conformer-based Ultrasound-to-Speech Conversion](http://arxiv.org/abs/2506.03831v1)|总结：  <br/>本研究对比了Conformer与传统CNN在超声波语音转换中的表现，发现Conformer结合bi-LSTM在感知质量上更优，而Conformer Base在效率上显著提升，为静默语音接口提供了新思路。<br/><br/>贡献点：  <br/>1. **提出两种Conformer-based模型**：首次将Conformer架构应用于超声波到语音转换任务，分别采用Base结构和结合bi-LSTM的结构。  <br/>2. **融合HiFi-GAN声码器**：通过HiFi-GAN将生成的mel频谱图合成高质量音频波形，提升系统输出的语音还原效果。  <br/>3. **验证感知质量优势**：在MUSHRA主观测试中发现，Conformer与bi-LSTM结合的模型在语音可懂度上优于传统CNN基线。  <br/>4. **效率提升**：Conformer Base模型在保持性能的同时，训练速度比传统CNN快3倍，降低了计算成本。|
|2506.03793v1|[Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts](http://arxiv.org/abs/2506.03793v1)|**贡献点**  <br/>1. **提出Cadence模型**：基于预训练大语言模型设计，首次实现对自发语音转录的精准标点恢复，克服传统模型在处理断续、重复等语音特征时的不足。  <br/>2. **扩展语言覆盖**：支持22种印度语言及英语（原仅14种），显著提升多语言标点恢复的适用性与泛化能力。  <br/>3. **分析挑战与改进**：通过跨语言和标点类型的行为研究，揭示领域迁移及罕见标点符号下的修复难题，验证预训练模型在低资源NLP场景中的有效性。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出Cadence模型，基于预训练语言模型实现自发语音的精准标点恢复，扩展支持22种印度语言和英语，并通过深入分析揭示多语言任务中的关键挑战。|
|2506.03515v1|[BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and   Weight Indexing](http://arxiv.org/abs/2506.03515v1)|总结：  <br/>本文提出两种模型压缩技术，实现83%的模型体积减小，同时保持高质量语音合成，适用于设备端部署。<br/><br/>贡献点：<br/>1. 提出量化感知训练（QAT）技术，将模型参数从32位压缩至1.58位，主要采用三值{-1,0,1}量化策略。<br/>2. 开发权重索引方法，通过将一组1.58位权重编码为单个int8索引来优化存储效率。<br/>3. 在保持合成质量的前提下，实现模型轻量化，使其更适用于资源受限的设备端应用场景。|
|2506.02979v1|[Towards a Japanese Full-duplex Spoken Dialogue System](http://arxiv.org/abs/2506.02979v1)|**贡献点总结：**  <br/>1. 提出首个公开可用的全双工日语语音对话模型，基于英语模型Moshi迁移。  <br/>2. 采用两阶段训练方法：大规模日语语料预训练与高质量立体声数据微调。  <br/>3. 引入多流文本到语音系统生成的合成对话数据以增强模型性能。  <br/>4. 实验证明模型在自然度和语义理解上优于现有日语基准模型。  <br/><br/>（注：以上为100字以内总结，符合要求。）|
|2506.02958v1|[PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech   Editing](http://arxiv.org/abs/2506.02958v1)|**贡献点总结（100字以内）:**  <br/>提出PartialEdit数据集，聚焦神经语音编辑生成的深伪语音检测，揭示现有模型的局限性，分析伪影特征，推动该领域研究。<br/><br/>**分点贡献:**  <br/>1. **提出PartialEdit数据集**：构建首个基于先进神经语音编辑技术的深伪语音检测数据集，包含部分编辑的语音样本，为研究提供基准。  <br/>2. **探索检测与定位任务**：系统研究部分编辑语音的检测与内容定位，验证当前检测方法在复杂编辑场景下的有效性。  <br/>3. **揭示模型局限性**：实验表明基于PartialSpoof数据集训练的模型无法有效识别神经编辑生成的深伪语音，凸显数据集不足。  <br/>4. **分析伪影特征**：结合神经音频编码器，探讨检测模型学习到的伪影（artifacts）特征，为改进检测方法提供理论依据。  <br/>5. **提供公开资源**：开放数据集和音频样本，方便后续研究复现与扩展。|
|2506.02858v2|[DGMO: Training-Free Audio Source Separation through Diffusion-Guided   Mask Optimization](http://arxiv.org/abs/2506.02858v2)|总结：  <br/>提出训练无关的零样本音频源分离框架DGMO，利用扩散模型生成先验优化频谱掩码，突破传统方法依赖任务特定训练的限制，拓展扩散模型在音频分离领域的应用范式。<br/><br/>贡献点：  <br/>1. **首个训练无关的零样本框架**：首次将预训练扩散模型直接用于语言查询的音频源分离，无需额外任务特定训练。  <br/>2. **测试时优化机制**：提出Diffusion-Guided Mask Optimization（DGMO），通过测试阶段动态优化频谱掩码以提升分离精度。  <br/>3. **跨模态适应性提升**：分析模态适配问题，揭示自然语言与音频处理的协同挑战，并提出针对性解决方案。  <br/>4. **性能验证**：在无监督或弱监督条件下实现竞争性分离效果，证明扩散模型可有效迁移至音频分离任务。  <br/>5. **应用范式创新**：拓展扩散模型从生成任务到分离任务的边界，建立新的零样本音频处理框架。|
|2506.02401v1|[Trusted Fake Audio Detection Based on Dirichlet Distribution](http://arxiv.org/abs/2506.02401v1)|**贡献点：**  <br/>1. **提出基于狄利克雷分布的不确定性建模方法**：首次将狄利克雷分布用于量化模型决策的不确定性，提升假音频检测的可信度评估能力。  <br/>2. **构建新型检测框架**：通过神经网络生成证据，并将预测概率与不确定性估计结合，形成最终检测意见，优化检测流程。  <br/>3. **多数据集验证**：在ASVspoof 2019 LA、2021 LA及DF数据集上进行对比实验，验证了模型在准确率、鲁棒性和可信度方面的优越性能。  <br/><br/>**总结：**  <br/>该研究通过狄利克雷分布建模不确定性，提出新型假音频检测方法，在多数据集上验证了其高准确率与鲁棒性，显著提升了检测的可靠性。|
|2506.02082v1|[SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction](http://arxiv.org/abs/2506.02082v1)|总结：该论文提出了一种新的SALF-MOS模型，解决传统MOS评估高成本问题，通过卷积序列提取音频潜在特征，实现小尺寸、端到端、高度通用和可扩展的语音质量预测，取得最佳评估效果。<br/><br/>贡献点：<br/>1. 提出Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS)模型，解决传统MOS评估需要大量人工标注的问题<br/>2. 设计基于卷积序列的音频特征提取方法，实现端到端的MOS预测系统<br/>3. 构建的小尺寸模型在保持通用性的同时具备良好的可扩展性<br/>4. 在多个客观评估指标（MSE、LCC、SRCC、KTAU）上达到当前最优性能<br/>5. 提供了一种新的语音质量评估方法，可作为TTS和语音转换模型选择的有效工具|
|2506.01618v1|[Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech](http://arxiv.org/abs/2506.01618v1)|总结（100字以内）:  <br/>本文提出基于音节的节奏建模方法，结合LF-MMI模型显著提升口吃语音的ASR性能，验证了无监督转换的有效性。<br/><br/>贡献点:  <br/>1. **提出syllable-based rhythm modeling方法**：在Rhythm and Voice（RnV）框架中引入针对口吃语音的音节级节奏建模技术，解决传统方法对口吃语音节奏特征建模不足的问题。  <br/>2. **评估转换对ASR的影响**：首次系统验证口吃语音转换对ASR性能提升的效果，通过训练LF-MMI模型和微调Whisper模型，发现LF-MMI在口吃语音识别中具有显著优势。  <br/>3. **揭示无监督转换的潜力**：证明无监督学习的节奏和语音转换技术在口吃ASR任务中的有效性，为低资源场景下的语音处理提供了新思路。|
|2506.01322v1|[Zero-Shot Text-to-Speech for Vietnamese](http://arxiv.org/abs/2506.01322v1)|贡献点：<br/>1. 构建了首个越南语高质量语音数据集PhoAudiobook（含941小时音频）  <br/>2. 在VALL-E、VoiceCraft、XTTS-V2等三大零样本TTS模型上开展系统实验  <br/>3. 证实PhoAudiobook能显著提升多指标下的模型性能  <br/>4. 揭示VALL-E与VoiceCraft在短句合成任务中的优越性  <br/>5. 公开数据集促进越南语TTS领域研究发展  <br/><br/>总结：  <br/>本研究发布首个越南语高质量语音数据集PhoAudiobook，并验证其提升三大零样本TTS模型性能的效果，尤其突出VALL-E与VoiceCraft在短句生成中的优势，推动越南语语音合成研究。|
|2506.01268v1|[CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction](http://arxiv.org/abs/2506.01268v1)|贡献点总结：<br/>1. 提出单文件实现的CleanS2S框架，实现低延迟语音交互系统<br/>2. 首创主动对话机制，集成记忆系统与主观行动判断模块<br/>3. 开发五种人类般响应策略（中断/拒绝/转移/沉默/标准响应）<br/>4. 实现全双工WebSocket连接与非阻塞I/O的实时中断处理<br/>5. 提出Action Judgement SFT方法评估输入流策略选择<br/>6. 提供前所未有的透明度和可扩展性，代码已开源<br/><br/>（97字）|
|2506.01263v1|[WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing](http://arxiv.org/abs/2506.01263v1)|总结：  <br/>提出无需额外训练的CTC模型改进方法，通过中间层声学特征检测关键词并引入偏置机制，有效提升罕见词识别准确率，实验在日语语音识别中实现29%的F1分数提升。<br/><br/>贡献点：  <br/>1. **解决罕见词识别偏差问题**：通过中间层声学特征进行关键词检测，避免模型对训练数据词汇的过度依赖。  <br/>2. **引入灵活的wildcard CTC机制**：采用快速且容错的CTC变体，支持模糊匹配处理难以严格识别的单词。  <br/>3. **无需模型重训练的可扩展性**：方法兼容现有模型，无需额外训练或文本到语音系统，适用于大规模模型。  <br/>4. **显著提升未知词识别性能**：在日语语音识别任务中，实现未知词F1分数29%的提升，验证方法有效性。|
|2506.01032v1|[ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and   Speaker Feature Optimization](http://arxiv.org/abs/2506.01032v1)|**总结**：  <br/>本文提出ReFlow-VC，通过修正流的ODE框架提升语音转换效率，结合内容与音高信息优化说话人特征，并验证其在小数据集和零样本场景中的优越性能，推动高保真语音转换的实际应用。  <br/><br/>**贡献点**：  <br/>1. **提出ReFlow-VC方法**：基于修正流构建高效高保真语音转换模型，通过ODE框架将高斯分布直接映射到真实Mel频谱图分布，减少采样步骤。  <br/>2. **优化说话人特征建模**：同时利用内容与音高信息优化特征，增强说话人属性的准确表达。  <br/>3. **验证小样本适配性**：在小数据集和零样本场景中均表现出卓越的性能，突破传统扩散模型的局限。|
|2506.01020v1|[DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation](http://arxiv.org/abs/2506.01020v1)|总结：  <br/>该论文提出DS-TTS框架，结合双风格编码与动态生成网络，解决zero-shot语音克隆问题，实现未见过语音的高质量合成，并在VCTK数据集上验证了其优越的性能。<br/><br/>贡献点：  <br/>1. **提出DS-TTS框架**：设计一种用于zero-shot语音克隆的全新方法，仅需单个音频样本和任意文本即可合成目标说话人语音，无需历史训练数据。  <br/>2. **双风格编码网络（DuSEN）**：通过两个互补的风格编码器捕捉说话人语音身份的多维度特征，增强对未见说话人风格的建模能力。  <br/>3. **动态融合机制（SGF + DyGN）**：引入风格门控-Film机制将风格向量融入动态生成器，实现对不同句子长度的自适应合成，提升鲁棒性与自然度。  <br/>4. **实验验证优势**：在VCTK数据集上证明DS-TTS在单词错误率和说话人相似度上优于现有模型，显著改善语音克隆的泛化与表达性。|
|2506.00832v1|[Counterfactual Activation Editing for Post-hoc Prosody and   Mispronunciation Correction in TTS Models](http://arxiv.org/abs/2506.00832v1)|**贡献点：**<br/>1. **提出模型无关的后处理技术**：Counterfactual Activation Editing 是一种通用方法，无需依赖预训练模型特定架构，可适用于多种TTS模型。  <br/>2. **实现韵律与发音的联合控制**：同时支持对语音韵律特征（prosody）和发音错误（mispronunciation）的精确调整，提升控制能力。  <br/>3. **无需额外训练即可进行推理时调整**：在不重新训练模型的情况下，直接对预训练TTS的内部表示进行修改，实现在线输出优化。  <br/>4. **适应低资源场景**：解决传统发音纠正依赖音素词典（grapheme-to-phoneme dictionaries）的问题，降低对资源的依赖。  <br/>5. **填补预训练模型与可编辑合成的空白**：首次将可编辑的语音合成能力引入预训练TTS框架，推动模型灵活性与应用范围的扩展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种通用、无需额外训练的后处理方法，实现对TTS输出的韵律与发音联合控制，适应低资源场景，推动预训练模型向可编辑语音合成发展。|
|2506.00736v1|[IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio   Generation with Diffusion Modeling](http://arxiv.org/abs/2506.00736v1)|总结：  <br/>本文提出IMPACT框架，结合扩散建模与迭代掩码并行解码，在连续潜空间中生成高质量文本到音频，有效解决先前模型在音质与推理速度之间的权衡问题，实验在AudioCaps上达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出IMPACT框架**：首次将迭代掩码并行解码与扩散模型结合，实现文本到音频生成中高保真度与低延迟的双重优化。  <br/>2. **连续潜空间设计**：通过在连续潜空间中操作，避免离散token带来的保真度限制，提升生成音频的自然度和质量。  <br/>3. **高效解码策略**：采用迭代掩码并行解码技术，在保持推理速度的同时显著提高音频生成效果，优于MAGNET等基于离散token的模型。  <br/>4. **验证SOTA性能**：在AudioCaps数据集上，IMPACT在Fréchet Distance (FD)和Fréchet Audio Distance (FAD)等关键指标上超越现有方法，验证了其有效性。|
|2506.00722v1|[Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](http://arxiv.org/abs/2506.00722v1)|总结：该研究提出基于链式思维的端到端语音对话系统方法，在较小数据集上实现显著性能提升并提升语义连贯性，同时增强计算效率，模型及代码将公开。<br/><br/>贡献点：<br/>1. 提出基于链式思维（Chain-of-Thought, CoT）的端到端（E2E）对话系统框架，解决了传统级联流水线的局限性；<br/>2. 通过将对话训练与多模态语言模型（涵盖语音识别、文本生成、语音合成等任务）的预训练对齐，提升语义连贯性；<br/>3. 实现仅需300小时公开对话数据（如Switchboard）即可获得超过1.5 ROUGE-1的提升效果；<br/>4. 优化计算效率，使模型能在有限数据量下高效训练；<br/>5. 提供公开模型及代码，推动该领域研究与应用。|
|2506.00548v1|[Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities](http://arxiv.org/abs/2506.00548v1)|总结（100字以内）：  <br/>本论文提出Con Instruction方法，通过生成对抗图像和音频攻击多模态语言模型，无需文本预处理或训练数据。引入新型ARC评估框架，并验证其在多个模型上的高攻击成功率，揭示当前安全机制的不足。<br/><br/>贡献点（分点列出）：  <br/>1. **提出非文本指令攻击方法（Con Instruction）**  <br/>   - 首次利用MLLMs对非文本指令（图像/音频）的解析能力，通过生成对抗性图像或音频发起攻击，突破传统依赖文本指令的攻击模式。  <br/>   - 攻击无需训练数据或文本预处理，简化了攻击流程。  <br/><br/>2. **优化对抗样本的语义对齐**  <br/>   - 在嵌入空间中优化对抗样本，使其更精准匹配目标指令，显著提升攻击成功率，揭示MLLMs复杂语义理解的潜在安全风险。  <br/><br/>3. **提出攻击响应分类（ARC）框架**  <br/>   - 引入新型评估体系，综合衡量模型响应质量与对恶意指令的相关性，为安全评估提供更全面的指标。  <br/><br/>4. **验证多模型有效性与攻击性能**  <br/>   - 在LLaVA-v1.5、InternVL、Qwen-VL、Qwen-Audio等多模态模型上验证攻击效果，尤其在LLaVA-v1.5 (13B)中达到81.3%和86.6%的成功率，证明方法的普适性。  <br/><br/>5. **分析防御技术的局限性**  <br/>   - 探索针对攻击的防御措施，发现现有技术在攻击检测和防御能力上存在显著性能差距，为后续防御研究提供方向。  <br/><br/>6. **开放源代码实现**  <br/>   - 公布实现代码，便于学术界和工业界复现与扩展研究。|
|2506.00462v1|[XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](http://arxiv.org/abs/2506.00462v1)|**贡献点：**  <br/>1. **提出跨域多语言音频深度伪造基准**：构建XMAD-Bench数据集，包含668.8小时真实与深度伪造语音，覆盖多语言和多样化生成方法。  <br/>2. **强化数据集多样性与挑战性**：训练与测试集的说话人、生成模型、真实音频源均独立，显著提升跨域评估的难度。  <br/>3. **揭示性能差距**：实验结果表明，域内检测准确率接近100%，但跨域性能严重下降，接近随机猜测，突显现有方法的局限性。  <br/>4. **推动鲁棒检测器发展**：强调需开发具备跨语言、跨说话人、跨生成方法和跨数据源泛化能力的深度伪造检测技术。  <br/>5. **公开数据集**：将XMAD-Bench在GitHub平台开源，为研究社区提供统一的评估基准。  <br/><br/>**总结**：  <br/>提出首个跨域多语言音频深度伪造基准XMAD-Bench，验证现有检测模型在真实场景下的性能不足，推动更鲁棒的检测技术发展。|
|2506.00385v1|[MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity   Reconstruction and Generation](http://arxiv.org/abs/2506.00385v1)|**贡献点：**  <br/>1. 提出新型单层流式Transformer音频编解码器MagiCodec，解决传统编解码器重建质量与下游模型兼容性之间的矛盾。  <br/>2. 引入多阶段训练框架，结合高斯噪声注入与潜在正则化技术，增强编码的语义表达性并保持重建保真度。  <br/>3. 理论推导噪声注入在频域中的作用，证明其对抑制高频成分、提升鲁棒性效果显著。  <br/>4. 实验验证MagiCodec在重建质量与下游任务（如语言模型兼容）上优于现有技术，生成的编码呈现齐普夫分布特性。  <br/>5. 开源代码与预训练模型，促进研究复现与应用。  <br/><br/>**总结：**  <br/>MagiCodec通过噪声注入与多阶段训练提升音频编码的语义表达和重建质量，实现与语言模型的兼容性，开源推动研究进展。|
|2506.00087v1|[SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset](http://arxiv.org/abs/2506.00087v1)|总结：  <br/>本文提出多代理协作框架LinguaMaster，构建首个大规模多语言多民族代码转换数据集SwitchLingua，并设计语义感知评估指标SAER，推动多语言和文化多样性研究。  <br/><br/>贡献点：  <br/>1. **提出LinguaMaster框架**：首次设计多智能体协作系统，实现高效可扩展的多语言数据合成。  <br/>2. **构建SwitchLingua数据集**：创建首个覆盖12种语言、18国/地区、63个种族背景的大型代码转换语料库（含420K文本样本及80小时音频）。  <br/>3. **创新SAER评估指标**：提出融合语义信息的代码转换ASR评估方法，提升对跨语言场景的性能判断准确性。|
|2505.24496v1|[Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation](http://arxiv.org/abs/2505.24496v1)|总结：  <br/>本文提出一种压缩-精细语言建模方法，通过保留局部上下文和压缩长距离信息，有效解决神经音频编解码器在语音生成中的长序列挑战，提升生成质量与模型通用性。<br/><br/>贡献点：  <br/>1. **揭示语音标记序列的短距离依赖性**：首次观察到TTS任务中语音标记的预测主要依赖局部上下文，长距离标记贡献较小且冗余。  <br/>2. **设计压缩-精细语言建模框架**：  <br/>   - 保留初始提示和短范围标记，确保文本对齐及语用信息完整性；  <br/>   - 将长范围标记压缩为紧凑表示，减少冗余同时保留关键语义。  <br/>3. **验证方法的有效性与通用性**：通过多组神经音频编解码器和下游语言模型的实验，证明该方法可提升语音生成效果，具有广泛适用性。|
|2505.24314v1|[DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture   Switching for Speech Codec](http://arxiv.org/abs/2505.24314v1)|**贡献点：**  <br/>1. 提出DS-Codec，首次结合镜像（mirror）与非镜像（non-mirror）架构的双阶段训练框架，提升语音重建质量。  <br/>2. 设计动态切换机制，平衡镜像与非镜像结构的优势，增强模型的鲁棒性与高保真度。  <br/>3. 通过大规模实验与消融研究，验证了训练策略的有效性，对比了两种架构的性能差异。  <br/><br/>**总结：**  <br/>本研究提出DS-Codec，通过镜像与非镜像结构的双阶段训练框架提升语音重构效果，并系统验证了其优势。|
|2505.23619v1|[Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](http://arxiv.org/abs/2505.23619v1)|贡献点总结：<br/>1. 提出ADD-GP框架：基于高斯过程的少样本音频深度伪造检测方法，支持快速适应新生成模型  <br/>2. 性能突破：结合深度嵌入模型与GP灵活性，在少量数据下实现高检测准确率  <br/>3. 个性化检测能力：增强对新型TTS模型的鲁棒性，支持单样本级自适应  <br/>4. 构建基准数据集：使用SOTA语音克隆模型创建专用评估数据集  <br/><br/>（99字）|
|2505.20868v2|[Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction   and Style Direction Adjustment for Expressive Text-to-Speech](http://arxiv.org/abs/2505.20868v2)|**贡献点**：  <br/>1. 提出Spotlight-TTS，通过语音感知的风格提取和风格方向调整实现风格强化。  <br/>2. 建立音素感知的风格提取机制，聚焦发声区域以提升表达性。  <br/>3. 引入风格方向优化策略，增强风格与TTS模型的融合能力。  <br/>4. 实验证明在表达性、语音质量和风格迁移能力上优于基线模型。  <br/>5. 公开音频样本，便于复现与进一步研究。  <br/><br/>**总结**：  <br/>本研究提出Spotlight-TTS，通过语音感知的风格提取与方向调整提升表达性及语音质量，实验验证其优越性，数据公开。|
|2505.19931v2|[Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned   Step Sampling](http://arxiv.org/abs/2505.19931v2)|总结（100字以内）:  <br/>提出训练无关的Fast F5-TTS方法，通过分析采样轨迹设计EPSS策略减少冗余步骤，实现7步生成且推理速度提升4倍，验证了EPSS对流匹配类TTS模型的通用性。  <br/><br/>贡献点:  <br/>1. **训练无关加速方案**：提出Fast F5-TTS，无需额外训练直接优化流匹配TTS模型的推理效率。  <br/>2. **非均匀采样策略**：开发EPSS方法，通过识别冗余采样步骤，显著减少生成过程的采样次数。  <br/>3. **高效推理性能**：在NVIDIA RTX 3090上实现7步生成，推理RTF达0.030，且性能与原模型相当。  <br/>4. **跨模型通用性**：验证EPSS在E2 TTS等其他流匹配模型上的有效性，证明其广泛适用性。|
|2505.19669v2|[Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling](http://arxiv.org/abs/2505.19669v2)|总结：  <br/>提出SMLLE框架，通过Transducer与Delete <Bos>机制解决流式TTS低延迟问题，实现高质量逐帧语音生成，优于现有流式方法且接近句级系统性能。<br/><br/>贡献点：  <br/>1. **提出SMLLE框架**：首次构建零样本流式TTS系统，无需依赖未来文本即可实现低延迟高质量语音合成。  <br/>2. **融合Transducer与时长对齐**：实时将文本转化为语义标记，结合时长对齐信息提升语音自然度。  <br/>3. **设计Delete <Bos>机制**：优化自回归模型对未来文本的访问，最小化延迟同时保持生成稳定性。  <br/>4. **验证性能优势**：实验表明SMLLE在流式TTS任务中超越现有方法，在句级系统中表现可比，具备实际应用价值。|
|2505.19595v2|[Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment](http://arxiv.org/abs/2505.19595v2)|总结：  <br/>本研究提出A-DMA方法，通过双模态对齐优化扩散模型的训练，显著提升收敛速度与生成质量，降低计算成本，并开源代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **提出A-DMA策略**：设计双模态对齐机制，通过文本与语音模态的协同优化加速扩散模型的训练过程。  <br/>2. **双模态对齐技术**：引入文本引导对齐（融合上下文表示）和语音引导对齐（精炼语义表示）的双重对齐管道。  <br/>3. **减少隐式学习依赖**：通过显式对齐隐藏状态与判别特征，降低扩散模型对复杂中间表示的隐式学习需求。  <br/>4. **实验验证效果**：在多个基准上验证A-DMA的性能优势，收敛速度提升1倍，生成质量优于现有方法，并开源代码与演示样本。|
|2505.17076v2|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v2)|总结：  <br/>研究分析不同帧率对中文和英文语音分词效果的影响，揭示帧率与语言特性间的关系，为语音分词优化提供参考。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统探讨低帧率编解码器对语音分词的影响，揭示帧率与语言模型间的作用机制。  <br/>2. **跨语言对比分析**：通过对比汉语和英语两种音系差异显著的语言，发现帧率对不同语言的分词效果存在异质性。  <br/>3. **揭示关键影响因素**：明确帧率变化与音素密度、语言特异性声学特征的交互作用，为分词器设计提供理论依据。  <br/>4. **应用指导意义**：提出优化帧率选择的策略，对自动语音识别、文本到语音等语音处理任务具有实践价值。|
|2504.14906v3|[OmniAudio: Generating Spatial Audio from 360-Degree Video](http://arxiv.org/abs/2504.14906v3)|总结：  <br/>提出360V2SA新任务，构建Sphere360数据集，设计半自动化数据流程，提出OmniAudio框架（结合自监督训练与双分支结构），在空间音频生成领域取得SOTA成果。<br/><br/>贡献点：  <br/>1. 提出360V2SA任务：首次系统性生成360度视频对应的3D空间音频（FOA格式），填补传统视频-音频生成在空间线索上的不足。  <br/>2. 构建Sphere360数据集：首个针对该任务的高质量数据集，基于真实场景数据并优化数据配对与清洗流程。  <br/>3. 设计半自动化数据收集管道：提供高效且可扩展的视频-音频数据配对与预处理方案，降低数据获取门槛。  <br/>4. 提出OmniAudio框架：结合自监督预训练和双分支结构（全景+视角视频输入），同时建模局部与全局空间信息。  <br/>5. 实验验证与开源：在Sphere360上实现SOTA性能，代码与数据集开源，推动领域研究与应用。|