|Source|Title|Summary|
|---|---|---|
|2507.06483v1|[Learning Japanese with Jouzu: Interaction Outcomes with Stylized   Dialogue Fictional Agents](http://arxiv.org/abs/2507.06483v1)|**贡献点：**  <br/>1. **理论创新**：首次系统探讨风格化、有声代理在多模态语言学习环境中的作用，揭示了声音、人格和语言风格对用户体验、动机与学习策略的关键影响。  <br/>2. **跨文化实证分析**：通过混合方法验证了代理设计对不同语言水平和文化背景用户的影响差异，为文化敏感性设计提供了数据支持。  <br/>3. **设计指导**：提出了针对社交响应性及用户参与度的语音交互系统优化方向，强调情感化与文化风格化的重要性。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过混合方法分析了风格化有声代理在语言学习中的影响，发现其设计要素显著提升用户体验与动机，为跨文化、多模态交互系统的优化提供了理论依据与实践指导。|
|2507.04094v1|[MMMOS: Multi-domain Multi-axis Audio Quality Assessment](http://arxiv.org/abs/2507.04094v1)|总结（100字以内）:  <br/>提出MMMOS系统，实现跨多领域音频质量评估，引入四个正交维度，融合多编码器并采用多种策略，显著提升评估性能。<br/><br/>贡献点：  <br/>1. **多领域、无参考评估**：首次构建跨语音、音乐、环境音的无参考音频质量评估系统，突破传统语音MOS评估的局限性。  <br/>2. **四维质量指标**：定义四个正交评估维度（生产质量、生产复杂度、内容愉悦度、内容有用性），覆盖更全面的听觉感知因素。  <br/>3. **多模态编码器融合**：结合WavLM、MuQ和M2D三个预训练模型的帧级嵌入，提升多领域音频特征提取能力。  <br/>4. **优化聚合与损失策略**：设计三种聚合方法和四种损失函数，系统性优化评估模型性能。  <br/>5. **模型集成优势**：通过集成前八名模型，实现20-30%均方误差下降和4-5%Kendall's τ提升，部分指标登顶国际榜单。|
|2507.04036v1|[PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)|总结:  <br/>提出跨模态生成系统PresentAgent，突破传统幻灯片/摘要生成局限，实现视频内容精准同步与人类风格演示，同时构建统一评估框架PresentEval，验证了方法在内容、视觉和理解维度的竞争力。<br/><br/>贡献点:  <br/>1. **提出新型多模态转换框架**：首次实现将长文档直接转化为具有同步视觉与语音内容的演示视频，突破传统静态幻灯片和文本摘要的生成限制。  <br/>2. **模块化生成流程设计**：构建系统化管道，包括文档分段、视觉帧渲染、语境化语音生成及音视频精准对齐的全流程技术整合。  <br/>3. **创新评估体系PresentEval**：基于视觉语言模型设计统一的三维度（内容忠実度、视觉清晰度、观众理解）评估框架，填补多模态演示视频评价空白。  <br/>4. **实验证明性能优势**：在30组文档-演示数据对上验证，展示系统在多项指标上接近人类表现，体现可控多模态代理的潜力。  <br/>5. **开源实现促进应用**：提供完整代码库，推动技术复现与实际落地。|
|2507.03912v1|[Prosody Labeling with Phoneme-BERT and Speech Foundation Models](http://arxiv.org/abs/2507.03912v1)|总结（100字以内）:  <br/>提出结合声学与语言特征的模型，用于自动韵律标注，通过SSL及预训练语言模型提升预测准确性，实验验证在日语数据中效果显著，为TTS提供可控标注数据。<br/><br/>贡献点:  <br/>1. **多模态特征融合**：首次结合声学特征（自监督学习模型/Whisper编码器）和语言特征（PnG BERT、PL-BERT），提升韵律标注的鲁棒性。  <br/>2. **预训练模型应用**：引入预训练语言基础模型作为语音输入处理模块，增强对韵律特征的语义建模能力。  <br/>3. **音素级标注方法**：提出针对音素级别的韵律标签预测框架，支持更细粒度的语音控制需求。  <br/>4. **实验验证突破**：在日语韵律标注任务中（含音高重音、短语边界），验证结合声学与语言模型的方案显著优于单一模态方法，准确率达94.3%（短语边界）。|
|2507.03887v1|[Traceable TTS: Toward Watermark-Free TTS with Strong Traceability](http://arxiv.org/abs/2507.03887v1)|**总结（100字以内）:**  <br/>本文提出首个无水印的可追溯TTS框架，通过联合训练提升模型泛化能力与音频质量，解决传统水印方法对质量的破坏和易被攻击的问题，并开源代码推动相关研究发展。<br/><br/>**贡献点分点:**  <br/>1. **首次提出无水印TTS可追溯框架**：实现对合成语音的追溯而无需依赖传统水印技术，解决水印对语音质量的干扰和被伪造的风险。  <br/>2. **引入联合训练方法**：通过同步训练TTS模型和判别器，显著增强模型的可追溯性泛化能力。  <br/>3. **保持或提升音频质量**：在实现可追溯性的同时，避免了现有方法对语音质量的负面影响，甚至优化了音质。  <br/>4. **开创性技术方案**：为水印-free且具有强可追溯性的TTS技术提供了全新思路，填补领域研究空白。  <br/>5. **代码开源以促进发展**：通过释放代码，推动语音安全和模型可追溯性相关技术的进一步研究与应用。|
|2507.01805v1|[A Dataset for Automatic Assessment of TTS Quality in Spanish](http://arxiv.org/abs/2507.01805v1)|**贡献点：**  <br/>1. **首个西班牙语TTS评估数据集**：构建了包含4,326个音频样本（涵盖52种TTS系统和人类语音）的数据库，填补了西班牙语领域空白。  <br/>2. **标准化主观测试**：基于ITU-T Rec. P.807标准设计测试流程，由92名参与者完成标注，确保数据质量。  <br/>3. **验证数据集实用性**：通过训练自动自然度预测模型（两种方法：英语模型微调、自监督语音模型下游网络），证明其有效性。  <br/>4. **模型性能指标**：在五点MOS尺度上达成0.8的平均绝对误差，体现评估体系的可靠性。  <br/>5. **数据集分析**：展示了数据集的质量、多样性及其对推动西班牙语TTS研究的潜力。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了首个西班牙语TTS系统自然度评估数据集，结合标准化主观测试与模型验证，显著提升了自然度预测的准确性，为该领域的研究提供了高质量、多样化的资源基础。|
|2507.00808v2|[Multi-interaction TTS toward professional recording reproduction](http://arxiv.org/abs/2507.00808v2)|**贡献点（分点）:**  <br/>1. 提出首个基于多步骤用户交互的TTS框架，模拟语音导演与演员的协作流程，实现语音风格的迭代优化。  <br/>2. 构建支持用户反馈的新型数据集，为细粒度风格调整提供训练与验证资源。  <br/>3. 验证了模型在实际场景中通过用户指令进行风格修正的有效性，证明其具备多交互能力。  <br/>4. 实现了用户对合成语音的直观、快速编辑，弥补传统TTS系统缺乏反馈机制的不足。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出多步骤交互TTS方法，通过模拟语音导演与演员的协作过程，支持用户对合成语音的迭代风格调整，解决了传统TTS缺乏反馈机制的问题，并验证了其有效性。|
|2507.00227v1|[Investigating Stochastic Methods for Prosody Modeling in Speech   Synthesis](http://arxiv.org/abs/2507.00227v1)|**贡献点：**  <br/>1. 探索性提出将随机方法（如Normalizing Flows、Conditional Flow Matching、Rectified Flows）应用于生成具有表现力的韵律（prosody）；  <br/>2. 与传统确定性方法及真实人类语音进行对比实验，验证随机方法在自然度与表现力上的有效性；  <br/>3. 首次通过严格评估证明，随机方法能捕捉人类语音的内在变异性，生成的韵律质量可媲美人类；  <br/>4. 引入可调采样温度参数，显著提升韵律生成的可控性与灵活性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出随机方法在文本到语音合成中生成自然韵律的潜力，通过对比实验验证其与人类语音相当的表现力，并通过温度参数调控增强系统的可控性，为语音生成提供新方向。|
|2506.23869v1|[Scaling Self-Supervised Representation Learning for Symbolic Piano   Performance](http://arxiv.org/abs/2506.23869v1)|总结：  <br/>该研究提出基于大规模符号钢琴数据训练的生成模型，创新性地应用SimCLR框架生成对比MIDI嵌入，在音乐生成和分类任务中均展现优越性能与高效泛化能力。<br/><br/>贡献点：  <br/>1. 提出使用60,000小时符号钢琴数据预训练的生成模型，结合微调策略提升音乐生成质量。  <br/>2. 首次将SimCLR框架拓展至符号音乐领域，生成通用对比MIDI嵌入。  <br/>3. 在钢琴延续一致性任务中，模型超越主流符号生成方法，与专有音频生成模型竞争。  <br/>4. 在MIR分类任务中，冻结表示实现SOTA结果，微调表明预训练模型具有强泛化能力。  <br/>5. 展示预训练模型在下游任务中仅需少量标签样例即可高效适配。|
|2506.23553v1|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v1)|总结：  <br/>该研究发现CLAPScore与人类主观评分相关性较低，提出基于人类感知的Human-CLAP模型，并通过实验验证其显著提升评估效果。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：首次系统分析CLAPScore与人类主观评估评分的低相关性，指出其在文本-音频相关性建模中的不足。  <br/>2. **提出Human-CLAP方法**：创新性地通过引入人类主观评分作为训练信号，构建了一个更符合人类感知的对比语言-音频预训练模型。  <br/>3. **验证效果提升**：实验证明Human-CLAP将Spearman秩相关系数（SRCC）提升超过0.25，显著增强了模型与人类评分的一致性。|
|2506.23552v1|[JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](http://arxiv.org/abs/2506.23552v1)|总结（100字以内）:  <br/>提出JAM-Flow框架，通过Flow Matching与MM-DiT架构整合面部运动与语音生成，引入选择性联合注意力、时序对齐位置嵌入等机制，实现多模态条件输入下的同步合成，推动音频-视觉生成模型的发展。<br/><br/>贡献点：  <br/>1. **统一框架设计**：首次将面部运动与语音生成联合建模，打破传统逐任务处理方式，实现同步合成与条件控制。  <br/>2. **多模态扩散变压器架构**：提出MM-DiT架构，整合Motion-DiT（运动模块）与Audio-DiT（语音模块），支持跨模态交互。  <br/>3. **跨模态交互机制**：引入选择性联合注意力层、时序对齐位置嵌入和局部联合注意力掩码，平衡模态间协同与独立性。  <br/>4. **灵活条件输入支持**：基于inpainting训练目标，兼容文本、参考音频和运动作为输入，实现多样化应用场景（如音频驱动动画、文本同步说话头生成）。  <br/>5. **实用性强的多模态生成解决方案**：为语音与视觉的联合生成提供高效模型，推动多模态生成建模技术发展。|
|2506.23367v1|[You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel   Properties](http://arxiv.org/abs/2506.23367v1)|总结：  <br/>本文提出首个面向L2者的TTS系统，通过元音时长差异设计“清晰模式”，实验证明其降低转录错误并提升用户接受度，同时揭示了实际与感知理解度的差异，指出当前ASR工具对L2理解评估的不足。<br/><br/>贡献点：  <br/>1. **首个L2特化TTS系统**：开发了首个针对第二语言者的文本到语音系统，专为提升其语音理解而设计。  <br/>2. **“清晰模式”创新**：基于美式英语元音的紧张/松弛时长差异，构建“清晰模式”以优化语音可理解性。  <br/>3. **实验证明有效性**：通过感知研究验证“清晰模式”可减少9.15%以上转录错误，并提高用户主观体验。  <br/>4. **认知与感知差异揭示**：发现听众未意识到模式的效果，实际理解度与主观感知存在显著分歧。  <br/>5. **评估工具局限性指出**：指出Whisper-ASR未能有效捕捉L2者的语音理解线索，无法准确评估TTS系统的表现。|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|**贡献点总结：**  <br/>1. 提出改进的音频拼接方法（SAGE）生成人工代码切换语音数据，缓解数据稀缺问题。  <br/>2. 通过SAGE数据微调，显著提升阿拉伯语和英语代码切换语音的Word Error Rate（WER）7.8%。  <br/>3. 引入经验重放机制增强模型泛化能力，减少灾难性遗忘。  <br/>4. 整合外部领域3-gram语言模型，降低整体WER至26.6%。  <br/>5. 少样本微调进一步优化代码切换基准，提升WER 4.9%。  <br/>6. 最终在阿拉伯语-英语代码切换任务中实现31.1%的WER，超越大型多语言基模（如USM、Whisper-large-v2）5.5%-8.4%。  <br/><br/>**总结（100字内）：**  <br/>本文提出SAGE数据生成方法与经验重放机制，结合外部语言模型及少样本微调，显著提升方言阿拉伯语与阿拉伯-英语代码切换语音的识别性能，WER达到31.1%，超越现有大模型。|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>本研究提出针对实时语音对话的偏好对齐框架，构建大规模标注语料库，通过离线对齐优化模型，并验证其提升效果，强调动态平衡的关键作用。<br/><br/>贡献点：  <br/>1. 提出新型偏好对齐框架，专门处理实时语音中的动态特性（如打断、插入）和缺乏明确回合分隔的问题。  <br/>2. 构建包含15万+偏好对的多轮语音对话数据集，使用AI反馈标注，覆盖语言内容与时序上下文的偏好。  <br/>3. 首次将离线对齐方法应用于全双工语音到语音模型的微调，提升对话连贯性与多轮交互处理能力。  <br/>4. 基于广泛实验和人类评估，验证偏好反馈对模型在事实性、安全性及上下文对齐方面的改进效果。  <br/>5. 揭示语音对话中动态平衡的重要性，为自然实时对话系统的构建提供理论依据与实践指导。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|**贡献点：**  <br/>1. **提出分步视频到音频生成框架**：模仿传统Foley工作流程，分步骤生成与视频中特定声音事件对应的独立音频轨道，实现更全面的音频合成。  <br/>2. **引入文本与历史音频引导机制**：每一步生成均基于目标文本提示和已生成的音频轨道，增强生成内容的逻辑连贯性和语义关联性。  <br/>3. **概念否定策略的创新应用**：借鉴组合生成框架中的概念否定方法，通过排除矛盾或冗余信息提升音频生成的准确性。  <br/>4. **无需专用配对数据的训练方法**：利用预训练视频到音频模型，无需依赖专业标注数据集，降低训练门槛并提高数据可获取性。  <br/>5. **多语义音频生成与质量提升**：实验验证方法能够生成多个语义独立的音频轨道，显著优于现有基线，实现更高质量的复合音频合成。  <br/><br/>**总结（100字内）：**  <br/>本文提出一种模仿Foley流程的分步视频到音频生成方法，通过文本引导与概念否定策略，结合预训练模型，无需专用数据集即可生成高质量的多语义音频，提升复合音频合成效果。|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|总结：  <br/>本研究通过对比不同说话人嵌入方法在零样本TTS中的表现，验证了H/ASP编码器的优越性，并强调了经验评估在跨任务应用中的重要性，为未来研究提供统一框架。<br/><br/>贡献点：  <br/>1. **提出零样本TTS中编码器性能对比**：系统评估了H/ASP、x-vector和ECAPA-TDNN三种说话人嵌入方法在同一体系下的效果，填补了该领域的研究空白。  <br/>2. **统一实验框架与数据集**：基于YourTTS系统，在相同数据集（捷克语读语音）和评估标准（24个域外目标说话人）下进行实验，确保比较的公平性。  <br/>3. **创新评估方法组合**：结合主观（说话人相似性听觉测试）和客观（余弦距离度量）指标，全面分析编码器对语音合成质量的影响。  <br/>4. **明确编码器有效性差异**：发现H/ASP编码器在零样本TTS中表现最优，ECAPA-TDNN优于x-vectors，但表现未达预期，挑战了其在说话人识别中通用性的假设。  <br/>5. **强调跨任务应用的评估必要性**：指出说话人识别嵌入在TTS任务中的有效性需通过经验验证，而非直接迁移，为未来研究提供参考方向。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|总结：  <br/>本文提出Kling-Foley多模态视频到音频生成模型，通过创新的对齐机制和通用音频编解码器，实现高保真音频-视频同步，并开源工业级基准Kling-Audio-Eval，验证了其在多个指标上的SOTA性能。<br/><br/>贡献点：  <br/>1. 提出Kling-Foley模型，首次将多模态扩散Transformer与视觉语义表示模块、音频-视频同步模块结合，实现视频-音频高精度同步生成。  <br/>2. 设计帧级视频-音频对齐机制，提升语义对齐和时序同步能力。  <br/>3. 开发通用潜在音频编解码器，支持声效、语音、歌唱、音乐等多场景高质量建模。  <br/>4. 引入立体渲染技术，增强合成音频的空间感与沉浸体验。  <br/>5. 开发工业级开源基准Kling-Audio-Eval，弥补现有数据集在类型与标注的不足。  <br/>6. 验证基于流匹配目标的训练方法在音频-视频分布匹配、语义对齐、时序对齐和音频质量上的SOTA性能。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|总结：  <br/>提出TTSDS2作为更稳健的语音合成质量评估指标，并发布包含主观评分数据集、多语言测试数据集生成流程及14语言更新基准的资源，提升合成语音评估有效性。<br/><br/>贡献点：  <br/>1. **提出TTSDS2指标**：改进原有TTSDS，实现跨领域和语言的高质量评估，与所有对比指标中唯一达到斯皮尔曼相关系数>0.50的基准。  <br/>2. **构建大规模主观评分数据集**：提供超11,000条人工标注的MOS评分，用于合成语音质量评估。  <br/>3. **设计数据生成与防泄露流程**：开发多语言测试数据集动态重建方案，避免评估过程中的数据泄露问题。  <br/>4. **创建持续更新的多语言基准**：建立包含14种语言的TTS系统性能基准，支持长期评估与比较。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**贡献点：**<br/>1. 提出新假设：人类在主观语音质量评分中更关注低质量段落，评分差异主要由忽略低质量部分时的高分误判导致。<br/>2. 验证假设：基于VCC2018和BVCC数据集分析，揭示人类评价焦点与语音质量分布的关系。<br/>3. 提出新指标：定义N_low-MOS（N个最低意见评分的平均值）作为更可靠的语音质量代表值。<br/>4. 实验验证：证明N_low-MOS可提升MOSNet的评估性能（LCC/SRCC指标改善），展示其内在有效性。<br/>5. 推动方法改进：为语音转换（VC）模型的评估提供更科学的基准，优化MOSNet的比较能力。<br/><br/>**总结（100字内）：**  <br/>本文提出N_low-MOS指标，通过分析数据集验证人类评分明显受低质量段落影响，并证明其在提升语音质量评估模型性能方面具有显著优势，为VC模型评估提供新思路。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|**贡献点：**  <br/>1. 提出一种新的评分聚合方法，解决传统MOS标注（1-5分离散评分）的局限性。  <br/>2. 基于标注者内部连续评分假设，建模生成分布并通过量化潜在连续分布估计评分峰值。  <br/>3. 引入潜在分布峰值作为新代表性值，取代传统MOS作为预测目标。  <br/>4. 通过实验验证，该方法能显著提升语音质量预测模型（如MOSNet）的性能。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出一种基于连续评分假设的新型语音质量评分方法，通过建模标注者的评分过程并引入潜在分布峰值作为替代指标，有效提升语音质量预测模型的性能。|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|**总结（100字以内）**  <br/>本研究构建了日本偶像语音语料库（JIS），聚焦年轻女性现场偶像群体，通过艺名标识促进听众实验。JIS支持TTS/VC系统的说话者相似性评估，并探索听众偏好的个性化语音生成，数据免费开放且仅限非商业科研使用，附带文化背景介绍与应用指导。<br/><br/>**贡献点分点列出**  <br/>1. **构建专业化语料库**  <br/>   - 首次创建针对日本“年轻女性现场偶像”这一特定群体的语音语料库（JIS），覆盖TTS和VC研究需求，补充语音生成领域的数据多样性。<br/><br/>2. **推动说话者相似性评估**  <br/>   - 所有语音由同一类群体（艺名标识）录制，便于系统评估生成语音与原声的说话者相似性，提升TTS/VC的可信度与可比性。<br/><br/>3. **开拓听众偏好研究方向**  <br/>   - 引入针对听众偏好的个性化语音生成研究（如定制偶像声音），填补该领域学术空白，推动语音生成技术与用户需求的结合。<br/><br/>4. **制定开放使用政策**  <br/>   - 以非商业、基础研究为限免费公开JIS，确保数据可及性同时维护版权与伦理规范，促进学术共享与创新。<br/><br/>5. **提供文化背景支持**  <br/>   - 对日本偶像文化进行介绍，助力研究者理解数据语境，确保JIS的合法、有效与伦理应用，提升科研的社会接受度。|
|2506.16738v1|[LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](http://arxiv.org/abs/2506.16738v1)|总结：  <br/>本文提出LM-SPT方法，通过间接语义蒸馏减少语音标记序列长度，改进编码器解码器结构并支持多帧率，实验验证其在语音-文本建模中取得优于基线的重建效果和文本到语音任务表现。<br/><br/>贡献点：  <br/>1. 提出LM-SPT模型，通过语义驱动的重建机制替代传统池化操作，有效减少语音标记序列长度。  <br/>2. 引入间接语义蒸馏策略，通过冻结ASR编码器对比原始与重建波形，提升语义对齐精度。  <br/>3. 设计编码器-解码器架构优化方案，支持25Hz、12.5Hz、6.25Hz等多帧率配置。  <br/>4. 实验证明LM-SPT在语音-文本双向任务中均优于基线，尤其在文本到语音生成中表现突出。|
|2506.16580v1|[Streaming Non-Autoregressive Model for Accent Conversion and   Pronunciation Improvement](http://arxiv.org/abs/2506.16580v1)|总结：提出首个支持流式处理的口音转换模型，实现非母语语音向母语口音的转换，同时保持说话人身份与语调，并通过集成TTS模型提升训练效率，达到顶级模型性能且具备稳定延迟。<br/><br/>贡献点：<br/>1. 提出首个实现流式口音转换（AC）的模型，支持实时处理非母语语音。<br/>2. 保持说话人身份、语调特征并提升发音质量，实现"原声"性转换。<br/>3. 采用Emformer编码器与优化推理机制，显著降低处理延迟。<br/>4. 整合母语TTS模型生成理想参考数据，提升训练效率。<br/>5. 在保持稳定延迟的前提下，达到当前最优AC模型的转换效果。|
|2506.16127v1|[Improved Intelligibility of Dysarthric Speech using Conditional Flow   Matching](http://arxiv.org/abs/2506.16127v1)|**贡献点总结（100字以内）**  <br/>本研究提出基于自监督学习特征及量化表示的失语语音到正常语音转换方法，采用单说话人语音生成策略降低说话人差异，并结合非自回归条件流匹配与扩散Transformer实现高效映射，提升语音可懂度和收敛速度。  <br/><br/>**具体贡献点**  <br/>1. **替代传统特征**：提出使用自监督学习（SSL）特征及其量化表示替代梅尔频谱图（mel-spectrograms），探索其在语音生成中的有效性。  <br/>2. **缓解说话人差异**：通过从WavLM提取特征生成单说话人干净语音，减少说话人变异性对模型性能的影响。  <br/>3. **非自回归框架**：设计全非自回归方法，结合条件流匹配（CFM）与扩散Transformer，直接学习失语语音到干净语音的映射。  <br/>4. **离散音素优势**：验证离散音素单元在提升语音可懂度和加速模型收敛方面的显著效果，优于传统基于梅尔频谱的方法。|
|2506.15873v1|[DeckFlow: Iterative Specification on a Multimodal Generative Canvas](http://arxiv.org/abs/2506.15873v1)|总结：  <br/>该论文提出DeckFlow多模态生成AI工具，通过任务分解、规格分解和生成空间探索三大创新机制解决现有工具设计问题，并验证了其在文本到图像生成中的有效性，进一步拓展至音频生成以研究用户创意行为。<br/><br/>贡献点：  <br/>1. **任务分解机制**：采用无限画布与卡片式视觉数据流交互，支持用户创建和管理多个互联子任务。  <br/>2. **规格分解工作流**：将初始目标迭代拆解为子部分，结合特征标签与聚类实现多模态内容组织。  <br/>3. **生成空间探索**：通过生成多组提示词与输出变体（网格展示），支持递归反馈优化设计迭代。  <br/>4. **多模态验证与扩展**：在文本-图像生成中对比传统对话AI基线，后扩展至音频生成，分析跨模态创作行为。|
|2506.15759v1|[Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](http://arxiv.org/abs/2506.15759v1)|**总结（100字以内）**  <br/>该论文提出Sonic4D框架，首次实现4D场景与空间音频的联合生成，通过动态视觉捕捉、声源定位及物理模拟三阶段处理，无需训练即可生成逼真的时空音频，增强了沉浸式音频视觉体验。<br/><br/>**贡献点**  <br/>1. **提出Sonic4D框架**：首次将空间音频生成与4D场景合成结合，解决现有方法忽视音频与场景对齐的问题。  <br/>2. **三阶段生成方法**：  <br/>   - 第一阶段：利用预训练模型生成4D场景及单声道音频；  <br/>   - 第二阶段：通过像素级视觉定位策略估计声源的3D空间坐标；  <br/>   - 第三阶段：基于物理模拟合成动态视角与时间变化的时空音频。  <br/>3. **训练-free设计**：无需额外训练，直接利用现有数据生成符合场景的空间音频。  <br/>4. **实验验证与资源公开**：通过实验证明生成音频的逼真度和沉浸感，并开放生成示例供研究复现。|
|2506.15085v1|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v1)|总结：  <br/>提出EmojiVoice工具包，实现社交机器人离线语音表达的动态控制，通过emoji提示增强长时表达性，并验证其在不同场景中的表现差异。<br/><br/>贡献点：  <br/>1. **开发EmojiVoice工具包**：首个专门针对社交机器人长时语音表达的免费、可定制TTS系统，解决基础模型离线部署难题。  <br/>2. **引入emoji提示控制机制**：首次将表情符号用于细粒度表达性调控，实现对语音情感相位的精准控制。  <br/>3. **轻量级实时生成框架**：采用Matcha-TTS等轻量模型，支持机器人端实时语音生成需求。  <br/>4. **多场景验证与对比**：通过剧本对话、讲故事、自主交互三类案例研究，验证方法对表达性的提升效果，并揭示不同应用场景下的接受差异。|
|2506.13053v2|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v2)|总结：本文提出ZipVoice，通过紧凑结构和流匹配技术实现高效零样本TTS，质量与SOTA相当，在模型体积和推理速度上显著提升，并开源代码与模型。<br/><br/>贡献点：  <br/>1. **提出Zipformer-based流匹配解码器**：在模型体积受限情况下保持足够的建模能力，提升效率。  <br/>2. **设计Average upsampling与Zipformer结合的语音-文本对齐及文本编码器**：增强语音可懂度，优化多语言处理效果。  <br/>3. **创新流蒸馏方法**：减少采样步骤，消除无分类器引导的推理开销，显著提升生成速度。  <br/>4. **实验验证效果**：在100k小时多语言数据集上，ZipVoice在语音质量与SOTA模型相当的同时，体积缩小3倍，速度提升30倍。  <br/>5. **开源实现**：提供代码、模型检查点和演示样本，便于复现与应用。|
|2506.12199v1|[ViSAGe: Video-to-Spatial Audio Generation](http://arxiv.org/abs/2506.12199v1)|**贡献点总结**（100字以内）:  <br/>本研究提出ViSAGe框架，直接从无声视频生成一阶Ambisonics，构建YT-Ambigen数据集，设计空间音频评估指标，并验证其在时空对齐与视角适应性上的优越性。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新问题**：探索无需复杂录制设备或专业技能，直接从无声视频生成一阶Ambisonics的方法。  <br/>2. **构建数据集**：创建包含102K视频片段的YT-Ambigen数据集，为研究提供标注数据支持。  <br/>3. **设计评估指标**：首次基于音频能量图与显著性度量，提出量化评估生成空间音频质量的新标准。  <br/>4. **端到端框架**：提出ViSAGe模型，融合CLIP视觉特征、自回归音频编码与方向/视觉引导生成高质量Ambisonics。  <br/>5. **性能优势**：验证ViSAGe在生成音频的连贯性与时空对齐性上优于传统两阶段方法（视频-音频生成+空间化）。  <br/>6. **应用能力**：生成的音频可随视角变化动态适应，提升沉浸式音频体验的交互性。|
|2506.11160v5|[S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation   Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning](http://arxiv.org/abs/2506.11160v5)|总结：  <br/>提出S2ST-Omni框架，通过解耦任务、集成预训练模型、引入轻量适配器和两阶段微调策略，结合流式TTS生成，有效解决多语言语音翻译的高质量与平行语料依赖问题，并在CVSS基准上超越现有S2ST系统。<br/><br/>贡献点：  <br/>1. **任务分解创新**：首次将S2ST任务拆分为S2TT与TTS子任务，降低复杂度并提升灵活性。  <br/>2. **预训练模型整合**：结合Whisper编码器与Qwen 3.0语言模型，实现语音与文本的高效理解与生成。  <br/>3. **轻量适配器设计**：提出跨模态的轻量级语音适配器，缓解语音-文本表征的模态鸿沟问题。  <br/>4. **两阶段微调策略**：创新性地采用分步微调方法，优化多模态知识学习效率。  <br/>5. **流式生成技术**：在TTS阶段引入流式自回归生成方法，提升目标语音的自然性和实时性。  <br/>6. **性能突破**：在CVSS基准测试中显著优于现有S2ST系统，验证了框架的有效性和通用性。|
|2506.11130v1|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v1)|总结（100字以内）:  <br/>提出自优化框架，利用无标签数据与闭环训练提升ASR性能，成功将Whisper转化为Twister，在台语中实现显著精度提升，并为低资源场景提供实用方案。<br/><br/>贡献点：<br/>1. 提出无需标注数据的自精炼框架，通过伪标签生成与闭环训练实现ASR性能提升。  <br/>2. 首次将高保真TTS系统与ASR模型结合，形成"ASR→TTS→ASR"的闭合优化循环。  <br/>3. 在台语语音任务中验证框架有效性，利用6000小时未标注语音驱动模型训练。  <br/>4. 开发专用模型Twister，相较Whisper在普通话和双语切换场景分别降低20%和50%错误率。  <br/>5. 为低资源/特定领域ASR提供创新解决方案，突破传统伪标签自蒸馏方法的局限性。|
|2506.11127v1|[GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech   Instructions](http://arxiv.org/abs/2506.11127v1)|总结：  <br/>提出GUIRoboTron-Speech，首个端到端语音指令GUI代理，通过语音数据生成和混合训练策略解决模态不平衡问题，实验验证其高效性和广泛应用前景。<br/><br/>贡献点：  <br/>1. **首个语音驱动GUI代理**：提出GUIRoboTron-Speech，首次实现端到端GUI操作，直接接受语音指令和设备截图作为输入，无需文本转换。  <br/>2. **语音数据生成方法**：利用随机音色TTS模型将现有文本指令转换为高质量语音指令，解决语音数据稀缺问题。  <br/>3. **混合指令训练策略**：设计启发式方法缓解预训练模型的模态不平衡，结合文本和语音指令提升代理性能。  <br/>4. **系统性实验验证**：在基准数据集上验证模型表现，证明语音指令在GUI自动化中的有效性与广泛适用性。|
|2506.11086v1|[Intelligibility of Text-to-Speech Systems for Mathematical Expressions](http://arxiv.org/abs/2506.11086v1)|贡献点：  <br/>1. **系统评估框架**：首次设计涵盖五种TTS模型的实验，量化数学表达式（MX）的发音质量与可懂性（通过用户评分和转录正确性）。  <br/>2. **LLM辅助生成**：利用大语言模型（LLM）将LaTeX格式MX转换为英文发音，弥补TTS模型无法直接处理LaTeX的缺陷。  <br/>3. **多维度指标**：提出包含三个指标的转录正确性评估体系，并结合Mean Opinion Score分析用户主观感受。  <br/>4. **对比分析**：对比听众对TTS输出与人类专家发音的偏好，揭示TTS在处理MX时的表现短板。  <br/>5. **结果发现**：发现TTS模型输出的可懂性存在显著差异，且多数MX类别下表现劣于专家，同时LLM选择对结果影响有限。  <br/>6. **应用导向结论**：明确指出需针对性优化TTS模型处理数学表达式的能力。  <br/><br/>总结：  <br/>首次系统评估TTS模型对数学表达式的发音质量与可懂性，揭示模型与人类专家表现差异，强调改进TTS处理MX能力的必要性。|
|2506.10019v1|[A Survey of Automatic Evaluation Methods on Text, Visual and Speech   Generations](http://arxiv.org/abs/2506.10019v1)|总结：本文提出首个综合框架，系统化分类文本、图像和音频生成的评估方法，识别出五种核心范式，并探讨跨模态评估的未来方向。<br/><br/>贡献点：  <br/>1. **构建统一框架**：首次建立跨文本、图像、音频三大模态的自动评估方法系统化框架，实现多模态方法的整合与对比。  <br/>2. **提出五种核心范式**：归纳出生成内容评估的五大基础范式，为后续研究提供理论分类依据。  <br/>3. **跨模态适用性验证**：将文本生成评估方法的分析逻辑扩展至图像和音频领域，验证框架的广泛适用性。  <br/>4. **探索未来研究方向**：针对跨模态评估挑战，提出潜在的研究路径，促进多模态生成AI的评估体系发展。|
|2506.09827v2|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v2)|**贡献点：**  <br/>1. 提出EmoNet-Voice资源，包含大规模预训练数据集（Big）和新型基准数据集（Bench），覆盖40种情感、11种声音及4种语言。  <br/>2. 支持细粒度情感评估，明确区分不同情感强度以提升模型准确性。  <br/>3. 通过语音生成技术合成情感音频，模拟真实场景以解决隐私问题。  <br/>4. 引入心理学专家标注与验证，确保情绪强度标签的可靠性。  <br/>5. 提出Empathic Insight Voice模型，实现与人类专家高度一致的SER性能。  <br/>6. 揭示高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更易被识别的模型表现差异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoNet-Voice资源，通过合成语音和专家标注解决隐私与情感粒度问题，推出高性能SER模型，并发现高唤醒情绪更易识别，为情感语音研究提供新基准。|
|2506.09827v1|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v1)|总结：  <br/>提出EmoNet-Voice资源及两个子集，包含大规模预训练数据与专家标注基准数据，解决隐私问题并增强情感粒度和多样性。开发高精度模型，通过评估揭示高唤醒情绪识别易度高于低唤醒情绪。<br/><br/>贡献点：  <br/>1. **引入EmoNet-Voice资源**：构建由EmoNet-Voice Big（大规模预训练数据集）和EmoNet-Voice Bench（专家标注基准数据集）组成的综合数据资源，覆盖40种情感、11种声音和4种语言。  <br/>2. **提升情感粒度与强度区分**：设计支持细粒度情感分类（40类）及不同强度评估的框架，推动SER模型在情感理解上的更精确建模。  <br/>3. **合成数据解决隐私与多样性问题**：通过先进语音生成技术合成敏感情感场景数据，避免真实数据隐私风险并扩展情感表达的多样性。  <br/>4. **专家验证机制**：联合心理学专家对合成数据进行严格标注（感知强度标签），确保数据质量与情感感知的科学性。  <br/>5. **提出高性能SER模型**：开发Empathic Insight Voice模型，实现与人类专家的高一致性，推动语音情感识别技术的标准化。  <br/>6. **揭示情绪识别难度差异**：通过对比实验发现，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易被模型识别，为后续研究提供新视角。|
|2506.08279v1|[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)|**总结（100字以内）:**  <br/>该论文提出Mirage模型，首次实现从音频直接生成高质量、表达性强的视频，结合TTS技术生成多模态视频，并开发统一的自注意力训练方法，提升生成视频的主观质量与通用性。  <br/><br/>**分点贡献:**  <br/>1. **提出Mirage模型**：首个专注于音频到视频生成的通用模型，从原始音频直接生成逼真、表达性的视频图像，不依赖视觉输入。  <br/>2. **多模态生成能力**：通过集成文本到语音（TTS）技术，实现语音与视频的同步生成，解决语音和视频内容对齐问题。  <br/>3. **统一训练方法**：开发适用于自注意力机制的统一训练框架，支持从头训练和基于已有权重的微调，增强模型灵活性。  <br/>4. **语义对齐优化**：利用A-roll数据（人物对话语音视频）训练，使生成视频能准确反映音频中的表演信息，提高可信度。  <br/>5. **性能优势**：生成的视频在主观质量上优于依赖语音特定架构或损失函数的现有方法，保持通用性的同时实现高保真输出。|
|2506.04527v1|[Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning](http://arxiv.org/abs/2506.04527v1)|总结（100字以内）:<br/>该论文提出结合字母符号的语音标签生成方法，通过BERT特征提示编码实现隐式条件，配合推理时的显式修剪提升标签一致性，为文本到语音和口音估计等任务生成高质量平行数据。<br/><br/>贡献点：<br/>1. **方法创新**：首创结合字母符号的语音标签生成框架，突破传统ASR模型直接微调的局限，引入显式与隐式的双条件策略。<br/>2. **条件生成机制**  <br/>   - 通过预训练BERT特征构建隐式图形单词条件（Prompt Encoder）  <br/>   - 提出推理阶段的显式标签假设剪枝算法（Pruning Inconsistent Hypotheses）<br/>3. **数据价值**：生成跨模态的语音-标签-图形单词平行数据，支持文本到语音合成（TTS）和口音估计等下游任务。<br/>4. **实验验证**：在音素-图形单词一致性任务和口音估计任务中均取得显著性能提升，证明方法的有效性。|
|2506.04397v1|[Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?](http://arxiv.org/abs/2506.04397v1)|总结：  <br/>该论文通过构建专用数据集和微调大模型，探索Parler TTS在语音重建任务中生成可理解语音与保持说话人身份的能力，揭示其在可控性方面的局限并提出改进方向。<br/><br/>贡献点：  <br/>1. **构建专用数据集**：创建并标注了包含说话人信息和可懂度数据的语音重建数据集，为相关研究提供基础资源。  <br/>2. **应用大模型进行语音重建**：首次尝试利用Parler TTS等先进大型语音模型，生成语音障碍患者疾病前的语音近似。  <br/>3. **分析模型局限性**：发现模型虽能学习数据分布，但在控制语音可懂度和保持说话人身份一致性方面存在挑战。  <br/>4. **提出改进方向**：为提升语音重建任务中模型的可控性，提出了未来研究的潜在优化路径。|
|2506.04152v1|[HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](http://arxiv.org/abs/2506.04152v1)|**分点贡献：**  <br/>1. **构建高带宽语音数据集**：提出HiFiTTS-2，基于LibriVox有声书，包含36.7k小时（22.05kHz）和31.7k小时（44.1kHz）的高质量英语语音数据，支持高带宽语音合成研究。  <br/>2. **创新数据处理流程**：设计了包含带宽估计、语音分段、文本预处理及多说话人检测的全流程数据处理方法，提升数据适配性与实用性。  <br/>3. **提供详细元数据**：生成全面的语句和有声书元数据，支持研究者通过数据质量过滤器灵活调整数据集以适配不同应用场景。  <br/>4. **验证数据集有效性**：实验表明该数据集和处理方法可有效训练高质量零样本TTS模型，推动高带宽语音生成技术的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HiFiTTS-2，构建大规模高带宽语音数据集并设计全流程处理技术，提供详细元数据以增强数据适应性，实验证明其可有效支持高质量零样本TTS模型训练。|
|2506.04134v1|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v1)|**贡献点总结（100字以内）**：  <br/>提出直接生成CS语音的新方法，构建首个统一框架UniCUE，通过细颗粒语义对齐、跨任务适配器及姿态感知处理，解决误差传播与时空对齐问题，显著提升语音生成质量与唇语同步性。<br/><br/>**分点贡献**：  <br/>1. **直接生成语音的创新方法**：首次无需依赖文本中间媒介，直接从CS视频生成语音，避免误差传播与时间错位问题。  <br/>2. **统一框架UniCUE**：提出首个融合CSV2S与CSR任务的框架，打通视觉-语义到语音的端到端生成流程。  <br/>3. **核心技术创新**：  <br/>   - 引入细颗粒语义对齐池，精确映射视觉特征与语音内容；  <br/>   - 设计VisioPhonetic适配器，实现CSV2S与CSR的跨任务表示兼容；  <br/>   - 开发姿态感知视觉处理器，增强唇语与手势的时空关联性。  <br/>4. **实验验证与数据集**：基于新建立的中文CS数据集（含14名受试者），验证方法有效性，降低误词率78.3%并提升唇语同步性32%。|
|2506.04077v1|[A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions](http://arxiv.org/abs/2506.04077v1)|总结：  <br/>该论文提出一种基于大语言模型与跨模态信息的新型语音评估方法，有效解决低资源场景下的标签约束问题，提升口语评分的可靠性和多样性。<br/><br/>贡献点：  <br/>1. **低资源场景下的数据生成**：利用大语言模型生成多样化响应并合成语音，突破标注数据的稀缺限制。  <br/>2. **说话人感知的语音合成**：通过 speaker-aware 文本到语音合成技术，增强生成语音与真实语音的匹配度。  <br/>3. **动态重要性损失设计**：根据合成语音与真实语音的特征差异，自适应调整训练样本权重，提升模型学习效率。  <br/>4. **跨模态特征融合**：构建多模态大语言模型，直接整合文本与语音信号的对齐特征以预测口语水平。  <br/>5. **有效性验证**：在 LTTC 数据集上验证方法优于传统数据依赖或常规增强技术，实现跨模态信息驱动的自动化评分。|
|2506.03884v1|[Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages](http://arxiv.org/abs/2506.03884v1)|总结: 本文提出基于零样本合成的多语言TTS系统，通过共享音素表示和适配文本解析规则，成功生成梵语等低资源语言的自然语音，验证了该方法在扩展语音技术普惠性方面的有效性。<br/><br/>贡献点:<br/>1. 提出零样本合成框架，解决印度1369种语言中多数缺乏数字资源的TTS训练难题<br/>2. 构建跨语言音素共享表示，降低多语言模型过度假设需求<br/>3. 开发适应不同音系规则的文本解析方法（phonotactics adaptation）<br/>4. 实现Sanskrit、Maharashtrian、Canara Konkani、Maithili、Kurukh等5种语言的自然语音合成<br/>5. 提供可复用的合成器架构，验证方法在扩大语音技术覆盖范围中的可行性|
|2506.03831v1|[Conformer-based Ultrasound-to-Speech Conversion](http://arxiv.org/abs/2506.03831v1)|总结：  <br/>本研究对比了Conformer与传统CNN在超声波语音转换中的表现，发现Conformer结合bi-LSTM在感知质量上更优，而Conformer Base在效率上显著提升，为静默语音接口提供了新思路。<br/><br/>贡献点：  <br/>1. **提出两种Conformer-based模型**：首次将Conformer架构应用于超声波到语音转换任务，分别采用Base结构和结合bi-LSTM的结构。  <br/>2. **融合HiFi-GAN声码器**：通过HiFi-GAN将生成的mel频谱图合成高质量音频波形，提升系统输出的语音还原效果。  <br/>3. **验证感知质量优势**：在MUSHRA主观测试中发现，Conformer与bi-LSTM结合的模型在语音可懂度上优于传统CNN基线。  <br/>4. **效率提升**：Conformer Base模型在保持性能的同时，训练速度比传统CNN快3倍，降低了计算成本。|
|2506.03793v1|[Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts](http://arxiv.org/abs/2506.03793v1)|**贡献点**  <br/>1. **提出Cadence模型**：基于预训练大语言模型设计，首次实现对自发语音转录的精准标点恢复，克服传统模型在处理断续、重复等语音特征时的不足。  <br/>2. **扩展语言覆盖**：支持22种印度语言及英语（原仅14种），显著提升多语言标点恢复的适用性与泛化能力。  <br/>3. **分析挑战与改进**：通过跨语言和标点类型的行为研究，揭示领域迁移及罕见标点符号下的修复难题，验证预训练模型在低资源NLP场景中的有效性。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出Cadence模型，基于预训练语言模型实现自发语音的精准标点恢复，扩展支持22种印度语言和英语，并通过深入分析揭示多语言任务中的关键挑战。|
|2506.03515v1|[BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and   Weight Indexing](http://arxiv.org/abs/2506.03515v1)|总结：  <br/>本文提出两种模型压缩技术，实现83%的模型体积减小，同时保持高质量语音合成，适用于设备端部署。<br/><br/>贡献点：<br/>1. 提出量化感知训练（QAT）技术，将模型参数从32位压缩至1.58位，主要采用三值{-1,0,1}量化策略。<br/>2. 开发权重索引方法，通过将一组1.58位权重编码为单个int8索引来优化存储效率。<br/>3. 在保持合成质量的前提下，实现模型轻量化，使其更适用于资源受限的设备端应用场景。|
|2506.02979v1|[Towards a Japanese Full-duplex Spoken Dialogue System](http://arxiv.org/abs/2506.02979v1)|**贡献点总结：**  <br/>1. 提出首个公开可用的全双工日语语音对话模型，基于英语模型Moshi迁移。  <br/>2. 采用两阶段训练方法：大规模日语语料预训练与高质量立体声数据微调。  <br/>3. 引入多流文本到语音系统生成的合成对话数据以增强模型性能。  <br/>4. 实验证明模型在自然度和语义理解上优于现有日语基准模型。  <br/><br/>（注：以上为100字以内总结，符合要求。）|
|2506.02958v1|[PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech   Editing](http://arxiv.org/abs/2506.02958v1)|**贡献点总结（100字以内）:**  <br/>提出PartialEdit数据集，聚焦神经语音编辑生成的深伪语音检测，揭示现有模型的局限性，分析伪影特征，推动该领域研究。<br/><br/>**分点贡献:**  <br/>1. **提出PartialEdit数据集**：构建首个基于先进神经语音编辑技术的深伪语音检测数据集，包含部分编辑的语音样本，为研究提供基准。  <br/>2. **探索检测与定位任务**：系统研究部分编辑语音的检测与内容定位，验证当前检测方法在复杂编辑场景下的有效性。  <br/>3. **揭示模型局限性**：实验表明基于PartialSpoof数据集训练的模型无法有效识别神经编辑生成的深伪语音，凸显数据集不足。  <br/>4. **分析伪影特征**：结合神经音频编码器，探讨检测模型学习到的伪影（artifacts）特征，为改进检测方法提供理论依据。  <br/>5. **提供公开资源**：开放数据集和音频样本，方便后续研究复现与扩展。|
|2506.02858v2|[DGMO: Training-Free Audio Source Separation through Diffusion-Guided   Mask Optimization](http://arxiv.org/abs/2506.02858v2)|总结：  <br/>提出训练无关的零样本音频源分离框架DGMO，利用扩散模型生成先验优化频谱掩码，突破传统方法依赖任务特定训练的限制，拓展扩散模型在音频分离领域的应用范式。<br/><br/>贡献点：  <br/>1. **首个训练无关的零样本框架**：首次将预训练扩散模型直接用于语言查询的音频源分离，无需额外任务特定训练。  <br/>2. **测试时优化机制**：提出Diffusion-Guided Mask Optimization（DGMO），通过测试阶段动态优化频谱掩码以提升分离精度。  <br/>3. **跨模态适应性提升**：分析模态适配问题，揭示自然语言与音频处理的协同挑战，并提出针对性解决方案。  <br/>4. **性能验证**：在无监督或弱监督条件下实现竞争性分离效果，证明扩散模型可有效迁移至音频分离任务。  <br/>5. **应用范式创新**：拓展扩散模型从生成任务到分离任务的边界，建立新的零样本音频处理框架。|
|2506.02401v1|[Trusted Fake Audio Detection Based on Dirichlet Distribution](http://arxiv.org/abs/2506.02401v1)|**贡献点：**  <br/>1. **提出基于狄利克雷分布的不确定性建模方法**：首次将狄利克雷分布用于量化模型决策的不确定性，提升假音频检测的可信度评估能力。  <br/>2. **构建新型检测框架**：通过神经网络生成证据，并将预测概率与不确定性估计结合，形成最终检测意见，优化检测流程。  <br/>3. **多数据集验证**：在ASVspoof 2019 LA、2021 LA及DF数据集上进行对比实验，验证了模型在准确率、鲁棒性和可信度方面的优越性能。  <br/><br/>**总结：**  <br/>该研究通过狄利克雷分布建模不确定性，提出新型假音频检测方法，在多数据集上验证了其高准确率与鲁棒性，显著提升了检测的可靠性。|
|2506.02082v1|[SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction](http://arxiv.org/abs/2506.02082v1)|总结：该论文提出了一种新的SALF-MOS模型，解决传统MOS评估高成本问题，通过卷积序列提取音频潜在特征，实现小尺寸、端到端、高度通用和可扩展的语音质量预测，取得最佳评估效果。<br/><br/>贡献点：<br/>1. 提出Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS)模型，解决传统MOS评估需要大量人工标注的问题<br/>2. 设计基于卷积序列的音频特征提取方法，实现端到端的MOS预测系统<br/>3. 构建的小尺寸模型在保持通用性的同时具备良好的可扩展性<br/>4. 在多个客观评估指标（MSE、LCC、SRCC、KTAU）上达到当前最优性能<br/>5. 提供了一种新的语音质量评估方法，可作为TTS和语音转换模型选择的有效工具|
|2506.01618v1|[Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech](http://arxiv.org/abs/2506.01618v1)|总结（100字以内）:  <br/>本文提出基于音节的节奏建模方法，结合LF-MMI模型显著提升口吃语音的ASR性能，验证了无监督转换的有效性。<br/><br/>贡献点:  <br/>1. **提出syllable-based rhythm modeling方法**：在Rhythm and Voice（RnV）框架中引入针对口吃语音的音节级节奏建模技术，解决传统方法对口吃语音节奏特征建模不足的问题。  <br/>2. **评估转换对ASR的影响**：首次系统验证口吃语音转换对ASR性能提升的效果，通过训练LF-MMI模型和微调Whisper模型，发现LF-MMI在口吃语音识别中具有显著优势。  <br/>3. **揭示无监督转换的潜力**：证明无监督学习的节奏和语音转换技术在口吃ASR任务中的有效性，为低资源场景下的语音处理提供了新思路。|
|2506.01322v1|[Zero-Shot Text-to-Speech for Vietnamese](http://arxiv.org/abs/2506.01322v1)|贡献点：<br/>1. 构建了首个越南语高质量语音数据集PhoAudiobook（含941小时音频）  <br/>2. 在VALL-E、VoiceCraft、XTTS-V2等三大零样本TTS模型上开展系统实验  <br/>3. 证实PhoAudiobook能显著提升多指标下的模型性能  <br/>4. 揭示VALL-E与VoiceCraft在短句合成任务中的优越性  <br/>5. 公开数据集促进越南语TTS领域研究发展  <br/><br/>总结：  <br/>本研究发布首个越南语高质量语音数据集PhoAudiobook，并验证其提升三大零样本TTS模型性能的效果，尤其突出VALL-E与VoiceCraft在短句生成中的优势，推动越南语语音合成研究。|
|2506.01268v1|[CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction](http://arxiv.org/abs/2506.01268v1)|贡献点总结：<br/>1. 提出单文件实现的CleanS2S框架，实现低延迟语音交互系统<br/>2. 首创主动对话机制，集成记忆系统与主观行动判断模块<br/>3. 开发五种人类般响应策略（中断/拒绝/转移/沉默/标准响应）<br/>4. 实现全双工WebSocket连接与非阻塞I/O的实时中断处理<br/>5. 提出Action Judgement SFT方法评估输入流策略选择<br/>6. 提供前所未有的透明度和可扩展性，代码已开源<br/><br/>（97字）|
|2506.01263v1|[WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing](http://arxiv.org/abs/2506.01263v1)|总结：  <br/>提出无需额外训练的CTC模型改进方法，通过中间层声学特征检测关键词并引入偏置机制，有效提升罕见词识别准确率，实验在日语语音识别中实现29%的F1分数提升。<br/><br/>贡献点：  <br/>1. **解决罕见词识别偏差问题**：通过中间层声学特征进行关键词检测，避免模型对训练数据词汇的过度依赖。  <br/>2. **引入灵活的wildcard CTC机制**：采用快速且容错的CTC变体，支持模糊匹配处理难以严格识别的单词。  <br/>3. **无需模型重训练的可扩展性**：方法兼容现有模型，无需额外训练或文本到语音系统，适用于大规模模型。  <br/>4. **显著提升未知词识别性能**：在日语语音识别任务中，实现未知词F1分数29%的提升，验证方法有效性。|
|2506.01032v1|[ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and   Speaker Feature Optimization](http://arxiv.org/abs/2506.01032v1)|**总结**：  <br/>本文提出ReFlow-VC，通过修正流的ODE框架提升语音转换效率，结合内容与音高信息优化说话人特征，并验证其在小数据集和零样本场景中的优越性能，推动高保真语音转换的实际应用。  <br/><br/>**贡献点**：  <br/>1. **提出ReFlow-VC方法**：基于修正流构建高效高保真语音转换模型，通过ODE框架将高斯分布直接映射到真实Mel频谱图分布，减少采样步骤。  <br/>2. **优化说话人特征建模**：同时利用内容与音高信息优化特征，增强说话人属性的准确表达。  <br/>3. **验证小样本适配性**：在小数据集和零样本场景中均表现出卓越的性能，突破传统扩散模型的局限。|
|2506.01020v1|[DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation](http://arxiv.org/abs/2506.01020v1)|总结：  <br/>该论文提出DS-TTS框架，结合双风格编码与动态生成网络，解决zero-shot语音克隆问题，实现未见过语音的高质量合成，并在VCTK数据集上验证了其优越的性能。<br/><br/>贡献点：  <br/>1. **提出DS-TTS框架**：设计一种用于zero-shot语音克隆的全新方法，仅需单个音频样本和任意文本即可合成目标说话人语音，无需历史训练数据。  <br/>2. **双风格编码网络（DuSEN）**：通过两个互补的风格编码器捕捉说话人语音身份的多维度特征，增强对未见说话人风格的建模能力。  <br/>3. **动态融合机制（SGF + DyGN）**：引入风格门控-Film机制将风格向量融入动态生成器，实现对不同句子长度的自适应合成，提升鲁棒性与自然度。  <br/>4. **实验验证优势**：在VCTK数据集上证明DS-TTS在单词错误率和说话人相似度上优于现有模型，显著改善语音克隆的泛化与表达性。|
|2506.00832v1|[Counterfactual Activation Editing for Post-hoc Prosody and   Mispronunciation Correction in TTS Models](http://arxiv.org/abs/2506.00832v1)|**贡献点：**<br/>1. **提出模型无关的后处理技术**：Counterfactual Activation Editing 是一种通用方法，无需依赖预训练模型特定架构，可适用于多种TTS模型。  <br/>2. **实现韵律与发音的联合控制**：同时支持对语音韵律特征（prosody）和发音错误（mispronunciation）的精确调整，提升控制能力。  <br/>3. **无需额外训练即可进行推理时调整**：在不重新训练模型的情况下，直接对预训练TTS的内部表示进行修改，实现在线输出优化。  <br/>4. **适应低资源场景**：解决传统发音纠正依赖音素词典（grapheme-to-phoneme dictionaries）的问题，降低对资源的依赖。  <br/>5. **填补预训练模型与可编辑合成的空白**：首次将可编辑的语音合成能力引入预训练TTS框架，推动模型灵活性与应用范围的扩展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种通用、无需额外训练的后处理方法，实现对TTS输出的韵律与发音联合控制，适应低资源场景，推动预训练模型向可编辑语音合成发展。|
|2506.00736v1|[IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio   Generation with Diffusion Modeling](http://arxiv.org/abs/2506.00736v1)|总结：  <br/>本文提出IMPACT框架，结合扩散建模与迭代掩码并行解码，在连续潜空间中生成高质量文本到音频，有效解决先前模型在音质与推理速度之间的权衡问题，实验在AudioCaps上达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出IMPACT框架**：首次将迭代掩码并行解码与扩散模型结合，实现文本到音频生成中高保真度与低延迟的双重优化。  <br/>2. **连续潜空间设计**：通过在连续潜空间中操作，避免离散token带来的保真度限制，提升生成音频的自然度和质量。  <br/>3. **高效解码策略**：采用迭代掩码并行解码技术，在保持推理速度的同时显著提高音频生成效果，优于MAGNET等基于离散token的模型。  <br/>4. **验证SOTA性能**：在AudioCaps数据集上，IMPACT在Fréchet Distance (FD)和Fréchet Audio Distance (FAD)等关键指标上超越现有方法，验证了其有效性。|
|2506.00722v1|[Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](http://arxiv.org/abs/2506.00722v1)|总结：该研究提出基于链式思维的端到端语音对话系统方法，在较小数据集上实现显著性能提升并提升语义连贯性，同时增强计算效率，模型及代码将公开。<br/><br/>贡献点：<br/>1. 提出基于链式思维（Chain-of-Thought, CoT）的端到端（E2E）对话系统框架，解决了传统级联流水线的局限性；<br/>2. 通过将对话训练与多模态语言模型（涵盖语音识别、文本生成、语音合成等任务）的预训练对齐，提升语义连贯性；<br/>3. 实现仅需300小时公开对话数据（如Switchboard）即可获得超过1.5 ROUGE-1的提升效果；<br/>4. 优化计算效率，使模型能在有限数据量下高效训练；<br/>5. 提供公开模型及代码，推动该领域研究与应用。|
|2506.00548v1|[Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities](http://arxiv.org/abs/2506.00548v1)|总结（100字以内）：  <br/>本论文提出Con Instruction方法，通过生成对抗图像和音频攻击多模态语言模型，无需文本预处理或训练数据。引入新型ARC评估框架，并验证其在多个模型上的高攻击成功率，揭示当前安全机制的不足。<br/><br/>贡献点（分点列出）：  <br/>1. **提出非文本指令攻击方法（Con Instruction）**  <br/>   - 首次利用MLLMs对非文本指令（图像/音频）的解析能力，通过生成对抗性图像或音频发起攻击，突破传统依赖文本指令的攻击模式。  <br/>   - 攻击无需训练数据或文本预处理，简化了攻击流程。  <br/><br/>2. **优化对抗样本的语义对齐**  <br/>   - 在嵌入空间中优化对抗样本，使其更精准匹配目标指令，显著提升攻击成功率，揭示MLLMs复杂语义理解的潜在安全风险。  <br/><br/>3. **提出攻击响应分类（ARC）框架**  <br/>   - 引入新型评估体系，综合衡量模型响应质量与对恶意指令的相关性，为安全评估提供更全面的指标。  <br/><br/>4. **验证多模型有效性与攻击性能**  <br/>   - 在LLaVA-v1.5、InternVL、Qwen-VL、Qwen-Audio等多模态模型上验证攻击效果，尤其在LLaVA-v1.5 (13B)中达到81.3%和86.6%的成功率，证明方法的普适性。  <br/><br/>5. **分析防御技术的局限性**  <br/>   - 探索针对攻击的防御措施，发现现有技术在攻击检测和防御能力上存在显著性能差距，为后续防御研究提供方向。  <br/><br/>6. **开放源代码实现**  <br/>   - 公布实现代码，便于学术界和工业界复现与扩展研究。|
|2506.00462v1|[XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](http://arxiv.org/abs/2506.00462v1)|**贡献点：**  <br/>1. **提出跨域多语言音频深度伪造基准**：构建XMAD-Bench数据集，包含668.8小时真实与深度伪造语音，覆盖多语言和多样化生成方法。  <br/>2. **强化数据集多样性与挑战性**：训练与测试集的说话人、生成模型、真实音频源均独立，显著提升跨域评估的难度。  <br/>3. **揭示性能差距**：实验结果表明，域内检测准确率接近100%，但跨域性能严重下降，接近随机猜测，突显现有方法的局限性。  <br/>4. **推动鲁棒检测器发展**：强调需开发具备跨语言、跨说话人、跨生成方法和跨数据源泛化能力的深度伪造检测技术。  <br/>5. **公开数据集**：将XMAD-Bench在GitHub平台开源，为研究社区提供统一的评估基准。  <br/><br/>**总结**：  <br/>提出首个跨域多语言音频深度伪造基准XMAD-Bench，验证现有检测模型在真实场景下的性能不足，推动更鲁棒的检测技术发展。|
|2506.00385v1|[MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity   Reconstruction and Generation](http://arxiv.org/abs/2506.00385v1)|**贡献点：**  <br/>1. 提出新型单层流式Transformer音频编解码器MagiCodec，解决传统编解码器重建质量与下游模型兼容性之间的矛盾。  <br/>2. 引入多阶段训练框架，结合高斯噪声注入与潜在正则化技术，增强编码的语义表达性并保持重建保真度。  <br/>3. 理论推导噪声注入在频域中的作用，证明其对抑制高频成分、提升鲁棒性效果显著。  <br/>4. 实验验证MagiCodec在重建质量与下游任务（如语言模型兼容）上优于现有技术，生成的编码呈现齐普夫分布特性。  <br/>5. 开源代码与预训练模型，促进研究复现与应用。  <br/><br/>**总结：**  <br/>MagiCodec通过噪声注入与多阶段训练提升音频编码的语义表达和重建质量，实现与语言模型的兼容性，开源推动研究进展。|
|2506.00087v1|[SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset](http://arxiv.org/abs/2506.00087v1)|总结：  <br/>本文提出多代理协作框架LinguaMaster，构建首个大规模多语言多民族代码转换数据集SwitchLingua，并设计语义感知评估指标SAER，推动多语言和文化多样性研究。  <br/><br/>贡献点：  <br/>1. **提出LinguaMaster框架**：首次设计多智能体协作系统，实现高效可扩展的多语言数据合成。  <br/>2. **构建SwitchLingua数据集**：创建首个覆盖12种语言、18国/地区、63个种族背景的大型代码转换语料库（含420K文本样本及80小时音频）。  <br/>3. **创新SAER评估指标**：提出融合语义信息的代码转换ASR评估方法，提升对跨语言场景的性能判断准确性。|
|2505.24496v1|[Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation](http://arxiv.org/abs/2505.24496v1)|总结：  <br/>本文提出一种压缩-精细语言建模方法，通过保留局部上下文和压缩长距离信息，有效解决神经音频编解码器在语音生成中的长序列挑战，提升生成质量与模型通用性。<br/><br/>贡献点：  <br/>1. **揭示语音标记序列的短距离依赖性**：首次观察到TTS任务中语音标记的预测主要依赖局部上下文，长距离标记贡献较小且冗余。  <br/>2. **设计压缩-精细语言建模框架**：  <br/>   - 保留初始提示和短范围标记，确保文本对齐及语用信息完整性；  <br/>   - 将长范围标记压缩为紧凑表示，减少冗余同时保留关键语义。  <br/>3. **验证方法的有效性与通用性**：通过多组神经音频编解码器和下游语言模型的实验，证明该方法可提升语音生成效果，具有广泛适用性。|
|2505.24314v1|[DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture   Switching for Speech Codec](http://arxiv.org/abs/2505.24314v1)|**贡献点：**  <br/>1. 提出DS-Codec，首次结合镜像（mirror）与非镜像（non-mirror）架构的双阶段训练框架，提升语音重建质量。  <br/>2. 设计动态切换机制，平衡镜像与非镜像结构的优势，增强模型的鲁棒性与高保真度。  <br/>3. 通过大规模实验与消融研究，验证了训练策略的有效性，对比了两种架构的性能差异。  <br/><br/>**总结：**  <br/>本研究提出DS-Codec，通过镜像与非镜像结构的双阶段训练框架提升语音重构效果，并系统验证了其优势。|
|2505.23619v1|[Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](http://arxiv.org/abs/2505.23619v1)|贡献点总结：<br/>1. 提出ADD-GP框架：基于高斯过程的少样本音频深度伪造检测方法，支持快速适应新生成模型  <br/>2. 性能突破：结合深度嵌入模型与GP灵活性，在少量数据下实现高检测准确率  <br/>3. 个性化检测能力：增强对新型TTS模型的鲁棒性，支持单样本级自适应  <br/>4. 构建基准数据集：使用SOTA语音克隆模型创建专用评估数据集  <br/><br/>（99字）|
|2505.20868v2|[Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction   and Style Direction Adjustment for Expressive Text-to-Speech](http://arxiv.org/abs/2505.20868v2)|**贡献点：**<br/>1. 提出**声带感知的风格提取方法**，专门关注与风格强相关的发声区域，提升表达性。<br/>2. 引入**风格方向调整机制**，优化风格嵌入与TTS模型的融合以改善音质。<br/>3. 实验证明Spotlight-TTS在**表现力、整体语音质量和风格迁移能力**上优于现有方法。<br/>4. **公开音频样本**，促进研究社区验证与复现。<br/><br/>**总结：**  <br/>本文提出Spotlight-TTS，通过声带感知风格提取与方向调整显著提升表达性语音合成效果，实验验证其优于基线模型，数据公开共享。|
|2505.19931v2|[Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned   Step Sampling](http://arxiv.org/abs/2505.19931v2)|总结（100字以内）:  <br/>提出训练无关的Fast F5-TTS方法，通过分析采样轨迹设计EPSS策略减少冗余步骤，实现7步生成且推理速度提升4倍，验证了EPSS对流匹配类TTS模型的通用性。  <br/><br/>贡献点:  <br/>1. **训练无关加速方案**：提出Fast F5-TTS，无需额外训练直接优化流匹配TTS模型的推理效率。  <br/>2. **非均匀采样策略**：开发EPSS方法，通过识别冗余采样步骤，显著减少生成过程的采样次数。  <br/>3. **高效推理性能**：在NVIDIA RTX 3090上实现7步生成，推理RTF达0.030，且性能与原模型相当。  <br/>4. **跨模型通用性**：验证EPSS在E2 TTS等其他流匹配模型上的有效性，证明其广泛适用性。|
|2505.19669v2|[Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling](http://arxiv.org/abs/2505.19669v2)|总结：  <br/>提出SMLLE框架，通过Transducer与Delete <Bos>机制解决流式TTS低延迟问题，实现高质量逐帧语音生成，优于现有流式方法且接近句级系统性能。<br/><br/>贡献点：  <br/>1. **提出SMLLE框架**：首次构建零样本流式TTS系统，无需依赖未来文本即可实现低延迟高质量语音合成。  <br/>2. **融合Transducer与时长对齐**：实时将文本转化为语义标记，结合时长对齐信息提升语音自然度。  <br/>3. **设计Delete <Bos>机制**：优化自回归模型对未来文本的访问，最小化延迟同时保持生成稳定性。  <br/>4. **验证性能优势**：实验表明SMLLE在流式TTS任务中超越现有方法，在句级系统中表现可比，具备实际应用价值。|
|2505.19595v2|[Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment](http://arxiv.org/abs/2505.19595v2)|总结：  <br/>本研究提出A-DMA方法，通过双模态对齐优化扩散模型的训练，显著提升收敛速度与生成质量，降低计算成本，并开源代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **提出A-DMA策略**：设计双模态对齐机制，通过文本与语音模态的协同优化加速扩散模型的训练过程。  <br/>2. **双模态对齐技术**：引入文本引导对齐（融合上下文表示）和语音引导对齐（精炼语义表示）的双重对齐管道。  <br/>3. **减少隐式学习依赖**：通过显式对齐隐藏状态与判别特征，降低扩散模型对复杂中间表示的隐式学习需求。  <br/>4. **实验验证效果**：在多个基准上验证A-DMA的性能优势，收敛速度提升1倍，生成质量优于现有方法，并开源代码与演示样本。|
|2505.17076v2|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v2)|总结：  <br/>研究分析不同帧率对中文和英文语音分词效果的影响，揭示帧率与语言特性间的关系，为语音分词优化提供参考。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统探讨低帧率编解码器对语音分词的影响，揭示帧率与语言模型间的作用机制。  <br/>2. **跨语言对比分析**：通过对比汉语和英语两种音系差异显著的语言，发现帧率对不同语言的分词效果存在异质性。  <br/>3. **揭示关键影响因素**：明确帧率变化与音素密度、语言特异性声学特征的交互作用，为分词器设计提供理论依据。  <br/>4. **应用指导意义**：提出优化帧率选择的策略，对自动语音识别、文本到语音等语音处理任务具有实践价值。|
|2504.14906v3|[OmniAudio: Generating Spatial Audio from 360-Degree Video](http://arxiv.org/abs/2504.14906v3)|总结：  <br/>提出360V2SA新任务，构建Sphere360数据集，设计半自动化数据流程，提出OmniAudio框架（结合自监督训练与双分支结构），在空间音频生成领域取得SOTA成果。<br/><br/>贡献点：  <br/>1. 提出360V2SA任务：首次系统性生成360度视频对应的3D空间音频（FOA格式），填补传统视频-音频生成在空间线索上的不足。  <br/>2. 构建Sphere360数据集：首个针对该任务的高质量数据集，基于真实场景数据并优化数据配对与清洗流程。  <br/>3. 设计半自动化数据收集管道：提供高效且可扩展的视频-音频数据配对与预处理方案，降低数据获取门槛。  <br/>4. 提出OmniAudio框架：结合自监督预训练和双分支结构（全景+视角视频输入），同时建模局部与全局空间信息。  <br/>5. 实验验证与开源：在Sphere360上实现SOTA性能，代码与数据集开源，推动领域研究与应用。|