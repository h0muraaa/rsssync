|Source|Title|Summary|
|---|---|---|
|2512.05126v1|[SyncVoice: Towards Video Dubbing with Vision-Augmented Pretrained TTS Model](https://arxiv.org/abs/2512.05126v1)||
|2511.17555v1|[Speech Recognition Model Improves Text-to-Speech Synthesis using Fine-Grained Reward](https://arxiv.org/abs/2511.17555v1)||
|2511.14948v1|[RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems](https://arxiv.org/abs/2511.14948v1)|总结：提出一种低成本跨模态多摄像头同步方法，实现毫秒级精度，显著优于现有技术并验证于复杂手术场景。<br/><br/>贡献点：<br/>1. 首创通用低成本同步方案，兼容异构设备（专业/消费级、可见光/红外）<br/>2. 设计LED Clock时序编码系统，通过视觉解码实现跨模态毫秒级同步<br/>3. 在多模态视频同步任务中取得1.34ms RMSE的业界领先精度<br/>4. 在25+摄像头的大型手术场景中验证系统鲁棒性与实用性<br/>5. 为工业/临床等无约束环境提供可靠的时间空间对齐解决方案|
|2511.14312v1|[H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata](https://arxiv.org/abs/2511.14312v1)|**贡献点：**  <br/>1. **多尺度VAE架构**：构建生理特征解耦的潜在空间，实现心律、心音、杂音等多模态信号的分离表征。  <br/>2. **分层文本到生物信号管道**：基于临床结构化元数据，精细控制17种病理条件的生成，提升数据可控性。  <br/>3. **医学注意力引导的扩散过程**：引入可解释的Medical Attention模块，增强生成信号的临床可解释性与真实性。  <br/><br/>**总结（100字内）：**  <br/>提出H-LDM模型，通过多尺度VAE、分层文本控制和医学注意力模块生成高临床效用的PCG信号，解决病理数据稀缺问题，显著提升罕见疾病分类准确率，推动心脏诊断数据增强技术发展。|
|2511.14223v2|[StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model](https://arxiv.org/abs/2511.14223v2)|**贡献点：**  <br/>1. 提出首个流式自回归扩散模型，解决长音频处理时的训练限制与延迟问题。  <br/>2. 引入动态条件机制，结合历史运动上下文与音频输入生成实时面部动作。  <br/>3. 实现低延迟、高灵活性的实时交互演示，验证方法的高效性与高质量输出。  <br/>4. 公开代码库，推动语音驱动3D动画技术的落地与应用。  <br/><br/>**总结：**  <br/>本论文提出流式自回归扩散模型，有效解决长音频生成的延迟与训练限制问题，实现高质量实时面部动画生成，并提供开源代码促进应用。|
|2511.14223v1|[StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model](https://arxiv.org/abs/2511.14223v1)|**贡献点**（分点列出）：  <br/>1. **提出新型流式自回归扩散模型**：首次将自回归机制引入语音驱动3D面部动画任务，实现对音频输入的分段流式处理，解决传统方法处理超长音频序列时的性能下降问题。  <br/>2. **动态条件生成**：通过结合有限历史帧的运动上下文与实时音频输入，构建动态条件引导扩散过程，提升生成结果的同步性和自然度。  <br/>3. **低延迟实时合成**：设计流式处理框架，确保模型延迟与音频长度无关，支持高效实时生成高质量面部动画。  <br/>4. **交互式演示与代码开源**：提供实时交互演示验证方法有效性，并通过公开代码促进技术复用与进一步研究。  <br/><br/>**总结（100字以内）**：  <br/>本文提出流式自回归扩散模型，解决语音驱动3D面部动画中长音频处理的延迟和泛化问题，通过动态条件设计实现高质量实时生成，并开放代码促进应用与研究。|
|2511.13912v1|[Compute-in-Memory Implementation of State Space Models for Event Sequence Processing](https://arxiv.org/abs/2511.13912v1)|总结:  <br/>该论文提出一种基于能量高效CIM硬件的SSM实现方法，通过参数优化和算法-硬件协同设计，在事件驱动任务中实现高能效与高准确性的实时处理。<br/><br/>贡献点：  <br/>1. **SSM硬件实现创新**：首次将状态空间模型（SSM）部署至计算存储一体（CIM）硬件，支持实时、事件驱动的处理。  <br/>2. **模型参数优化**：重新参数化模型为实系数与共享衰减常数，降低硬件映射复杂度。  <br/>3. **硬件-算法协同设计**：利用忆阻器的短期记忆效应与对角化状态转移参数，直接在交叉条系统中实现状态演化。  <br/>4. **高能效与异步支持**：系统在事件驱动视觉/音频任务中同时实现高准确率、高能效和全异步处理。|
|2511.13802v1|[Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video](https://arxiv.org/abs/2511.13802v1)|总结：  <br/>提出无需语言或文本的面部微动态分析方法，构建跨设备文化可移植的轻量级模型，在YT DemTalk数据集上实现高性能的痴呆症筛查。<br/><br/>贡献点：  <br/>1. **语言无关检测**：首次通过面部微动态（眨眼、口部运动、眼球波动、头部微调）实现无语音/文本的早期神经认知变化筛查。  <br/>2. **跨场景泛化能力**：方法适用于非临床、无剧本的自然视频，可跨设备、主题和文化迁移，无需临床干预。  <br/>3. **微动态时间序列建模**：通过信号稳定化、平滑处理及窗口统计，将微动作转化为可解释的片段级特征。  <br/>4. **新数据集YT DemTalk**：构建首个公开可用的面向野生视频的痴呆症筛查数据集，提供基准测试。  <br/>5. **轻量级分类器性能**：使用浅层模型在数据集上达成高指标（AUROC 0.953，F1 0.851），验证方法有效性。|
|2511.13127v1|[VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127v1)|**贡献点：**  <br/>1. **提出VEIL框架**：首次设计针对T2V模型的隐蔽性 jailbreak 攻击方法，通过模块化提示设计利用跨模态关联模式。  <br/>2. **三组件提示结构**：创新性结合中性场景锚点（保持表面合理性）、潜在听觉触发器（隐含危险视觉概念的音频线索）和风格调节器（增强触发效果）。  <br/>3. **约束优化生成**：将攻击生成形式化为跨模态提示空间的优化问题，通过引导搜索算法平衡攻击隐蔽性与效果。  <br/>4. **实验验证有效性**：在7个T2V模型上验证，显著提升商业模型攻击成功率（平均提升23%），揭示模型安全盲区。  <br/><br/>**总结（100字内）：**  <br/>本文提出VEIL框架，通过模块化提示设计实现隐蔽性T2V jailbreak攻击，有效绕过安全机制并提升攻击成功率，揭示模型在跨模态隐含线索下的脆弱性。|
|2511.12662v1|[Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662v1)|总结：  <br/>本文提出了一种高保真实时交互数字人系统，通过异步执行框架与多模态技术整合，实现感知、语音和对话的协同，具备唤醒词检测、情感语调生成及上下文感知对话能力，适用于沉浸式通信、教育与娱乐场景。<br/><br/>贡献点：  <br/>1. **多模态一体化系统**：首次将高保真3D视觉化身、情感驱动语音合成与知识导向对话生成无缝整合，实现视觉与语音的实时交互同步。  <br/>2. **异步执行框架**：设计低延迟异步管道，协调多模态组件的高效协作，提升系统实时响应能力。  <br/>3. **上下文感知对话**：引入意图路由与历史增强机制，实现精准知识检索及自然对话流维护。  <br/>4. **高级交互功能**：支持唤醒词检测、情感语调表达和高度准确的实时对话生成，扩展数字人的应用场景。|
|2511.11285v1|[Language-Aided State Estimation](https://arxiv.org/abs/2511.11285v1)|总结（100字以内）:  <br/>提出Language-Aided Particle Filter（LAPF）框架，融合自然语言处理与粒子滤波，实现基于人类观测的物理系统状态估计，并通过灌溉渠水位估计验证其有效性。  <br/><br/>**贡献点**：  <br/>1. **提出LAPF框架**：首次将自然语言处理（NLP）与粒子滤波结合，通过结构化人类观测（如文本/语音）提升状态估计性能。  <br/>2. **人类作为感知代理**：创新性地将人类自然语言观测视为传感数据，突破传统传感器依赖模式，适用于复杂场景。  <br/>3. **实际应用验证**：在灌溉渠水位估计任务中验证方法有效性，展示其在工程中的可迁移性与实用性。|
|2511.10693v1|[Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693v1)|贡献点：  <br/>1. 首次系统验证文本到语音系统是否能隐式学习人类礼貌的非显性语音线索（语速变化）。  <br/>2. 通过跨平台实验（AI Studio和OpenAI）发现礼貌提示显著降低合成语音语速，且效果普遍显著。  <br/>3. 揭示AI语音系统已内化社会规范（如减缓语速传递礼貌），表明其具备模仿人类心理细微差别的能力。  <br/>4. 突出AI作为社会行为者的潜力，可主动强化或复制人类社交规范。  <br/><br/>总结（100字内）：  <br/>本研究证实先进语音系统能隐式学习并模仿人类礼貌的非显性语音特征（如减缓语速），跨平台实验证明其对社会规范的普遍适应性，凸显AI在社交互动中的潜在角色及对人类行为模式的复制能力。|
|2511.10212v1|[Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization](https://arxiv.org/abs/2511.10212v1)|总结：本文提出单阶段训练框架与窗口级注意力机制，提升多模态深度伪造检测的泛化能力与局部伪影识别，实验证明其在时空定位上的有效性。<br/><br/>贡献点：<br/>1. 提出单阶段训练框架，通过结合下一帧预测（uni-modal/cross-modal features）增强模型对未见操控和数据集的泛化能力。<br/>2. 引入窗口级注意力机制，精准捕捉预测帧与实际帧间的差异，有效检测局部模态内部伪影。<br/>3. 在多基准数据集验证中，模型展现强泛化性与高精度的时空定位性能，解决现有方法对音视频对齐攻击的失效问题。|
|2511.09918v1|[MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection](https://arxiv.org/abs/2511.09918v1)|总结：  <br/>本文提出Norm-RAG框架，结合检索增强与代理机制，实现多轮对话中的社会规范推理，并发布MINDS多语言数据集，推动跨文化社交规范建模与对话系统研究。<br/><br/>贡献点：  <br/>1. **提出Norm-RAG框架**：首个面向多轮对话的社会规范推理系统，融合检索增强（Retrieval-Augmented）与代理机制（Agentic），实现对交际意图、角色、框架及语言线索的综合建模。  <br/>2. **创新语义分块方法**：开发Semtic Chunking技术，从结构化规范文档中精准检索上下文相关的规范信息，增强模型对规范依据的解释性。  <br/>3. **构建MINDS数据集**：首次发布包含31个多语种（中英、西英）多轮对话的双语数据集，通过多人标注确保跨文化规范标注的准确性与真实性。  <br/>4. **跨文化规范建模**：验证模型在跨语言环境下的适应性，凸显社会规范推理对文化差异的敏感度与泛化能力。  <br/>5. **提升对话系统性能**：实验证明Norm-RAG在规范检测与跨文化任务中表现优异，为文化适应性与社会智能的对话系统提供新方法。|
|2511.09552v1|[Intelligent Carrier Allocation: A Cross-Modal Reasoning Framework for Adaptive Multimodal Steganography](https://arxiv.org/abs/2511.09552v1)|**总结**：提出基于跨模态推理引擎的智能载体分配框架，实现多模态载体自适应优化，提升隐写系统安全性与抗分析能力。  <br/><br/>**贡献点**：  <br/>1. **提出智能载体分配框架**：基于跨模态推理引擎（CMR），动态评估多模态载体（图像、音频、文本）的适配性。  <br/>2. **多模态可靠性评估**：引入熵、信号复杂度、词汇丰富度等指标，量化载体安全性与隐蔽性。  <br/>3. **自适应数据分配策略**：根据可靠性分数动态分配秘密数据，优先利用高复杂度载体提升隐蔽性。  <br/>4. **对抗隐写分析的优化**：通过动态调整载体负载，增强系统抗检测能力，优于传统静态方法。  <br/>5. **实验验证优势**：证明该框架在安全性和数据保护性能上显著优于现有非自适应多模态技术。|
|2511.08654v1|[AI-generated podcasts: Synthetic Intimacy and Cultural Translation in NotebookLM's Audio Overviews](https://arxiv.org/abs/2511.08654v1)|1. 首次将AI生成的播客视为媒体进行系统分析，填补了AI音频生成领域研究空白；  <br/>2. 揭示NotebookLM播客遵循固定模板结构，体现算法对内容形式的标准化控制；  <br/>3. 发现AI通过语言转换强化了白人、受教育的中产阶级文化默认值；  <br/>4. 质疑传统多公共领域理论，指出AI播客消解了人类播客中社区针对性与互动性。  <br/><br/>总结：该研究揭示AI播客的标准化模板与文化偏见，挑战传统公共领域理论，推动对AI生成媒体社会影响的反思。|
|2511.08031v1|[Multi-modal Deepfake Detection and Localization with FPN-Transformer](https://arxiv.org/abs/2511.08031v1)|总结：  <br/>提出一种基于FPN-Transformer的多模态框架，结合预训练模型与多尺度特征金字塔，实现跨模态泛化和帧级伪造检测定位，在IJCAI'25基准上取得0.7535的高分表现。<br/><br/>贡献点：  <br/>1. **提出多模态检测定位框架**：设计FPN-Transformer架构，解决跨模态特征融合与细粒度时间边界回归问题。  <br/>2. **利用预训练自监督模型**：采用WavLM（音频）、CLIP（视频）提取层次化时序特征，提升特征表征能力。  <br/>3. **构建多尺度特征金字塔**：通过R-TLM块与局部注意力机制，联合建模跨上下文时序依赖关系。  <br/>4. **双分支预测机制**：同时预测伪造概率与时间偏移量，实现帧级精确定位。  <br/>5. **实验验证有效性**：在IJCAI'25 DDL-AV基准测试中取得0.7535的高检测定位得分，证实方法有效性。|
|2511.07993v1|[Private Chat in a Public Space of Metaverse Systems](https://arxiv.org/abs/2511.07993v1)|总结：  <br/>本文提出Hushhub系统，解决社交VR中私密对话缺失问题，通过整合私聊功能提升沉浸式交互体验，并验证其对丰富社交互动的价值。<br/><br/>贡献点：  <br/>1. **识别关键问题**：指出社交VR环境中缺乏私密对话功能，限制用户进行非公开交流。  <br/>2. **设计Hushhub系统**：开发集成式私密聊天系统，允许用户在共享VR空间中选择性发起私人音频对话。  <br/>3. **实证验证有效性**：通过用户研究验证系统对提升社交互动质量的实际效果。  <br/>4. **推动社交VR发展**：证明私密对话功能的必要性，为更复杂的沉浸式社交场景提供技术基础。|
|2511.07940v1|[Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?](https://arxiv.org/abs/2511.07940v1)|**贡献点总结：**  <br/>1. **挑战传统假设**：揭示参考视频时长并非关键，短时长高质量片段可达到甚至超越全视频效果，强调信息质量优于长度。  <br/>2. **提出ISExplore方法**：基于音频特征多样性、嘴唇运动幅度、相机视角数量三个维度，开发高效且自动的片段选择策略。  <br/>3. **显著提升效率**：在NeRF和3DGS方法中，数据处理与训练速度提升超5倍，同时保持高保真输出，降低计算成本。  <br/>4. **开放项目资源**：提供可复现实验与代码，促进社区进一步探索与应用。  <br/><br/>（总结：本研究通过提出ISExplore策略，证明短时长高质量参考片段可有效提升TFG效率，同时保持输出质量，推动实际应用。）|
|2511.07619v1|[CAVER: Curious Audiovisual Exploring Robot](https://arxiv.org/abs/2511.07619v1)|总结：  <br/>本研究提出CAVER，一种结合3D打印末端执行器、音频视觉表示及好奇心驱动算法的机器人，显著提升材料分类和音频演示模仿效率。<br/><br/>贡献点：  <br/>1. **新型硬件设计**：开发可附加于并行夹爪的3D打印末端执行器，主动激发物体的音频响应。  <br/>2. **多模态表示方法**：融合局部/全局视觉特征与声音特征，构建更丰富的音频视觉物体表征。  <br/>3. **好奇心驱动探索**：提出优先探索高不确定性的物体的算法，以更少交互实现对意外音频的广泛覆盖。|
|2511.07205v1|[Twenty-Five Years of MIR Research: Achievements, Practices, Evaluations, and Future Challenges](https://arxiv.org/abs/2511.07205v1)|**贡献点：**  <br/>1. 回顾MIR领域25年发展历程，梳理其研究重点与趋势。  <br/>2. 强调MIR与IEEE音频信号处理技术委员会的紧密关联。  <br/>3. 系统总结MIR在音乐分析、处理、生成三大EDICS领域的核心成果。  <br/>4. 提出可重复研究、开放实践、产业协作及多样性推动等成功经验。  <br/><br/>**总结（100字内）：**  <br/>本论文回顾MIR领域25年发展，系统总结其在音乐分析、处理和生成的成果，分析可重复研究、产业协作及多样性推动等成功实践，并展望未来挑战，为该领域提供全面研究脉络与启示。|
|2511.07116v1|[BridgeVoC: Revitalizing Neural Vocoder from a Restoration Perspective](https://arxiv.org/abs/2511.07116v1)|总结：  <br/>本文提出BridgeVoC，将神经声码器任务重新定义为音频修复问题，通过引入Schrödinger桥框架、子带感知卷积扩散网络及单步推理蒸馏损失，实现了参数更少、计算成本更低且性能领先的语音生成方法。<br/><br/>贡献点：  <br/>1. **任务重构**：首次将神经声码器任务视为音频修复的特例，通过秩分析揭示Mel频谱与其他退化因素的差异特性。  <br/>2. **框架创新**：提出基于Schrödinger桥的扩散模型框架，将RSS与目标频谱定义为生成轨迹的双端点，增强模型对音频修复的适配性。  <br/>3. **子带感知设计**：开发子带意识卷积扩散网络，采用不均衡子带划分策略与大核卷积注意力模块，提升T-F域的语义建模效率。  <br/>4. **单步推理优化**：设计多目标蒸馏损失（含目标相关性与双射一致性），实现单步推理且保持高质量生成效果。  <br/>5. **高效性能**：在少参数、低计算成本下，仅需4步采样即可超越GAN/DDPM/流匹配基线，在多个基准和分布外数据集上验证性能领先。|
|2511.05516v1|[Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation](https://arxiv.org/abs/2511.05516v1)||
|2511.05143v1|[Synthesizing speech with selected perceptual voice qualities - A case study with creaky voice](https://arxiv.org/abs/2511.05143v1)||
|2511.03423v1|[Seeing What You Say: Expressive Image Generation from Speech](https://arxiv.org/abs/2511.03423v1)||
|2511.03295v2|[How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295v2)|总结：  <br/>本文首次系统研究源感知ST评估指标，提出ASR转录与反向翻译的双重生成策略，并设计跨语言重分段算法解决对齐问题，实验证明其有效性，推动ST评估方法的改进。<br/><br/>贡献点：  <br/>1. **首次系统性研究源感知指标**：针对ST领域源输入（音频）特性，提出首个专门研究源感知评估的框架，强调在无参考转录的现实场景中的应用。  <br/>2. **双生成策略**：探索ASR转录与参考翻译反向翻译两种文本代理方法，构建合成源文本以弥补真实源信息缺失。  <br/>3. **跨语言重分段算法**：提出新型两阶段跨语言对齐算法，解决合成源与参考译文间的对齐不匹配问题，提升指标可靠性。  <br/>4. **实证分析**：在79语言对、6系统上的实验表明，ASR转录在低WER场景更优，反向翻译则兼具效率与效果，为评估方法选择提供依据。  <br/>5. **方法迁移与推广**：验证源感知MT指标在ST中的可行性，为构建更准确、理论化的评估体系奠定基础。|
|2511.02104v1|[Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach](https://arxiv.org/abs/2511.02104v1)||
|2511.01310v2|[From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310v2)|**贡献点总结（分点）：**  <br/>1. **提出共享生成多模态世界模型（MWM）**：通过可扩展注意力机制融合多智能体多模态观测，学习压缩环境动态的潜变量表示。  <br/>2. **解耦表示与策略学习**：利用MWM作为快速模拟器，在其潜空间内训练协作策略，提升样本效率。  <br/>3. **引入挑战性多模态多智能体基准**：基于3D物理模拟器构建新基准，推动领域发展。  <br/>4. **显著提升样本效率**：实验显示，相比模型无关MARL基线，样本效率提升数量级。  <br/>5. **强调多模态融合的关键性**：证明其在感官不对称环境中的任务成功作用，及对传感器丢包的鲁棒性优势。  <br/><br/>**总结（100字内）：**  <br/>本文提出MWM-MARL框架，通过多模态融合与共享世界模型解耦表示与策略学习，显著提升协作MARL的样本效率，验证多模态融合对复杂环境的鲁棒性与任务关键性，为实际部署提供新方案。|
|2511.01261v1|[Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play](https://arxiv.org/abs/2511.01261v1)||
|2510.26190v1|[SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level](https://arxiv.org/abs/2510.26190v1)||
|2510.25178v1|[SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution](https://arxiv.org/abs/2510.25178v1)||
|2510.25054v2|[Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054v2)||
|2510.24992v1|[POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992v1)||
|2510.24372v1|[Bayesian Speech synthesizers Can Learn from Multiple Teachers](https://arxiv.org/abs/2510.24372v1)||
|2510.23969v1|[emg2speech: synthesizing speech from electromyography using self-supervised speech models](https://arxiv.org/abs/2510.23969v1)||
|2510.23541v2|[SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity](https://arxiv.org/abs/2510.23541v2)||
|2510.23056v2|[Enabling American Sign Language Communication Under Low Data Rates](https://arxiv.org/abs/2510.23056v2)|总结（100字以内）：  <br/>本文提出VC4ASL系统，通过音频传输人体姿态信息重建手语内容，使ASL用户在低带宽下实现无障碍通信，无需修改现有视频会议平台，并设计了基于姿态结构的接收端纠错机制。<br/><br/>贡献点：  <br/>1. **跨平台兼容性**：开发无需修改现有视频会议应用的ASL音频通信系统（VC4ASL），适应低带宽场景。  <br/>2. **姿态编码传输**：创新性地通过音频通道编码和传输人体姿态信息，实现手语内容的音频重建。  <br/>3. **接收端纠错机制**：提出利用手语姿态数据固有结构约束的误差检测与校正方法，提升通信鲁棒性。  <br/>4. **系统验证**：设计网络退化模拟与用户研究实验，验证系统在低带宽下的可理解性与实用性。|
|2510.22961v1|[Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition](https://arxiv.org/abs/2510.22961v1)||
|2510.22588v1|[UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models](https://arxiv.org/abs/2510.22588v1)||
|2510.21014v2|[ReFESS-QI: Reference-Free Evaluation For Speech Separation With Joint Quality And Intelligibility Scoring](https://arxiv.org/abs/2510.21014v2)||
|2510.20860v1|[Data-Centric Lessons To Improve Speech-Language Pretraining](https://arxiv.org/abs/2510.20860v1)||
|2510.19471v1|[Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition](https://arxiv.org/abs/2510.19471v1)||
|2510.19414v1|[EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection](https://arxiv.org/abs/2510.19414v1)||
|2510.18308v1|[ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation](https://arxiv.org/abs/2510.18308v1)||
|2510.16893v1|[Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893v1)||
|2510.16756v1|[End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756v1)||
|2510.16387v1|[Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387v1)||
|2510.14922v1|[TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922v1)||