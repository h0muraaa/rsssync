|Source|Title|Summary|
|---|---|---|
|2511.03080v1|[PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](http://arxiv.org/abs/2511.03080v1)|**贡献点总结：**  <br/>1. 提出基于大语言模型的文本规范化方法 PolyNorm，减少人工规则依赖，提升跨语言适用性；  <br/>2. 构建语言无关的自动数据构建与评估管道，支持多语言可扩展实验；  <br/>3. 发布 PolyNorm-Benchmark 多语言数据集，覆盖丰富文本规范化现象以促进研究。  <br/><br/>**摘要总结（100字内）：**  <br/>提出 PolyNorm 方法利用 LLM 实现高效文本规范化，构建语言无关数据流程，发布多语言基准数据集，解决传统系统工程成本高、语言覆盖局限等痛点，实验验证在八种语言中显著降低 WER。|
|2511.01299v1|[Towards General Auditory Intelligence: Large Multimodal Models for   Machine Listening and Speaking](http://arxiv.org/abs/2511.01299v1)|总结：  <br/>本文系统梳理了音频与大语言模型融合的最新进展，聚焦四大领域，揭示LLMs对音频感知与交互的革新，并探讨跨模态融合对多模态智能的推动作用，为构建音频原生的AGI系统提供研究方向与挑战分析。<br/><br/>贡献点：  <br/>1. **提出核心研究方向**：明确界定音频与LLMs融合的四大关键领域（音频理解、生成、语音交互、音视频联合理解），为后续研究提供框架。  <br/>2. **分析LLMs对音频领域的革新**：探讨大语言模型如何重构音频感知与推理能力，实现更深层语义理解与更自然的语音交互。  <br/>3. **跨模态融合的潜力挖掘**：研究音视频联合理解对情景意识与跨模态推理的增强作用，推动多模态智能边界拓展。  <br/>4. **批判性总结与展望**：系统归纳现有成果，识别构建音频原生AGI系统的挑战，并提出未来研究的优先方向。|
|2511.01091v1|[Feedback-driven Retrieval-augmented Audio Generation with Large Audio   Language Models](http://arxiv.org/abs/2511.01091v1)|总结：  <br/>提出一种反馈驱动的RAG方法，利用LALMs结合外部数据库检索优化音频生成，提升特定声音事件合成能力，并在多种模型上超越现有RAG专用方法。<br/><br/>贡献点：  <br/>1. 提出通用反馈驱动的RAG框架，首次将检索模块与音频生成过程动态结合。  <br/>2. 无需从头训练专用模型，直接利用预训练的LALMs分析生成结果并检索补充信息。  <br/>3. 设计反馈机制以识别预训练模型缺失或错误的声音事件，提升生成准确性。  <br/>4. 实验验证方法在多类音频生成任务（如语音、音乐、环境音）中的泛化能力。  <br/>5. 在多个基准模型上实现优于现有RAG专用方法的性能提升。|
|2510.25163v1|[Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD   Generation](http://arxiv.org/abs/2510.25163v1)|**总结**（100字以内）：  <br/>本文提出TGBFN框架，实现统一连续可微参数空间下的多模态CAD生成，引入引导贝叶斯流控制参数更新，构建新数据集并验证其在高保真、条件感知CAD生成任务中的优越性能。<br/><br/>**贡献点**：  <br/>1. 提出首个在统一连续可微参数空间下处理CAD序列多模态特性的框架TGBFN。  <br/>2. 引入引导贝叶斯流机制，实现对CAD参数的精细化控制。  <br/>3. 构建了用于量化约束CAD生成的新数据集。  <br/>4. 在单条件和多条件约束生成任务中均取得SOTA性能。|
|2510.24530v1|[Levée d'ambiguïtés par grammaires locales](http://arxiv.org/abs/2510.24530v1)|**贡献点总结：**<br/><br/>1. 提出一种适应零沉默率目标的词性消歧方法。  <br/>2. 强调需考虑词性消歧语法的交互作用而非单独路径。  <br/>3. 指出多个转换器组合的结果无法单独预测。  <br/>4. 强调需详细规格化语法以确保正确标签不被遗漏。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出一种适应零沉默率目标的词性消歧方法，并在INTEX系统中实现。强调语法交互验证的重要性，指出组合转换器的不可预测性，并建议详细语法规格以确保正确标签不被丢弃。|
|2510.24372v1|[Bayesian Speech synthesizers Can Learn from Multiple Teachers](http://arxiv.org/abs/2510.24372v1)|**贡献点总结：**<br/><br/>1. 提出BELLE，一种基于贝叶斯证据学习和语言建模的连续值自回归TTS框架。  <br/>2. 直接从文本预测梅尔频谱图，无需编码器-解码器结构。  <br/>3. 引入不确定性估计机制，提升模型在平行数据下的鲁棒性。  <br/>4. 利用多预训练TTS模型合成多样语音样本进行训练，提升泛化能力。  <br/>5. 在较少训练数据下实现高性能，具有实际应用潜力。|
|2510.24103v1|[Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain   Video-to-Audio Generation](http://arxiv.org/abs/2510.24103v1)|**贡献点总结：**<br/><br/>- 提出MGAudio，首个基于流的开放域视频到音频生成框架。<br/>- 引入模型引导的双角色对齐机制，提升生成质量与跨模态一致性。<br/>- 在VGGSound上取得SOTA结果，FAD降至0.40，优于现有方法。<br/>- 框架包含可扩展的Transformer模型、双角色对齐机制与模型引导目标。<br/>- 在UnAV-100等挑战性数据集上表现良好，具备良好泛化能力。|
|2510.23802v1|[Learning Interpretable Features in Audio Latent Spaces via Sparse   Autoencoders](http://arxiv.org/abs/2510.23802v1)|总结：  <br/>提出基于稀疏自编码器的音频生成模型解释框架，通过映射潜变量至可解释声学属性实现可控生成与分析，验证于多种音频模型并揭示音高、音色等特征演化规律，支持跨模态扩展。<br/><br/>贡献点：  <br/>1. **提出音频生成模型解释框架**：通过将语言模型中的稀疏自编码器（SAEs）迁移到音频自编码器潜变量，实现对生成过程的可解释性映射。  <br/>2. **设计离散声学属性建模方法**：学习从SAEs特征到音高、振幅、音色等离散声学属性的线性映射，支持对AI音乐生成的可控操作与分析。  <br/>3. **验证于主流音频模型**：在DiffRhythm-VAE（连续）和EnCodec/WavTokenizer（离散）等音频潜空间中验证框架有效性，展现其普适性。  <br/>4. **分析生成过程特征演化**：以DiffRhythm为案例，揭示音高、音色和响度等声学属性在音乐生成中的动态演化规律。  <br/>5. **提出跨模态扩展潜力**：框架可应用于视觉等其他模态的潜空间分析，为多模态模型解释提供方法论支持。|
|2510.23541v2|[SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and   Paralinguistic Diversity](http://arxiv.org/abs/2510.23541v2)|**贡献点：**<br/><br/>1. 提出SoulX-Podcast系统，专门用于多说话人、多轮对话的播客式语音生成。  <br/>2. 支持多种语言和方言，包括普通话、粤语、四川话、河南话等，提升个性化能力。  <br/>3. 实现自然的语调和韵律变化，支持上下文自适应的语音表达。  <br/>4. 在多轮对话中保持稳定的声调，实现超过90分钟的连贯生成。  <br/>5. 在传统TTS任务和多轮对话生成中均达到SOTA性能。  <br/><br/>**总结（100字以内）：**  <br/>SoulX-Podcast是首个支持多说话人、多轮对话的播客式语音生成系统，覆盖多种语言和方言，实现自然语调和稳定声调，性能达到SOTA。|
|2510.22588v1|[UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations   for Spoken Dialogue Models](http://arxiv.org/abs/2510.22588v1)|总结：  <br/>UltraVoice是首个支持多维度语音风格控制的大型对话数据集，显著提升模型的风格控制能力与对话表现。<br/><br/>贡献点：  <br/>1. 提出首个大规模语音对话数据集UltraVoice，支持六种细粒度语音风格控制（情感、语速、音量、口音、语言、复合风格）。  <br/>2. 在SLAM-Omni和VocalNet等领先模型上进行微调，显著提升其风格控制效果，同时保持对话能力。  <br/>3. 在多维度控制任务中，模型在MOS和IFR指标上分别提升29.12-42.33%和14.61-40.09个百分点。  <br/>4. 在URO-Bench基准上，模型在基础和进阶设置下分别提升10.84%和7.87%的核心理解、推理和对话能力。  <br/>5. 数据集适用于训练可控的TTS模型，具有高质量和广泛的应用性，支持表达性语音合成。|
|2510.21115v1|[Robust Distortion-Free Watermark for Autoregressive Audio Generation   Models](http://arxiv.org/abs/2510.21115v1)|**贡献点：**  <br/>1. 提出Aligned-IS，一种针对音频生成模型的无失真水印技术。  <br/>2. 采用聚类方法，解决传统水印方法在音频模型中的“retokenization mismatch”问题。  <br/>3. 在保持音频质量的同时，显著提升水印检测能力。  <br/>4. 在主流音频生成平台上的全面测试验证了其有效性，建立了新的安全音频技术基准。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Aligned-IS，一种无失真音频水印方法，通过聚类解决生成模型中的token不匹配问题，有效提升水印检测能力，为安全音频技术提供新基准。|
|2510.20210v1|[Vox-Evaluator: Enhancing Stability and Fidelity for Zero-shot TTS with A   Multi-Level Evaluator](http://arxiv.org/abs/2510.20210v1)|**贡献点（分点）:**  <br/>1. 提出Vox-Evaluator，一个结合多级评估机制的语音质量检测工具，可定位错误语音段的时间边界并进行整体质量评估。  <br/>2. 设计语音修正流程：通过自动识别音素错误、掩码处理错误片段并基于正确部分重新生成语音，提升零样本TTS的鲁棒性。  <br/>3. 构建首个细粒度标注的合成文本-语音数据集，用于训练和验证Vox-Evaluator，解决该领域缺乏标注数据的问题。  <br/>4. 实验证明Vox-Evaluator在增强TTS系统稳定性与保真度方面有效，通过语音修正和偏好优化减少合成语音缺陷。  <br/><br/>**总结（100字以内）:**  <br/>本文提出Vox-Evaluator，通过多级评估与修正机制提升零样本TTS的稳定性与质量，构建专用数据集支持训练，并实验证明其有效性，解决了误发音、噪声和质量下降等关键问题。|
|2510.19641v1|[Style Attack Disguise: When Fonts Become a Camouflage for Adversarial   Intent](http://arxiv.org/abs/2510.19641v1)|**贡献点：**<br/><br/>1. 识别出人类与NLP模型在处理风格化字体和emoji时的感知差异。  <br/>2. 提出一种基于风格的对抗攻击方法——Style Attack Disguise (SAD)。  <br/>3. 设计两种攻击强度：轻量版提升查询效率，强版增强攻击效果。  <br/>4. 在情感分类、机器翻译及多模态任务中验证SAD的有效性与潜在威胁。|
|2510.19414v1|[EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection](http://arxiv.org/abs/2510.19414v1)|总结：  <br/>本研究提出EchoFake数据集，包含真实物理重放攻击和零样本TTS语音，提升模型泛化能力，为语音反欺骗提供更现实的数据基础。<br/><br/>贡献点：  <br/>1. **构建现实攻击场景数据集**：创建EchoFake，包含超过120小时真实物理重放录音及零样本TTS语音，覆盖多设备和实际环境，弥补现有数据集对现实攻击场景的不足。  <br/>2. **验证模型泛化能力提升**：通过对比实验，证明基于EchoFake训练的检测模型在多数据集上的平均EER更低，表明其在应对实际攻击时具有更优泛化性能。  <br/>3. **推动反欺骗技术发展**：引入更贴近实际部署的挑战（如物理重放攻击），为语音伪装检测研究提供更具现实意义的基准，促进技术落地与改进。|
|2510.18355v1|[KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory   Call Center for Bengali Farmers](http://arxiv.org/abs/2510.18355v1)|总结：  <br/>KrishokBondhu 是一个基于RAG框架的语音农业咨询平台，通过电话和孟加拉语语音交互为偏远农民提供精准指导，显著提升回答质量并推动AI驱动的农业生态系统发展。<br/><br/>贡献点：  <br/>1. **构建RAG框架语音平台**：首次将检索增强生成（RAG）技术应用于农业领域，开发语音交互系统 KrishokBondhu，支持Bengali农民获取实时建议。  <br/>2. **多源文档结构化处理**：整合权威农业手册、政策文件和NGO资料，利用OCR和解析技术进行内容数字化与语义索引，提升信息可用性。  <br/>3. **多语言语音交互能力**：实现从Bengali语音到文本的转换、响应生成及自然语言语音输出，解决语言障碍问题。  <br/>4. **高效上下文关联回答**：通过RAG模块与大语言模型（Gemma 3-4B）结合，显著提升回答的上下文丰富度（+367%）和完整性（+100.4%）。  <br/>5. **验证AI农业可行性**：在试点中表现优于基准系统（综合得分4.53 vs. 3.13），证明RAG与电话服务中心的结合可有效支持远程农业指导。|
|2510.16497v1|[Edge-Based Speech Transcription and Synthesis for Kinyarwanda and   Swahili Languages](http://arxiv.org/abs/2510.16497v1)|总结（100字以内）:  <br/>该论文提出一种基于边缘-云并行的语音处理框架，通过级联机制优化资源分配，显著降低延迟和内存占用，在东非技术基础设施薄弱地区实现高效且准确的语音转录与合成。<br/><br/>贡献点：  <br/>1. **提出边缘-云协同框架**：首次将边缘计算与云计算结合，提升语音处理速度与可访问性，解决资源匮乏地区的需求。  <br/>2. **解决语言工具稀缺问题**：针对基库亚马语和斯瓦希里语在东非国家缺乏高效处理工具的挑战，提供可行方案。  <br/>3. **整合预训练模型**：利用Whisper与SpeechT5模型实现双向语音-文本转换，提高系统通用性与性能。  <br/>4. **级联架构设计**：通过任务分发机制降低计算延迟与资源消耗，优化边缘端与云端协同效率。  <br/>5. **内存压缩技术**：在边缘设备实现SpeechT5（9.5%）和Whisper（14%）的显著内存缩减，最大RAM仅149MB。  <br/>6. **实验证明可行性**：在低性能硬件（1.7GHz CPU+1MB/s网络）下验证系统处理能力，支持分钟级文本转录与合成。  <br/>7. **真实场景验证**：基于肯尼亚调研数据，证明框架在实际应用中的有效性与用户满意度。|
|2510.14628v1|[RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF](http://arxiv.org/abs/2510.14628v1)|总结（100字以内）:  <br/>提出RLAIF-SPA框架，通过RLAIF机制结合ASR和LLM直接优化情感表达与可理解性，在四维评价标准下提升语音质量，并在Libri Speech数据集上取得显著性能提升。<br/><br/>贡献点:  <br/>1. **框架创新**：设计RLAIF-SPA框架，整合强化学习与AI反馈机制，直接优化情感表达和语音可理解性，突破传统依赖人工标注或间接目标的限制。  <br/>2. **多维度评价**：引入Structure、Emotion、Speed、Tone四类细粒度维度，联合考量语义准确性与韵律-情感对齐，系统提升表达质量。  <br/>3. **双反馈机制**：结合ASR（语义准确性）与LLM（情感对齐）提供双重奖励信号，兼顾语音清晰度与自然情感传递。  <br/>4. **实验验证**：在Libri Speech数据集上对比Chat-TTS等方法，实验证明WER降低26.1%、SIM-O提升9.1%、人类评价提高超10%，展现有效性。|
|2510.13747v1|[InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn   Dialogue](http://arxiv.org/abs/2510.13747v1)|总结：  <br/>InteractiveOmni是一款轻量级多模态大语言模型，参数从4B到8B，通过统一架构和多阶段训练策略，实现高效音频-视觉交互与语音生成，展现优异的多轮记忆能力，性能媲美更大模型，为智能交互系统提供开放基础。<br/><br/>贡献点：  <br/>1. **模型架构统一**：首次将视觉/音频编码器、大语言模型和语音解码器集成，实现多模态理解与生成的统一框架。  <br/>2. **多阶段训练策略**：采用分步训练（预训练+后训练）增强跨模态对齐与多轮对话能力，提升复杂交互效果。  <br/>3. **多轮交互数据集**：构建高质量多跳对话数据集，优化模型处理长序列交互与上下文理解的能力。  <br/>4. **多模态基准测试**：提出专门针对多轮记忆与语音交互的评估基准，完善模型性能验证体系。  <br/>5. **轻量级性能优势**：4B版本性能接近7B级模型（如Qwen2.5-Omni），仅需50%模型规模即可达到97%的8B版本性能。  <br/>6. **开源与应用价值**：作为开放基础模型，推动下一代智能交互系统的发展，适用于多模态任务处理。|
|2510.13344v1|[UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity   MoE](http://arxiv.org/abs/2510.13344v1)|**贡献点总结（100字以内）:**  <br/>提出UniMoE-Audio统一语音音乐生成模型，结合动态容量MoE框架与三阶段训练策略，有效解决数据不平衡和领域冲突问题，实现跨模态协同学习并达到SOTA性能。<br/><br/>**分点贡献:**  <br/>1. **提出UniMoE-Audio模型**：首次构建统一的语音与音乐生成框架，突破传统分离训练模式，推动通用音频合成研究。  <br/>2. **创新Dynamic-Capacity MoE架构**：  <br/>   - 引入Top-P路由策略动态分配专家数量；  <br/>   - 设计混合专家结构（领域特异性专家、领域无关专家、自适应计算跳过专家）提升模型灵活性。  <br/>3. **三阶段训练策略**：  <br/>   - 阶段一：独立专家训练，建立各领域基础能力；  <br/>   - 阶段二：MoE集成与预热，优化门控模块与共享专家；  <br/>   - 阶段三：端到端协同联合训练，增强跨领域协同效应。  <br/>4. **实验证明有效性**：在语音与音乐生成基准测试中达成SOTA性能，显著缓解传统联合训练的性能退化问题。|
|2510.13293v1|[Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models](http://arxiv.org/abs/2510.13293v1)|**贡献点：**  <br/>1. 提出了一种自适应的Classifier-Free Guidance（CFG）方案，用于解决AR TTS模型中风格提示与语义内容不匹配的问题。  <br/>2. 通过大语言模型或自然语言推理模型检测风格-内容不匹配程度，并据此调整CFG。  <br/>3. 在提升情感表达的同时，保持了音频质量和可理解性。  <br/><br/>**总结（100字以内）:**  <br/>本文提出自适应CFG方案，解决AR TTS中情感提示与语义冲突问题，提升情感表达效果，同时维持音质和可理解性。|
|2510.12210v2|[DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation](http://arxiv.org/abs/2510.12210v2)|**贡献点总结**（100字以内）:  <br/>提出DISTAR框架，在离散RVQ空间结合自回归与扩散模型，无需对齐与持续预测器，实现块级并行生成与填充，增强鲁棒性与可控性，支持变比特率及多样性平衡，超越SOTA零样本TTS系统性能。<br/><br/>**分点贡献**:<br/>1. **零样本框架设计**  <br/>   采用纯离散残差向量量化（RVQ）代码空间，无需强制对齐或持续预测器，直接通过离散编码实现文本到语音的生成。<br/><br/>2. **块级并行生成机制**  <br/>   使用自回归模型生成块级RVQ令牌，再通过条件扩散模型并行填充后续块，提升长文本合成效率，缓解AR暴露偏差问题。<br/><br/>3. **显式可控性**  <br/>   - 支持贪心解码和样本基础解码（分类器无引导）  <br/>   - 提供稳健性与输出多样性的权衡调节  <br/>   - 通过RVQ层剪枝实现变比特率和可控计算资源消耗<br/><br/>4. **实验性能突破**  <br/>   在分布偏移、语音自然度、说话人/风格一致性等指标上超越当前最优的零样本TTS系统，同时保持丰富输出多样性。|
|2510.10396v3|[MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations](http://arxiv.org/abs/2510.10396v3)|**贡献点总结（100字以内）：**  <br/>提出MRSAudio多模态空间音频数据集，覆盖真实场景及多类型数据，支持五项空间音频研究任务，推动空间音频生成与理解发展，并开放数据集与演示资源。<br/><br/>**分点贡献：**  <br/>1. **填补研究空白**：首个大规模多模态空间音频数据集，突破传统单耳音频限制，促进空间音频生成与理解研究。  <br/>2. **多场景覆盖**：包含MRSLife、MRSSpeech、MRSMusic和MRSSing四个子集，覆盖生活、语音、音乐、歌唱等多样化现实场景。  <br/>3. **多模态整合**：同步提供双耳音频、Ambisonic音频、外在/内在视频、运动轨迹及细粒度注释（如转录、音素边界、歌词、乐谱等）。  <br/>4. **五项核心任务**：建立音频空间化、空间文本到语音、空间歌唱合成、空间音乐生成及声音事件定位检测等任务框架，验证数据集的多功能性。  <br/>5. **开放共享与应用**：公开数据集访问链接及演示，便于研究社区使用，加速空间音频技术的发展与落地。|
|2510.08078v3|[Detecting and Mitigating Insertion Hallucination in Video-to-Audio   Generation](http://arxiv.org/abs/2510.08078v3)|总结：  <br/>该论文首次定义并系统检测视频到音频生成中的插入幻觉问题，提出评估框架与新指标，开发了训练无关的PFC方法有效缓解该现象，显著提升生成音频的准确性和可靠性。<br/><br/>贡献点：  <br/>1. **提出新问题定义**：首次识别视频到音频生成中的"插入幻觉"（Insertion Hallucination），指出其由数据集偏差（如离屏声音）引发，并暴露现有评估指标的盲区。  <br/>2. **构建系统评估框架**：设计基于多音频事件检测器多数投票的评估体系，通过综合判断提高对幻觉的检测能力。  <br/>3. **定义新型评估指标**：提出两个量化指标（IH@vid、IH@dur），分别衡量幻觉的视频覆盖率和持续时间。  <br/>4. **开发训练无关修正方法**：提出PFC（Posterior Feature Correction）方法，通过两次音频生成过程与视频特征掩码，有效减少幻觉。  <br/>5. **验证方法有效性**：在主流V2A基准上证明，PFC可降低幻觉 prevalence 和 duration 超过50%，同时保持或提升传统音频质量与同步指标。  <br/>6. **推动领域发展**：首次实现对插入幻觉的系统性测量和缓解，为构建更可靠、忠实的V2A模型奠定基础。|
|2509.13068v2|[MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement](http://arxiv.org/abs/2509.13068v2)|总结：  <br/>提出多尺度残差语音编解码器，实现低比特率下的高保真语音重建与信息解耦，推动TTS系统在WER和说话人相似度上的突破，并开放代码与模型资源。<br/><br/>贡献点：  <br/>1. **提出多尺度残差编解码器架构**：将语音分解为语义、音色、韵律和残差四个独立流，实现高效编码与信息解耦。  <br/>2. **低比特率下保持高保真度**：在竞争性低比特率下达到高质量语音重建，优于现有方法。  <br/>3. **两阶段语言模型设计**：结合该编解码器构建轻量级TTS模型，以更少数据实现SOTA的WER和更高的说话人相似度。  <br/>4. **语音转换能力提升**：支持独立操控音色与韵律，提高语音转换任务的灵活性与效果。  <br/>5. **开源实现与资源共享**：提供推理代码、预训练模型和音频样本，促进研究复现与应用。|
|2509.12171v2|[Preservation of Language Understanding Capabilities in Speech-aware   Large Language Models](http://arxiv.org/abs/2509.12171v2)|贡献点：<br/>1. 提出C3T基准，首次系统评估语音感知大语言模型的语言理解能力保持性<br/>2. 创新性结合文本任务与语音克隆TTS模型进行跨模态能力验证<br/>3. 引入说话者类别维度，量化模型的语音输入公平性表现<br/>4. 建立跨模态鲁棒性评估框架，分析文本与语音输入的性能一致性<br/><br/>总结：该研究提出C3T基准，从能力保持、公平性和鲁棒性三方面创新评估语音感知大语言模型，为语音AI发展提供新型测试标准。|