|Source|Title|Summary|
|---|---|---|
|2511.10212v1|[Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization](https://arxiv.org/abs/2511.10212v1)|总结：本文提出单阶段训练框架与窗口级注意力机制，提升多模态深度伪造检测的泛化能力与局部伪影识别，实验证明其在时空定位上的有效性。<br/><br/>贡献点：<br/>1. 提出单阶段训练框架，通过结合下一帧预测（uni-modal/cross-modal features）增强模型对未见操控和数据集的泛化能力。<br/>2. 引入窗口级注意力机制，精准捕捉预测帧与实际帧间的差异，有效检测局部模态内部伪影。<br/>3. 在多基准数据集验证中，模型展现强泛化性与高精度的时空定位性能，解决现有方法对音视频对齐攻击的失效问题。|
|2511.09918v1|[MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection](https://arxiv.org/abs/2511.09918v1)|总结：  <br/>本文提出Norm-RAG框架，结合检索增强与代理机制，实现多轮对话中的社会规范推理，并发布MINDS多语言数据集，推动跨文化社交规范建模与对话系统研究。<br/><br/>贡献点：  <br/>1. **提出Norm-RAG框架**：首个面向多轮对话的社会规范推理系统，融合检索增强（Retrieval-Augmented）与代理机制（Agentic），实现对交际意图、角色、框架及语言线索的综合建模。  <br/>2. **创新语义分块方法**：开发Semtic Chunking技术，从结构化规范文档中精准检索上下文相关的规范信息，增强模型对规范依据的解释性。  <br/>3. **构建MINDS数据集**：首次发布包含31个多语种（中英、西英）多轮对话的双语数据集，通过多人标注确保跨文化规范标注的准确性与真实性。  <br/>4. **跨文化规范建模**：验证模型在跨语言环境下的适应性，凸显社会规范推理对文化差异的敏感度与泛化能力。  <br/>5. **提升对话系统性能**：实验证明Norm-RAG在规范检测与跨文化任务中表现优异，为文化适应性与社会智能的对话系统提供新方法。|
|2511.09552v1|[Intelligent Carrier Allocation: A Cross-Modal Reasoning Framework for Adaptive Multimodal Steganography](https://arxiv.org/abs/2511.09552v1)|**总结**：提出基于跨模态推理引擎的智能载体分配框架，实现多模态载体自适应优化，提升隐写系统安全性与抗分析能力。  <br/><br/>**贡献点**：  <br/>1. **提出智能载体分配框架**：基于跨模态推理引擎（CMR），动态评估多模态载体（图像、音频、文本）的适配性。  <br/>2. **多模态可靠性评估**：引入熵、信号复杂度、词汇丰富度等指标，量化载体安全性与隐蔽性。  <br/>3. **自适应数据分配策略**：根据可靠性分数动态分配秘密数据，优先利用高复杂度载体提升隐蔽性。  <br/>4. **对抗隐写分析的优化**：通过动态调整载体负载，增强系统抗检测能力，优于传统静态方法。  <br/>5. **实验验证优势**：证明该框架在安全性和数据保护性能上显著优于现有非自适应多模态技术。|
|2511.08654v1|[AI-generated podcasts: Synthetic Intimacy and Cultural Translation in NotebookLM's Audio Overviews](https://arxiv.org/abs/2511.08654v1)|1. 首次将AI生成的播客视为媒体进行系统分析，填补了AI音频生成领域研究空白；  <br/>2. 揭示NotebookLM播客遵循固定模板结构，体现算法对内容形式的标准化控制；  <br/>3. 发现AI通过语言转换强化了白人、受教育的中产阶级文化默认值；  <br/>4. 质疑传统多公共领域理论，指出AI播客消解了人类播客中社区针对性与互动性。  <br/><br/>总结：该研究揭示AI播客的标准化模板与文化偏见，挑战传统公共领域理论，推动对AI生成媒体社会影响的反思。|
|2511.08031v1|[Multi-modal Deepfake Detection and Localization with FPN-Transformer](https://arxiv.org/abs/2511.08031v1)|总结：  <br/>提出一种基于FPN-Transformer的多模态框架，结合预训练模型与多尺度特征金字塔，实现跨模态泛化和帧级伪造检测定位，在IJCAI'25基准上取得0.7535的高分表现。<br/><br/>贡献点：  <br/>1. **提出多模态检测定位框架**：设计FPN-Transformer架构，解决跨模态特征融合与细粒度时间边界回归问题。  <br/>2. **利用预训练自监督模型**：采用WavLM（音频）、CLIP（视频）提取层次化时序特征，提升特征表征能力。  <br/>3. **构建多尺度特征金字塔**：通过R-TLM块与局部注意力机制，联合建模跨上下文时序依赖关系。  <br/>4. **双分支预测机制**：同时预测伪造概率与时间偏移量，实现帧级精确定位。  <br/>5. **实验验证有效性**：在IJCAI'25 DDL-AV基准测试中取得0.7535的高检测定位得分，证实方法有效性。|
|2511.07993v1|[Private Chat in a Public Space of Metaverse Systems](https://arxiv.org/abs/2511.07993v1)|总结：  <br/>本文提出Hushhub系统，解决社交VR中私密对话缺失问题，通过整合私聊功能提升沉浸式交互体验，并验证其对丰富社交互动的价值。<br/><br/>贡献点：  <br/>1. **识别关键问题**：指出社交VR环境中缺乏私密对话功能，限制用户进行非公开交流。  <br/>2. **设计Hushhub系统**：开发集成式私密聊天系统，允许用户在共享VR空间中选择性发起私人音频对话。  <br/>3. **实证验证有效性**：通过用户研究验证系统对提升社交互动质量的实际效果。  <br/>4. **推动社交VR发展**：证明私密对话功能的必要性，为更复杂的沉浸式社交场景提供技术基础。|
|2511.07940v1|[Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?](https://arxiv.org/abs/2511.07940v1)|**贡献点总结：**  <br/>1. **挑战传统假设**：揭示参考视频时长并非关键，短时长高质量片段可达到甚至超越全视频效果，强调信息质量优于长度。  <br/>2. **提出ISExplore方法**：基于音频特征多样性、嘴唇运动幅度、相机视角数量三个维度，开发高效且自动的片段选择策略。  <br/>3. **显著提升效率**：在NeRF和3DGS方法中，数据处理与训练速度提升超5倍，同时保持高保真输出，降低计算成本。  <br/>4. **开放项目资源**：提供可复现实验与代码，促进社区进一步探索与应用。  <br/><br/>（总结：本研究通过提出ISExplore策略，证明短时长高质量参考片段可有效提升TFG效率，同时保持输出质量，推动实际应用。）|
|2511.07619v1|[CAVER: Curious Audiovisual Exploring Robot](https://arxiv.org/abs/2511.07619v1)|总结：  <br/>本研究提出CAVER，一种结合3D打印末端执行器、音频视觉表示及好奇心驱动算法的机器人，显著提升材料分类和音频演示模仿效率。<br/><br/>贡献点：  <br/>1. **新型硬件设计**：开发可附加于并行夹爪的3D打印末端执行器，主动激发物体的音频响应。  <br/>2. **多模态表示方法**：融合局部/全局视觉特征与声音特征，构建更丰富的音频视觉物体表征。  <br/>3. **好奇心驱动探索**：提出优先探索高不确定性的物体的算法，以更少交互实现对意外音频的广泛覆盖。|
|2511.07205v1|[Twenty-Five Years of MIR Research: Achievements, Practices, Evaluations, and Future Challenges](https://arxiv.org/abs/2511.07205v1)|**贡献点：**  <br/>1. 回顾MIR领域25年发展历程，梳理其研究重点与趋势。  <br/>2. 强调MIR与IEEE音频信号处理技术委员会的紧密关联。  <br/>3. 系统总结MIR在音乐分析、处理、生成三大EDICS领域的核心成果。  <br/>4. 提出可重复研究、开放实践、产业协作及多样性推动等成功经验。  <br/><br/>**总结（100字内）：**  <br/>本论文回顾MIR领域25年发展，系统总结其在音乐分析、处理和生成的成果，分析可重复研究、产业协作及多样性推动等成功实践，并展望未来挑战，为该领域提供全面研究脉络与启示。|
|2511.07116v1|[BridgeVoC: Revitalizing Neural Vocoder from a Restoration Perspective](https://arxiv.org/abs/2511.07116v1)|总结：  <br/>本文提出BridgeVoC，将神经声码器任务重新定义为音频修复问题，通过引入Schrödinger桥框架、子带感知卷积扩散网络及单步推理蒸馏损失，实现了参数更少、计算成本更低且性能领先的语音生成方法。<br/><br/>贡献点：  <br/>1. **任务重构**：首次将神经声码器任务视为音频修复的特例，通过秩分析揭示Mel频谱与其他退化因素的差异特性。  <br/>2. **框架创新**：提出基于Schrödinger桥的扩散模型框架，将RSS与目标频谱定义为生成轨迹的双端点，增强模型对音频修复的适配性。  <br/>3. **子带感知设计**：开发子带意识卷积扩散网络，采用不均衡子带划分策略与大核卷积注意力模块，提升T-F域的语义建模效率。  <br/>4. **单步推理优化**：设计多目标蒸馏损失（含目标相关性与双射一致性），实现单步推理且保持高质量生成效果。  <br/>5. **高效性能**：在少参数、低计算成本下，仅需4步采样即可超越GAN/DDPM/流匹配基线，在多个基准和分布外数据集上验证性能领先。|
|2511.03295v2|[How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295v2)|总结：  <br/>本文首次系统研究源感知ST评估指标，提出ASR转录与反向翻译的双重生成策略，并设计跨语言重分段算法解决对齐问题，实验证明其有效性，推动ST评估方法的改进。<br/><br/>贡献点：  <br/>1. **首次系统性研究源感知指标**：针对ST领域源输入（音频）特性，提出首个专门研究源感知评估的框架，强调在无参考转录的现实场景中的应用。  <br/>2. **双生成策略**：探索ASR转录与参考翻译反向翻译两种文本代理方法，构建合成源文本以弥补真实源信息缺失。  <br/>3. **跨语言重分段算法**：提出新型两阶段跨语言对齐算法，解决合成源与参考译文间的对齐不匹配问题，提升指标可靠性。  <br/>4. **实证分析**：在79语言对、6系统上的实验表明，ASR转录在低WER场景更优，反向翻译则兼具效率与效果，为评估方法选择提供依据。  <br/>5. **方法迁移与推广**：验证源感知MT指标在ST中的可行性，为构建更准确、理论化的评估体系奠定基础。|
|2511.01310v2|[From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310v2)|**贡献点总结（分点）：**  <br/>1. **提出共享生成多模态世界模型（MWM）**：通过可扩展注意力机制融合多智能体多模态观测，学习压缩环境动态的潜变量表示。  <br/>2. **解耦表示与策略学习**：利用MWM作为快速模拟器，在其潜空间内训练协作策略，提升样本效率。  <br/>3. **引入挑战性多模态多智能体基准**：基于3D物理模拟器构建新基准，推动领域发展。  <br/>4. **显著提升样本效率**：实验显示，相比模型无关MARL基线，样本效率提升数量级。  <br/>5. **强调多模态融合的关键性**：证明其在感官不对称环境中的任务成功作用，及对传感器丢包的鲁棒性优势。  <br/><br/>**总结（100字内）：**  <br/>本文提出MWM-MARL框架，通过多模态融合与共享世界模型解耦表示与策略学习，显著提升协作MARL的样本效率，验证多模态融合对复杂环境的鲁棒性与任务关键性，为实际部署提供新方案。|
|2510.23056v2|[Enabling American Sign Language Communication Under Low Data Rates](https://arxiv.org/abs/2510.23056v2)|总结（100字以内）：  <br/>本文提出VC4ASL系统，通过音频传输人体姿态信息重建手语内容，使ASL用户在低带宽下实现无障碍通信，无需修改现有视频会议平台，并设计了基于姿态结构的接收端纠错机制。<br/><br/>贡献点：  <br/>1. **跨平台兼容性**：开发无需修改现有视频会议应用的ASL音频通信系统（VC4ASL），适应低带宽场景。  <br/>2. **姿态编码传输**：创新性地通过音频通道编码和传输人体姿态信息，实现手语内容的音频重建。  <br/>3. **接收端纠错机制**：提出利用手语姿态数据固有结构约束的误差检测与校正方法，提升通信鲁棒性。  <br/>4. **系统验证**：设计网络退化模拟与用户研究实验，验证系统在低带宽下的可理解性与实用性。|
|2510.13630v2|[AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset](https://arxiv.org/abs/2510.13630v2)|**总结（100字以内）**  <br/>提出AVAR-Net多模态异常识别框架，结合音频视频数据提升复杂场景下的可靠性，并构建VAAR数据集推动研究进展，实验验证其高效性与优越性能。<br/><br/>**贡献点分点**  <br/>1. **提出AVAR-Net框架**：首次设计轻量高效的音频-视觉多模态异常识别框架，通过音频（Wav2Vec2）与视频（MobileViT）特征提取、早期融合及多阶段时序卷积网络（MTCN）建模跨模态关系，实现稳健时空推理。  <br/>2. **构建VAAR数据集**：发布首个中规模同步音频-视觉异常数据集（3,000条视频，10类异常），填补领域数据缺口，为多模态研究提供基准。  <br/>3. **性能提升**：在VAAR和XD-Violance数据集上取得89.29%准确率和88.56% AP，相比现有方法提升2.8% AP，验证框架有效性与泛化能力。  <br/>4. **解决实际挑战**：有效应对遮挡、低光、恶劣天气等复杂场景，提升多模态异常检测在真实环境中的可靠性与鲁棒性。|
|2509.20072v2|[From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](https://arxiv.org/abs/2509.20072v2)|**论文贡献（分点列出）**  <br/><br/>1. **统一的音文框架 Text‑to‑Talk (TtT)**  <br/>   - 在同一 Transformer 中同时实现自回归 (AR) 文本生成和非自回归 (NAR) 音频扩散，解决了文本依赖目标‑目标关系、音频依赖源‑目标关系的不同建模需求。<br/><br/>2. **基于离散扩散的任意序列自回归特性**  <br/>   - 通过吸收式离散扩散 (absorbing discrete diffusion)，构建统一的训练目标，使文本和音频可以共享同一损失函数进行端到端优化。<br/><br/>3. **模态感知注意力机制**  <br/>   - 设计了能够对文本执行因果解码、对音频块执行双向建模的注意力结构，兼顾两种模态的不同解码约束。<br/><br/>4. **三项训练策略以削减训练‑推理差距**  <br/>   - 包括（a）噪声调度对齐、（b）块级教师强制、（c）可变长输出对齐，提升模型在实际推理时的稳健性。<br/><br/>5. **块式扩散并行生成**  <br/>   - 在推理阶段采用块级扩散，实现音频的并行合成，同时支持可变长度的输出，显著加速生成过程。<br/><br/>6. **广泛实验与细致消融**  <br/>   - 在 Audio‑QA 与 ASR 任务上进行全面评估，实验结果验证了框架及各组件的有效性。<br/><br/>7. **开源资源**  <br/>   - 将公开模型、数据及代码，为后续多模态语音研究提供基准与复现平台。<br/><br/>---<br/><br/>**100字以内的中文总结**  <br/>提出 Text‑to‑Talk，统一 AR 文本生成与 NAR 音频扩散，创新模态感知注意力和三种训练策略，块式并行扩散提升生成效率，并在 Audio‑QA 与 ASR 上验证效果，代码、模型将开源。|
|2509.14684v1|[DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis](https://arxiv.org/abs/2509.14684v1)|**主要贡献**  <br/>1. **提出 DAIEN‑TTS 零-shot TTS 框架**：实现了在合成语音时可独立控制说话人音色和背景环境，实现环境感知的语音合成。  <br/>2. **引入预训练的语音‑环境分离 (SES) 模块**：将混合音频分解为干净语音和环境音的 mel‑spectrogram，为后续的 disentangled audio infilling 提供基础。  <br/>3. **采用双随机跨度掩码并与文本嵌入共同条件**：在干净语音和环境音的 mel‑spectrogram 上同时施加不同长度的掩码，实现对两类音频的同步填充与延续。  <br/>4. **提出双类无监督引导 (DCFG)**：在推理阶段分别对语音和环境两个分支施加类‑free guidance，提升可控性与合成质量。  <br/>5. **设计信噪比 (SNR) 自适应策略**：根据环境提示动态调节合成语音的 SNR，使生成的语音与目标环境更匹配。  <br/>6. **实验验证**：在自然度、说话人相似度和环境保真度方面均显著优于基线，证明了该框架在环境个性化语音合成上的有效性。<br/><br/>**100字以内摘要**  <br/>DAIEN‑TTS 基于 F5‑TTS，融合预训练语音‑环境分离、双随机掩码填充与双类无监督引导，实现说话人与环境的独立、零-shot 控制，并通过 SNR 自适应提升匹配度，实验表明其在自然度、说话人相似度和环境保真度上均表现优异。|