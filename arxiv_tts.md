|Source|Title|Summary|
|---|---|---|
|2507.06826v1|[Physics-Informed Direction-Aware Neural Acoustic Fields](http://arxiv.org/abs/2507.06826v1)|**贡献点：**  <br/>1. **提出物理信息神经网络建模FOA RIRs**：首次将PINN框架应用于一阶Ambisonic房间脉冲响应建模，结合神经网络与声学物理模型提升预测精度。  <br/>2. **构建两个物理先验约束**：基于粒子速度与(X, Y, Z)通道的对应关系，推导出通过偏导数关联W通道与其他通道的物理先验，确保四通道间的物理一致性。  <br/>3. **实验验证有效性**：通过对比实验证明所提方法在建模FOA RIRs时优于无物理约束的神经网络，提升沉浸式音频生成的性能。  <br/><br/>**总结（100字以内）：**  <br/>本论文提出一种物理信息神经网络模型，用于建模一阶Ambisonic房间脉冲响应，成功推导出两个基于粒子速度与通道关系的物理先验，并通过实验验证其有效性优于传统方法。|
|2507.06483v1|[Learning Japanese with Jouzu: Interaction Outcomes with Stylized   Dialogue Fictional Agents](http://arxiv.org/abs/2507.06483v1)|总结（100字以内）:  <br/>本研究探索风格化语音代理在多模态语言学习中的影响，发现其设计（声音、人设、语言风格）显著提升用户体验和学习动机，为构建更具社会响应性的智能语言学习系统提供理论支持和设计指导。<br/><br/>贡献点：  <br/>1. **理论贡献**：揭示风格化语音代理在语言学习场景中对用户体验、学习策略和情感反应的影响机制，拓展人机交互中文化与情感因素的研究范畴。  <br/>2. **实证研究**：通过混合方法分析54名跨语言水平与文化背景的用户，验证语音风格、情感表达与互动效果的关联性。  <br/>3. **实践指导**：提出基于语音、人设及语言风格的设计原则，为开发更自然、文化适配的多模态语言学习系统提供参考。  <br/>4. **技术整合**：结合大语言模型与情感语音合成技术，构建具有异步对话能力的动漫风格化虚拟角色，推动语音交互在教育领域的应用创新。|
|2507.06116v1|[Speech Quality Assessment Model Based on Mixture of Experts:   System-Level Performance Enhancement and Utterance-Level Challenge Analysis](http://arxiv.org/abs/2507.06116v1)|总结：  <br/>本文提出基于自监督学习的增强MOS预测系统，结合MoE架构与多模型合成数据，揭示句子级评估局限并探索性能差异原因，为语音质量评估提供新方向。<br/><br/>贡献点：  <br/>1. **提出增强的MOS预测系统**：融合自监督学习模型（如wav2vec2）与Mixture of Experts（MoE）分类头，提升语音质量评估的准确性。  <br/>2. **构建多模态合成数据集**：收集涵盖文本到语音、语音转换及语音增强系统的海量合成语音数据，用于数据增强与模型训练。  <br/>3. **揭示方法局限性**：发现当前模型在句子级预测任务上性能提升有限，明确其在处理细粒度质量评估中的瓶颈。  <br/>4. **探索性能差异根源**：分析不同评估粒度（如句子级与帧级）下模型性能差异的根本原因，为后续研究提供理论依据。|
|2507.05911v1|[Differentiable Reward Optimization for LLM based TTS system](http://arxiv.org/abs/2507.05911v1)|总结：  <br/>本文提出DiffRO方法，通过神经编码器token直接计算奖励并结合Gumbel-Softmax技术优化RLHF训练，引入多任务奖励模型提升TTS系统的发音准确性和指令遵循能力，实验表现SOTA性能。<br/><br/>贡献点：  <br/>1. **提出DiffRO方法**：首次将可微奖励机制应用于神经编码器语言模型的TTS系统，直接基于神经编码器token计算奖励而非依赖合成音频。  <br/>2. **引入Gumbel-Softmax技术**：使奖励函数可微化，简化RLHF训练流程，提升训练效率和稳定性。  <br/>3. **设计多任务奖励（MTR）模型**：从多视角提供反馈，增强系统对指令的遵循能力，实现更灵活的控制。  <br/>4. **实验验证效果**：在seed-tts-eval基准上取得SOTA WER结果，并展示通过MTR模型零样本控制情感与质量属性的能力。|
|2507.05177v2|[OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech   Language Model](http://arxiv.org/abs/2507.05177v2)|**贡献点：**  <br/>1. **提出OpenS2S框架**：首个完全开源、透明且端到端的 empathetic LSLM（低延迟语音语言模型），解决现有模型架构与数据不透明的问题。  <br/>2. **创新流式解码架构**：基于BLSP-Emo模型，采用流式交错解码实现低延迟语音生成，提升实时交互体验。  <br/>3. **自动化数据构建**：设计低成本、高效率的自动化管道，生成多样化的高质量情感语音对话数据。  <br/>4. **弱监督训练方法**：结合大语言模型与可控文本到语音系统，减少人工标注，构建富含非语言特征的可扩展语料库。  <br/>5. **全面开源释放**：公开数据集、模型权重、预训练与微调代码，促进研究社区共享与加速情感语音系统的创新。  <br/><br/>**总结（100字内）**  <br/>OpenS2S是首个开源透明的 empathetic LSLM，通过流式解码和自动化数据构建实现低延迟生成与弱监督训练，推动情感语音系统研究与应用。|
|2507.04598v1|[Multi-Step Prediction and Control of Hierarchical Emotion Distribution   in Text-to-Speech Synthesis](http://arxiv.org/abs/2507.04598v1)|贡献点总结（100字以内）:  <br/>提出分层情感分布模型，通过多步预测实现语句、单词、音素三级情感控制；融合全局语境优化局部情感变化；模块化设计兼容多种TTS系统；实验验证显著提升情感表达与多粒度控制效果。<br/><br/>分点贡献：  <br/>1. **提出分层情感分布框架**：首次构建三级（语句、单词、音素）情感量化控制机制，实现语音情感的多粒度建模。  <br/>2. **创新多步情感预测方法**：通过层级化预测流程，结合全局上下文与局部细节，提升情感表达的层次性与自然度。  <br/>3. **模块化适配性设计**：开发可兼容不同TTS系统的模块，支持灵活集成与扩展，增强技术通用性。  <br/>4. **实证效果验证**：通过客观与主观实验，证明框架在情感渲染精准控制与表达增强方面具有显著优势。|
|2507.04349v1|[TTS-CtrlNet: Time varying emotion aligned text-to-speech generation with   ControlNet](http://arxiv.org/abs/2507.04349v1)|总结：  <br/>提出TTS-CtrlNet，实现可控流匹配与情感控制，无需全模型微调；提供架构设计、情感特定flow step和控制规模的实用方案；实验验证在情感相似度指标上达到SOTA，支持零样本语音克隆与自然语音生成。<br/><br/>贡献点：<br/>1. **首个ControlNet-Flow Matching TTS方法**  <br/>   提出TTS-CtrlNet，首次将ControlNet的条件控制引入流匹配TTS框架，通过冻结原始模型并引入可训练的条件模块，实现细粒度、时间可变的情感控制，同时保留原模型的零样本语音克隆和自然语音生成能力。<br/><br/>2. **高效的情感控制策略设计**  <br/>   提出三项实用建议：（1）基于模块分析的架构设计，（2）情感特异性flow步骤，（3）可扩展的控制尺度调节，支持情感控制的直观性、可扩展性与时间动态性。<br/><br/>3. **优越的实验表现与指标验证**  <br/>   在情感相似度（Emo-SIM、Aro-Val SIM）上达到SOTA性能，证明了该方法在情感生成质量与语音自然度上的有效性，并提供项目页面供复现实验。|
|2507.04094v1|[MMMOS: Multi-domain Multi-axis Audio Quality Assessment](http://arxiv.org/abs/2507.04094v1)|总结（100字以内）:  <br/>该论文提出MMMOS，首个多领域无参考音频质量评估系统，涵盖四个正交维度，并融合多编码器与聚合策略，通过模型集成显著提升评估性能，获得多个指标前列。<br/><br/>贡献点:  <br/>1. **提出首个多领域无参考评估系统**：MMMOS可应用于语音、音乐和环境声音，突破以往仅针对语音的限制。  <br/>2. **定义四维正交质量指标**：涵盖生产质量、生产复杂度、内容愉悦度和内容实用性，提供更全面的音频质量分析框架。  <br/>3. **多编码器融合方法**：结合WavLM、MuQ和M2D三个预训练模型的帧级嵌入，增强特征表示能力。  <br/>4. **创新损失函数与聚合策略组合**：设计三种聚合策略及四种损失函数，提升模型训练效果。  <br/>5. **模型集成性能优势**：通过集成前八名模型，实现20-30% MSE下降和4-5% Kendall's τ提升，表现优异。|
|2507.04036v1|[PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)|总结：  <br/>PresentAgent通过多模态整合生成高质量演示视频，突破传统方法局限，引入统一评估框架验证效果，证明其接近人类水平，展现可控多模态代理的应用潜力。<br/><br/>贡献点：  <br/>1. **提出新型多模态生成框架**：PresentAgent首次实现将长文档转换为同步的视觉与语音内容，突破生成静态幻灯片或文本摘要的局限，贴近人类式演讲风格。  <br/>2. **设计模块化处理流程**：系统化分割文档、规划视觉帧、生成上下文相关语音，并通过精确的音频-视觉对齐技术整合为连贯视频。  <br/>3. **构建统一评估体系**：推出PresentEval，基于视觉语言模型，从内容保真度、视觉清晰度和观众理解三维度评估生成视频的性能。  <br/>4. **实验验证有效性**：在30个文档-演示视频配对数据集上验证，结果显示该方法在所有指标上接近人类表现，证明其可行性与优势。  <br/>5. **开源代码促进应用**：提供完整代码库，推动技术复用与进一步研究。|
|2507.03912v1|[Prosody Labeling with Phoneme-BERT and Speech Foundation Models](http://arxiv.org/abs/2507.03912v1)|**总结**（100字以内）:  <br/>本研究提出结合声学与语言学特征的模型，用于自动韵律标签注释，显著提升日语数据集的预测准确率，达到89.8%、93.2%和94.3%。<br/><br/>**贡献点**:<br/>1. **跨模态特征融合**：首次将声学特征（SSL/Whisper模型）与语言学特征（PnG BERT、PL-BERT）结合，用于音素级韵律标签预测，突破单一模态限制。  <br/>2. **预训练模型适配**：引入语音领域预训练语言模型作为语言学特征提取器，提升对韵律结构（如音调重音、短语边界）的建模能力。  <br/>3. **实验验证有效性**：在日语韵律标签任务中，证明声学-语言学联合模型优于单源模型，验证跨模态协同的必要性。  <br/>4. **高精度性能**：实现吐字音调（89.8%）、高低音调（93.2%）、短语边界（94.3%）的高准确率预测，为可控TTS提供高质量标注数据。|
|2507.03887v1|[Traceable TTS: Toward Watermark-Free TTS with Strong Traceability](http://arxiv.org/abs/2507.03887v1)|**贡献点**  <br/>1. 提出首个无需水印的TTS模型属性框架，通过联合训练TTS模型与鉴别器提升可追溯性，同时保持或改善音频质量。  <br/>2. 解决现有水印技术导致的语音质量下降和伪造攻击问题，实现更强的抗欺骗性与泛化能力。  <br/>3. 开源代码以推动语音安全与模型可追溯性相关技术的研究发展。  <br/><br/>**总结**  <br/>本研究提出首个无水印TTS框架，通过联合训练提升可追溯性与音质，解决了现有技术缺陷，并开源代码促进领域进展。|
|2507.02530v1|[Open-Source System for Multilingual Translation and Cloned Speech   Synthesis](http://arxiv.org/abs/2507.02530v1)|**贡献点分点总结：**  <br/>1. **系统集成创新**：结合Whisper语音识别与VAD检测，构建多语言翻译与语音再生的端到端解决方案。  <br/>2. **模块化翻译架构**：采用双LLM流水线（分句+翻译），支持高效、准确的多语言处理流程。  <br/>3. **语音再生技术**：引入带语音克隆能力的TTS模块，保留原说话人身份与自然语音特性。  <br/>4. **灵活部署方案**：开源组件支持本地运行与API调用，降低多场景部署成本。  <br/>5. **应用广泛性**：覆盖实时会议翻译、公共广播再生、蓝牙设备播放等实际需求场景。  <br/>6. **性能分析与开源共享**：提供延迟与词准确率数据，通过开源推动社区创新与无障碍通信发展。  <br/><br/>**总结（100字以内）：**  <br/>提出集成语音识别、多语言翻译与语音克隆的开源系统，支持灵活部署，适用于实时会议、广播及个人设备等场景，通过性能分析验证其在多语言通信中的实用性与普适性。|
|2507.02380v1|[JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](http://arxiv.org/abs/2507.02380v1)|**贡献点：**  <br/>1. 提出端到端语音聊天机器人框架，整合大语言模型（LLM）与文本到语音（TTS）技术，实现语音克隆功能。  <br/>2. 基于开源模型MiniCPM-o和CosyVoice2，并在2000小时对话数据上进行训练。  <br/>3. 开源完整的训练代码与模型，支持社区进一步开发和优化。  <br/>4. 在测试平台seed-tts-zh中取得SS（语音相似度）0.73和WER（词错误率）5.09的性能表现。  <br/>5. 提供代码、模型及训练推理脚本的完整访问链接（GitHub仓库）。  <br/><br/>**总结（100字以内）：**  <br/>开发了整合LLM与TTS的端到端语音聊天机器人JoyTTS，支持语音克隆。基于开源模型和大规模对话数据训练，开源代码促进社区协作，实测性能达SS 0.73和WER 5.09，并开放完整资源至GitHub。|
|2507.01805v1|[A Dataset for Automatic Assessment of TTS Quality in Spanish](http://arxiv.org/abs/2507.01805v1)|总结（100字以内）:  <br/>本研究构建了首个西班牙语TTS系统自动评估数据库，包含4326个音频样本，采用标准化主观测试方法，验证了数据集的效用，探索了跨语言模型迁移与自监督模型结合的两种训练策略，为西班牙语TTS自然度评估提供新资源。<br/><br/>贡献点:  <br/>1. **首个西班牙语TTS评估数据库**：创建了包含52种TTS系统与人类语音的4,326个音频样本数据集，填补该领域空白。  <br/>2. **标准化主观标注流程**：基于ITU-T Rec. P.807标准设计测试方案，由92名参与者完成标注，保障数据可靠性。  <br/>3. **双路径模型验证方法**：通过微调英文预训练模型和构建下游网络两种方式验证数据集，探索跨语言迁移有效性。  <br/>4. **高质量与多样性分析**：从数据分布、标注一致性等方面证明数据集的科学性，明确其对西班牙语TTS研究的推动作用。|
|2507.01348v2|[SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and   Text to Speech](http://arxiv.org/abs/2507.01348v2)|总结：  <br/>本文提出了SpeechAccentLLM框架，创新性地整合CTC与代码本离散化，采用多任务学习解决FAC数据稀缺问题，并引入后处理模块提升输出质量，为语音口音转换提供了新方法。<br/><br/>贡献点：  <br/>1. **提出SpeechAccentLLM框架**：首次将大语言模型（LLM）技术应用于语音领域，探索LLM在文本到语音（TTS）任务中的口音转换能力。  <br/>2. **创新SpeechCodeVAE模型**：首次将连接主义时序分类（CTC）直接嵌入代码本离散化过程，实现具有“局部性”特征的语音内容分词，并验证其在内容忠实度、时间一致性与结构恢复性间的平衡。  <br/>3. **多任务学习策略**：联合训练FAC与TTS模块，缓解数据稀缺问题，同时提升收敛速度与语音质量。  <br/>4. **设计SpeechRestorer后处理架构**：利用离散语音表征特性优化LLM生成结果，减少随机误差并增强韵律连续性，经消融实验验证其有效性。|
|2507.00808v2|[Multi-interaction TTS toward professional recording reproduction](http://arxiv.org/abs/2507.00808v2)|**贡献点总结（100字以内）**  <br/>本研究提出一种具有多步骤交互的TTS方法，模拟语音导演与演员的协作关系，解决风格优化不足问题，通过实验验证其迭代调整能力并提供样本音频。<br/><br/>**分点贡献**  <br/>1. **首次引入多步交互机制**：在TTS中模拟语音导演与演员的协作流程，通过用户反馈实现语音风格的迭代优化，弥补传统TTS单次生成的不足。  <br/>2. **构建配套数据集**：提出专门用于支持多交互风格调整的数据集，为研究提供真实、多样化的训练和评估资源。  <br/>3. **验证多交互能力**：实验表明该方法能根据用户指令进行符合预期的风格修正，证明其在迭代优化中的有效性。  <br/>4. **提升用户交互体验**：设计直观的用户界面，使用户能够快速、灵活地调整合成语音的风格细节。  <br/>5. **提供实际音频验证**：通过公开样本音频直观展示模型性能，增强研究的可验证性和应用价值。|
|2507.00227v1|[Investigating Stochastic Methods for Prosody Modeling in Speech   Synthesis](http://arxiv.org/abs/2507.00227v1)|**贡献点：**  <br/>1. **提出随机方法在语音生成中的新应用**：探索基于归一化流、条件流匹配和修正流的随机方法，用于生成更具表现力的语调（prosody）。  <br/>2. **对比实验验证有效性**：通过主观与客观评估，验证随机方法在生成自然语调方面与人类表现相当，并优于传统确定性基线。  <br/>3. **提升可控性**：展示随机方法可通过调节采样温度实现更灵活的语调控制，增强系统对语音特征的调节能力。  <br/><br/>**总结**（100字以内）：  <br/>该研究比较了随机方法与传统基线及人类语调，发现随机方法生成自然语调且提升可控性。|
|2506.23869v1|[Scaling Self-Supervised Representation Learning for Symbolic Piano   Performance](http://arxiv.org/abs/2506.23869v1)|总结（100字以内）:  <br/>该研究提出基于大规模符号钢琴数据的生成模型，创新性地应用SimCLR框架生成对比MIDI嵌入，显著提升钢琴延续生成和MIR分类性能，并验证了预训练表示在下游任务中的泛化能力，降低数据需求。<br/><br/>贡献点:  <br/>1. **大规模预训练与微调策略**：利用约60,000小时符号独奏钢琴数据进行预训练，并通过高质量子集微调，提升模型在音乐生成和分类任务中的表现。  <br/>2. **SimCLR框架的符号音乐适配**：首次将对比学习框架（SimCLR）应用于符号音乐领域，生成通用的对比MIDI嵌入（contrastive MIDI embeddings）。  <br/>3. **性能验证**：生成模型在钢琴延续连贯性评估中超越主流符号生成方法，对比模型在MIR分类任务中达到SOTA，且微调后仅需少量标注样本即可完成下游任务。|
|2506.23553v1|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v1)|总结:  <br/>该研究提出Human-CLAP模型，通过结合人类主观评分优化CLAP，显著提升了CLAPScore与主观评价的相关性，解决了语音生成与识别任务中评估指标与人类感知脱节的问题。<br/><br/>贡献点:  <br/>1. **揭示CLAPScore局限性**：首次论证CLAPScore与人类主观评分的相关性较低，指出其在语音-文本相关性评估中的不足。  <br/>2. **提出Human-CLAP模型**：设计基于人类感知的对比学习框架，借助主观评分数据训练语言-音频模型，增强对人类偏好的建模能力。  <br/>3. **实验验证有效性**：通过对比实验证明，Human-CLAP将Spearman相关系数(SRCC)提升超0.25，显著改善语音生成任务的评估一致性。|
|2506.23552v1|[JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](http://arxiv.org/abs/2506.23552v1)|总结：  <br/>提出JAM-Flow框架，实现语音与面部运动的统一生成，结合flow matching和MM-DiT架构，支持多种条件输入，提升跨模态交互效果与模型实用性。<br/><br/>贡献点：  <br/>1. **统一框架设计**：首次将说话头合成与文本到语音（TTS）任务整合，构建单一模型同时处理音频-视觉生成。  <br/>2. **多模态架构创新**：引入Multi-Modal Diffusion Transformer（MM-DiT），结合Motion-DiT和Audio-DiT模块，增强模态间交互能力。  <br/>3. **跨模态交互优化**：通过选择性联合注意力层和时序对齐位置嵌入、局部联合注意力掩码等机制，实现高效跨模态信息融合。  <br/>4. **灵活训练目标**：采用图像修复（inpainting）风格的训练方法，支持文本、参考音频和运动等多种条件输入，扩展应用场景。|
|2506.23367v1|[You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel   Properties](http://arxiv.org/abs/2506.23367v1)|**贡献点总结（100字以内）:**  <br/>本文提出首个面向L2学习者的TTS系统Matcha-TTS，通过调整元音时长创建"清晰度模式"，显著降低转录错误率并提升听众体验。揭示实际与感知可懂度的差异，同时指出现有Whisper-ASR在评估L2可懂度上的不足。<br/><br/>**分点贡献:**  <br/>1. **首个L2专用TTS系统**：开发Matcha-TTS，专为第二语言说话者设计，填补该领域空白。  <br/>2. **"清晰度模式"创新**：基于美式英语元音的时长差异，针对性优化语音输出以提升可懂度。  <br/>3. **显著降低错误率**：实验显示该模式使法国母语者听英语L2时的转录错误率减少至少9.15%。  <br/>4. **感知差异研究**：发现听众虽实际可懂度提高，仍认为整体放慢语速更易理解，揭示实际与感知可懂度的脱节。  <br/>5. **批判现有评估工具**：指出Whisper-ASR未采用L2特有的语音线索，无法有效评估此类TTS系统的可懂度。|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|**贡献点：**  <br/>1. 提出改进的音频拼接方法（SAGE）生成阿拉伯语-英语代码切换语音的合成数据，缓解数据稀缺问题。  <br/>2. 引入经验回放（ER）策略，增强模型在方言阿拉伯语和代码切换语音上的泛化能力，减少灾难性遗忘。  <br/>3. 集成领域外3-gram语言模型，显著降低阿拉伯语-英语代码切换语音的平均Word Error Rate（WER）至26.6%。  <br/>4. 开发少样本微调方法，进一步提升代码切换语音识别性能（WER再降4.9%）。  <br/>5. 在阿拉伯语-英语代码切换语音识别任务中，实现31.1%的WER，优于大模型USM和Whisper-large-v2（分别高5.5%和8.4%）。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过改进的数据生成方法和训练策略，提升了方言阿拉伯语及阿拉伯-英语代码切换语音的识别性能，实验证明效果显著优于大模型，为低资源语音任务提供有效解决方案。|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|总结：  <br/>本文提出面向真实语音场景的综合评估框架，通过构建多样化语音数据集和查询感知评估方法，揭示主流语音模型在不同场景的性能差异，为语音模型开发提供新方向。<br/><br/>贡献点：  <br/>1. **构建真实场景语音基准**：系统收集并整理真实语音对话数据，覆盖口语化场景、多样化说话者属性及复杂声学环境。  <br/>2. **增强语音特性多样性**：在数据集中引入韵律、同音词、口吃等语音特有的现象，提升数据集对语音挑战的全面性。  <br/>3. **设计查询感知评估方法**：通过定制化评估清单与提示，结合任务相关性优化自动评估流程，提高评价精度。  <br/>4. **揭示模型场景性能差异**：对主流语音模型进行全面测试与分析，明确不同语音场景下的表现差异。  <br/>5. **推动语音模型优化方向**：提出的评估框架为语音模型的开发、调优及实际应用中的用户体验提升提供理论支撑与实践参考。|
|2506.21619v1|[IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](http://arxiv.org/abs/2506.21619v1)|总结:  <br/>IndexTTS2提出了一种支持精确时长控制和情感解耦的自回归语音合成方法，结合GPT潜表示与自然语言指令机制，实现零样本情感复现与音色控制，实验表现优于现有SOTA模型。<br/><br/>贡献点:  <br/>1. **时长控制双模式**：支持显式指定生成token数量（精确控制）和自动自由生成（保留韵律特征）  <br/>2. **情感-身份解耦**：实现音色与情感的独立控制，允许跨说话人情感迁移  <br/>3. **零样本情感复现**：能够完美还原输入提示的情感特征，支持分离情感指令输入  <br/>4. **GPT潜表示增强**：通过融合GPT潜在表示提升强烈情感表达的语音稳定性  <br/>5. **软指令机制设计**：基于文本描述的指令系统（微调Qwen3）实现自然语言情感引导|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>提出面向实时语音对话的偏好对齐框架，构建大规模语音偏好数据集，结合离线对齐和全双工模型训练，验证反馈机制在提升对话模型事实性、安全性和上下文对齐方面的作用。<br/><br/>贡献点：  <br/>1. **提出新的偏好对齐框架**：专为实时语音对话设计，解决现有文本模型难以应对语音交互中复杂动态（如打断、插入）及隐式回合分割的问题。  <br/>2. **构建大规模语音偏好数据集**：从多轮对话中提取超15万对偏好样本，标注AI反馈，涵盖语言内容与时间上下文的变化。  <br/>3. **引入离线对齐与全双工模型训练**：利用离线对齐方法优化全双工自动回归语音到语音模型，提升其处理实时对话的能力。  <br/>4. **实验证明反馈机制有效性**：通过大量实验验证泛化对话反馈对提高模型事实性、安全性和上下文对齐效果的显著性。  <br/>5. **全人评估验证实际应用价值**：通过综合人类评估，验证模型在超出现有单轮对话场景下的整体性能提升。  <br/>6. **强调动态平衡的理论意义**：揭示语音对话系统中动态因素（如语言内容、时间上下文）平衡的重要性，为自然对话设计提供指导。|
|2506.21448v2|[ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing](http://arxiv.org/abs/2506.21448v2)|总结：  <br/>本文提出ThinkSound框架，结合CoT推理实现视频音频生成与编辑，引入AudioCoT数据集，并在生成质量与基准测试中取得SOTA结果。<br/><br/>贡献点：  <br/>1. **框架创新**：提出ThinkSound，利用CoT推理分三阶段（基础音效生成、交互式对象精炼、定向编辑）实现视频音频的逐步生成与编辑。  <br/>2. **数据集构建**：开发AudioCoT，提供结构化推理标注，建立视觉、文本与音频合成的关联。  <br/>3. **性能突破**：在视频到音频生成任务中达到当前最优性能，尤其在跨分布电影音频基准测试中表现卓越。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|贡献点总结：  <br/>1. 提出模仿传统 Foley 工作流程的视频-音频生成方法，实现对视频中所有音效的全面捕捉。  <br/>2. 设计分步骤生成机制，每步基于文本提示和历史音频进行条件化引导合成。  <br/>3. 引入概念否定机制，提升生成音频的多样性与语义区分度。  <br/>4. 构建无需专属配对数据集的训练框架，利用预训练模型实现更高效的数据利用。  <br/>5. 实验证明方法在生成质量与复合音频合成效果上优于现有基线。  <br/><br/>（总结：该论文提出了一种新型视频-音频生成方法，通过分步骤生成结合文本提示和历史音频，引入概念否定机制与灵活训练框架，实现了高质量、多样化音效合成，优于现有技术。）|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|**贡献点**：  <br/>1. 首次系统比较H/ASP、x-vector和ECAPA-TDNN三种说话人编码器在零样本TTS场景下的有效性。  <br/>2. 构建统一的零样本TTS框架，验证说话人嵌入对语音合成质量的影响。  <br/>3. 引入主客观双维度评估方法（主观聆听测试与客观余弦距离）量化说话人相似度。  <br/>4. 发现H/ASP编码器在零样本TTS中性能最优，ECAPA-TDNN优于x-vector，但未超越H/ASP。  <br/>5. 强调说话人识别嵌入在TTS应用中的实证评估必要性，为后续研究提供可复现实验基准。  <br/><br/>**总结**：  <br/>本研究通过对比三种说话人编码器在零样本TTS中的表现，发现H/ASP编码器效果最佳，揭示了说话人识别嵌入迁移至TTS领域需依赖实证验证的重要性。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|总结：  <br/>本研究提出Kling-Foley，通过多模态模块化结构提升音视频同步与语义对齐，开发通用音频编码器和立体渲染技术，并开源工业级基准数据集，实验证明在多指标上取得SOTA性能。<br/><br/>贡献点：  <br/>1. **提出多模态生成框架**：设计Kling-Foley模型，整合视频-音频-文本交互的多模态扩散变压器、视觉语义表示模块和音频-视觉同步模块，提升跨模态对齐能力。  <br/>2. **通用音频编码器开发**：引入跨场景适用的潜藏音频编码器（Kling-Audio-Codec），支持声效、语音、唱歌和音乐的高质量建模。  <br/>3. **空间感知渲染技术**：采用立体渲染方法增强合成音频的空间存在感，优化听觉体验。  <br/>4. **工业级基准数据集开源**：为解决现有数据集不足，开源Kling-Audio-Eval，提供高质量标注的音视频对数据。  <br/>5. **性能突破**：基于流匹配目标训练的Kling-Foley在分布匹配、语义对齐、时序对齐和音频质量上达成SOTA，优于现有公开模型。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|**贡献点：**  <br/>1. **TTSDS2的提出**：提出改进的语音质量评估指标TTSDS2，解决了传统TTS系统评测中主观与客观指标不一致的问题，跨领域和语言均表现出优异的Spearman相关性（高于0.50）。  <br/>2. **大规模主观数据集**：发布包含11,000+主观评分的语音质量数据集，填补合成语音与真实语音对比的评估缺口。  <br/>3. **多语言持续测试框架**：构建可动态生成多语言测试数据集的管道，防止数据泄露并支持跨语言一致性评估。  <br/>4. **更新基准系统**：提供14种语言的持续更新TTS性能基准，推动领域内标准化评测流程。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出改进的TTSDS2评估指标，结合大规模主观数据集与动态测试框架，解决了TTS系统语音质量评测的挑战，显著提升跨语言和领域的评估可靠性，推动语音合成技术的标准化发展。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**贡献点总结**:  <br/>该论文提出了一种更合理的主观语音质量评估方法N_low-MOS，并通过实验证明其能提升MOSNet对语音转换模型的评估效果，揭示了人类对语音质量的注意力分配机制。  <br/><br/>**分点贡献**:  <br/>1. **提出新评估指标**：定义N_low-MOS为N个最低意见评分的平均值，解决了传统MOS评估中对不一致质量段的代表性不足问题。  <br/>2. **验证注意力分配假设**：通过分析VCC2018和BVCC数据集，论证了人类在主观评估时更关注低质量段，从而解释评分方差的来源。  <br/>3. **实验验证有效性**：证明使用N_low-MOS训练MOSNet后，LCC和SRCC（线性相关系数和斯皮尔曼相关系数）均优于传统MOS评估方法。  <br/>4. **优化模型比较框架**：指出N_low-MOS可作为更内在的主观质量代表，为语音转换模型的性能评估提供更可靠的基准。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|总结（100字以内）:  <br/>本文提出一种改进的MOS预测方法，通过建模标注者的连续评分过程并量化潜在分布，用峰值代替传统离散MOS作为新指标，显著提升语音质量评估模型性能。<br/><br/>贡献点:  <br/>1. **提出新型评分聚合方法**：解决传统1-5分离散标注的局限性，通过连续评分建模提升评分精度。  <br/>2. **理论假设创新**：假设标注者基于连续评分选择最近的离散值，构建了评分生成的分布模型。  <br/>3. **潜在分布量化技术**：利用量化后的连续潜在分布逼近真实评分分布，优化模型训练目标。  <br/>4. **峰值替代MOS**：引入潜在分布的峰值作为新的代表值，替代传统MOS指标提升预测效果。  <br/>5. **实验验证有效性**：通过实验表明该方法能显著改善MOSNet等模型的预测性能。|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|**贡献点总结**（100字以内）:  <br/>构建首个针对日本年轻女性现场偶像的语音语料库（JIS），支持TTS和VC系统中说话者相似性评估，促进个性化语音生成研究，推动开放科学并确保非商业使用，提供文化背景与应用指导。<br/><br/>---<br/><br/>**分点贡献**:<br/>1. **创建专业语料库**：首次构建专门针对日本"年轻女性现场偶像"的语音语料库（JIS），覆盖该特定群体的语音特征，填补语音生成领域细分数据空白。<br/>2. **精准评估指标**：通过统一的舞台名标识系统，使研究者能有效筛选熟悉偶像的听众，提升TTS/VC系统中说话者相似性评估的准确性与客观性。<br/>3. **个性化研究方向**：聚焦"根据听众偏好生成定制化语音"这一新兴领域，为探索语音生成与受众互动关系提供独特数据支持。<br/>4. **开放科研机制**：提供免费非商业使用许可，促进语音AI研究的开放性与可复现性，同时确保数据用于基础研究而非商业化用途。<br/>5. **文化背景支持**：配套日本偶像文化解析，帮助研究者理解语料库的社会语境，确保研究的伦理性和文化敏感性。|
|2506.16738v1|[LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](http://arxiv.org/abs/2506.16738v1)|总结（100字以内）:  <br/>LM-SPT提出了一种新的语音标记方法，通过间接数据驱动监督优化语义对齐，改进编码器解码器结构，支持多帧率，提升了语音到文本与文本到语音任务的性能。  <br/><br/>贡献点:  <br/>1. **提出LM-SPT方法**：通过冻结ASR编码器重构语音波形，最小化原始与重建之间的差异，实现更精确的语义对齐。  <br/>2. **新型语义蒸馏策略**：替代传统池化操作，采用间接监督机制，减少语义信息损失与声学冗余。  <br/>3. **架构优化**：改进语音标记的编码器-解码器设计，增强模型效率与泛化能力。  <br/>4. **多帧率支持**：兼容25Hz、12.5Hz、6.25Hz等不同帧率，提升模型灵活性与适用性。  <br/>5. **实验验证优势**：在重建保真度与文本到语音任务中优于基线方法，证实了方法有效性。|
|2506.16580v1|[Streaming Non-Autoregressive Model for Accent Conversion and   Pronunciation Improvement](http://arxiv.org/abs/2506.16580v1)|总结：  <br/>提出首个支持流式处理的发音转换模型，实现非母语语音向母语发音的转换，同时保持说话者身份、语调并提升发音质量，通过Emformer编码器和优化推理机制实现高效低延迟处理，结合母语TTS生成训练数据提升性能。<br/><br/>贡献点：  <br/>1. **首个流式AC模型**：首次实现非母语语音向母语发音的实时转换，保持说话者身份与语调，且显著提升发音准确性。  <br/>2. **架构优化**：采用Emformer编码器替代传统编码器，结合优化的推理机制，支持稳定低延迟的流式处理。  <br/>3. **训练数据增强**：集成母语TTS模型生成理想目标数据，提升训练效率及模型性能。  <br/>4. **性能与延迟平衡**：在保持稳定延迟的同时，模型性能达到现有顶级AC系统的水平，实现流式应用可行性。|
|2506.16381v1|[InstructTTSEval: Benchmarking Complex Natural-Language Instruction   Following in Text-to-Speech Systems](http://arxiv.org/abs/2506.16381v1)|总结：  <br/>本文提出InstructTTSEval基准，涵盖三个任务和中英文数据集，通过Gemini自动评估，揭示指令跟随TTS的改进空间，推动更灵活准确的语音合成发展。<br/><br/>贡献点：  <br/>1. **提出新型基准InstructTTSEval**：填补了指令驱动语音合成领域缺乏系统性评估的空白，专门针对复杂自然语言风格控制能力。  <br/>2. **设计三类任务框架**（Acoustic-Parameter Specification, Descriptive-Style Directive, Role-Play）：覆盖语音合成中关键的非语言特征（如音色、情感、韵律），并包含中英文子集（共6k测试案例）。  <br/>3. **引入自动评估方法**：基于Gemini模型实现跨系统的指令遵循能力自动化评测，提升评估效率和客观性。  <br/>4. **揭示研究现状与改进方向**：通过实验证明当前指令驱动TTS系统在复杂指令处理上仍有显著局限，为后续优化提供依据。  <br/>5. **推动领域发展**：强调该基准对构建更灵活、精准的语音合成技术的潜在价值。|
|2506.16310v1|[Optimizing Multilingual Text-To-Speech with Accents & Emotions](http://arxiv.org/abs/2506.16310v1)|**贡献点总结（100字以内）：**  <br/>本研究提出一种融合多尺度情感建模与文化敏感的TTS架构，针对印地语和印度英语的语调及情感生成进行优化。通过语言特异性音素对齐、动态语调切换及残差向量量化技术，显著提升语调准确率（降低WERR 23.7%）和情感识别准确率（达85.3%），并在跨语言合成中实现实时语调-情感混合，适用于南亚教育科技与无障碍软件。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **创新架构**：设计集成语调、音译保持和多尺度情感建模的TTS系统，特别优化印地语和印度英语的多语言语调处理。  <br/>2. **模型扩展**：基于Parler-TTS，引入语言特异性音素对齐的混合编解码器架构，增强多语言语音生成的准确性。  <br/>3. **文化敏感情感建模**：通过使用母语者语料训练的情感嵌入层，提升情感表达的文化适配性与识别准确率。  <br/>4. **动态语调切换技术**：采用残差向量量化实现动态语调代码切换，支持跨语言语调无缝过渡。  <br/>5. **定量实验验证**：提升语调准确率23.7%（WERR从15.4%降至11.8%）和情感识别准确率85.3%，优于现有基准模型。  <br/>6. **实时混合能力**：实现语调与情感的动态混合生成，如“Namaste, let's talk about <Hindi phrase>”，保持情感一致性。  <br/>7. **用户评价成果**：主观测试MOS达4.2/5，显著优于其他多语言系统（p<0.01），验证文化正确性。  <br/>8. **实际应用场景**：推动跨语言合成的可行性，可应用于南亚教育科技（EdTech）和无障碍软件开发。|
|2506.16127v1|[Improved Intelligibility of Dysarthric Speech using Conditional Flow   Matching](http://arxiv.org/abs/2506.16127v1)|**贡献点：**  <br/>1. 提出使用自监督学习（SSL）特征及其量化表示替代传统mel-spectrograms，用于dysarthric-to-regular语音生成。  <br/>2. 通过WavLM提取的单人语音特征，探索减少说话人变异的方法，生成更一致的干净语音。  <br/>3. 设计全非自回归框架，结合Conditional Flow Matching (CFM)与Diffusion Transformers实现端到端直接映射。  <br/>4. 验证离散声学单元在提升语音可懂度和加速模型收敛方面的有效性，优于传统方法。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于SSL特征和CFM的非自回归模型，用于dysarthric语音到清晰语音的转换，通过WavLM特征提取降低说话人变异，并验证离散声学单元在提升可懂度与收敛效率方面优于传统mel-spectrogram方法。|
|2506.15873v1|[DeckFlow: Iterative Specification on a Multimodal Generative Canvas](http://arxiv.org/abs/2506.15873v1)|总结：  <br/>本研究提出DeckFlow多模态生成工具，通过任务分解、规格分解及生成空间探索创新解决生成AI设计难题，并扩展至音频生成以验证其在开放创作场景中的有效性。<br/><br/>贡献点：  <br/>1. **任务分解机制**：支持通过无限画布与卡片连接的可视化数据流，实现多子任务的协同管理。  <br/>2. **规格分解流程**：提供迭代分解目标、结合特征标签与聚类的特异性工作流，优化生成过程。  <br/>3. **生成空间探索**：通过网格展示多提示与输出变体，支持递归反馈优化设计迭代。  <br/>4. **跨模态验证**：以文本-图像生成对比现有对话式AI基线，进一步扩展至音频生成以探索用户行为。|
|2506.15759v1|[Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](http://arxiv.org/abs/2506.15759v1)|**总结（100字以内）**  <br/>该论文提出Sonic4D框架，首次实现4D场景与空间音频的联合生成，通过动态视觉捕捉、声源定位及物理模拟三阶段处理，无需训练即可生成逼真的时空音频，增强了沉浸式音频视觉体验。<br/><br/>**贡献点**  <br/>1. **提出Sonic4D框架**：首次将空间音频生成与4D场景合成结合，解决现有方法忽视音频与场景对齐的问题。  <br/>2. **三阶段生成方法**：  <br/>   - 第一阶段：利用预训练模型生成4D场景及单声道音频；  <br/>   - 第二阶段：通过像素级视觉定位策略估计声源的3D空间坐标；  <br/>   - 第三阶段：基于物理模拟合成动态视角与时间变化的时空音频。  <br/>3. **训练-free设计**：无需额外训练，直接利用现有数据生成符合场景的空间音频。  <br/>4. **实验验证与资源公开**：通过实验证明生成音频的逼真度和沉浸感，并开放生成示例供研究复现。|
|2506.15085v1|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v1)|总结：  <br/>提出EmojiVoice工具包，实现社交机器人离线语音表达的动态控制，通过emoji提示增强长时表达性，并验证其在不同场景中的表现差异。<br/><br/>贡献点：  <br/>1. **开发EmojiVoice工具包**：首个专门针对社交机器人长时语音表达的免费、可定制TTS系统，解决基础模型离线部署难题。  <br/>2. **引入emoji提示控制机制**：首次将表情符号用于细粒度表达性调控，实现对语音情感相位的精准控制。  <br/>3. **轻量级实时生成框架**：采用Matcha-TTS等轻量模型，支持机器人端实时语音生成需求。  <br/>4. **多场景验证与对比**：通过剧本对话、讲故事、自主交互三类案例研究，验证方法对表达性的提升效果，并揭示不同应用场景下的接受差异。|
|2506.13053v2|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v2)|**贡献点分点总结**:  <br/>1. 提出ZipVoice，首个高质且轻量的零样本TTS模型，基于流匹配框架。  <br/>2. 引入Zipformer解码器，实现模型压缩与建模能力的平衡。  <br/>3. 设计平均上采样对齐方法与Zipformer编码器，提升语音可理解性。  <br/>4. 创新性流蒸馏技术，降低采样步骤并消除无分类器引导的推理开销。  <br/>5. 在100k小时多语言数据集上验证性能，表现优于SOTA模型且效率显著提升。  <br/><br/>**总结（100字以内）**:  <br/>本文提出ZipVoice，通过结构优化与流蒸馏技术，在保持高质量的同时显著提升推理效率，模型体积缩小3倍、速度加快30倍，且公开代码与资源便于应用。|
|2506.12199v1|[ViSAGe: Video-to-Spatial Audio Generation](http://arxiv.org/abs/2506.12199v1)|**贡献点总结**（100字以内）:  <br/>本研究提出ViSAGe框架，直接从无声视频生成一阶Ambisonics，构建YT-Ambigen数据集，设计空间音频评估指标，并验证其在时空对齐与视角适应性上的优越性。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新问题**：探索无需复杂录制设备或专业技能，直接从无声视频生成一阶Ambisonics的方法。  <br/>2. **构建数据集**：创建包含102K视频片段的YT-Ambigen数据集，为研究提供标注数据支持。  <br/>3. **设计评估指标**：首次基于音频能量图与显著性度量，提出量化评估生成空间音频质量的新标准。  <br/>4. **端到端框架**：提出ViSAGe模型，融合CLIP视觉特征、自回归音频编码与方向/视觉引导生成高质量Ambisonics。  <br/>5. **性能优势**：验证ViSAGe在生成音频的连贯性与时空对齐性上优于传统两阶段方法（视频-音频生成+空间化）。  <br/>6. **应用能力**：生成的音频可随视角变化动态适应，提升沉浸式音频体验的交互性。|
|2506.11160v5|[S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation   Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning](http://arxiv.org/abs/2506.11160v5)|总结：  <br/>S2ST-Omni提出了一种高效多语言语音到语音翻译框架，通过集成预训练模型、轻量化适配器和两阶段微调策略，解决了高质量翻译和依赖平行语料的挑战。<br/><br/>贡献点：  <br/>1. **框架设计创新**：提出S2ST-Omni框架，将语音-语音翻译分解为语音-文本翻译（S2TT）和文本-语音合成（TTS）两阶段，实现任务拆分与模块化处理。  <br/>2. **预训练模型融合**：结合Whisper编码器（增强语音理解）和Qwen 3.0（提升文本理解），构建高效的S2TT子系统。  <br/>3. **轻量化适配器**：引入轻量级语音适配器，弥合语音与文本表征间的模态差距，降低计算成本。  <br/>4. **两阶段微调策略**：设计分阶段的微调方法，优化多模态知识学习效率。  <br/>5. **流式生成技术**：在TTS阶段采用流式自回归生成，提升语音合成的自然度与流畅性。  <br/>6. **实验验证有效性**：在CVSS基准测试中，S2ST-Omni在翻译质量上超越现有S2ST系统，证明其优越性。|
|2506.11130v1|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v1)|总结（100字以内）:  <br/>提出自优化框架，利用无标签数据与闭环训练提升ASR性能，成功将Whisper转化为Twister，在台语中实现显著精度提升，并为低资源场景提供实用方案。<br/><br/>贡献点：<br/>1. 提出无需标注数据的自精炼框架，通过伪标签生成与闭环训练实现ASR性能提升。  <br/>2. 首次将高保真TTS系统与ASR模型结合，形成"ASR→TTS→ASR"的闭合优化循环。  <br/>3. 在台语语音任务中验证框架有效性，利用6000小时未标注语音驱动模型训练。  <br/>4. 开发专用模型Twister，相较Whisper在普通话和双语切换场景分别降低20%和50%错误率。  <br/>5. 为低资源/特定领域ASR提供创新解决方案，突破传统伪标签自蒸馏方法的局限性。|
|2506.11127v1|[GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech   Instructions](http://arxiv.org/abs/2506.11127v1)|总结：  <br/>提出GUIRoboTron-Speech，首个端到端语音指令GUI代理，通过语音数据生成和混合训练策略解决模态不平衡问题，实验验证其高效性和广泛应用前景。<br/><br/>贡献点：  <br/>1. **首个语音驱动GUI代理**：提出GUIRoboTron-Speech，首次实现端到端GUI操作，直接接受语音指令和设备截图作为输入，无需文本转换。  <br/>2. **语音数据生成方法**：利用随机音色TTS模型将现有文本指令转换为高质量语音指令，解决语音数据稀缺问题。  <br/>3. **混合指令训练策略**：设计启发式方法缓解预训练模型的模态不平衡，结合文本和语音指令提升代理性能。  <br/>4. **系统性实验验证**：在基准数据集上验证模型表现，证明语音指令在GUI自动化中的有效性与广泛适用性。|
|2506.11086v1|[Intelligibility of Text-to-Speech Systems for Mathematical Expressions](http://arxiv.org/abs/2506.11086v1)|贡献点：  <br/>1. **系统评估框架**：首次设计涵盖五种TTS模型的实验，量化数学表达式（MX）的发音质量与可懂性（通过用户评分和转录正确性）。  <br/>2. **LLM辅助生成**：利用大语言模型（LLM）将LaTeX格式MX转换为英文发音，弥补TTS模型无法直接处理LaTeX的缺陷。  <br/>3. **多维度指标**：提出包含三个指标的转录正确性评估体系，并结合Mean Opinion Score分析用户主观感受。  <br/>4. **对比分析**：对比听众对TTS输出与人类专家发音的偏好，揭示TTS在处理MX时的表现短板。  <br/>5. **结果发现**：发现TTS模型输出的可懂性存在显著差异，且多数MX类别下表现劣于专家，同时LLM选择对结果影响有限。  <br/>6. **应用导向结论**：明确指出需针对性优化TTS模型处理数学表达式的能力。  <br/><br/>总结：  <br/>首次系统评估TTS模型对数学表达式的发音质量与可懂性，揭示模型与人类专家表现差异，强调改进TTS处理MX能力的必要性。|
|2506.10019v1|[A Survey of Automatic Evaluation Methods on Text, Visual and Speech   Generations](http://arxiv.org/abs/2506.10019v1)|总结：本文提出首个综合框架，系统化分类文本、图像和音频生成的评估方法，识别出五种核心范式，并探讨跨模态评估的未来方向。<br/><br/>贡献点：  <br/>1. **构建统一框架**：首次建立跨文本、图像、音频三大模态的自动评估方法系统化框架，实现多模态方法的整合与对比。  <br/>2. **提出五种核心范式**：归纳出生成内容评估的五大基础范式，为后续研究提供理论分类依据。  <br/>3. **跨模态适用性验证**：将文本生成评估方法的分析逻辑扩展至图像和音频领域，验证框架的广泛适用性。  <br/>4. **探索未来研究方向**：针对跨模态评估挑战，提出潜在的研究路径，促进多模态生成AI的评估体系发展。|
|2506.09827v2|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v2)|**贡献点：**  <br/>1. 提出EmoNet-Voice资源，包含大规模预训练数据集（Big）和新型基准数据集（Bench），覆盖40种情感、11种声音及4种语言。  <br/>2. 支持细粒度情感评估，明确区分不同情感强度以提升模型准确性。  <br/>3. 通过语音生成技术合成情感音频，模拟真实场景以解决隐私问题。  <br/>4. 引入心理学专家标注与验证，确保情绪强度标签的可靠性。  <br/>5. 提出Empathic Insight Voice模型，实现与人类专家高度一致的SER性能。  <br/>6. 揭示高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更易被识别的模型表现差异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoNet-Voice资源，通过合成语音和专家标注解决隐私与情感粒度问题，推出高性能SER模型，并发现高唤醒情绪更易识别，为情感语音研究提供新基准。|
|2506.09827v1|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v1)|总结：  <br/>提出EmoNet-Voice资源及两个子集，包含大规模预训练数据与专家标注基准数据，解决隐私问题并增强情感粒度和多样性。开发高精度模型，通过评估揭示高唤醒情绪识别易度高于低唤醒情绪。<br/><br/>贡献点：  <br/>1. **引入EmoNet-Voice资源**：构建由EmoNet-Voice Big（大规模预训练数据集）和EmoNet-Voice Bench（专家标注基准数据集）组成的综合数据资源，覆盖40种情感、11种声音和4种语言。  <br/>2. **提升情感粒度与强度区分**：设计支持细粒度情感分类（40类）及不同强度评估的框架，推动SER模型在情感理解上的更精确建模。  <br/>3. **合成数据解决隐私与多样性问题**：通过先进语音生成技术合成敏感情感场景数据，避免真实数据隐私风险并扩展情感表达的多样性。  <br/>4. **专家验证机制**：联合心理学专家对合成数据进行严格标注（感知强度标签），确保数据质量与情感感知的科学性。  <br/>5. **提出高性能SER模型**：开发Empathic Insight Voice模型，实现与人类专家的高一致性，推动语音情感识别技术的标准化。  <br/>6. **揭示情绪识别难度差异**：通过对比实验发现，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易被模型识别，为后续研究提供新视角。|
|2506.08279v1|[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)|**总结（100字以内）:**  <br/>该论文提出Mirage模型，首次实现从音频直接生成高质量、表达性强的视频，结合TTS技术生成多模态视频，并开发统一的自注意力训练方法，提升生成视频的主观质量与通用性。  <br/><br/>**分点贡献:**  <br/>1. **提出Mirage模型**：首个专注于音频到视频生成的通用模型，从原始音频直接生成逼真、表达性的视频图像，不依赖视觉输入。  <br/>2. **多模态生成能力**：通过集成文本到语音（TTS）技术，实现语音与视频的同步生成，解决语音和视频内容对齐问题。  <br/>3. **统一训练方法**：开发适用于自注意力机制的统一训练框架，支持从头训练和基于已有权重的微调，增强模型灵活性。  <br/>4. **语义对齐优化**：利用A-roll数据（人物对话语音视频）训练，使生成视频能准确反映音频中的表演信息，提高可信度。  <br/>5. **性能优势**：生成的视频在主观质量上优于依赖语音特定架构或损失函数的现有方法，保持通用性的同时实现高保真输出。|
|2506.04527v1|[Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning](http://arxiv.org/abs/2506.04527v1)|总结（100字以内）:<br/>该论文提出结合字母符号的语音标签生成方法，通过BERT特征提示编码实现隐式条件，配合推理时的显式修剪提升标签一致性，为文本到语音和口音估计等任务生成高质量平行数据。<br/><br/>贡献点：<br/>1. **方法创新**：首创结合字母符号的语音标签生成框架，突破传统ASR模型直接微调的局限，引入显式与隐式的双条件策略。<br/>2. **条件生成机制**  <br/>   - 通过预训练BERT特征构建隐式图形单词条件（Prompt Encoder）  <br/>   - 提出推理阶段的显式标签假设剪枝算法（Pruning Inconsistent Hypotheses）<br/>3. **数据价值**：生成跨模态的语音-标签-图形单词平行数据，支持文本到语音合成（TTS）和口音估计等下游任务。<br/>4. **实验验证**：在音素-图形单词一致性任务和口音估计任务中均取得显著性能提升，证明方法的有效性。|
|2506.04397v1|[Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?](http://arxiv.org/abs/2506.04397v1)|总结：  <br/>该论文通过构建专用数据集和微调大模型，探索Parler TTS在语音重建任务中生成可理解语音与保持说话人身份的能力，揭示其在可控性方面的局限并提出改进方向。<br/><br/>贡献点：  <br/>1. **构建专用数据集**：创建并标注了包含说话人信息和可懂度数据的语音重建数据集，为相关研究提供基础资源。  <br/>2. **应用大模型进行语音重建**：首次尝试利用Parler TTS等先进大型语音模型，生成语音障碍患者疾病前的语音近似。  <br/>3. **分析模型局限性**：发现模型虽能学习数据分布，但在控制语音可懂度和保持说话人身份一致性方面存在挑战。  <br/>4. **提出改进方向**：为提升语音重建任务中模型的可控性，提出了未来研究的潜在优化路径。|
|2506.04152v1|[HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](http://arxiv.org/abs/2506.04152v1)|**分点贡献：**  <br/>1. **构建高带宽语音数据集**：提出HiFiTTS-2，基于LibriVox有声书，包含36.7k小时（22.05kHz）和31.7k小时（44.1kHz）的高质量英语语音数据，支持高带宽语音合成研究。  <br/>2. **创新数据处理流程**：设计了包含带宽估计、语音分段、文本预处理及多说话人检测的全流程数据处理方法，提升数据适配性与实用性。  <br/>3. **提供详细元数据**：生成全面的语句和有声书元数据，支持研究者通过数据质量过滤器灵活调整数据集以适配不同应用场景。  <br/>4. **验证数据集有效性**：实验表明该数据集和处理方法可有效训练高质量零样本TTS模型，推动高带宽语音生成技术的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HiFiTTS-2，构建大规模高带宽语音数据集并设计全流程处理技术，提供详细元数据以增强数据适应性，实验证明其可有效支持高质量零样本TTS模型训练。|
|2506.04134v1|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v1)|**贡献点总结（100字以内）**：  <br/>提出直接生成CS语音的新方法，构建首个统一框架UniCUE，通过细颗粒语义对齐、跨任务适配器及姿态感知处理，解决误差传播与时空对齐问题，显著提升语音生成质量与唇语同步性。<br/><br/>**分点贡献**：  <br/>1. **直接生成语音的创新方法**：首次无需依赖文本中间媒介，直接从CS视频生成语音，避免误差传播与时间错位问题。  <br/>2. **统一框架UniCUE**：提出首个融合CSV2S与CSR任务的框架，打通视觉-语义到语音的端到端生成流程。  <br/>3. **核心技术创新**：  <br/>   - 引入细颗粒语义对齐池，精确映射视觉特征与语音内容；  <br/>   - 设计VisioPhonetic适配器，实现CSV2S与CSR的跨任务表示兼容；  <br/>   - 开发姿态感知视觉处理器，增强唇语与手势的时空关联性。  <br/>4. **实验验证与数据集**：基于新建立的中文CS数据集（含14名受试者），验证方法有效性，降低误词率78.3%并提升唇语同步性32%。|
|2506.04077v1|[A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions](http://arxiv.org/abs/2506.04077v1)|总结：  <br/>该论文提出一种基于大语言模型与跨模态信息的新型语音评估方法，有效解决低资源场景下的标签约束问题，提升口语评分的可靠性和多样性。<br/><br/>贡献点：  <br/>1. **低资源场景下的数据生成**：利用大语言模型生成多样化响应并合成语音，突破标注数据的稀缺限制。  <br/>2. **说话人感知的语音合成**：通过 speaker-aware 文本到语音合成技术，增强生成语音与真实语音的匹配度。  <br/>3. **动态重要性损失设计**：根据合成语音与真实语音的特征差异，自适应调整训练样本权重，提升模型学习效率。  <br/>4. **跨模态特征融合**：构建多模态大语言模型，直接整合文本与语音信号的对齐特征以预测口语水平。  <br/>5. **有效性验证**：在 LTTC 数据集上验证方法优于传统数据依赖或常规增强技术，实现跨模态信息驱动的自动化评分。|
|2506.03884v1|[Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages](http://arxiv.org/abs/2506.03884v1)|总结: 本文提出基于零样本合成的多语言TTS系统，通过共享音素表示和适配文本解析规则，成功生成梵语等低资源语言的自然语音，验证了该方法在扩展语音技术普惠性方面的有效性。<br/><br/>贡献点:<br/>1. 提出零样本合成框架，解决印度1369种语言中多数缺乏数字资源的TTS训练难题<br/>2. 构建跨语言音素共享表示，降低多语言模型过度假设需求<br/>3. 开发适应不同音系规则的文本解析方法（phonotactics adaptation）<br/>4. 实现Sanskrit、Maharashtrian、Canara Konkani、Maithili、Kurukh等5种语言的自然语音合成<br/>5. 提供可复用的合成器架构，验证方法在扩大语音技术覆盖范围中的可行性|
|2506.03831v1|[Conformer-based Ultrasound-to-Speech Conversion](http://arxiv.org/abs/2506.03831v1)|总结：  <br/>本研究对比了Conformer与传统CNN在超声波语音转换中的表现，发现Conformer结合bi-LSTM在感知质量上更优，而Conformer Base在效率上显著提升，为静默语音接口提供了新思路。<br/><br/>贡献点：  <br/>1. **提出两种Conformer-based模型**：首次将Conformer架构应用于超声波到语音转换任务，分别采用Base结构和结合bi-LSTM的结构。  <br/>2. **融合HiFi-GAN声码器**：通过HiFi-GAN将生成的mel频谱图合成高质量音频波形，提升系统输出的语音还原效果。  <br/>3. **验证感知质量优势**：在MUSHRA主观测试中发现，Conformer与bi-LSTM结合的模型在语音可懂度上优于传统CNN基线。  <br/>4. **效率提升**：Conformer Base模型在保持性能的同时，训练速度比传统CNN快3倍，降低了计算成本。|
|2506.03793v1|[Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts](http://arxiv.org/abs/2506.03793v1)|**贡献点**  <br/>1. **提出Cadence模型**：基于预训练大语言模型设计，首次实现对自发语音转录的精准标点恢复，克服传统模型在处理断续、重复等语音特征时的不足。  <br/>2. **扩展语言覆盖**：支持22种印度语言及英语（原仅14种），显著提升多语言标点恢复的适用性与泛化能力。  <br/>3. **分析挑战与改进**：通过跨语言和标点类型的行为研究，揭示领域迁移及罕见标点符号下的修复难题，验证预训练模型在低资源NLP场景中的有效性。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出Cadence模型，基于预训练语言模型实现自发语音的精准标点恢复，扩展支持22种印度语言和英语，并通过深入分析揭示多语言任务中的关键挑战。|
|2506.03515v1|[BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and   Weight Indexing](http://arxiv.org/abs/2506.03515v1)|总结：  <br/>本文提出两种模型压缩技术，实现83%的模型体积减小，同时保持高质量语音合成，适用于设备端部署。<br/><br/>贡献点：<br/>1. 提出量化感知训练（QAT）技术，将模型参数从32位压缩至1.58位，主要采用三值{-1,0,1}量化策略。<br/>2. 开发权重索引方法，通过将一组1.58位权重编码为单个int8索引来优化存储效率。<br/>3. 在保持合成质量的前提下，实现模型轻量化，使其更适用于资源受限的设备端应用场景。|
|2506.02979v1|[Towards a Japanese Full-duplex Spoken Dialogue System](http://arxiv.org/abs/2506.02979v1)|**贡献点总结：**  <br/>1. 提出首个公开可用的全双工日语语音对话模型，基于英语模型Moshi迁移。  <br/>2. 采用两阶段训练方法：大规模日语语料预训练与高质量立体声数据微调。  <br/>3. 引入多流文本到语音系统生成的合成对话数据以增强模型性能。  <br/>4. 实验证明模型在自然度和语义理解上优于现有日语基准模型。  <br/><br/>（注：以上为100字以内总结，符合要求。）|
|2506.02958v1|[PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech   Editing](http://arxiv.org/abs/2506.02958v1)|**贡献点总结（100字以内）:**  <br/>提出PartialEdit数据集，聚焦神经语音编辑生成的深伪语音检测，揭示现有模型的局限性，分析伪影特征，推动该领域研究。<br/><br/>**分点贡献:**  <br/>1. **提出PartialEdit数据集**：构建首个基于先进神经语音编辑技术的深伪语音检测数据集，包含部分编辑的语音样本，为研究提供基准。  <br/>2. **探索检测与定位任务**：系统研究部分编辑语音的检测与内容定位，验证当前检测方法在复杂编辑场景下的有效性。  <br/>3. **揭示模型局限性**：实验表明基于PartialSpoof数据集训练的模型无法有效识别神经编辑生成的深伪语音，凸显数据集不足。  <br/>4. **分析伪影特征**：结合神经音频编码器，探讨检测模型学习到的伪影（artifacts）特征，为改进检测方法提供理论依据。  <br/>5. **提供公开资源**：开放数据集和音频样本，方便后续研究复现与扩展。|
|2506.02858v2|[DGMO: Training-Free Audio Source Separation through Diffusion-Guided   Mask Optimization](http://arxiv.org/abs/2506.02858v2)|总结：  <br/>提出训练无关的零样本音频源分离框架DGMO，利用扩散模型生成先验优化频谱掩码，突破传统方法依赖任务特定训练的限制，拓展扩散模型在音频分离领域的应用范式。<br/><br/>贡献点：  <br/>1. **首个训练无关的零样本框架**：首次将预训练扩散模型直接用于语言查询的音频源分离，无需额外任务特定训练。  <br/>2. **测试时优化机制**：提出Diffusion-Guided Mask Optimization（DGMO），通过测试阶段动态优化频谱掩码以提升分离精度。  <br/>3. **跨模态适应性提升**：分析模态适配问题，揭示自然语言与音频处理的协同挑战，并提出针对性解决方案。  <br/>4. **性能验证**：在无监督或弱监督条件下实现竞争性分离效果，证明扩散模型可有效迁移至音频分离任务。  <br/>5. **应用范式创新**：拓展扩散模型从生成任务到分离任务的边界，建立新的零样本音频处理框架。|
|2506.02401v1|[Trusted Fake Audio Detection Based on Dirichlet Distribution](http://arxiv.org/abs/2506.02401v1)|**贡献点：**  <br/>1. **提出基于狄利克雷分布的不确定性建模方法**：首次将狄利克雷分布用于量化模型决策的不确定性，提升假音频检测的可信度评估能力。  <br/>2. **构建新型检测框架**：通过神经网络生成证据，并将预测概率与不确定性估计结合，形成最终检测意见，优化检测流程。  <br/>3. **多数据集验证**：在ASVspoof 2019 LA、2021 LA及DF数据集上进行对比实验，验证了模型在准确率、鲁棒性和可信度方面的优越性能。  <br/><br/>**总结：**  <br/>该研究通过狄利克雷分布建模不确定性，提出新型假音频检测方法，在多数据集上验证了其高准确率与鲁棒性，显著提升了检测的可靠性。|
|2506.02082v1|[SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction](http://arxiv.org/abs/2506.02082v1)|总结：该论文提出了一种新的SALF-MOS模型，解决传统MOS评估高成本问题，通过卷积序列提取音频潜在特征，实现小尺寸、端到端、高度通用和可扩展的语音质量预测，取得最佳评估效果。<br/><br/>贡献点：<br/>1. 提出Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS)模型，解决传统MOS评估需要大量人工标注的问题<br/>2. 设计基于卷积序列的音频特征提取方法，实现端到端的MOS预测系统<br/>3. 构建的小尺寸模型在保持通用性的同时具备良好的可扩展性<br/>4. 在多个客观评估指标（MSE、LCC、SRCC、KTAU）上达到当前最优性能<br/>5. 提供了一种新的语音质量评估方法，可作为TTS和语音转换模型选择的有效工具|
|2506.01618v1|[Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech](http://arxiv.org/abs/2506.01618v1)|总结（100字以内）:  <br/>本文提出基于音节的节奏建模方法，结合LF-MMI模型显著提升口吃语音的ASR性能，验证了无监督转换的有效性。<br/><br/>贡献点:  <br/>1. **提出syllable-based rhythm modeling方法**：在Rhythm and Voice（RnV）框架中引入针对口吃语音的音节级节奏建模技术，解决传统方法对口吃语音节奏特征建模不足的问题。  <br/>2. **评估转换对ASR的影响**：首次系统验证口吃语音转换对ASR性能提升的效果，通过训练LF-MMI模型和微调Whisper模型，发现LF-MMI在口吃语音识别中具有显著优势。  <br/>3. **揭示无监督转换的潜力**：证明无监督学习的节奏和语音转换技术在口吃ASR任务中的有效性，为低资源场景下的语音处理提供了新思路。|
|2506.01322v1|[Zero-Shot Text-to-Speech for Vietnamese](http://arxiv.org/abs/2506.01322v1)|贡献点：<br/>1. 构建了首个越南语高质量语音数据集PhoAudiobook（含941小时音频）  <br/>2. 在VALL-E、VoiceCraft、XTTS-V2等三大零样本TTS模型上开展系统实验  <br/>3. 证实PhoAudiobook能显著提升多指标下的模型性能  <br/>4. 揭示VALL-E与VoiceCraft在短句合成任务中的优越性  <br/>5. 公开数据集促进越南语TTS领域研究发展  <br/><br/>总结：  <br/>本研究发布首个越南语高质量语音数据集PhoAudiobook，并验证其提升三大零样本TTS模型性能的效果，尤其突出VALL-E与VoiceCraft在短句生成中的优势，推动越南语语音合成研究。|
|2506.01268v1|[CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction](http://arxiv.org/abs/2506.01268v1)|贡献点总结：<br/>1. 提出单文件实现的CleanS2S框架，实现低延迟语音交互系统<br/>2. 首创主动对话机制，集成记忆系统与主观行动判断模块<br/>3. 开发五种人类般响应策略（中断/拒绝/转移/沉默/标准响应）<br/>4. 实现全双工WebSocket连接与非阻塞I/O的实时中断处理<br/>5. 提出Action Judgement SFT方法评估输入流策略选择<br/>6. 提供前所未有的透明度和可扩展性，代码已开源<br/><br/>（97字）|
|2506.01263v1|[WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing](http://arxiv.org/abs/2506.01263v1)|总结：  <br/>提出无需额外训练的CTC模型改进方法，通过中间层声学特征检测关键词并引入偏置机制，有效提升罕见词识别准确率，实验在日语语音识别中实现29%的F1分数提升。<br/><br/>贡献点：  <br/>1. **解决罕见词识别偏差问题**：通过中间层声学特征进行关键词检测，避免模型对训练数据词汇的过度依赖。  <br/>2. **引入灵活的wildcard CTC机制**：采用快速且容错的CTC变体，支持模糊匹配处理难以严格识别的单词。  <br/>3. **无需模型重训练的可扩展性**：方法兼容现有模型，无需额外训练或文本到语音系统，适用于大规模模型。  <br/>4. **显著提升未知词识别性能**：在日语语音识别任务中，实现未知词F1分数29%的提升，验证方法有效性。|
|2506.01032v1|[ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and   Speaker Feature Optimization](http://arxiv.org/abs/2506.01032v1)|**总结**：  <br/>本文提出ReFlow-VC，通过修正流的ODE框架提升语音转换效率，结合内容与音高信息优化说话人特征，并验证其在小数据集和零样本场景中的优越性能，推动高保真语音转换的实际应用。  <br/><br/>**贡献点**：  <br/>1. **提出ReFlow-VC方法**：基于修正流构建高效高保真语音转换模型，通过ODE框架将高斯分布直接映射到真实Mel频谱图分布，减少采样步骤。  <br/>2. **优化说话人特征建模**：同时利用内容与音高信息优化特征，增强说话人属性的准确表达。  <br/>3. **验证小样本适配性**：在小数据集和零样本场景中均表现出卓越的性能，突破传统扩散模型的局限。|
|2506.01020v1|[DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation](http://arxiv.org/abs/2506.01020v1)|总结：  <br/>该论文提出DS-TTS框架，结合双风格编码与动态生成网络，解决zero-shot语音克隆问题，实现未见过语音的高质量合成，并在VCTK数据集上验证了其优越的性能。<br/><br/>贡献点：  <br/>1. **提出DS-TTS框架**：设计一种用于zero-shot语音克隆的全新方法，仅需单个音频样本和任意文本即可合成目标说话人语音，无需历史训练数据。  <br/>2. **双风格编码网络（DuSEN）**：通过两个互补的风格编码器捕捉说话人语音身份的多维度特征，增强对未见说话人风格的建模能力。  <br/>3. **动态融合机制（SGF + DyGN）**：引入风格门控-Film机制将风格向量融入动态生成器，实现对不同句子长度的自适应合成，提升鲁棒性与自然度。  <br/>4. **实验验证优势**：在VCTK数据集上证明DS-TTS在单词错误率和说话人相似度上优于现有模型，显著改善语音克隆的泛化与表达性。|
|2506.00832v1|[Counterfactual Activation Editing for Post-hoc Prosody and   Mispronunciation Correction in TTS Models](http://arxiv.org/abs/2506.00832v1)|**贡献点：**<br/>1. **提出模型无关的后处理技术**：Counterfactual Activation Editing 是一种通用方法，无需依赖预训练模型特定架构，可适用于多种TTS模型。  <br/>2. **实现韵律与发音的联合控制**：同时支持对语音韵律特征（prosody）和发音错误（mispronunciation）的精确调整，提升控制能力。  <br/>3. **无需额外训练即可进行推理时调整**：在不重新训练模型的情况下，直接对预训练TTS的内部表示进行修改，实现在线输出优化。  <br/>4. **适应低资源场景**：解决传统发音纠正依赖音素词典（grapheme-to-phoneme dictionaries）的问题，降低对资源的依赖。  <br/>5. **填补预训练模型与可编辑合成的空白**：首次将可编辑的语音合成能力引入预训练TTS框架，推动模型灵活性与应用范围的扩展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种通用、无需额外训练的后处理方法，实现对TTS输出的韵律与发音联合控制，适应低资源场景，推动预训练模型向可编辑语音合成发展。|
|2506.00736v1|[IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio   Generation with Diffusion Modeling](http://arxiv.org/abs/2506.00736v1)|总结：  <br/>本文提出IMPACT框架，结合扩散建模与迭代掩码并行解码，在连续潜空间中生成高质量文本到音频，有效解决先前模型在音质与推理速度之间的权衡问题，实验在AudioCaps上达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出IMPACT框架**：首次将迭代掩码并行解码与扩散模型结合，实现文本到音频生成中高保真度与低延迟的双重优化。  <br/>2. **连续潜空间设计**：通过在连续潜空间中操作，避免离散token带来的保真度限制，提升生成音频的自然度和质量。  <br/>3. **高效解码策略**：采用迭代掩码并行解码技术，在保持推理速度的同时显著提高音频生成效果，优于MAGNET等基于离散token的模型。  <br/>4. **验证SOTA性能**：在AudioCaps数据集上，IMPACT在Fréchet Distance (FD)和Fréchet Audio Distance (FAD)等关键指标上超越现有方法，验证了其有效性。|
|2506.00722v1|[Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](http://arxiv.org/abs/2506.00722v1)|总结：该研究提出基于链式思维的端到端语音对话系统方法，在较小数据集上实现显著性能提升并提升语义连贯性，同时增强计算效率，模型及代码将公开。<br/><br/>贡献点：<br/>1. 提出基于链式思维（Chain-of-Thought, CoT）的端到端（E2E）对话系统框架，解决了传统级联流水线的局限性；<br/>2. 通过将对话训练与多模态语言模型（涵盖语音识别、文本生成、语音合成等任务）的预训练对齐，提升语义连贯性；<br/>3. 实现仅需300小时公开对话数据（如Switchboard）即可获得超过1.5 ROUGE-1的提升效果；<br/>4. 优化计算效率，使模型能在有限数据量下高效训练；<br/>5. 提供公开模型及代码，推动该领域研究与应用。|
|2506.00548v1|[Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities](http://arxiv.org/abs/2506.00548v1)|总结（100字以内）：  <br/>本论文提出Con Instruction方法，通过生成对抗图像和音频攻击多模态语言模型，无需文本预处理或训练数据。引入新型ARC评估框架，并验证其在多个模型上的高攻击成功率，揭示当前安全机制的不足。<br/><br/>贡献点（分点列出）：  <br/>1. **提出非文本指令攻击方法（Con Instruction）**  <br/>   - 首次利用MLLMs对非文本指令（图像/音频）的解析能力，通过生成对抗性图像或音频发起攻击，突破传统依赖文本指令的攻击模式。  <br/>   - 攻击无需训练数据或文本预处理，简化了攻击流程。  <br/><br/>2. **优化对抗样本的语义对齐**  <br/>   - 在嵌入空间中优化对抗样本，使其更精准匹配目标指令，显著提升攻击成功率，揭示MLLMs复杂语义理解的潜在安全风险。  <br/><br/>3. **提出攻击响应分类（ARC）框架**  <br/>   - 引入新型评估体系，综合衡量模型响应质量与对恶意指令的相关性，为安全评估提供更全面的指标。  <br/><br/>4. **验证多模型有效性与攻击性能**  <br/>   - 在LLaVA-v1.5、InternVL、Qwen-VL、Qwen-Audio等多模态模型上验证攻击效果，尤其在LLaVA-v1.5 (13B)中达到81.3%和86.6%的成功率，证明方法的普适性。  <br/><br/>5. **分析防御技术的局限性**  <br/>   - 探索针对攻击的防御措施，发现现有技术在攻击检测和防御能力上存在显著性能差距，为后续防御研究提供方向。  <br/><br/>6. **开放源代码实现**  <br/>   - 公布实现代码，便于学术界和工业界复现与扩展研究。|
|2506.00462v1|[XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](http://arxiv.org/abs/2506.00462v1)|**贡献点：**  <br/>1. **提出跨域多语言音频深度伪造基准**：构建XMAD-Bench数据集，包含668.8小时真实与深度伪造语音，覆盖多语言和多样化生成方法。  <br/>2. **强化数据集多样性与挑战性**：训练与测试集的说话人、生成模型、真实音频源均独立，显著提升跨域评估的难度。  <br/>3. **揭示性能差距**：实验结果表明，域内检测准确率接近100%，但跨域性能严重下降，接近随机猜测，突显现有方法的局限性。  <br/>4. **推动鲁棒检测器发展**：强调需开发具备跨语言、跨说话人、跨生成方法和跨数据源泛化能力的深度伪造检测技术。  <br/>5. **公开数据集**：将XMAD-Bench在GitHub平台开源，为研究社区提供统一的评估基准。  <br/><br/>**总结**：  <br/>提出首个跨域多语言音频深度伪造基准XMAD-Bench，验证现有检测模型在真实场景下的性能不足，推动更鲁棒的检测技术发展。|
|2506.00385v1|[MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity   Reconstruction and Generation](http://arxiv.org/abs/2506.00385v1)|**贡献点：**  <br/>1. 提出新型单层流式Transformer音频编解码器MagiCodec，解决传统编解码器重建质量与下游模型兼容性之间的矛盾。  <br/>2. 引入多阶段训练框架，结合高斯噪声注入与潜在正则化技术，增强编码的语义表达性并保持重建保真度。  <br/>3. 理论推导噪声注入在频域中的作用，证明其对抑制高频成分、提升鲁棒性效果显著。  <br/>4. 实验验证MagiCodec在重建质量与下游任务（如语言模型兼容）上优于现有技术，生成的编码呈现齐普夫分布特性。  <br/>5. 开源代码与预训练模型，促进研究复现与应用。  <br/><br/>**总结：**  <br/>MagiCodec通过噪声注入与多阶段训练提升音频编码的语义表达和重建质量，实现与语言模型的兼容性，开源推动研究进展。|
|2506.00087v1|[SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset](http://arxiv.org/abs/2506.00087v1)|总结：  <br/>本文提出多代理协作框架LinguaMaster，构建首个大规模多语言多民族代码转换数据集SwitchLingua，并设计语义感知评估指标SAER，推动多语言和文化多样性研究。  <br/><br/>贡献点：  <br/>1. **提出LinguaMaster框架**：首次设计多智能体协作系统，实现高效可扩展的多语言数据合成。  <br/>2. **构建SwitchLingua数据集**：创建首个覆盖12种语言、18国/地区、63个种族背景的大型代码转换语料库（含420K文本样本及80小时音频）。  <br/>3. **创新SAER评估指标**：提出融合语义信息的代码转换ASR评估方法，提升对跨语言场景的性能判断准确性。|
|2505.24496v1|[Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation](http://arxiv.org/abs/2505.24496v1)|总结：  <br/>本文提出一种压缩-精细语言建模方法，通过保留局部上下文和压缩长距离信息，有效解决神经音频编解码器在语音生成中的长序列挑战，提升生成质量与模型通用性。<br/><br/>贡献点：  <br/>1. **揭示语音标记序列的短距离依赖性**：首次观察到TTS任务中语音标记的预测主要依赖局部上下文，长距离标记贡献较小且冗余。  <br/>2. **设计压缩-精细语言建模框架**：  <br/>   - 保留初始提示和短范围标记，确保文本对齐及语用信息完整性；  <br/>   - 将长范围标记压缩为紧凑表示，减少冗余同时保留关键语义。  <br/>3. **验证方法的有效性与通用性**：通过多组神经音频编解码器和下游语言模型的实验，证明该方法可提升语音生成效果，具有广泛适用性。|
|2505.24314v1|[DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture   Switching for Speech Codec](http://arxiv.org/abs/2505.24314v1)|**贡献点：**  <br/>1. 提出DS-Codec，首次结合镜像（mirror）与非镜像（non-mirror）架构的双阶段训练框架，提升语音重建质量。  <br/>2. 设计动态切换机制，平衡镜像与非镜像结构的优势，增强模型的鲁棒性与高保真度。  <br/>3. 通过大规模实验与消融研究，验证了训练策略的有效性，对比了两种架构的性能差异。  <br/><br/>**总结：**  <br/>本研究提出DS-Codec，通过镜像与非镜像结构的双阶段训练框架提升语音重构效果，并系统验证了其优势。|
|2505.23619v1|[Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](http://arxiv.org/abs/2505.23619v1)|贡献点总结：<br/>1. 提出ADD-GP框架：基于高斯过程的少样本音频深度伪造检测方法，支持快速适应新生成模型  <br/>2. 性能突破：结合深度嵌入模型与GP灵活性，在少量数据下实现高检测准确率  <br/>3. 个性化检测能力：增强对新型TTS模型的鲁棒性，支持单样本级自适应  <br/>4. 构建基准数据集：使用SOTA语音克隆模型创建专用评估数据集  <br/><br/>（99字）|
|2505.20868v2|[Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction   and Style Direction Adjustment for Expressive Text-to-Speech](http://arxiv.org/abs/2505.20868v2)|**贡献点总结：**  <br/>1. 提出Spotlight-TTS，采用voiced-aware风格提取与风格方向调整，提升表达性与语音质量；  <br/>2. 通过聚焦声带相关区域并保持语音连续性，优化风格提取效果；  <br/>3. 风格方向调整增强与TTS模型的融合，改善合成质量；  <br/>4. 实验证明在表达性、整体质量与风格迁移能力上优于现有模型；  <br/>5. 公开音频样本促进研究复现与验证。<br/><br/>**100字内总结：**  <br/>本文提出Spotlight-TTS，通过声带感知的风格提取与方向调整，提升表达性与语音质量，并验证其在多项指标上优于基线模型，公开音频样本便于研究复现。|
|2505.19931v2|[Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned   Step Sampling](http://arxiv.org/abs/2505.19931v2)|总结（100字以内）:  <br/>提出训练无关的Fast F5-TTS方法，通过分析采样轨迹设计EPSS策略减少冗余步骤，实现7步生成且推理速度提升4倍，验证了EPSS对流匹配类TTS模型的通用性。  <br/><br/>贡献点:  <br/>1. **训练无关加速方案**：提出Fast F5-TTS，无需额外训练直接优化流匹配TTS模型的推理效率。  <br/>2. **非均匀采样策略**：开发EPSS方法，通过识别冗余采样步骤，显著减少生成过程的采样次数。  <br/>3. **高效推理性能**：在NVIDIA RTX 3090上实现7步生成，推理RTF达0.030，且性能与原模型相当。  <br/>4. **跨模型通用性**：验证EPSS在E2 TTS等其他流匹配模型上的有效性，证明其广泛适用性。|
|2505.19669v2|[Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling](http://arxiv.org/abs/2505.19669v2)|总结：  <br/>提出SMLLE框架，通过Transducer与Delete <Bos>机制解决流式TTS低延迟问题，实现高质量逐帧语音生成，优于现有流式方法且接近句级系统性能。<br/><br/>贡献点：  <br/>1. **提出SMLLE框架**：首次构建零样本流式TTS系统，无需依赖未来文本即可实现低延迟高质量语音合成。  <br/>2. **融合Transducer与时长对齐**：实时将文本转化为语义标记，结合时长对齐信息提升语音自然度。  <br/>3. **设计Delete <Bos>机制**：优化自回归模型对未来文本的访问，最小化延迟同时保持生成稳定性。  <br/>4. **验证性能优势**：实验表明SMLLE在流式TTS任务中超越现有方法，在句级系统中表现可比，具备实际应用价值。|
|2505.19595v2|[Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment](http://arxiv.org/abs/2505.19595v2)|总结：  <br/>本研究提出A-DMA方法，通过双模态对齐优化扩散模型的训练，显著提升收敛速度与生成质量，降低计算成本，并开源代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **提出A-DMA策略**：设计双模态对齐机制，通过文本与语音模态的协同优化加速扩散模型的训练过程。  <br/>2. **双模态对齐技术**：引入文本引导对齐（融合上下文表示）和语音引导对齐（精炼语义表示）的双重对齐管道。  <br/>3. **减少隐式学习依赖**：通过显式对齐隐藏状态与判别特征，降低扩散模型对复杂中间表示的隐式学习需求。  <br/>4. **实验验证效果**：在多个基准上验证A-DMA的性能优势，收敛速度提升1倍，生成质量优于现有方法，并开源代码与演示样本。|
|2505.17076v2|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v2)|总结：  <br/>研究分析不同帧率对中文和英文语音分词效果的影响，揭示帧率与语言特性间的关系，为语音分词优化提供参考。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统探讨低帧率编解码器对语音分词的影响，揭示帧率与语言模型间的作用机制。  <br/>2. **跨语言对比分析**：通过对比汉语和英语两种音系差异显著的语言，发现帧率对不同语言的分词效果存在异质性。  <br/>3. **揭示关键影响因素**：明确帧率变化与音素密度、语言特异性声学特征的交互作用，为分词器设计提供理论依据。  <br/>4. **应用指导意义**：提出优化帧率选择的策略，对自动语音识别、文本到语音等语音处理任务具有实践价值。|
|2505.11079v2|[ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for   Audio Deepfake Detection](http://arxiv.org/abs/2505.11079v2)|**贡献点总结**：  <br/>1. **首次提出ALLM4ADD框架**：将音频深度伪造检测（ADD）转化为音频问答任务，通过"Is this audio fake or real?"引导模型判断。  <br/>2. **系统性评估ALLMs在ADD中的性能**：通过零样本实验揭示其局限性，为后续研究奠定基础。  <br/>3. **监督微调策略优化**：设计监督训练方法提升模型对音频真实性的评估能力。  <br/>4. **数据稀缺场景下的有效性验证**：在少量数据条件下实现优于传统方法的检测效果，表明ALLM方法的泛化潜力。  <br/>5. **开源代码与实践推动**：提供代码库促进研究社区开发更高效的ADD系统。  <br/><br/>**摘要总结**：  <br/>本研究首次将音频深度伪造检测转化为音频问答任务，提出ALLM4ADD框架，通过零样本评估与监督微调优化，验证了其在数据稀缺场景下优于传统方法的检测性能，为构建更有效的ADD系统提供了新思路。|
|2504.14906v3|[OmniAudio: Generating Spatial Audio from 360-Degree Video](http://arxiv.org/abs/2504.14906v3)|总结：  <br/>提出360V2SA新任务，构建Sphere360数据集，设计半自动化数据流程，提出OmniAudio框架（结合自监督训练与双分支结构），在空间音频生成领域取得SOTA成果。<br/><br/>贡献点：  <br/>1. 提出360V2SA任务：首次系统性生成360度视频对应的3D空间音频（FOA格式），填补传统视频-音频生成在空间线索上的不足。  <br/>2. 构建Sphere360数据集：首个针对该任务的高质量数据集，基于真实场景数据并优化数据配对与清洗流程。  <br/>3. 设计半自动化数据收集管道：提供高效且可扩展的视频-音频数据配对与预处理方案，降低数据获取门槛。  <br/>4. 提出OmniAudio框架：结合自监督预训练和双分支结构（全景+视角视频输入），同时建模局部与全局空间信息。  <br/>5. 实验验证与开源：在Sphere360上实现SOTA性能，代码与数据集开源，推动领域研究与应用。|