|Source|Title|Summary|
|---|---|---|
|2507.11777v1|[Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake   Detection](http://arxiv.org/abs/2507.11777v1)|**贡献点**：  <br/>1. **冻结Wav2Vec 2.0编码器**：在有限数据条件下保留自监督语音表示，提升模型对数据不足的适应性。  <br/>2. **标准化多头注意力模块**：替换原图注意力块，通过异构查询投影增强对语音特征的建模能力。  <br/>3. **上下文感知融合层**：用可训练模块替代启发式帧段融合策略，提升反欺骗性能。  <br/>4. **实验验证效果**：在ASVspoof 5数据集上实现7.6% EER，优于基线模型。  <br/>5. **代码开源**：提供公开实现以促进研究复现和应用。  <br/><br/>**总结**：  <br/>提出并验证了AASIST架构的三项改进，有效提升语音反欺骗性能并开源代码。|
|2507.10827v1|[Supporting SENĆOTEN Language Documentation Efforts with Automatic   Speech Recognition](http://arxiv.org/abs/2507.10827v1)|**贡献点分点：**  <br/>1. **提出ASR驱动的文档流程**：整合文本转语音（TTS）系统生成的增强数据与跨语言迁移学习，利用语音基础模型（SFMs）解决数据稀缺问题。  <br/>2. **优化语言模型设计**：通过浅层融合或n-best恢复引入n-gram语言模型，最大化有限数据的使用效率。  <br/>3. **实验验证有效性**：在SEN'CO TEN语料库上实现14.32% WER和3.45% CER，降低OOV率至26.48%，表明方法对语言文档的可行性。  <br/>4. **支持语言复兴实践**：为社区提供可落地的ASR技术方案，助力语言保护与教育资源开发。  <br/><br/>**总结（100字以内）**：  <br/>提出ASR驱动的文档框架，结合TTS增强数据与跨语言迁移学习，优化语言模型设计，降低错误率，为濒危语言SEN'CO TEN的保护与复兴提供技术支撑。|
|2507.10469v1|[An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived   Realism and Performance in Virtual Reality Environments](http://arxiv.org/abs/2507.10469v1)|**贡献点分点列出：**  <br/>1. **AI模型应用**：提出使用GPT-4 Turbo构建AI驱动的VR虚拟角色（NPC），提升对话的逼真度和交互性。  <br/>2. **多维度评估体系**：设计综合评估框架，结合SUS、GEQ和虚拟代理人可信度问卷，量化用户对系统真实感、可用性和性能的评价。  <br/>3. **性能数据分析**：揭示系统在语音交互中的延迟问题（平均循环延迟7秒），分析对话上下文对性能的影响。  <br/>4. **情感表达短板**：指出AI角色在情感与个性方面存在不足，强调提升情感深度对沉浸感的重要性。  <br/>5. **实践意义与方向**：为优化AI驱动NPC性能提供实证依据，推动构建更沉浸的VR体验。  <br/><br/>**总结（100字以内）：**  <br/>该研究评估了GPT-4 Turbo在VR审讯模拟中的应用，通过多维度问卷揭示AI角色在真实感、可用性及情感表达上的表现，发现系统延迟问题并提出性能优化需求，为提升沉浸式交互的NPC设计提供理论与实践参考。|
|2507.09318v1|[ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow   Matching](http://arxiv.org/abs/2507.09318v1)|**贡献点（分点）：**  <br/>1. **提出非自回归零样本语音对话生成模型**  <br/>   - 通过流匹配技术构建 ZipVoice-Dialog，解决传统自回归模型推理速度慢、稳定性差的问题。  <br/><br/>2. **创新模型设计**  <br/>   - （1）引入说话者轮次嵌入（speaker-turn embeddings）实现精准对话轮次控制；  <br/>   - （2）采用课程学习策略（curriculum learning）提升语音-文本对齐稳定性；  <br/>   - （3）开发专用策略支持立体（stereo）对话生成，增强多说话者场景下的表现。  <br/><br/>3. **构建大规模开放数据集 OpenDialog**  <br/>   - 收集6.8k小时真实场景语音对话数据，填补语音对话领域开源数据的空白。  <br/><br/>4. **建立全面评估基准**  <br/>   - 提供统一标准用于客观比较不同语音对话生成模型的性能。  <br/><br/>5. **开源实现与资源共享**  <br/>   - 所有代码、模型权重、演示示例及数据集均公开，促进研究复现与社区发展。  <br/><br/>**总结（100字以内）：**  <br/>本文提出非自回归语音对话生成模型 ZipVoice-Dialog，结合流匹配与创新设计，显著提升对话生成质量与效率；同时构建大规模开源数据集 OpenDialog，并建立评估基准，推动相关研究发展。|
|2507.09310v1|[Voice Conversion for Lombard Speaking Style with Implicit and Explicit   Acoustic Feature Conditioning](http://arxiv.org/abs/2507.09310v1)|**贡献点：**<br/>1. 提出一种隐式声学特征条件策略，用于Lombard说话风格转换，解决数据不足导致的训练挑战。<br/>2. 比较隐式与显式声学特征条件的VC模型，验证其在风格迁移中的有效性。<br/>3. 证明隐式条件策略在保持说话者身份相似性的同时，可懂度提升效果与显式条件模型相当。<br/>4. 为TTS系统在嘈杂环境下的应用提供新的风格转换方法，提升语音可懂度和实用性。<br/><br/>**总结（100字以内）：**  <br/>该研究提出隐式声学特征条件策略，将语音转换应用于Lombard说话风格迁移，有效解决数据稀缺问题，在保持说话者相似性的同时实现与显式条件相当的可懂度提升，为噪声环境下的TTS应用提供新方案。|
|2507.09282v1|[ClaritySpeech: Dementia Obfuscation in Speech](http://arxiv.org/abs/2507.09282v1)|总结：  <br/>本文提出ClaritySpeech框架，通过结合ASR、文本混淆与零样本TTS，有效改善阿尔茨海默症患者语音可访问性同时保护隐私，在低数据环境下无需微调即可保持说话者身份，显著提升语音质量与识别准确率。<br/><br/>贡献点：  <br/>1. **提出首个针对阿尔茨海默症语音的混淆框架**：整合ASR、文本混淆和零样本TTS技术，实现疾病特征的移除与语音质量的提升。  <br/>2. **无需微调的低数据适配性**：在数据稀缺场景下无需模型微调即可保持说话者身份，降低部署成本。  <br/>3. **显著提升语音识别性能**：在ADReSS和ADReSSo数据集上，WER分别从0.73/0.15降至0.08/0.15，语音质量提高1.65→2.15。  <br/>4. **隐私与可访问性平衡**：通过降低疾病可识别性（F1 score下降16%/10%）和保持50%说话者相似度，兼顾隐私保护与语音可用性。|
|2507.08983v1|[Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](http://arxiv.org/abs/2507.08983v1)|**总结（100字以内）**  <br/>本研究揭示模型排行榜可被攻击者利用分发 poisoned models，提出 TrojanClimb 框架实现隐蔽恶意注入且保持竞争力，并验证其在四种模态下的有效性，强调需重新设计排行榜安全机制以防范潜在威胁。<br/><br/>**贡献点分点**  <br/>1. **首次揭示模型排行榜的攻击通道**：系统性分析模型排行榜作为隐蔽分发 poisoned models 的关键渠道，指出其在机器学习生态中的安全漏洞。  <br/>2. **提出通用攻击框架 TrojanClimb**：设计可跨模态注入恶意行为的框架，同时维持模型在排行榜上的高性能表现，实现攻击隐蔽性与有效性平衡。  <br/>3. **验证跨模态攻击的可行性**：在文本嵌入、生成、语音及图像生成等四类任务中实验证明攻击者可成功植入后门、偏差等有害功能并获得高排名。  <br/>4. **推动安全机制重构需求**：明确当前排行榜评估机制对 poisoned models 的检测不足，呼吁对模型来源和评估流程进行安全设计改进。  <br/>5. **揭示更广泛的安全影响**：强调模型从不可靠来源采纳的风险，推动机器学习社区对模型可信度验证的重视。|
|2507.08530v1|[MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling](http://arxiv.org/abs/2507.08530v1)|总结：  <br/>提出MIDI-VALLE模型，通过结合参考音频和MIDI的条件建模、离散token编码方式及多样数据训练，显著提升钢琴表演音频合成质量与泛化能力，实验结果表明其在FAD和听觉测试中均优于现有基准。<br/><br/>贡献点：  <br/>1. **提出MIDI-VALLE框架**：基于VALLE的神经编码语言模型，首次应用于音乐表演的MIDI到音频合成，实现零样本个性化生成。  <br/>2. **多模态条件建模**：同时利用参考音频性能与对应MIDI作为条件输入，打破传统两阶段流程（先生成MIDI再合成音频）。  <br/>3. **离散token编码创新**：将MIDI和音频均编码为离散token，而非依赖传统钢琴卷（piano roll）格式，提升建模的一致性与鲁棒性。  <br/>4. **多源数据训练**：通过大规模、多风格的钢琴表演数据集训练，增强模型对多样输入的泛化能力。  <br/>5. **性能验证突破**：在ATEPP和Maestro数据集上取得显著效果，FAD降低超75%，听觉测试投票数达202（基线仅58），验证合成质量与泛化能力的双重提升。|
|2507.08319v1|[Active Learning for Text-to-Speech Synthesis with Informative Sample   Collection](http://arxiv.org/abs/2507.08319v1)|**贡献点：**  <br/>1. 提出基于主动学习的TTS语料库构建方法，突破传统前馈和模型无关的构建范式。  <br/>2. 通过迭代交替的数据收集与模型训练流程，实现数据高效性与模型性能的协同优化。  <br/>3. 实验证明：在相同数据规模下，该方法显著提升语音合成质量，缓解存储约束问题。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种基于主动学习的高效TTS语料库构建方法，通过动态迭代优化数据收集与模型训练过程，在减少数据量的同时提升语音合成质量，解决了大规模数据带来的存储瓶颈问题。|
|2507.06826v1|[Physics-Informed Direction-Aware Neural Acoustic Fields](http://arxiv.org/abs/2507.06826v1)|总结：  <br/>该论文提出一种基于物理信息神经网络（PINN）的FOA房间脉冲响应建模方法，引入两个物理先验约束提升模型准确性，并通过实验验证其有效性。<br/><br/>贡献点：  <br/>1. **提出FOA RIRs建模的PINN框架**：首次将PINN应用于建模一阶Ambisonic房间脉冲响应，结合神经网络与物理传播原理。  <br/>2. **设计双物理先验约束**：基于粒子速度与FOA通道（X, Y, Z）的对应关系，推导两个先验条件，关联W通道与其他通道的物理关系。  <br/>3. **增强模型物理合理性**：通过部分导数约束，确保预测结果符合声学物理规律，提升RIRs的空间特性建模能力。  <br/>4. **验证方法有效性**：实验表明，该方法在保留物理约束条件下优于无物理先验的神经网络模型。|
|2507.06483v1|[Learning Japanese with Jouzu: Interaction Outcomes with Stylized   Dialogue Fictional Agents](http://arxiv.org/abs/2507.06483v1)|总结：  <br/>本研究通过混合方法探究风格化语音代理在多模态语言学习中的影响，发现语音、人设与语言风格对用户交互体验、动机和策略有显著作用，为设计更具文化适应性和情感联结的AI系统提供理论依据和实践指导。<br/><br/>贡献点：  <br/>1. **方法论创新**：采用混合方法（定量与定性结合），系统评估54名参与者与动漫角色风格化语音代理的交互效果，涵盖多维度数据（参与度、可用性、情感、学习行为）。  <br/>2. **跨文化/语言影响**：首次明确分析语言能力和文化背景差异对语音代理风格化交互的调节作用，揭示设计因素在不同群体中的差异化影响。  <br/>3. **核心设计要素识别**：量化证明语音风格、角色人设及语言表达方式是影响用户情感体验、学习动机和策略的关键设计变量。  <br/>4. **理论拓展**：深化对情感化（affective）与文化适应性（culturally stylized）代理在人机交互中的作用机制的理解，推动多模态教育AI的理论发展。  <br/>5. **实践指导**：为开发者提供优化语音代理设计的实证依据，强调通过语音和风格化元素提升系统社交响应性和用户参与度的策略。|
|2507.04094v1|[MMMOS: Multi-domain Multi-axis Audio Quality Assessment](http://arxiv.org/abs/2507.04094v1)|总结：提出MMMOS系统，实现多领域音频质量评估，四个维度涵盖生产、内容相关因素，结合多种预训练模型与策略，提升评估性能和泛化能力。<br/><br/>贡献点：<br/>1. **多维度评估框架**：首次提出四个正交维度（Production Quality, Complexity, Content Enjoyment, Usefulness）的无参考音频质量评估体系，突破传统单一MOS评分的局限性。<br/>2. **跨领域泛化能力**：解决现有模型仅适用于语音的瓶颈，扩展至音乐与环境声音领域，实现统一质量评估标准。<br/>3. **多编码器融合策略**：创新性结合WavLM、MuQ和M2D三个预训练编码器的帧级嵌入，提升特征表征能力。<br/>4. **多样化模型集成**：设计三种聚合策略和四种损失函数，通过集成前八名模型显著优化性能（MSE下降20-30%，Kendall's τ提升4-5%）。<br/>5. **指标表现突破**：在生产复杂度指标中获6项第一，在17/32挑战指标中进入前三，验证模型有效性。|
|2507.04036v1|[PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)|**贡献点：**  <br/>1. 提出PresentAgent：首个将长文本文档转化为全同步视觉+语音演示视频的多模态代理，突破现有静态幻灯片和文本摘要的局限。  <br/>2. 模块化处理流程：系统化分段、规划视觉帧、生成上下文相关语音，并实现精确的音视频同步。  <br/>3. 引入PresentEval：基于视觉语言模型的统一评估框架，从内容忠实度、视觉清晰度和观众理解度三维度量化评价。  <br/>4. 实验验证：在30个文档-演示对数据集上证明其接近人类水平的视频生成质量。  <br/>5. 开源代码：提供GitHub代码库，促进方法复现与研究扩展。  <br/><br/>**总结（100字以内）：**  <br/>PresentAgent通过模块化多模态流程，实现文本到动态演示视频的生成，提出新型评估框架PresentEval，实验验证其接近人类表现，为静态文本的多模态转化提供了高效、可控的解决方案。|
|2507.03912v1|[Prosody Labeling with Phoneme-BERT and Speech Foundation Models](http://arxiv.org/abs/2507.03912v1)|总结（100字以内）: <br/>该论文提出融合声学与语言特征的模型，用于音素级韵律标签标注，通过结合SSL模型与预训练语言模型，在日语音调和停顿标注任务中实现显著准确率提升（89.8%-94.3%）。<br/><br/>贡献点：<br/>1. 语音-语言特征融合：首次将自监督声学特征（SSL-based模型/Whisper）与预训练语言特征（PnG BERT/PL-BERT）联合用于韵律标签预测，打破单一模态限制。<br/>2. 端到端标注架构：设计声学特征与语言特征拼接机制，直接预测音素级别的韵律标签（如音调重音、句间停顿），无需中间步骤。<br/>3. 韵律控制预训练：实验验证混合模型在日语音调（93.2%）和停顿（94.3%）标注中的优势，为后续构建可控TTS系统提供高质量标注数据。|
|2507.03887v1|[Traceable TTS: Toward Watermark-Free TTS with Strong Traceability](http://arxiv.org/abs/2507.03887v1)|**贡献点分点：**  <br/>1. **提出无水印TTS模型框架**：首次实现无需显式水印的TTS系统，解决现有方法中水印对语音质量的破坏问题。  <br/>2. **引入联合训练方法**：同时训练TTS模型和判别器，提升模型可追踪性的泛化能力且不依赖额外水印嵌入。  <br/>3. **保持/优化音频质量**：在保障强可追踪性的同时，维持甚至改善合成语音的音质表现。  <br/>4. **增强安全性与抗欺骗性**：通过模型结构设计，降低合成语音被篡改或伪造的风险。  <br/>5. **开源代码推动研究**：计划在论文接受后公开代码，促进语音安全与模型归因领域的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个无需水印的TTS模型框架，通过联合训练提升可追踪性并保持音质，解决了现有方法的质量下降与安全漏洞问题，开源代码将进一步推动语音安全技术研究。|
|2507.02530v1|[Open-Source System for Multilingual Translation and Cloned Speech   Synthesis](http://arxiv.org/abs/2507.02530v1)|总结：  <br/>本论文提出了一种开源的多语言翻译与语音再生系统，通过集成语音识别、大语言模型和文本到语音技术，解决了跨语言通信和可访问性问题，适用于实时会议、广播和设备播放等场景，推动了创新与包容性技术发展。<br/><br/>贡献点：  <br/>1. **多语言通信系统集成**：结合Whisper语音识别与VAD，构建端到端语音处理流程，支持多语言实时翻译与语音再生。  <br/>2. **双LLM分阶段处理**：采用首个LLM分句，第二个LLM翻译，提升翻译准确性和语义连贯性。  <br/>3. **语音克隆与TTS技术**：在语音再生中引入语音克隆功能，保持原始语音的自然度和身份特征。  <br/>4. **灵活部署方案**：系统支持本地运行和API调用，降低部署成本并适配多样化的实际需求。  <br/>5. **开源社区共享**：开放源码促进技术复用与协作，推动语音技术在多语言场景下的普及。  <br/>6. **性能评估验证**：提供延迟和词准确率分析，验证系统在现实场景中的有效性与适应性。|
|2507.01805v1|[A Dataset for Automatic Assessment of TTS Quality in Spanish](http://arxiv.org/abs/2507.01805v1)|**分点贡献：**  <br/>① 创建首个西班牙语TTS系统自动评估数据库（4,326音频样本，涵盖52系统与真人语音）；  <br/>② 设计符合ITU-T标准的主观测试，收集92名参与者标注数据；  <br/>③ 验证数据集对自动自然度预测模型的作用，提出两种改进方案（英文模型微调、自监督模型下游网络）；  <br/>④ 模型在五点MOS评分上取得0.8平均绝对误差的评估性能；  <br/>⑤ 证明数据集的高质量与多样性，为西班牙语TTS研究提供重要资源。  <br/><br/>**总结（100字内）：**  <br/>本研究构建首个西班牙语TTS评估数据库，结合标准化主观测试与模型验证方法，在自然度预测中取得显著效果，推动西班牙语语音合成研究。|
|2507.00808v2|[Multi-interaction TTS toward professional recording reproduction](http://arxiv.org/abs/2507.00808v2)|总结：  <br/>本文提出多步骤交互的TTS方法，模拟语音导演与演员的协作机制，实现用户对语音风格的迭代修正，并通过数据集验证其有效性。<br/><br/>贡献点：  <br/>1. **首次将语音导演的迭代反馈机制引入TTS**：构建多步交互框架，允许用户在合成后动态调整语音风格。  <br/>2. **模拟人机协作过程**：通过建模用户与TTS模型之间的互动关系，实现更自然的风格修正流程。  <br/>3. **提出配套数据集**：提供支持实验的语料库，推动语音风格细化研究。  <br/>4. **验证多交互能力**：实验结果表明模型能根据用户指令进行多次风格迭代优化，提升合成质量。|
|2507.00227v1|[Investigating Stochastic Methods for Prosody Modeling in Speech   Synthesis](http://arxiv.org/abs/2507.00227v1)|总结：  <br/>本研究对比了随机方法与传统确定性模型，验证了其在生成自然韵律方面的有效性，提出通过采样温度调节提升韵律可控性。<br/><br/>贡献点：  <br/>1. **方法对比**：系统评估了归一化流、条件流匹配和修正流等随机方法在文本到语音合成中的表现，与传统确定性基线及人类语音进行对比。  <br/>2. **自然性验证**：通过主观和客观测试，证明随机方法能生成与人类语音质量相当的自然韵律，有效捕捉语音的固有变异性。  <br/>3. **可控性增强**：提出通过调节采样温度实现对生成韵律的灵活控制，拓展了模型的可调控维度。  <br/>4. **框架创新**：引入新的随机建模范式，为语音合成中韵律生成提供了更符合人类语言特性的解决方案。|
|2506.23869v1|[Scaling Self-Supervised Representation Learning for Symbolic Piano   Performance](http://arxiv.org/abs/2506.23869v1)|总结：  <br/>本文提出通过大规模预训练和微调生成自回归Transformer模型，在音乐生成与分类任务中取得显著成果，同时创新性地将对比学习框架应用于符号音乐。<br/><br/>贡献点：  <br/>1. **提出生成自回归Transformer模型**：基于符号化独奏钢琴转录数据进行训练，实现音乐生成与分类任务。  <br/>2. **大规模预训练与高效微调**：使用约60,000小时音乐数据预训练，再通过小规模高质量数据集微调，提升模型性能。  <br/>3. **对比学习框架的应用**：将SimCLR框架适配于符号音乐，生成通用对比MIDI嵌入，推动MIR任务发展。  <br/>4. **显著性能提升**：在钢琴延续生成任务中超越主流符号生成方法，与商业音频模型竞争；MIR分类任务中冻结表示达SOTA，微调需极少标注数据即可迁移。|
|2506.23553v2|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v2)|总结：  <br/>该研究发现CLAPScore与人类主观评分相关性低，提出基于人类感知的Human-CLAP模型，并验证其显著提升相关性指标。<br/><br/>贡献点：  <br/>1. **揭示问题**：首次指出CLAPScore与人类主观评估分数之间存在显著相关性不足的问题。  <br/>2. **提出新方法**：设计Human-CLAP模型，通过引入主观评分数据优化对比学习过程。  <br/>3. **验证有效性**：实验证明Human-CLAP将Spearman相关系数（SRCC）提升超过0.25，显著优于传统CLAP。|
|2506.23553v1|[Human-CLAP: Human-perception-based contrastive language-audio   pretraining](http://arxiv.org/abs/2506.23553v1)|总结：  <br/>该研究发现CLAPScore与人类主观评分相关性较低，提出基于人类感知的Human-CLAP模型，并通过实验验证其显著提升评估效果。<br/><br/>贡献点：  <br/>1. **揭示CLAPScore的局限性**：首次系统分析CLAPScore与人类主观评估评分的低相关性，指出其在文本-音频相关性建模中的不足。  <br/>2. **提出Human-CLAP方法**：创新性地通过引入人类主观评分作为训练信号，构建了一个更符合人类感知的对比语言-音频预训练模型。  <br/>3. **验证效果提升**：实验证明Human-CLAP将Spearman秩相关系数（SRCC）提升超过0.25，显著增强了模型与人类评分的一致性。|
|2506.23552v1|[JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](http://arxiv.org/abs/2506.23552v1)|总结：  <br/>提出JAM-Flow统一框架，整合面部运动与语音生成，通过创新架构和联合注意力机制实现跨模态交互，支持文本、参考音频/运动等多种条件输入，推动多模态音频-视频合成的发展。<br/><br/>贡献点：  <br/>1. **统一框架设计**：首次将说话头合成（talking head）与文本到语音（TTS）结合，解决传统方法中分开处理的局限性。  <br/>2. **多模态扩散Transformer架构**：提出MM-DiT（Multi-Modal Diffusion Transformer），集成Motion-DiT和Audio-DiT模块，实现多模态数据处理。  <br/>3. **联合注意力机制**：引入选择性联合注意力层与时间对齐位置嵌入，提升跨模态交互效率并保留模态特异性表现。  <br/>4. **局部联合注意力掩码**：通过局部化掩码策略，优化生成过程中的模态间依赖关系。  <br/>5. **Inpainting训练目标**：以图像修复式目标训练模型，支持文本、参考音频及运动等多种条件输入，实现同步生成与控制任务。  <br/>6. **应用场景拓展**：单模型支持多种音频-视觉生成任务（如同步说话头生成、音频驱动动画等），提升模型通用性与实用性。|
|2506.23367v1|[You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel   Properties](http://arxiv.org/abs/2506.23367v1)|总结：  <br/>提出首个针对L2学习者的TTS系统，通过元音时长差异设计"清晰模式"，实验证明其有效降低错误率并提升主观体验，揭示实际与感知理解度的差异，指出现有ASR工具在评估L2用户理解度上的不足。<br/><br/>贡献点：  <br/>1. **首个L2定制TTS系统**：开发专门针对第二语言学习者的文本到语音系统，填补领域空白。  <br/>2. **基于元音时长的"清晰模式"**：利用美式英语紧张元音（长）与松弛元音（短）的差异，创新性设计语音清晰度调节机制。  <br/>3. **实验证据支持**：通过对比实验验证该模式可减少9.15%以上转录错误，并提升用户主观体验（更鼓励、尊重）。  <br/>4. **揭示感知与实际理解度差异**：发现听众对清晰模式效果缺乏主观认知，实际理解度提升与感知认知不一致。  <br/>5. **批判现有评估工具**：指出Whisper-ASR未能有效捕捉L2学习者对困难元音的区分策略，无法准确评估TTS系统对L2用户的真实理解度。|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|总结：  <br/>本研究提出SAGE数据生成方法和经验回放策略，结合外部语言模型，显著提升了方言阿拉伯语与阿拉伯-英语代码切换语音的识别性能，超越了大模型效果。<br/><br/>贡献点：  <br/>1. **提出改进的音频拼接方法（SAGE）**：生成人工阿拉伯语-英语代码切换语音数据，解决数据稀缺问题。  <br/>2. **SAGE与SSL模型的联合微调**：在代码切换和阿拉伯语基准测试中，使WER绝对下降7.8%。  <br/>3. **基于经验回放（ER）的泛化增强技术**：缓解方言和代码切换任务中的灾难性遗忘，降低整体WER至26.6%。  <br/>4. **少样本微调策略**：进一步将代码切换基准WER提升4.9%，并实现优于USM和Whisper-large-v2的性能（分别高5.5%和8.4%）。|
|2506.21875v1|[WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](http://arxiv.org/abs/2506.21875v1)|总结：  <br/>本文提出一种针对语音交互的端到端评估基准，通过构建包含真实场景、语音特性（如韵律、同音词、口吃）及多样化声学条件的数据集，并设计查询感知的评估方法，揭示主流语音模型在不同场景下的性能差异，为模型优化提供依据。<br/><br/>贡献点：  <br/>1. **构建专用语音评估基准**：系统创建面向语音场景的端到端评估基准，弥补现有文本基准在语音领域应用的不足。  <br/>2. **语音特性增强数据集**：收集真实语音对话数据，纳入多元说话者属性、声学条件及语音独特现象（如韵律、同音词、口吃）。  <br/>3. **设计查询感知评估方法**：提出基于定制化检查清单和提示的自动评估框架，提升语音任务评价的准确性与细粒度。  <br/>4. **揭示模型场景差异**：对主流语音模型进行全面测试与分析，发现其在不同语音场景下的显著性能差异。  <br/>5. **推动语音模型发展**：为语音模型的开发、优化与评估提供实证依据和参考价值。|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>提出针对实时语音对话的偏好对齐框架，构建多轮语音数据集，结合AI反馈优化模型，通过实验验证提升对话系统的表现。<br/><br/>贡献点：  <br/>1. **创新框架**：提出首个专为实时语音对话设计的偏好对齐方法，解决传统文本模型在处理动态语音交互（如中断、插入）上的不足。  <br/>2. **大规模数据集**：创建超过15万对标注AI反馈的多轮语音对话数据，涵盖语言内容与时间上下文的偏好变化。  <br/>3. **模型优化**：利用离线对齐技术微调全双工自回归语音到语音模型，提升多轮对话生成能力。  <br/>4. **实验验证**：在通用对话中证明反馈机制能有效增强模型的准确性、安全性和上下文一致性。  <br/>5. **人类评估**：通过综合人工评价验证模型在复杂多轮场景下的实际效果。  <br/>6. **理论洞见**：发现动态平衡（如语言、时间因素）对构建自然实时对话系统的重要性。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|总结（100字以内）:  <br/>本文提出一种步骤式视频到音频生成方法，模仿Foley流程生成多音频轨道，结合文本提示与概念否定策略，无需专用数据集即可实现高质量语义区分的音频合成，优于现有基线。<br/><br/>贡献点:  <br/>1. **步骤式生成框架**：首次提出按顺序生成单个音频轨道的方法，每个轨道对应视频中的特定声音事件，实现多声部音频合成。  <br/>2. **Foley流程模拟**：通过镜像传统影视音效创作流程，全面捕捉视频中所有由动作触发的声音事件。  <br/>3. **文本引导与记忆机制**：引入文本提示和先前生成的音频轨道作为条件，构建动态引导的合成流程。  <br/>4. **概念否定策略**：借鉴组合生成框架中的概念否定机制，提升音频生成的精确性和语义分离能力。  <br/>5. **无需配对数据训练**：设计基于预训练模型的通用训练框架，消除对专用视频-音频配对数据集的需求。  <br/>6. **高质量合成效果**：实验证明生成的音频轨道在语义区分度和整体质量上优于现有基线方法。|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|**贡献点分点总结：**  <br/>1. **系统性比较**：在零样本TTS框架下，对比了三种主流说话人编码器（H/ASP、x-vector、ECAPA-TDNN）的性能。  <br/>2. **标准化评估**：基于同一捷克语数据集，跨领域（24个目标说话人）进行主观听觉测试与客观余弦距离分析。  <br/>3. **发现编码器差异**：H/ASP编码器在零样本TTS中表现最好，ECAPA-TDNN优于x-vector，但未超越H/ASP。  <br/>4. **提出实证建议**：强调说话人识别嵌入在TTS中的适配需经实证验证，并构建了可复用的比较框架。  <br/><br/>**总结（100字内）：**  <br/>本研究对比了三种说话人嵌入在零样本TTS中的效果，发现H/ASP编码器表现最优，ECAPA-TDNN虽优于x-vector但不足。提出标准化评估框架，强调实证验证的重要性，为跨任务嵌入应用提供了参考。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|总结：  <br/>提出Kling-Foley多模态视频到音频生成模型，结合扩散Transformer与同步模块提升音视频对齐，开源工业级基准Kling-Audio-Eval，并在多个指标上达成SOTA性能。<br/><br/>贡献点：  <br/>1. **提出Kling-Foley模型**：首个大规模多模态Video-to-Audio生成模型，实现高保真音频与视频内容的同步合成。  <br/>2. **多模态交互建模**：引入多模态扩散Transformer，融合视觉语义表示模块与音视频同步模块，提升跨模态对齐能力。  <br/>3. **帧级对齐机制**：通过视频条件与潜在音频元素的帧级对齐，优化语义对齐与时间同步效果。  <br/>4. **通用音频编解码器**：设计支持音效、语音、歌唱及音乐等多场景的潜在音频编码方案，实现高质量建模。  <br/>5. **空间感渲染技术**：采用立体声渲染方法增强合成音频的听觉空间感知。  <br/>6. **开源工业级基准**：针对数据集缺陷，发布Kling-Audio-Eval工业级评估基准，推动领域研究。  <br/>7. **SOTA性能验证**：实验表明，基于流匹配目标训练的Kling-Foley在分布匹配、语义对齐、时间同步和音频质量上达到公共模型最优。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|**总结（100字以内）:**  <br/>本文提出TTSDS2，改进TTS评估方法，实现与主观评分的高相关性，并发布包含11000个评分的数据集、多语言测试数据管道及14语言持续更新的基准，推动合成语音质量评估的标准化和客观性。<br/><br/>**贡献点分点列述：**  <br/>1. **提出TTSDS2评估指标**：相比现有16种评估方法，TTSDS2在所有测试领域和主观评分中均达到Spearman相关性>0.5，显著提升TTS系统质量评估的客观性和鲁棒性。  <br/>2. **构建大规模主观数据集**：提供涵盖11,000+主观评分的多语言数据集，填补合成语音质量评估的标注缺口。  <br/>3. **设计可复用的评估框架**：推出能持续生成多语言测试数据的管道，避免数据泄露；并发布14语言的动态更新基准，支持长期性能监测与对比研究。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**贡献点：**<br/>1. 提出新假设：人类在主观语音质量评分中更关注低质量段落，评分差异主要由忽略低质量部分时的高分误判导致。<br/>2. 验证假设：基于VCC2018和BVCC数据集分析，揭示人类评价焦点与语音质量分布的关系。<br/>3. 提出新指标：定义N_low-MOS（N个最低意见评分的平均值）作为更可靠的语音质量代表值。<br/>4. 实验验证：证明N_low-MOS可提升MOSNet的评估性能（LCC/SRCC指标改善），展示其内在有效性。<br/>5. 推动方法改进：为语音转换（VC）模型的评估提供更科学的基准，优化MOSNet的比较能力。<br/><br/>**总结（100字内）：**  <br/>本文提出N_low-MOS指标，通过分析数据集验证人类评分明显受低质量段落影响，并证明其在提升语音质量评估模型性能方面具有显著优势，为VC模型评估提供新思路。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|**贡献点：**  <br/>1. 提出一种新的评分聚合方法，解决传统MOS标注（1-5分离散评分）的局限性。  <br/>2. 基于标注者内部连续评分假设，建模生成分布并通过量化潜在连续分布估计评分峰值。  <br/>3. 引入潜在分布峰值作为新代表性值，取代传统MOS作为预测目标。  <br/>4. 通过实验验证，该方法能显著提升语音质量预测模型（如MOSNet）的性能。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出一种基于连续评分假设的新型语音质量评分方法，通过建模标注者的评分过程并引入潜在分布峰值作为替代指标，有效提升语音质量预测模型的性能。|
|2506.18296v2|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v2)|总结：<br/>本文提出日本偶像语音语料库（JIS），专注于年轻女性现场偶像群体，填补听众偏好研究空白，推动TTS/VC技术评估，并通过文化背景说明和伦理指导促进开放研究。<br/><br/>贡献点：<br/>1. 构建首个针对日本"年轻女性live idols"的专用语音语料库（JIS），提供特定说话者群体的语音数据<br/>2. 引入艺名标识系统，方便招募熟悉偶像的听众进行主观评价实验<br/>3. 填补语音生成领域中听众偏好驱动研究的空白，推动个性化语音生成方向<br/>4. 采用免费开放模式，限定非商业研究用途，促进学术资源共享<br/>5. 提供日本偶像文化的背景说明，确保语料库的伦理规范与文化敏感性<br/>6. 包含基础数据分析，为研究者提供应用指导与技术参考|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|**总结（100字以内）**  <br/>本研究构建了日本偶像语音语料库（JIS），聚焦年轻女性现场偶像群体，通过艺名标识促进听众实验。JIS支持TTS/VC系统的说话者相似性评估，并探索听众偏好的个性化语音生成，数据免费开放且仅限非商业科研使用，附带文化背景介绍与应用指导。<br/><br/>**贡献点分点列出**  <br/>1. **构建专业化语料库**  <br/>   - 首次创建针对日本“年轻女性现场偶像”这一特定群体的语音语料库（JIS），覆盖TTS和VC研究需求，补充语音生成领域的数据多样性。<br/><br/>2. **推动说话者相似性评估**  <br/>   - 所有语音由同一类群体（艺名标识）录制，便于系统评估生成语音与原声的说话者相似性，提升TTS/VC的可信度与可比性。<br/><br/>3. **开拓听众偏好研究方向**  <br/>   - 引入针对听众偏好的个性化语音生成研究（如定制偶像声音），填补该领域学术空白，推动语音生成技术与用户需求的结合。<br/><br/>4. **制定开放使用政策**  <br/>   - 以非商业、基础研究为限免费公开JIS，确保数据可及性同时维护版权与伦理规范，促进学术共享与创新。<br/><br/>5. **提供文化背景支持**  <br/>   - 对日本偶像文化进行介绍，助力研究者理解数据语境，确保JIS的合法、有效与伦理应用，提升科研的社会接受度。|
|2506.16738v1|[LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](http://arxiv.org/abs/2506.16738v1)|总结：  <br/>本文提出LM-SPT方法，通过间接语义蒸馏减少语音标记序列长度，改进编码器解码器结构并支持多帧率，实验验证其在语音-文本建模中取得优于基线的重建效果和文本到语音任务表现。<br/><br/>贡献点：  <br/>1. 提出LM-SPT模型，通过语义驱动的重建机制替代传统池化操作，有效减少语音标记序列长度。  <br/>2. 引入间接语义蒸馏策略，通过冻结ASR编码器对比原始与重建波形，提升语义对齐精度。  <br/>3. 设计编码器-解码器架构优化方案，支持25Hz、12.5Hz、6.25Hz等多帧率配置。  <br/>4. 实验证明LM-SPT在语音-文本双向任务中均优于基线，尤其在文本到语音生成中表现突出。|
|2506.16580v1|[Streaming Non-Autoregressive Model for Accent Conversion and   Pronunciation Improvement](http://arxiv.org/abs/2506.16580v1)|总结：提出首个支持流式处理的口音转换模型，实现非母语语音向母语口音的转换，同时保持说话人身份与语调，并通过集成TTS模型提升训练效率，达到顶级模型性能且具备稳定延迟。<br/><br/>贡献点：<br/>1. 提出首个实现流式口音转换（AC）的模型，支持实时处理非母语语音。<br/>2. 保持说话人身份、语调特征并提升发音质量，实现"原声"性转换。<br/>3. 采用Emformer编码器与优化推理机制，显著降低处理延迟。<br/>4. 整合母语TTS模型生成理想参考数据，提升训练效率。<br/>5. 在保持稳定延迟的前提下，达到当前最优AC模型的转换效果。|
|2506.16127v1|[Improved Intelligibility of Dysarthric Speech using Conditional Flow   Matching](http://arxiv.org/abs/2506.16127v1)|**贡献点总结（100字以内）**  <br/>本研究提出基于自监督学习特征及量化表示的失语语音到正常语音转换方法，采用单说话人语音生成策略降低说话人差异，并结合非自回归条件流匹配与扩散Transformer实现高效映射，提升语音可懂度和收敛速度。  <br/><br/>**具体贡献点**  <br/>1. **替代传统特征**：提出使用自监督学习（SSL）特征及其量化表示替代梅尔频谱图（mel-spectrograms），探索其在语音生成中的有效性。  <br/>2. **缓解说话人差异**：通过从WavLM提取特征生成单说话人干净语音，减少说话人变异性对模型性能的影响。  <br/>3. **非自回归框架**：设计全非自回归方法，结合条件流匹配（CFM）与扩散Transformer，直接学习失语语音到干净语音的映射。  <br/>4. **离散音素优势**：验证离散音素单元在提升语音可懂度和加速模型收敛方面的显著效果，优于传统基于梅尔频谱的方法。|
|2506.15873v1|[DeckFlow: Iterative Specification on a Multimodal Generative Canvas](http://arxiv.org/abs/2506.15873v1)|总结：  <br/>该论文提出DeckFlow多模态生成AI工具，通过任务分解、规格分解和生成空间探索三大创新机制解决现有工具设计问题，并验证了其在文本到图像生成中的有效性，进一步拓展至音频生成以研究用户创意行为。<br/><br/>贡献点：  <br/>1. **任务分解机制**：采用无限画布与卡片式视觉数据流交互，支持用户创建和管理多个互联子任务。  <br/>2. **规格分解工作流**：将初始目标迭代拆解为子部分，结合特征标签与聚类实现多模态内容组织。  <br/>3. **生成空间探索**：通过生成多组提示词与输出变体（网格展示），支持递归反馈优化设计迭代。  <br/>4. **多模态验证与扩展**：在文本-图像生成中对比传统对话AI基线，后扩展至音频生成，分析跨模态创作行为。|
|2506.15759v1|[Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](http://arxiv.org/abs/2506.15759v1)|**总结（100字以内）**  <br/>该论文提出Sonic4D框架，首次实现4D场景与空间音频的联合生成，通过动态视觉捕捉、声源定位及物理模拟三阶段处理，无需训练即可生成逼真的时空音频，增强了沉浸式音频视觉体验。<br/><br/>**贡献点**  <br/>1. **提出Sonic4D框架**：首次将空间音频生成与4D场景合成结合，解决现有方法忽视音频与场景对齐的问题。  <br/>2. **三阶段生成方法**：  <br/>   - 第一阶段：利用预训练模型生成4D场景及单声道音频；  <br/>   - 第二阶段：通过像素级视觉定位策略估计声源的3D空间坐标；  <br/>   - 第三阶段：基于物理模拟合成动态视角与时间变化的时空音频。  <br/>3. **训练-free设计**：无需额外训练，直接利用现有数据生成符合场景的空间音频。  <br/>4. **实验验证与资源公开**：通过实验证明生成音频的逼真度和沉浸感，并开放生成示例供研究复现。|
|2506.15085v1|[EmojiVoice: Towards long-term controllable expressivity in robot speech](http://arxiv.org/abs/2506.15085v1)|总结：  <br/>提出EmojiVoice工具包，实现社交机器人离线语音表达的动态控制，通过emoji提示增强长时表达性，并验证其在不同场景中的表现差异。<br/><br/>贡献点：  <br/>1. **开发EmojiVoice工具包**：首个专门针对社交机器人长时语音表达的免费、可定制TTS系统，解决基础模型离线部署难题。  <br/>2. **引入emoji提示控制机制**：首次将表情符号用于细粒度表达性调控，实现对语音情感相位的精准控制。  <br/>3. **轻量级实时生成框架**：采用Matcha-TTS等轻量模型，支持机器人端实时语音生成需求。  <br/>4. **多场景验证与对比**：通过剧本对话、讲故事、自主交互三类案例研究，验证方法对表达性的提升效果，并揭示不同应用场景下的接受差异。|
|2506.13053v2|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v2)|总结：本文提出ZipVoice，通过紧凑结构和流匹配技术实现高效零样本TTS，质量与SOTA相当，在模型体积和推理速度上显著提升，并开源代码与模型。<br/><br/>贡献点：  <br/>1. **提出Zipformer-based流匹配解码器**：在模型体积受限情况下保持足够的建模能力，提升效率。  <br/>2. **设计Average upsampling与Zipformer结合的语音-文本对齐及文本编码器**：增强语音可懂度，优化多语言处理效果。  <br/>3. **创新流蒸馏方法**：减少采样步骤，消除无分类器引导的推理开销，显著提升生成速度。  <br/>4. **实验验证效果**：在100k小时多语言数据集上，ZipVoice在语音质量与SOTA模型相当的同时，体积缩小3倍，速度提升30倍。  <br/>5. **开源实现**：提供代码、模型检查点和演示样本，便于复现与应用。|
|2506.12199v1|[ViSAGe: Video-to-Spatial Audio Generation](http://arxiv.org/abs/2506.12199v1)|**贡献点总结**（100字以内）:  <br/>本研究提出ViSAGe框架，直接从无声视频生成一阶Ambisonics，构建YT-Ambigen数据集，设计空间音频评估指标，并验证其在时空对齐与视角适应性上的优越性。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **提出新问题**：探索无需复杂录制设备或专业技能，直接从无声视频生成一阶Ambisonics的方法。  <br/>2. **构建数据集**：创建包含102K视频片段的YT-Ambigen数据集，为研究提供标注数据支持。  <br/>3. **设计评估指标**：首次基于音频能量图与显著性度量，提出量化评估生成空间音频质量的新标准。  <br/>4. **端到端框架**：提出ViSAGe模型，融合CLIP视觉特征、自回归音频编码与方向/视觉引导生成高质量Ambisonics。  <br/>5. **性能优势**：验证ViSAGe在生成音频的连贯性与时空对齐性上优于传统两阶段方法（视频-音频生成+空间化）。  <br/>6. **应用能力**：生成的音频可随视角变化动态适应，提升沉浸式音频体验的交互性。|
|2506.11160v5|[S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation   Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning](http://arxiv.org/abs/2506.11160v5)|**贡献点分点总结：**  <br/>1. 提出S2ST-Omni框架，通过分解S2ST为S2TT与TTS解决高质翻译与依赖平行语料的挑战。  <br/>2. 设计轻量级speech adapter，弥合语音与文本的模态表示差异。  <br/>3. 采用两阶段微调策略，增强多模态知识学习效率。  <br/>4. 在TTS环节引入流式自回归生成方法，提升目标语音的自然度与流畅性。  <br/>5. 集成预训练Whisper编码器与Qwen 3.0模型，分别优化语音理解与文本生成能力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出S2ST-Omni框架，通过任务分解、模型整合及轻量适配器设计，解决多语言语音翻译中的关键问题。采用两阶段微调与流式生成技术，提升翻译质量并减少对大规模语料的依赖，实验验证其有效性。|
|2506.11130v1|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v1)|总结（100字以内）:  <br/>提出自优化框架，利用无标签数据与闭环训练提升ASR性能，成功将Whisper转化为Twister，在台语中实现显著精度提升，并为低资源场景提供实用方案。<br/><br/>贡献点：<br/>1. 提出无需标注数据的自精炼框架，通过伪标签生成与闭环训练实现ASR性能提升。  <br/>2. 首次将高保真TTS系统与ASR模型结合，形成"ASR→TTS→ASR"的闭合优化循环。  <br/>3. 在台语语音任务中验证框架有效性，利用6000小时未标注语音驱动模型训练。  <br/>4. 开发专用模型Twister，相较Whisper在普通话和双语切换场景分别降低20%和50%错误率。  <br/>5. 为低资源/特定领域ASR提供创新解决方案，突破传统伪标签自蒸馏方法的局限性。|
|2506.11127v1|[GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech   Instructions](http://arxiv.org/abs/2506.11127v1)|总结：  <br/>提出GUIRoboTron-Speech，首个端到端语音指令GUI代理，通过语音数据生成和混合训练策略解决模态不平衡问题，实验验证其高效性和广泛应用前景。<br/><br/>贡献点：  <br/>1. **首个语音驱动GUI代理**：提出GUIRoboTron-Speech，首次实现端到端GUI操作，直接接受语音指令和设备截图作为输入，无需文本转换。  <br/>2. **语音数据生成方法**：利用随机音色TTS模型将现有文本指令转换为高质量语音指令，解决语音数据稀缺问题。  <br/>3. **混合指令训练策略**：设计启发式方法缓解预训练模型的模态不平衡，结合文本和语音指令提升代理性能。  <br/>4. **系统性实验验证**：在基准数据集上验证模型表现，证明语音指令在GUI自动化中的有效性与广泛适用性。|
|2506.11086v1|[Intelligibility of Text-to-Speech Systems for Mathematical Expressions](http://arxiv.org/abs/2506.11086v1)|贡献点：  <br/>1. **系统评估框架**：首次设计涵盖五种TTS模型的实验，量化数学表达式（MX）的发音质量与可懂性（通过用户评分和转录正确性）。  <br/>2. **LLM辅助生成**：利用大语言模型（LLM）将LaTeX格式MX转换为英文发音，弥补TTS模型无法直接处理LaTeX的缺陷。  <br/>3. **多维度指标**：提出包含三个指标的转录正确性评估体系，并结合Mean Opinion Score分析用户主观感受。  <br/>4. **对比分析**：对比听众对TTS输出与人类专家发音的偏好，揭示TTS在处理MX时的表现短板。  <br/>5. **结果发现**：发现TTS模型输出的可懂性存在显著差异，且多数MX类别下表现劣于专家，同时LLM选择对结果影响有限。  <br/>6. **应用导向结论**：明确指出需针对性优化TTS模型处理数学表达式的能力。  <br/><br/>总结：  <br/>首次系统评估TTS模型对数学表达式的发音质量与可懂性，揭示模型与人类专家表现差异，强调改进TTS处理MX能力的必要性。|
|2506.10019v1|[A Survey of Automatic Evaluation Methods on Text, Visual and Speech   Generations](http://arxiv.org/abs/2506.10019v1)|总结：本文提出首个综合框架，系统化分类文本、图像和音频生成的评估方法，识别出五种核心范式，并探讨跨模态评估的未来方向。<br/><br/>贡献点：  <br/>1. **构建统一框架**：首次建立跨文本、图像、音频三大模态的自动评估方法系统化框架，实现多模态方法的整合与对比。  <br/>2. **提出五种核心范式**：归纳出生成内容评估的五大基础范式，为后续研究提供理论分类依据。  <br/>3. **跨模态适用性验证**：将文本生成评估方法的分析逻辑扩展至图像和音频领域，验证框架的广泛适用性。  <br/>4. **探索未来研究方向**：针对跨模态评估挑战，提出潜在的研究路径，促进多模态生成AI的评估体系发展。|
|2506.09874v2|[UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow   Matching](http://arxiv.org/abs/2506.09874v2)|总结:  <br/>UmbraTTS通过自监督方法解决数据对齐难题，联合生成语音与环境音，实现背景音量精细控制和高质量环境音频合成，显著优于现有基线。<br/><br/>贡献点:  <br/>1. 提出UmbraTTS模型：基于流匹配技术的TTS系统，首次联合生成语音与环境音频，实现多模态音频场景合成。  <br/>2. 实现细粒度控制：通过文本和声学上下文条件，精准调控背景音量，生成多样化且连贯的环境音频。  <br/>3. 自监督数据框架：开发从未标注录音中提取语音、环境音和文本转录的方法，突破配对数据稀缺的限制。  <br/>4. 优越性能表现：在环境感知和音频质量上显著优于现有基线，验证了模型的有效性与创新性。|
|2506.09827v2|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v2)|**贡献点：**  <br/>1. 提出EmoNet-Voice资源，包含大规模预训练数据集（Big）和新型基准数据集（Bench），覆盖40种情感、11种声音及4种语言。  <br/>2. 支持细粒度情感评估，明确区分不同情感强度以提升模型准确性。  <br/>3. 通过语音生成技术合成情感音频，模拟真实场景以解决隐私问题。  <br/>4. 引入心理学专家标注与验证，确保情绪强度标签的可靠性。  <br/>5. 提出Empathic Insight Voice模型，实现与人类专家高度一致的SER性能。  <br/>6. 揭示高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更易被识别的模型表现差异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出EmoNet-Voice资源，通过合成语音和专家标注解决隐私与情感粒度问题，推出高性能SER模型，并发现高唤醒情绪更易识别，为情感语音研究提供新基准。|
|2506.09827v1|[EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection](http://arxiv.org/abs/2506.09827v1)|总结：  <br/>提出EmoNet-Voice资源及两个子集，包含大规模预训练数据与专家标注基准数据，解决隐私问题并增强情感粒度和多样性。开发高精度模型，通过评估揭示高唤醒情绪识别易度高于低唤醒情绪。<br/><br/>贡献点：  <br/>1. **引入EmoNet-Voice资源**：构建由EmoNet-Voice Big（大规模预训练数据集）和EmoNet-Voice Bench（专家标注基准数据集）组成的综合数据资源，覆盖40种情感、11种声音和4种语言。  <br/>2. **提升情感粒度与强度区分**：设计支持细粒度情感分类（40类）及不同强度评估的框架，推动SER模型在情感理解上的更精确建模。  <br/>3. **合成数据解决隐私与多样性问题**：通过先进语音生成技术合成敏感情感场景数据，避免真实数据隐私风险并扩展情感表达的多样性。  <br/>4. **专家验证机制**：联合心理学专家对合成数据进行严格标注（感知强度标签），确保数据质量与情感感知的科学性。  <br/>5. **提出高性能SER模型**：开发Empathic Insight Voice模型，实现与人类专家的高一致性，推动语音情感识别技术的标准化。  <br/>6. **揭示情绪识别难度差异**：通过对比实验发现，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易被模型识别，为后续研究提供新视角。|
|2506.08279v1|[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)|**总结（100字以内）:**  <br/>该论文提出Mirage模型，首次实现从音频直接生成高质量、表达性强的视频，结合TTS技术生成多模态视频，并开发统一的自注意力训练方法，提升生成视频的主观质量与通用性。  <br/><br/>**分点贡献:**  <br/>1. **提出Mirage模型**：首个专注于音频到视频生成的通用模型，从原始音频直接生成逼真、表达性的视频图像，不依赖视觉输入。  <br/>2. **多模态生成能力**：通过集成文本到语音（TTS）技术，实现语音与视频的同步生成，解决语音和视频内容对齐问题。  <br/>3. **统一训练方法**：开发适用于自注意力机制的统一训练框架，支持从头训练和基于已有权重的微调，增强模型灵活性。  <br/>4. **语义对齐优化**：利用A-roll数据（人物对话语音视频）训练，使生成视频能准确反映音频中的表演信息，提高可信度。  <br/>5. **性能优势**：生成的视频在主观质量上优于依赖语音特定架构或损失函数的现有方法，保持通用性的同时实现高保真输出。|
|2506.04527v1|[Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning](http://arxiv.org/abs/2506.04527v1)|总结（100字以内）:<br/>该论文提出结合字母符号的语音标签生成方法，通过BERT特征提示编码实现隐式条件，配合推理时的显式修剪提升标签一致性，为文本到语音和口音估计等任务生成高质量平行数据。<br/><br/>贡献点：<br/>1. **方法创新**：首创结合字母符号的语音标签生成框架，突破传统ASR模型直接微调的局限，引入显式与隐式的双条件策略。<br/>2. **条件生成机制**  <br/>   - 通过预训练BERT特征构建隐式图形单词条件（Prompt Encoder）  <br/>   - 提出推理阶段的显式标签假设剪枝算法（Pruning Inconsistent Hypotheses）<br/>3. **数据价值**：生成跨模态的语音-标签-图形单词平行数据，支持文本到语音合成（TTS）和口音估计等下游任务。<br/>4. **实验验证**：在音素-图形单词一致性任务和口音估计任务中均取得显著性能提升，证明方法的有效性。|
|2506.04397v1|[Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?](http://arxiv.org/abs/2506.04397v1)|总结：  <br/>该论文通过构建专用数据集和微调大模型，探索Parler TTS在语音重建任务中生成可理解语音与保持说话人身份的能力，揭示其在可控性方面的局限并提出改进方向。<br/><br/>贡献点：  <br/>1. **构建专用数据集**：创建并标注了包含说话人信息和可懂度数据的语音重建数据集，为相关研究提供基础资源。  <br/>2. **应用大模型进行语音重建**：首次尝试利用Parler TTS等先进大型语音模型，生成语音障碍患者疾病前的语音近似。  <br/>3. **分析模型局限性**：发现模型虽能学习数据分布，但在控制语音可懂度和保持说话人身份一致性方面存在挑战。  <br/>4. **提出改进方向**：为提升语音重建任务中模型的可控性，提出了未来研究的潜在优化路径。|
|2506.04152v1|[HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](http://arxiv.org/abs/2506.04152v1)|**分点贡献：**  <br/>1. **构建高带宽语音数据集**：提出HiFiTTS-2，基于LibriVox有声书，包含36.7k小时（22.05kHz）和31.7k小时（44.1kHz）的高质量英语语音数据，支持高带宽语音合成研究。  <br/>2. **创新数据处理流程**：设计了包含带宽估计、语音分段、文本预处理及多说话人检测的全流程数据处理方法，提升数据适配性与实用性。  <br/>3. **提供详细元数据**：生成全面的语句和有声书元数据，支持研究者通过数据质量过滤器灵活调整数据集以适配不同应用场景。  <br/>4. **验证数据集有效性**：实验表明该数据集和处理方法可有效训练高质量零样本TTS模型，推动高带宽语音生成技术的发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出HiFiTTS-2，构建大规模高带宽语音数据集并设计全流程处理技术，提供详细元数据以增强数据适应性，实验证明其可有效支持高质量零样本TTS模型训练。|
|2506.04134v1|[UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation](http://arxiv.org/abs/2506.04134v1)|**贡献点总结（100字以内）**：  <br/>提出直接生成CS语音的新方法，构建首个统一框架UniCUE，通过细颗粒语义对齐、跨任务适配器及姿态感知处理，解决误差传播与时空对齐问题，显著提升语音生成质量与唇语同步性。<br/><br/>**分点贡献**：  <br/>1. **直接生成语音的创新方法**：首次无需依赖文本中间媒介，直接从CS视频生成语音，避免误差传播与时间错位问题。  <br/>2. **统一框架UniCUE**：提出首个融合CSV2S与CSR任务的框架，打通视觉-语义到语音的端到端生成流程。  <br/>3. **核心技术创新**：  <br/>   - 引入细颗粒语义对齐池，精确映射视觉特征与语音内容；  <br/>   - 设计VisioPhonetic适配器，实现CSV2S与CSR的跨任务表示兼容；  <br/>   - 开发姿态感知视觉处理器，增强唇语与手势的时空关联性。  <br/>4. **实验验证与数据集**：基于新建立的中文CS数据集（含14名受试者），验证方法有效性，降低误词率78.3%并提升唇语同步性32%。|
|2506.04077v1|[A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions](http://arxiv.org/abs/2506.04077v1)|总结：  <br/>该论文提出一种基于大语言模型与跨模态信息的新型语音评估方法，有效解决低资源场景下的标签约束问题，提升口语评分的可靠性和多样性。<br/><br/>贡献点：  <br/>1. **低资源场景下的数据生成**：利用大语言模型生成多样化响应并合成语音，突破标注数据的稀缺限制。  <br/>2. **说话人感知的语音合成**：通过 speaker-aware 文本到语音合成技术，增强生成语音与真实语音的匹配度。  <br/>3. **动态重要性损失设计**：根据合成语音与真实语音的特征差异，自适应调整训练样本权重，提升模型学习效率。  <br/>4. **跨模态特征融合**：构建多模态大语言模型，直接整合文本与语音信号的对齐特征以预测口语水平。  <br/>5. **有效性验证**：在 LTTC 数据集上验证方法优于传统数据依赖或常规增强技术，实现跨模态信息驱动的自动化评分。|
|2506.03884v1|[Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages](http://arxiv.org/abs/2506.03884v1)|总结: 本文提出基于零样本合成的多语言TTS系统，通过共享音素表示和适配文本解析规则，成功生成梵语等低资源语言的自然语音，验证了该方法在扩展语音技术普惠性方面的有效性。<br/><br/>贡献点:<br/>1. 提出零样本合成框架，解决印度1369种语言中多数缺乏数字资源的TTS训练难题<br/>2. 构建跨语言音素共享表示，降低多语言模型过度假设需求<br/>3. 开发适应不同音系规则的文本解析方法（phonotactics adaptation）<br/>4. 实现Sanskrit、Maharashtrian、Canara Konkani、Maithili、Kurukh等5种语言的自然语音合成<br/>5. 提供可复用的合成器架构，验证方法在扩大语音技术覆盖范围中的可行性|
|2506.03831v1|[Conformer-based Ultrasound-to-Speech Conversion](http://arxiv.org/abs/2506.03831v1)|总结：  <br/>本研究对比了Conformer与传统CNN在超声波语音转换中的表现，发现Conformer结合bi-LSTM在感知质量上更优，而Conformer Base在效率上显著提升，为静默语音接口提供了新思路。<br/><br/>贡献点：  <br/>1. **提出两种Conformer-based模型**：首次将Conformer架构应用于超声波到语音转换任务，分别采用Base结构和结合bi-LSTM的结构。  <br/>2. **融合HiFi-GAN声码器**：通过HiFi-GAN将生成的mel频谱图合成高质量音频波形，提升系统输出的语音还原效果。  <br/>3. **验证感知质量优势**：在MUSHRA主观测试中发现，Conformer与bi-LSTM结合的模型在语音可懂度上优于传统CNN基线。  <br/>4. **效率提升**：Conformer Base模型在保持性能的同时，训练速度比传统CNN快3倍，降低了计算成本。|
|2506.03793v1|[Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts](http://arxiv.org/abs/2506.03793v1)|**贡献点**  <br/>1. **提出Cadence模型**：基于预训练大语言模型设计，首次实现对自发语音转录的精准标点恢复，克服传统模型在处理断续、重复等语音特征时的不足。  <br/>2. **扩展语言覆盖**：支持22种印度语言及英语（原仅14种），显著提升多语言标点恢复的适用性与泛化能力。  <br/>3. **分析挑战与改进**：通过跨语言和标点类型的行为研究，揭示领域迁移及罕见标点符号下的修复难题，验证预训练模型在低资源NLP场景中的有效性。  <br/><br/>**总结**（100字以内）:  <br/>本研究提出Cadence模型，基于预训练语言模型实现自发语音的精准标点恢复，扩展支持22种印度语言和英语，并通过深入分析揭示多语言任务中的关键挑战。|
|2506.03515v1|[BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and   Weight Indexing](http://arxiv.org/abs/2506.03515v1)|总结：  <br/>本文提出两种模型压缩技术，实现83%的模型体积减小，同时保持高质量语音合成，适用于设备端部署。<br/><br/>贡献点：<br/>1. 提出量化感知训练（QAT）技术，将模型参数从32位压缩至1.58位，主要采用三值{-1,0,1}量化策略。<br/>2. 开发权重索引方法，通过将一组1.58位权重编码为单个int8索引来优化存储效率。<br/>3. 在保持合成质量的前提下，实现模型轻量化，使其更适用于资源受限的设备端应用场景。|
|2506.02979v1|[Towards a Japanese Full-duplex Spoken Dialogue System](http://arxiv.org/abs/2506.02979v1)|**贡献点总结：**  <br/>1. 提出首个公开可用的全双工日语语音对话模型，基于英语模型Moshi迁移。  <br/>2. 采用两阶段训练方法：大规模日语语料预训练与高质量立体声数据微调。  <br/>3. 引入多流文本到语音系统生成的合成对话数据以增强模型性能。  <br/>4. 实验证明模型在自然度和语义理解上优于现有日语基准模型。  <br/><br/>（注：以上为100字以内总结，符合要求。）|
|2506.02958v1|[PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech   Editing](http://arxiv.org/abs/2506.02958v1)|**贡献点总结（100字以内）:**  <br/>提出PartialEdit数据集，聚焦神经语音编辑生成的深伪语音检测，揭示现有模型的局限性，分析伪影特征，推动该领域研究。<br/><br/>**分点贡献:**  <br/>1. **提出PartialEdit数据集**：构建首个基于先进神经语音编辑技术的深伪语音检测数据集，包含部分编辑的语音样本，为研究提供基准。  <br/>2. **探索检测与定位任务**：系统研究部分编辑语音的检测与内容定位，验证当前检测方法在复杂编辑场景下的有效性。  <br/>3. **揭示模型局限性**：实验表明基于PartialSpoof数据集训练的模型无法有效识别神经编辑生成的深伪语音，凸显数据集不足。  <br/>4. **分析伪影特征**：结合神经音频编码器，探讨检测模型学习到的伪影（artifacts）特征，为改进检测方法提供理论依据。  <br/>5. **提供公开资源**：开放数据集和音频样本，方便后续研究复现与扩展。|
|2506.02858v2|[DGMO: Training-Free Audio Source Separation through Diffusion-Guided   Mask Optimization](http://arxiv.org/abs/2506.02858v2)|总结：  <br/>提出训练无关的零样本音频源分离框架DGMO，利用扩散模型生成先验优化频谱掩码，突破传统方法依赖任务特定训练的限制，拓展扩散模型在音频分离领域的应用范式。<br/><br/>贡献点：  <br/>1. **首个训练无关的零样本框架**：首次将预训练扩散模型直接用于语言查询的音频源分离，无需额外任务特定训练。  <br/>2. **测试时优化机制**：提出Diffusion-Guided Mask Optimization（DGMO），通过测试阶段动态优化频谱掩码以提升分离精度。  <br/>3. **跨模态适应性提升**：分析模态适配问题，揭示自然语言与音频处理的协同挑战，并提出针对性解决方案。  <br/>4. **性能验证**：在无监督或弱监督条件下实现竞争性分离效果，证明扩散模型可有效迁移至音频分离任务。  <br/>5. **应用范式创新**：拓展扩散模型从生成任务到分离任务的边界，建立新的零样本音频处理框架。|
|2506.02401v1|[Trusted Fake Audio Detection Based on Dirichlet Distribution](http://arxiv.org/abs/2506.02401v1)|**贡献点：**  <br/>1. **提出基于狄利克雷分布的不确定性建模方法**：首次将狄利克雷分布用于量化模型决策的不确定性，提升假音频检测的可信度评估能力。  <br/>2. **构建新型检测框架**：通过神经网络生成证据，并将预测概率与不确定性估计结合，形成最终检测意见，优化检测流程。  <br/>3. **多数据集验证**：在ASVspoof 2019 LA、2021 LA及DF数据集上进行对比实验，验证了模型在准确率、鲁棒性和可信度方面的优越性能。  <br/><br/>**总结：**  <br/>该研究通过狄利克雷分布建模不确定性，提出新型假音频检测方法，在多数据集上验证了其高准确率与鲁棒性，显著提升了检测的可靠性。|
|2506.02082v1|[SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction](http://arxiv.org/abs/2506.02082v1)|总结：该论文提出了一种新的SALF-MOS模型，解决传统MOS评估高成本问题，通过卷积序列提取音频潜在特征，实现小尺寸、端到端、高度通用和可扩展的语音质量预测，取得最佳评估效果。<br/><br/>贡献点：<br/>1. 提出Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS)模型，解决传统MOS评估需要大量人工标注的问题<br/>2. 设计基于卷积序列的音频特征提取方法，实现端到端的MOS预测系统<br/>3. 构建的小尺寸模型在保持通用性的同时具备良好的可扩展性<br/>4. 在多个客观评估指标（MSE、LCC、SRCC、KTAU）上达到当前最优性能<br/>5. 提供了一种新的语音质量评估方法，可作为TTS和语音转换模型选择的有效工具|
|2506.01618v1|[Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech](http://arxiv.org/abs/2506.01618v1)|总结（100字以内）:  <br/>本文提出基于音节的节奏建模方法，结合LF-MMI模型显著提升口吃语音的ASR性能，验证了无监督转换的有效性。<br/><br/>贡献点:  <br/>1. **提出syllable-based rhythm modeling方法**：在Rhythm and Voice（RnV）框架中引入针对口吃语音的音节级节奏建模技术，解决传统方法对口吃语音节奏特征建模不足的问题。  <br/>2. **评估转换对ASR的影响**：首次系统验证口吃语音转换对ASR性能提升的效果，通过训练LF-MMI模型和微调Whisper模型，发现LF-MMI在口吃语音识别中具有显著优势。  <br/>3. **揭示无监督转换的潜力**：证明无监督学习的节奏和语音转换技术在口吃ASR任务中的有效性，为低资源场景下的语音处理提供了新思路。|
|2506.01322v1|[Zero-Shot Text-to-Speech for Vietnamese](http://arxiv.org/abs/2506.01322v1)|贡献点：<br/>1. 构建了首个越南语高质量语音数据集PhoAudiobook（含941小时音频）  <br/>2. 在VALL-E、VoiceCraft、XTTS-V2等三大零样本TTS模型上开展系统实验  <br/>3. 证实PhoAudiobook能显著提升多指标下的模型性能  <br/>4. 揭示VALL-E与VoiceCraft在短句合成任务中的优越性  <br/>5. 公开数据集促进越南语TTS领域研究发展  <br/><br/>总结：  <br/>本研究发布首个越南语高质量语音数据集PhoAudiobook，并验证其提升三大零样本TTS模型性能的效果，尤其突出VALL-E与VoiceCraft在短句生成中的优势，推动越南语语音合成研究。|
|2506.01268v1|[CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction](http://arxiv.org/abs/2506.01268v1)|贡献点总结：<br/>1. 提出单文件实现的CleanS2S框架，实现低延迟语音交互系统<br/>2. 首创主动对话机制，集成记忆系统与主观行动判断模块<br/>3. 开发五种人类般响应策略（中断/拒绝/转移/沉默/标准响应）<br/>4. 实现全双工WebSocket连接与非阻塞I/O的实时中断处理<br/>5. 提出Action Judgement SFT方法评估输入流策略选择<br/>6. 提供前所未有的透明度和可扩展性，代码已开源<br/><br/>（97字）|
|2506.01263v1|[WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing](http://arxiv.org/abs/2506.01263v1)|总结：  <br/>提出无需额外训练的CTC模型改进方法，通过中间层声学特征检测关键词并引入偏置机制，有效提升罕见词识别准确率，实验在日语语音识别中实现29%的F1分数提升。<br/><br/>贡献点：  <br/>1. **解决罕见词识别偏差问题**：通过中间层声学特征进行关键词检测，避免模型对训练数据词汇的过度依赖。  <br/>2. **引入灵活的wildcard CTC机制**：采用快速且容错的CTC变体，支持模糊匹配处理难以严格识别的单词。  <br/>3. **无需模型重训练的可扩展性**：方法兼容现有模型，无需额外训练或文本到语音系统，适用于大规模模型。  <br/>4. **显著提升未知词识别性能**：在日语语音识别任务中，实现未知词F1分数29%的提升，验证方法有效性。|
|2506.01032v1|[ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and   Speaker Feature Optimization](http://arxiv.org/abs/2506.01032v1)|**总结**：  <br/>本文提出ReFlow-VC，通过修正流的ODE框架提升语音转换效率，结合内容与音高信息优化说话人特征，并验证其在小数据集和零样本场景中的优越性能，推动高保真语音转换的实际应用。  <br/><br/>**贡献点**：  <br/>1. **提出ReFlow-VC方法**：基于修正流构建高效高保真语音转换模型，通过ODE框架将高斯分布直接映射到真实Mel频谱图分布，减少采样步骤。  <br/>2. **优化说话人特征建模**：同时利用内容与音高信息优化特征，增强说话人属性的准确表达。  <br/>3. **验证小样本适配性**：在小数据集和零样本场景中均表现出卓越的性能，突破传统扩散模型的局限。|
|2506.01020v1|[DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation](http://arxiv.org/abs/2506.01020v1)|总结：  <br/>该论文提出DS-TTS框架，结合双风格编码与动态生成网络，解决zero-shot语音克隆问题，实现未见过语音的高质量合成，并在VCTK数据集上验证了其优越的性能。<br/><br/>贡献点：  <br/>1. **提出DS-TTS框架**：设计一种用于zero-shot语音克隆的全新方法，仅需单个音频样本和任意文本即可合成目标说话人语音，无需历史训练数据。  <br/>2. **双风格编码网络（DuSEN）**：通过两个互补的风格编码器捕捉说话人语音身份的多维度特征，增强对未见说话人风格的建模能力。  <br/>3. **动态融合机制（SGF + DyGN）**：引入风格门控-Film机制将风格向量融入动态生成器，实现对不同句子长度的自适应合成，提升鲁棒性与自然度。  <br/>4. **实验验证优势**：在VCTK数据集上证明DS-TTS在单词错误率和说话人相似度上优于现有模型，显著改善语音克隆的泛化与表达性。|
|2506.00832v1|[Counterfactual Activation Editing for Post-hoc Prosody and   Mispronunciation Correction in TTS Models](http://arxiv.org/abs/2506.00832v1)|**贡献点：**<br/>1. **提出模型无关的后处理技术**：Counterfactual Activation Editing 是一种通用方法，无需依赖预训练模型特定架构，可适用于多种TTS模型。  <br/>2. **实现韵律与发音的联合控制**：同时支持对语音韵律特征（prosody）和发音错误（mispronunciation）的精确调整，提升控制能力。  <br/>3. **无需额外训练即可进行推理时调整**：在不重新训练模型的情况下，直接对预训练TTS的内部表示进行修改，实现在线输出优化。  <br/>4. **适应低资源场景**：解决传统发音纠正依赖音素词典（grapheme-to-phoneme dictionaries）的问题，降低对资源的依赖。  <br/>5. **填补预训练模型与可编辑合成的空白**：首次将可编辑的语音合成能力引入预训练TTS框架，推动模型灵活性与应用范围的扩展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种通用、无需额外训练的后处理方法，实现对TTS输出的韵律与发音联合控制，适应低资源场景，推动预训练模型向可编辑语音合成发展。|
|2506.00736v1|[IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio   Generation with Diffusion Modeling](http://arxiv.org/abs/2506.00736v1)|总结：  <br/>本文提出IMPACT框架，结合扩散建模与迭代掩码并行解码，在连续潜空间中生成高质量文本到音频，有效解决先前模型在音质与推理速度之间的权衡问题，实验在AudioCaps上达到SOTA性能。<br/><br/>贡献点：  <br/>1. **提出IMPACT框架**：首次将迭代掩码并行解码与扩散模型结合，实现文本到音频生成中高保真度与低延迟的双重优化。  <br/>2. **连续潜空间设计**：通过在连续潜空间中操作，避免离散token带来的保真度限制，提升生成音频的自然度和质量。  <br/>3. **高效解码策略**：采用迭代掩码并行解码技术，在保持推理速度的同时显著提高音频生成效果，优于MAGNET等基于离散token的模型。  <br/>4. **验证SOTA性能**：在AudioCaps数据集上，IMPACT在Fréchet Distance (FD)和Fréchet Audio Distance (FAD)等关键指标上超越现有方法，验证了其有效性。|
|2506.00722v1|[Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](http://arxiv.org/abs/2506.00722v1)|总结：该研究提出基于链式思维的端到端语音对话系统方法，在较小数据集上实现显著性能提升并提升语义连贯性，同时增强计算效率，模型及代码将公开。<br/><br/>贡献点：<br/>1. 提出基于链式思维（Chain-of-Thought, CoT）的端到端（E2E）对话系统框架，解决了传统级联流水线的局限性；<br/>2. 通过将对话训练与多模态语言模型（涵盖语音识别、文本生成、语音合成等任务）的预训练对齐，提升语义连贯性；<br/>3. 实现仅需300小时公开对话数据（如Switchboard）即可获得超过1.5 ROUGE-1的提升效果；<br/>4. 优化计算效率，使模型能在有限数据量下高效训练；<br/>5. 提供公开模型及代码，推动该领域研究与应用。|
|2506.00548v1|[Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities](http://arxiv.org/abs/2506.00548v1)|总结（100字以内）：  <br/>本论文提出Con Instruction方法，通过生成对抗图像和音频攻击多模态语言模型，无需文本预处理或训练数据。引入新型ARC评估框架，并验证其在多个模型上的高攻击成功率，揭示当前安全机制的不足。<br/><br/>贡献点（分点列出）：  <br/>1. **提出非文本指令攻击方法（Con Instruction）**  <br/>   - 首次利用MLLMs对非文本指令（图像/音频）的解析能力，通过生成对抗性图像或音频发起攻击，突破传统依赖文本指令的攻击模式。  <br/>   - 攻击无需训练数据或文本预处理，简化了攻击流程。  <br/><br/>2. **优化对抗样本的语义对齐**  <br/>   - 在嵌入空间中优化对抗样本，使其更精准匹配目标指令，显著提升攻击成功率，揭示MLLMs复杂语义理解的潜在安全风险。  <br/><br/>3. **提出攻击响应分类（ARC）框架**  <br/>   - 引入新型评估体系，综合衡量模型响应质量与对恶意指令的相关性，为安全评估提供更全面的指标。  <br/><br/>4. **验证多模型有效性与攻击性能**  <br/>   - 在LLaVA-v1.5、InternVL、Qwen-VL、Qwen-Audio等多模态模型上验证攻击效果，尤其在LLaVA-v1.5 (13B)中达到81.3%和86.6%的成功率，证明方法的普适性。  <br/><br/>5. **分析防御技术的局限性**  <br/>   - 探索针对攻击的防御措施，发现现有技术在攻击检测和防御能力上存在显著性能差距，为后续防御研究提供方向。  <br/><br/>6. **开放源代码实现**  <br/>   - 公布实现代码，便于学术界和工业界复现与扩展研究。|
|2506.00462v1|[XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](http://arxiv.org/abs/2506.00462v1)|**贡献点：**  <br/>1. **提出跨域多语言音频深度伪造基准**：构建XMAD-Bench数据集，包含668.8小时真实与深度伪造语音，覆盖多语言和多样化生成方法。  <br/>2. **强化数据集多样性与挑战性**：训练与测试集的说话人、生成模型、真实音频源均独立，显著提升跨域评估的难度。  <br/>3. **揭示性能差距**：实验结果表明，域内检测准确率接近100%，但跨域性能严重下降，接近随机猜测，突显现有方法的局限性。  <br/>4. **推动鲁棒检测器发展**：强调需开发具备跨语言、跨说话人、跨生成方法和跨数据源泛化能力的深度伪造检测技术。  <br/>5. **公开数据集**：将XMAD-Bench在GitHub平台开源，为研究社区提供统一的评估基准。  <br/><br/>**总结**：  <br/>提出首个跨域多语言音频深度伪造基准XMAD-Bench，验证现有检测模型在真实场景下的性能不足，推动更鲁棒的检测技术发展。|
|2506.00385v1|[MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity   Reconstruction and Generation](http://arxiv.org/abs/2506.00385v1)|**贡献点：**  <br/>1. 提出新型单层流式Transformer音频编解码器MagiCodec，解决传统编解码器重建质量与下游模型兼容性之间的矛盾。  <br/>2. 引入多阶段训练框架，结合高斯噪声注入与潜在正则化技术，增强编码的语义表达性并保持重建保真度。  <br/>3. 理论推导噪声注入在频域中的作用，证明其对抑制高频成分、提升鲁棒性效果显著。  <br/>4. 实验验证MagiCodec在重建质量与下游任务（如语言模型兼容）上优于现有技术，生成的编码呈现齐普夫分布特性。  <br/>5. 开源代码与预训练模型，促进研究复现与应用。  <br/><br/>**总结：**  <br/>MagiCodec通过噪声注入与多阶段训练提升音频编码的语义表达和重建质量，实现与语言模型的兼容性，开源推动研究进展。|
|2506.00087v1|[SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset](http://arxiv.org/abs/2506.00087v1)|总结：  <br/>本文提出多代理协作框架LinguaMaster，构建首个大规模多语言多民族代码转换数据集SwitchLingua，并设计语义感知评估指标SAER，推动多语言和文化多样性研究。  <br/><br/>贡献点：  <br/>1. **提出LinguaMaster框架**：首次设计多智能体协作系统，实现高效可扩展的多语言数据合成。  <br/>2. **构建SwitchLingua数据集**：创建首个覆盖12种语言、18国/地区、63个种族背景的大型代码转换语料库（含420K文本样本及80小时音频）。  <br/>3. **创新SAER评估指标**：提出融合语义信息的代码转换ASR评估方法，提升对跨语言场景的性能判断准确性。|
|2505.24496v1|[Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation](http://arxiv.org/abs/2505.24496v1)|总结：  <br/>本文提出一种压缩-精细语言建模方法，通过保留局部上下文和压缩长距离信息，有效解决神经音频编解码器在语音生成中的长序列挑战，提升生成质量与模型通用性。<br/><br/>贡献点：  <br/>1. **揭示语音标记序列的短距离依赖性**：首次观察到TTS任务中语音标记的预测主要依赖局部上下文，长距离标记贡献较小且冗余。  <br/>2. **设计压缩-精细语言建模框架**：  <br/>   - 保留初始提示和短范围标记，确保文本对齐及语用信息完整性；  <br/>   - 将长范围标记压缩为紧凑表示，减少冗余同时保留关键语义。  <br/>3. **验证方法的有效性与通用性**：通过多组神经音频编解码器和下游语言模型的实验，证明该方法可提升语音生成效果，具有广泛适用性。|
|2505.24314v1|[DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture   Switching for Speech Codec](http://arxiv.org/abs/2505.24314v1)|**贡献点：**  <br/>1. 提出DS-Codec，首次结合镜像（mirror）与非镜像（non-mirror）架构的双阶段训练框架，提升语音重建质量。  <br/>2. 设计动态切换机制，平衡镜像与非镜像结构的优势，增强模型的鲁棒性与高保真度。  <br/>3. 通过大规模实验与消融研究，验证了训练策略的有效性，对比了两种架构的性能差异。  <br/><br/>**总结：**  <br/>本研究提出DS-Codec，通过镜像与非镜像结构的双阶段训练框架提升语音重构效果，并系统验证了其优势。|
|2505.23619v1|[Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](http://arxiv.org/abs/2505.23619v1)|贡献点总结：<br/>1. 提出ADD-GP框架：基于高斯过程的少样本音频深度伪造检测方法，支持快速适应新生成模型  <br/>2. 性能突破：结合深度嵌入模型与GP灵活性，在少量数据下实现高检测准确率  <br/>3. 个性化检测能力：增强对新型TTS模型的鲁棒性，支持单样本级自适应  <br/>4. 构建基准数据集：使用SOTA语音克隆模型创建专用评估数据集  <br/><br/>（99字）|
|2505.20868v2|[Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction   and Style Direction Adjustment for Expressive Text-to-Speech](http://arxiv.org/abs/2505.20868v2)|**贡献点总结（100字以内）：**  <br/>Spotlight-TTS通过语音感知的风格提取和方向调整，提升表达性与语音质量，实验验证其在关键指标上优于基线模型，并公开音频样本以促进研究。  <br/><br/>**分点贡献：**  <br/>1. **提出语音感知风格提取方法**：聚焦与风格强相关的发声区域（voiced regions），同时保持跨语音区域的连续性，增强语音表达性。  <br/>2. **设计风格方向调整机制**：优化提取的风格方向以更好地整合到TTS模型中，显著提升生成语音的整体质量。  <br/>3. **验证性能优越性**：通过实验表明，Spotlight-TTS在表达性、语音质量和风格迁移能力方面均优于现有基线方法。  <br/>4. **开放数据支持**：公开音频样本，便于社区验证成果并推动相关研究发展。|
|2505.19931v2|[Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned   Step Sampling](http://arxiv.org/abs/2505.19931v2)|总结（100字以内）:  <br/>提出训练无关的Fast F5-TTS方法，通过分析采样轨迹设计EPSS策略减少冗余步骤，实现7步生成且推理速度提升4倍，验证了EPSS对流匹配类TTS模型的通用性。  <br/><br/>贡献点:  <br/>1. **训练无关加速方案**：提出Fast F5-TTS，无需额外训练直接优化流匹配TTS模型的推理效率。  <br/>2. **非均匀采样策略**：开发EPSS方法，通过识别冗余采样步骤，显著减少生成过程的采样次数。  <br/>3. **高效推理性能**：在NVIDIA RTX 3090上实现7步生成，推理RTF达0.030，且性能与原模型相当。  <br/>4. **跨模型通用性**：验证EPSS在E2 TTS等其他流匹配模型上的有效性，证明其广泛适用性。|
|2505.19669v2|[Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling](http://arxiv.org/abs/2505.19669v2)|总结：  <br/>提出SMLLE框架，通过Transducer与Delete <Bos>机制解决流式TTS低延迟问题，实现高质量逐帧语音生成，优于现有流式方法且接近句级系统性能。<br/><br/>贡献点：  <br/>1. **提出SMLLE框架**：首次构建零样本流式TTS系统，无需依赖未来文本即可实现低延迟高质量语音合成。  <br/>2. **融合Transducer与时长对齐**：实时将文本转化为语义标记，结合时长对齐信息提升语音自然度。  <br/>3. **设计Delete <Bos>机制**：优化自回归模型对未来文本的访问，最小化延迟同时保持生成稳定性。  <br/>4. **验证性能优势**：实验表明SMLLE在流式TTS任务中超越现有方法，在句级系统中表现可比，具备实际应用价值。|
|2505.19595v2|[Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment](http://arxiv.org/abs/2505.19595v2)|总结：  <br/>本研究提出A-DMA方法，通过双模态对齐优化扩散模型的训练，显著提升收敛速度与生成质量，降低计算成本，并开源代码以促进研究复现。<br/><br/>贡献点：  <br/>1. **提出A-DMA策略**：设计双模态对齐机制，通过文本与语音模态的协同优化加速扩散模型的训练过程。  <br/>2. **双模态对齐技术**：引入文本引导对齐（融合上下文表示）和语音引导对齐（精炼语义表示）的双重对齐管道。  <br/>3. **减少隐式学习依赖**：通过显式对齐隐藏状态与判别特征，降低扩散模型对复杂中间表示的隐式学习需求。  <br/>4. **实验验证效果**：在多个基准上验证A-DMA的性能优势，收敛速度提升1倍，生成质量优于现有方法，并开源代码与演示样本。|
|2505.17076v2|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v2)|总结：  <br/>研究分析不同帧率对中文和英文语音分词效果的影响，揭示帧率与语言特性间的关系，为语音分词优化提供参考。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统探讨低帧率编解码器对语音分词的影响，揭示帧率与语言模型间的作用机制。  <br/>2. **跨语言对比分析**：通过对比汉语和英语两种音系差异显著的语言，发现帧率对不同语言的分词效果存在异质性。  <br/>3. **揭示关键影响因素**：明确帧率变化与音素密度、语言特异性声学特征的交互作用，为分词器设计提供理论依据。  <br/>4. **应用指导意义**：提出优化帧率选择的策略，对自动语音识别、文本到语音等语音处理任务具有实践价值。|
|2504.14906v3|[OmniAudio: Generating Spatial Audio from 360-Degree Video](http://arxiv.org/abs/2504.14906v3)|总结：  <br/>提出360V2SA新任务，构建Sphere360数据集，设计半自动化数据流程，提出OmniAudio框架（结合自监督训练与双分支结构），在空间音频生成领域取得SOTA成果。<br/><br/>贡献点：  <br/>1. 提出360V2SA任务：首次系统性生成360度视频对应的3D空间音频（FOA格式），填补传统视频-音频生成在空间线索上的不足。  <br/>2. 构建Sphere360数据集：首个针对该任务的高质量数据集，基于真实场景数据并优化数据配对与清洗流程。  <br/>3. 设计半自动化数据收集管道：提供高效且可扩展的视频-音频数据配对与预处理方案，降低数据获取门槛。  <br/>4. 提出OmniAudio框架：结合自监督预训练和双分支结构（全景+视角视频输入），同时建模局部与全局空间信息。  <br/>5. 实验验证与开源：在Sphere360上实现SOTA性能，代码与数据集开源，推动领域研究与应用。|