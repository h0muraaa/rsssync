|Source|Title|Summary|
|---|---|---|
|2512.17740v1|[When Pamplona sounds different: the soundscape transformation of San Fermin through intelligent acoustic sensors and a sound repository](https://arxiv.org/abs/2512.17740v1)||
|2512.17356v1|[Training Text-to-Speech Model with Purely Synthetic Data: Feasibility, Sensitivity, and Generalization Capability](https://arxiv.org/abs/2512.17356v1)||
|2512.17293v1|[Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track](https://arxiv.org/abs/2512.17293v1)||
|2512.17281v1|[LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection](https://arxiv.org/abs/2512.17281v1)||
|2512.16304v1|[CogSR: Semantic-Aware Speech Super-Resolution via Chain-of-Thought Guided Flow Matching](https://arxiv.org/abs/2512.16304v1)||
|2512.16271v1|[Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification](https://arxiv.org/abs/2512.16271v1)||
|2512.15830v1|[From Minutes to Days: Scaling Intracranial Speech Decoding with Supervised Pretraining](https://arxiv.org/abs/2512.15830v1)||
|2512.15313v1|[Time-Varying Audio Effect Modeling by End-to-End Adversarial Training](https://arxiv.org/abs/2512.15313v1)||
|2512.15229v1|[O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229v1)||
|2512.15180v1|[BEAT2AASIST model with layer fusion for ESDD 2026 Challenge](https://arxiv.org/abs/2512.15180v1)||
|2512.15124v1|[Synaspot: A Lightweight, Streaming Multi-modal Framework for Keyword Spotting with Audio-Text Synergy](https://arxiv.org/abs/2512.15124v1)||
|2512.14865v1|[Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction](https://arxiv.org/abs/2512.14865v1)||
|2512.14653v1|[Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty](https://arxiv.org/abs/2512.14653v1)||
|2512.14629v1|[MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation](https://arxiv.org/abs/2512.14629v1)||
|2512.14602v1|[Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis](https://arxiv.org/abs/2512.14602v1)||
|2512.14291v1|[GLM-TTS Technical Report](https://arxiv.org/abs/2512.14291v1)||
|2512.14115v1|[Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting](https://arxiv.org/abs/2512.14115v1)||
|2512.14085v1|[Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study](https://arxiv.org/abs/2512.14085v1)||
|2512.13905v1|[Ensemble-Guided Distillation for Compact and Robust Acoustic Scene Classification on Edge Devices](https://arxiv.org/abs/2512.13905v1)||
|2512.13880v1|[Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880v1)||
|2512.13744v1|[Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes](https://arxiv.org/abs/2512.13744v1)||
|2512.13284v1|[SAMAY: System for Acoustic Measurement and Analysis](https://arxiv.org/abs/2512.13284v1)||
|2512.13251v2|[DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec](https://arxiv.org/abs/2512.13251v2)||
|2512.13251v1|[DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec](https://arxiv.org/abs/2512.13251v1)||
|2512.13131v1|[Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131v1)||
|2512.13012v1|[HQ-MPSD: A Multilingual Artifact-Controlled Benchmark for Partial Deepfake Speech Detection](https://arxiv.org/abs/2512.13012v1)||
|2512.12875v1|[Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875v1)||
|2512.12834v1|[Procedural Music Generation Systems in Games](https://arxiv.org/abs/2512.12834v1)||
|2512.12471v1|[Privacy-Aware Ambient Audio Sensing for Healthy Indoor Spaces](https://arxiv.org/abs/2512.12471v1)||
|2512.12129v1|[A comparative study of generative models for child voice conversion](https://arxiv.org/abs/2512.12129v1)||
|2512.11545v1|[Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition](https://arxiv.org/abs/2512.11545v1)||
|2512.11457v1|[Processing through encoding: Quantum circuit approaches for point-wise multiplication and convolution](https://arxiv.org/abs/2512.11457v1)||
|2512.11348v2|[PhraseVAE and PhraseLDM: Latent Diffusion for Full-Song Multitrack Symbolic Music Generation](https://arxiv.org/abs/2512.11348v2)||
|2512.11348v1|[PhraseVAE and PhraseLDM: Latent Diffusion for Full-Song Multitrack Symbolic Music Generation](https://arxiv.org/abs/2512.11348v1)||
|2512.11229v1|[REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation](https://arxiv.org/abs/2512.11229v1)||
|2512.11165v1|[Mitigation of multi-path propagation artefacts in acoustic targets with cepstral adaptive filtering](https://arxiv.org/abs/2512.11165v1)||
|2512.11009v1|[The TCG CREST -- RKMVERI Submission for the NCIIPC Startup India AI Grand Challenge](https://arxiv.org/abs/2512.11009v1)||
|2512.10375v1|[Neural personal sound zones with flexible bright zone control](https://arxiv.org/abs/2512.10375v1)||
|2512.10264v2|[MR-FlowDPO: Multi-Reward Direct Preference Optimization for Flow-Matching Text-to-Music Generation](https://arxiv.org/abs/2512.10264v2)||
|2512.10264v1|[MR-FlowDPO: Multi-Reward Direct Preference Optimization for Flow-Matching Text-to-Music Generation](https://arxiv.org/abs/2512.10264v1)||
|2512.10170v1|[Semantic-Aware Confidence Calibration for Automated Audio Captioning](https://arxiv.org/abs/2512.10170v1)||
|2512.09504v1|[DMP-TTS: Disentangled multi-modal Prompting for Controllable Text-to-Speech with Chained Guidance](https://arxiv.org/abs/2512.09504v1)||
|2512.09327v1|[UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking](https://arxiv.org/abs/2512.09327v1)||
|2512.09299v1|[VABench: A Comprehensive Benchmark for Audio-Video Generation](https://arxiv.org/abs/2512.09299v1)||
|2512.09285v1|[Who Speaks What from Afar: Eavesdropping In-Person Conversations via mmWave Sensing](https://arxiv.org/abs/2512.09285v1)||
|2512.08282v1|[PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282v1)||
|2512.08238v1|[SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality](https://arxiv.org/abs/2512.08238v1)||
|2512.08203v1|[Error-Resilient Semantic Communication for Speech Transmission over Packet-Loss Networks](https://arxiv.org/abs/2512.08203v1)||
|2512.07741v1|[A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data](https://arxiv.org/abs/2512.07741v1)||
|2512.07627v1|[Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization](https://arxiv.org/abs/2512.07627v1)||
|2512.07570v1|[Introduction to Ambisonics, Part 1: The Part With No Math](https://arxiv.org/abs/2512.07570v1)||
|2512.07352v1|[MultiAPI Spoof: A Multi-API Dataset and Local-Attention Network for Speech Anti-spoofing Detection](https://arxiv.org/abs/2512.07352v1)||
|2512.07351v1|[DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351v1)||
|2512.07277v1|[Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277v1)||
|2512.07265v1|[TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265v1)||
|2512.07209v1|[Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits](https://arxiv.org/abs/2512.07209v1)||
|2512.07168v1|[JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention](https://arxiv.org/abs/2512.07168v1)||
|2512.07005v1|[Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition](https://arxiv.org/abs/2512.07005v1)||
|2512.06999v1|[Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model](https://arxiv.org/abs/2512.06999v1)||
|2512.06890v1|[What Needs to be Known in Order to Perform a Meaningful Scientific Comparison Between Animal Communications and Human Spoken Language](https://arxiv.org/abs/2512.06890v1)||
|2512.06757v1|[XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association](https://arxiv.org/abs/2512.06757v1)||
|2512.06417v1|[Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator](https://arxiv.org/abs/2512.06417v1)||
|2512.06304v1|[Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation](https://arxiv.org/abs/2512.06304v1)||
|2512.06041v1|[Technical Report of Nomi Team in the Environmental Sound Deepfake Detection Challenge 2026](https://arxiv.org/abs/2512.06041v1)||
|2512.06040v1|[Physics-Guided Deepfake Detection for Voice Authentication Systems](https://arxiv.org/abs/2512.06040v1)||
|2512.05701v1|[A Multi-Channel Auditory Signal Encoder with Adaptive Resolution Using Volatile Memristors](https://arxiv.org/abs/2512.05701v1)||
|2512.05592v1|[The T12 System for AudioMOS Challenge 2025: Audio Aesthetics Score Prediction System Using KAN- and VERSA-based Models](https://arxiv.org/abs/2512.05592v1)||
|2512.05528v1|[Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening](https://arxiv.org/abs/2512.05528v1)||
|2512.05355v1|[Noise Suppression for Time Difference of Arrival: Performance Evaluation of a Generalized Cross-Correlation Method Using Mean Signal and Inverse Filter](https://arxiv.org/abs/2512.05355v1)||
|2512.05201v2|[MuMeNet: A Network Simulator for Musical Metaverse Communications](https://arxiv.org/abs/2512.05201v2)||
|2512.04827v1|[Contract-Driven QoE Auditing for Speech and Singing Services: From MOS Regression to Service Graphs](https://arxiv.org/abs/2512.04827v1)||
|2512.04814v1|[Shared Multi-modal Embedding Space for Face-Voice Association](https://arxiv.org/abs/2512.04814v1)||
|2512.04793v1|[YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases](https://arxiv.org/abs/2512.04793v1)||
|2512.04779v1|[YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance](https://arxiv.org/abs/2512.04779v1)||
|2512.04616v1|[Standard audiogram classification from loudness scaling data using unsupervised, supervised, and explainable machine learning techniques](https://arxiv.org/abs/2512.04616v1)||
|2512.03783v2|[Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783v2)||
|2512.03783v1|[Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783v1)||
|2512.03637v1|[AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning](https://arxiv.org/abs/2512.03637v1)||
|2512.02783v1|[Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces](https://arxiv.org/abs/2512.02783v1)||
|2512.02669v1|[SAND Challenge: Four Approaches for Dysartria Severity Classification](https://arxiv.org/abs/2512.02669v1)||
|2512.02652v1|[Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training](https://arxiv.org/abs/2512.02652v1)||
|2512.02523v1|[Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation](https://arxiv.org/abs/2512.02523v1)||
|2512.02432v1|[Continual Learning for Singing Voice Separation with Human in the Loop Adaptation](https://arxiv.org/abs/2512.02432v1)||
|2512.02206v1|[WhAM: Towards A Translative Model of Sperm Whale Vocalization](https://arxiv.org/abs/2512.02206v1)||
|2512.02074v1|[Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074v1)||
|2512.01626v1|[Parallel Delayed Memory Units for Enhanced Temporal Modeling in Biomedical and Bioacoustic Signal Analysis](https://arxiv.org/abs/2512.01626v1)||
|2512.01537v1|[Q2D2: A Geometry-Aware Audio Codec Leveraging Two-Dimensional Quantization](https://arxiv.org/abs/2512.01537v1)||
|2512.01428v1|[Masked Symbol Modeling for Demodulation of Oversampled Baseband Communication Signals in Impulsive Noise-Dominated Channels](https://arxiv.org/abs/2512.01428v1)||
|2512.00883v1|[Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound](https://arxiv.org/abs/2512.00883v1)||
|2512.00563v1|[Explainable Multi-Modal Deep Learning for Automatic Detection of Lung Diseases from Respiratory Audio Signals](https://arxiv.org/abs/2512.00563v1)||
|2512.00451v2|[STCTS: Generative Semantic Compression for Ultra-Low Bitrate Speech via Explicit Text-Prosody-Timbre Decomposition](https://arxiv.org/abs/2512.00451v2)||
|2512.00451v1|[STCTS: Generative Semantic Compression for Ultra-Low Bitrate Speech via Explicit Text-Prosody-Timbre Decomposition](https://arxiv.org/abs/2512.00451v1)||
|2512.00115v1|[MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning](https://arxiv.org/abs/2512.00115v1)||
|2511.23142v1|[Adapting Neural Audio Codecs to EEG](https://arxiv.org/abs/2511.23142v1)||
|2511.22696v3|[Probabilistic Fusion and Calibration of Neural Speaker Diarization Models](https://arxiv.org/abs/2511.22696v3)||
|2511.22696v2|[Probabilistic Fusion and Calibration of Neural Speaker Diarization Models](https://arxiv.org/abs/2511.22696v2)||
|2511.22696v1|[Probabilistic Fusion and Calibration of Neural Speaker Diarization Models](https://arxiv.org/abs/2511.22696v1)||
|2511.21780v1|[3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation](https://arxiv.org/abs/2511.21780v1)||
|2511.21580v1|[Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension](https://arxiv.org/abs/2511.21580v1)||
|2511.21577v1|[HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal](https://arxiv.org/abs/2511.21577v1)||
|2511.21342v1|[Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures](https://arxiv.org/abs/2511.21342v1)||
|2511.21325v1|[SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection](https://arxiv.org/abs/2511.21325v1)||
|2511.21146v1|[AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control](https://arxiv.org/abs/2511.21146v1)||
|2511.21088v1|[ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features](https://arxiv.org/abs/2511.21088v1)||
|2511.21045v1|[CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation](https://arxiv.org/abs/2511.21045v1)||
|2511.20470v1|[Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model](https://arxiv.org/abs/2511.20470v1)||
|2511.20380v1|[Differentiable Attenuation Filters for Feedback Delay Networks](https://arxiv.org/abs/2511.20380v1)||
|2511.19974v1|[Continual Audio Deepfake Detection via Universal Adversarial Perturbation](https://arxiv.org/abs/2511.19974v1)||
|2511.19734v1|[Evaluating Objective Speech Quality Metrics for Neural Audio Codecs](https://arxiv.org/abs/2511.19734v1)||
|2511.19403v1|[Frequency-Invariant Beamforming in Elevation and Azimuth via Autograd and Concentric Circular Microphone Arrays](https://arxiv.org/abs/2511.19403v1)||
|2511.19396v1|[Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments](https://arxiv.org/abs/2511.19396v1)||
|2511.19342v1|[Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation](https://arxiv.org/abs/2511.19342v1)||
|2511.18698v1|[Multimodal Real-Time Anomaly Detection and Industrial Applications](https://arxiv.org/abs/2511.18698v1)||
|2511.18421v1|[DHAuDS: A Dynamic and Heterogeneous Audio Benchmark for Test-Time Adaptation](https://arxiv.org/abs/2511.18421v1)||
|2511.18384v1|[NSTR: Neural Spectral Transport Representation for Space-Varying Frequency Fields](https://arxiv.org/abs/2511.18384v1)||
|2511.18078v2|[Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels](https://arxiv.org/abs/2511.18078v2)||
|2511.18078v1|[Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels](https://arxiv.org/abs/2511.18078v1)||
|2511.17879v2|[Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879v2)||
|2511.17879v1|[Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879v1)||
|2511.17346v1|[Is Phase Really Needed for Weakly-Supervised Dereverberation ?](https://arxiv.org/abs/2511.17346v1)||
|2511.17337v1|[A new kid on the block: Distributional semantics predicts the word-specific tone signatures of monosyllabic words in conversational Taiwan Mandarin](https://arxiv.org/abs/2511.17337v1)||
|2511.17323v1|[MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core](https://arxiv.org/abs/2511.17323v1)||
|2511.17181v1|[Investigating self-supervised representations for audio-visual deepfake detection](https://arxiv.org/abs/2511.17181v1)||
|2511.17136v1|[Device-Guided Music Transfer](https://arxiv.org/abs/2511.17136v1)||
|2511.16849v1|[Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks](https://arxiv.org/abs/2511.16849v1)||
|2511.16228v2|[Difficulty-Controlled Simplification of Piano Scores with Synthetic Data for Inclusive Music Education](https://arxiv.org/abs/2511.16228v2)||
|2511.16228v1|[Difficulty-Controlled Simplification of Piano Scores with Synthetic Data for Inclusive Music Education](https://arxiv.org/abs/2511.16228v1)|总结：  <br/>本研究提出基于Transformer的MusicXML钢琴曲难度调整方法，通过合成数据集和开源资源解决专有数据限制，提升音乐教育AI的可访问性和创新性。<br/><br/>贡献点：  <br/>1. **挑战专有系统壁垒**：首次针对AI音乐教育中因专有系统导致的民主化障碍提出解决方案，推动技术开放共享。  <br/>2. **合成难度配对数据集**：构建首个基于MusicXML的合成钢琴曲难度配对数据集，以旋律与和声为条件生成挑战与简化版本，突破传统标注数据依赖。  <br/>3. **格式兼容性改进**：使用MusicXML替代MIDI，保留乐谱可读性与布局信息，提升生成内容对人类演奏者的实用性。  <br/>4. **开源促进复现与创新**：全面公开代码、数据集和模型，确保研究可复现，并为后续开源研究提供基础，助力技术普惠。|
|2511.15848v2|[Step-Audio-R1 Technical Report](https://arxiv.org/abs/2511.15848v2)||
|2511.15848v1|[Step-Audio-R1 Technical Report](https://arxiv.org/abs/2511.15848v1)|**贡献点：**  <br/>1. 提出首个音频推理模型 **Step-Audio-R1**，突破音频领域无推理优势的瓶颈。  <br/>2. 创新性提出 **Modality-Grounded Reasoning Distillation (MGRD)** 框架，实现基于音频特征的推理链生成，而非虚假推理。  <br/>3. 在语音、环境音、音乐等音频任务中性能超越 **Gemini 2.5 Pro**，接近 **Gemini 3 Pro**，验证高阶推理能力。  <br/>4. 证明推理能力可在模态间迁移，通过锚定实现从“累赘”到“核心资产”的转变。  <br/>5. 开启构建跨模态深度推理系统的全新路径，推动多模态人工智能发展。  <br/><br/>**总结（100字内）：**  <br/>本文提出首个音频推理模型Step-Audio-R1，通过MGRD框架实现基于音频特征的推理，超越Gemini 2.5 Pro并接近SOTA，证明推理能力可跨模态迁移，为构建多模态智能系统奠定基础。|
|2511.15485v1|[A Novel CustNetGC Boosted Model with Spectral Features for Parkinson's Disease Prediction](https://arxiv.org/abs/2511.15485v1)|总结（100字以内）:  <br/>本文提出CustNetGC模型，结合CNN、Grad-CAM和CatBoost，通过L-mHP频谱特征和Spectral Slopes实现帕金森病高效诊断，达到99.06%准确率，显著提升诊断准确性与模型可解释性，为早期检测提供新方法。<br/><br/>贡献点分点:  <br/>1. **提出新型诊断框架CustNetGC**：创新性整合CNN、Custom Network Grad-CAM与CatBoost算法，提升帕金森病诊断的效率和准确性。  <br/>2. **多模态频谱特征提取**：首次采用L-mHP特征（融合Log-Mel、谐波、打击性频谱图）和Spectral Slopes，增强语音信号表征能力。  <br/>3. **可解释性增强**：通过Grad-CAM可视化关键频谱区域，揭示PD检测中的重要特征位置，提升模型可信度。  <br/>4. **高精度实验验证**：在公开数据集（81名参与者）上实现99.06%准确率和95.83%精确率，AUC达0.90（PD）和0.89（HC），展现优越性能。  <br/>5. **CatBoost优化**：利用梯度提升算法增强模型鲁棒性，有效区分PD与非PD样本，改善预测稳定性。  <br/>6. **临床应用潜力**：为PD早期筛查提供高效、可解释的语音分析工具，推动非侵入式诊断方法的发展。|
|2511.15270v1|[LargeSHS: A large-scale dataset of music adaptation](https://arxiv.org/abs/2511.15270v1)|**贡献点总结**（100字以内）：  <br/>本文提出LargeSHS数据集，整合1.7百万元数据与90万音频链接，首次构建结构化的改编关系网络，支持改编树及表演簇分析，突破传统文本条件模型局限，推动参考驱动的音乐生成与改编感知MIR研究。<br/><br/>**分点贡献**：  <br/>1. **填补研究空白**：首个专注于参考驱动音乐生成（如歌曲改编）的大规模数据集，弥补现有文本条件模型的不足。  <br/>2. **超大规模数据**：基于SecondHandSongs构建，包含170万+元数据和约90万可访问音频链接，显著高于现有数据集。  <br/>3. **结构化改编关系**：提供歌曲间的明确改编关系，支持构建改编树与表演簇，增强对翻唱家族的语义建模能力。  <br/>4. **全面分析支持**：附带统计分析及与其他数据集的对比，凸显其规模与多样性，为后续研究提供基准。|
|2511.14824v1|[Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech](https://arxiv.org/abs/2511.14824v1)|**贡献点总结**  <br/>1. **声带感知风格提取**：聚焦与表达性高度相关的声带区域，保持语音区域连续性，提升表达性。  <br/>2. **风格方向优化**：调整提取的风格方向，增强与TTS模型的融合效果，提高语音质量。  <br/>3. **综合性能提升**：在表达性、语音质量和风格迁移能力上优于基线模型，验证方法有效性。  <br/><br/>**总结**（100字内）：  <br/>SpotlightTTS通过声带感知风格提取与方向调整，提升表达性与语音质量，优于现有模型，在风格迁移和整体表现上取得突破。|
|2511.14801v2|[IHearYou: Linking Acoustic Features to DSM-5 Depressive Behavior Indicators](https://arxiv.org/abs/2511.14801v2)||
|2511.14801v1|[IHearYou: Linking Acoustic Features to DSM-5 Depressive Behavior Indicators](https://arxiv.org/abs/2511.14801v1)|**贡献点分点总结：**  <br/>1. **提出基于语音声学的自动化抑郁症检测系统 IHearYou**，通过家庭环境被动采集语音数据，减少主观依赖。  <br/>2. **构建结构化 Linkage Framework**，将语音特征与 DSM-5 指标（特别是重度抑郁症）关联，实现临床指标量化。  <br/>3. **设计本地化运行系统**，保障用户隐私，支持实时处理与可视化（含持久性存储和仪表盘）。  <br/>4. **制定可重复性验证协议**，引入 FDR 校正和性别分层测试，提升方法的严谨性和普适性。  <br/>5. **通过真实数据集与实验验证**，在 DAIC-WOZ 数据集上展示特征-指标一致性，证明端到端可行性。  <br/>6. **实现黑箱检测的可解释性突破**，将被动语音感知转化为临床可理解的 DSM-5 指标评分。  <br/><br/>**总结（100字内）：**  <br/>IHearYou 提出基于语音声学的抑郁症检测系统，通过 Linkage Framework 关联 DSM-5 指标，实现本地隐私保护与可解释性分析，结合 DAIC-WOZ 数据集验证方法有效性，填补了自动检测与临床解释间的空白。|
|2511.14250v1|[Count The Notes: Histogram-Based Supervision for Automatic Music Transcription](https://arxiv.org/abs/2511.14250v1)|**贡献点：**  <br/>1. **提出CountEM框架**：首次利用音符事件直方图替代传统帧级标注，消除对显式局部对齐的依赖，提升计算效率与模型灵活性。  <br/>2. **创新EM优化方法**：通过仅依赖音符出现次数的迭代优化策略，显著降低标注成本，同时保持高转录精度。  <br/>3. **实验证明有效性**：在钢琴、吉他及多乐器数据集上验证，CountEM匹配或超越现有弱监督方法，增强AMT的鲁棒性、可扩展性与效率。  <br/><br/>**总结（100字内）：**  <br/>本文提出CountEM框架，通过音符直方图弱监督和EM算法优化，减少对局部对齐的依赖，提升AMT的效率与灵活性，在多乐器数据集上表现优异，验证了其鲁棒性与可扩展性。|
|2511.13146v1|[Towards Practical Real-Time Low-Latency Music Source Separation](https://arxiv.org/abs/2511.13146v1)|**贡献点：**<br/>1. 提出轻量化实时低延迟模型RT-STT（基于DTTNet），适用于需实时处理的场景（如助听器、音频流混音等）；  <br/>2. 引入基于通道扩展的特征融合技术，提升模型性能；  <br/>3. 验证单路径建模在实时模型中的有效性，优于传统双路径结构；  <br/>4. 通过量化方法进一步降低推理时间，减少参数量且保持高性能；  <br/>5. 在参数量和推理速度上超越现有SOTA模型，实现效率与效果的平衡。  <br/><br/>**总结（100字内）：**  <br/>本文提出轻量化实时音乐分离模型RT-STT，通过通道扩展融合、单路径设计及量化优化，在参数量和推理速度上超越现有方法，适用于低延迟音频处理场景。|
|2511.12072v1|[ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation](https://arxiv.org/abs/2511.12072v1)|**贡献点：**  <br/>1. 提出**ProAV-DiT**，通过多尺度时空自编码器与扩散Transformer结合，实现高效同步音视频生成。  <br/>2. 引入**正交分解**的统一潜在空间，解决音频-视频模态语义对齐问题。  <br/>3. 设计**多尺度注意力机制**（含时间自注意力和跨模态分组注意力），增强时序连贯性与模态融合能力。  <br/>4. 构建**3D潜空间**整合音频与视频的多尺度信息，优化时空依赖建模。  <br/>5. 在标准数据集上验证了提升的**生成质量**与**计算效率**，超越现有方法。  <br/><br/>**总结（100字以内）：**  <br/>ProAV-DiT通过统一潜在空间、多尺度注意力机制与3D潜空间设计，解决音视频结构对齐与计算效率问题，实现高质量同步生成。|
|2511.11930v1|[Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering](https://arxiv.org/abs/2511.11930v1)|**贡献点：**  <br/>1. 提出SAMOSA系统：首个基于设备的实时空间音频渲染系统，动态适应物理环境以提升XR听觉真实感。  <br/>2. 多模态融合技术：整合房间几何、表面材料及语义驱动的声学场景信息，构建协同的场景表示。  <br/>3. 高效校准方法：通过场景先验知识优化房间冲激响应（RIR）合成，减少计算资源占用。  <br/>4. 全面验证体系：结合技术评估（多房间配置与声学指标）和专家评价（N=12），证明系统可行性与效果。  <br/><br/>**总结（100字以内）：**  <br/>SAMOSA是首个设备端实时空间音频系统，通过多模态场景融合与语义驱动校准，动态适应物理环境，显著提升XR的听觉真实感，并经实验验证其有效性。|
|2511.11527v1|[Evaluation of Audio Compression Codecs](https://arxiv.org/abs/2511.11527v1)|**贡献点：**  <br/>1. **提出感知质量的综合定义**：将音频感知质量定义为听觉准确性、可懂度和保真度的综合，强调其对用户选择压缩编解码器的影响。  <br/>2. **多维度评估方法**：通过编解码器性能测量、可视化分析及PEAQ评分，系统评估不同压缩算法的感知质量与压缩效率。  <br/>3. **揭示压缩技术的影响机制**：分析数字音频压缩对感知质量的具体影响，为用户选择压缩方案提供理论依据。  <br/>4. **实用指导意义**：为用户在实际场景中权衡压缩效率与感知质量提供决策参考。  <br/><br/>**总结（100字以内）：**  <br/>论文提出音频感知质量的综合定义，通过多维度评估方法分析压缩编解码器的性能与感知特性，揭示压缩技术对音质的影响，为用户选择压缩方案提供理论与实践指导。|
|2511.11124v1|[AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124v1)|总结：  <br/>提出AV-Dialog框架，结合音频与视觉线索实现鲁棒对话，解决嘈杂环境下的说话者追踪与轮次预测问题，提升对话质量。<br/><br/>贡献点：<br/>1. **首个多模态对话框架**：首次融合音频与视觉信息，实现说话者感知的对话交互，提升环境鲁棒性。<br/>2. **核心技术创新**：结合声学标记化与多任务多阶段训练策略，突破单模态模型局限。<br/>3. **多场景数据训练**：基于单模态、合成与真实音频-视觉数据集进行联合训练，增强模型泛化能力。<br/>4. **性能提升**：在干扰环境下显著降低转录错误，提升轮次预测精度与人类评估的对话质量。<br/>5. **应用价值**：为真实世界噪声场景下的对话代理提供新范式，推动语音交互向自然流畅发展。|
|2511.11104v1|[CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation](https://arxiv.org/abs/2511.11104v1)|**总结（100字以内）**：  <br/>CLARITY提出双信号优化框架，解决TTS中口音与语言偏见，通过本地化文本和检索增强口音提示，提升多方言准确性和公平性，保持语音质量。  <br/><br/>**贡献点**：  <br/>1. **问题识别**：首次系统揭示TTS中的口音偏见（默认主导口音）与语言偏见（忽略方言文化特征）的耦合关系。  <br/>2. **方法创新**：提出CLARITY框架（背骨无关），通过双信号优化策略解决偏见：  <br/>   - **上下文语言适应**：将输入文本本地化至目标方言。  <br/>   - **检索增强口音提示（RAAP）**：生成口音一致的语音提示。  <br/>3. **效果验证**：在12种英语口音上验证，显著提升口音准确性、公平性，同时保持高质量感知语音。|
|2511.11039v2|[Listening Between the Frames: Bridging Temporal Gaps in Large Audio-Language Models](https://arxiv.org/abs/2511.11039v2)||
|2511.10697v1|[Graph Neural Field with Spatial-Correlation Augmentation for HRTF Personalization](https://arxiv.org/abs/2511.10697v1)|总结：提出基于图神经网络的空间相关性增强HRTF个性化框架，实现高效、高质量的未见主体HRTF生成。<br/><br/>贡献点：<br/>1. **提出GraphNF-SCA框架**：首次将图神经网络与空间相关性建模结合，用于生成个性化HRTF，解决传统HRTF测量耗时且依赖个体数据的问题。<br/>2. **三模块协同设计**：包含HRTF-P（编解码结构提取通用与目标特征）、HRTF-U（建模HRTF间空间相关性）和微调阶段，形成完整个性化生成流程。<br/>3. **空间一致性优化**：通过空间相关性建模提升预测HRTF的空间一致性，显著优于传统逐位置估计方法。<br/>4. **跨位置特征融合**：利用GNN的图结构隐式捕捉HRTF的跨位置依赖关系，无需显式标注位置信息即可实现高质量渲染。<br/>5. **SOTA性能验证**：实验结果在多个数据集上达到当前最优水平，证明了框架的有效性与实用性。|
|2511.10482v1|[Proceedings of The third international workshop on eXplainable AI for the Arts (XAIxArts)](https://arxiv.org/abs/2511.10482v1)|**总结（100字以内）：**  <br/>第三次XAIxArts工作坊推动跨学科合作，探讨可解释AI在艺术领域的应用，促进AI、数字艺术与人机交互领域的知识交流，为艺术创作与技术融合提供新思路。<br/><br/>---<br/><br/>**贡献点分点：**  <br/>1. **跨学科社区构建**：汇聚HCI、AI、数字艺术等领域的研究人员，促进多学科对话与合作。  <br/>2. **研究方向聚焦**：明确XAI在艺术领域的角色，推动可解释性技术与艺术创作、体验设计的结合。  <br/>3. **学术平台搭建**：作为ACM C&C 2025子活动，提供学术交流与实践探索的平台，强化领域影响力。|
|2511.09802v1|[Investigation of Feature Selection and Pooling Methods for Environmental Sound Classification](https://arxiv.org/abs/2511.09802v1)|总结：  <br/>提出SSRP及其变体（SSRP-B、SSRP-T）用于环境声音分类，验证其在轻量CNN中的高效性与高准确性，特别是在资源受限场景下优于PCA和传统CNN方法。<br/><br/>贡献点：  <br/>1. **提出新型稀疏池化方法**：设计Sparse Salient Region Pooling（SSRP）及其变体SSRP-Basic和SSRP-Top-K，探索其在环境声音分类中的应用。  <br/>2. **对比实验验证有效性**：通过在ESC-50数据集上的实验，证明SSRP-T在准确率（80.69%）上显著优于基线CNN（66.75%）和PCA模型（37.60%）。  <br/>3. **参数优化与适应性分析**：评估不同超参数设置下的SSRP性能，展示其在调参后对计算成本与精度的平衡能力。  <br/>4. **资源约束场景的解决方案**：强调SSRP在轻量级模型中的优势，为低功耗或边缘设备的环境声音分类任务提供高效且高性能的方法。|
|2511.09682v1|[Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models](https://arxiv.org/abs/2511.09682v1)|总结：  <br/>本研究首次探讨音频推理模型（ARMs）的抗 jailbreak 攻击安全性，提出 Rebellion 方法通过抵消表征漂移提升鲁棒性，在保障良性任务性能的同时显著优化准确率与安全性的平衡。<br/><br/>贡献点：  <br/>1. **首次系统分析 ARM 的安全性**：揭示标准推理训练（RT）在对抗常规音频 jailbreak 攻击时有效，但无法抵御新颖的高级攻击，因表征漂移导致模型生成有害响应。  <br/>2. **提出 Rebellion 方法**：构建一种鲁棒 RT 框架，专门训练模型抵御最坏情况下的表征漂移，增强其对抗 jailbreak 攻击的能力。  <br/>3. **验证效果与性能平衡**：在 Qwen2-Audio 上证明 Rebellion 能有效防御高级攻击，同时不降低对良性任务的性能表现。  <br/>4. **优化准确率-安全权衡**：相比标准 RT，Rebellion 显著改善准确率与安全性的平衡对比，提升模型的实用性。|
|2511.09585v4|[Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation](https://arxiv.org/abs/2511.09585v4)||
|2511.09585v3|[Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation](https://arxiv.org/abs/2511.09585v3)||
|2511.09585v2|[Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation](https://arxiv.org/abs/2511.09585v2)|总结：本文提出VeM模型，通过分层视频解析和跨模态注意力机制解决视频音乐生成中的语义-时间对齐和节奏同步问题，创新性构建严格同步数据集并设计专用评估指标，显著提升生成音轨的高质量与同步性能。<br/><br/>贡献点：<br/>1. 提出Video Echoed in Music (VeM)框架，首次将多模态信息（视频+音乐）整合到潜在扩散模型中，实现语义、时间与节奏三重精准对齐。<br/>2. 设计分层视频解析机制作为"音乐指挥"，通过多级视频信息协调生成具有丰富语义的背景音乐。<br/>3. 开发语义引导的跨模态注意力机制(SG-CAtt)，结合模态编码器与位置/持续时间编码，确保时间连贯性。<br/>4. 引入帧级转换节拍对齐器与适配器(TB-As)，动态实现视觉场景转换与音乐节拍的精确同步。<br/>5. 构建首个严格要求过渡节拍同步的视频-音乐配对数据集，来自电商广告和视频平台，推动任务标准化。<br/>6. 提出针对性评估指标，量化衡量视频音乐生成的语义相关性与节奏精准度。|
|2511.09585v1|[Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation](https://arxiv.org/abs/2511.09585v1)|总结：  <br/>该论文提出VeM方法，通过分层视频解析、跨模态注意力机制和帧级节拍对齐技术，解决视频-音乐生成中的语义对齐与节奏同步问题，并构建了首个符合电商广告严格节拍要求的视频-音乐配对数据集。<br/><br/>贡献点：  <br/>1. **提出VeM模型**：首次构建基于潜变量扩散的视频-音乐生成框架，实现语义、时间和节奏的多维度对齐。  <br/>2. **分层视频解析**：设计多级视频解析模块，作为音乐“指挥”协调跨模态多级信息融合。  <br/>3. **SG-CAtt机制**：引入故事板引导的跨模态注意力机制，结合位置与持续时间编码确保语义与时间一致性。  <br/>4. **TB-As对齐器**：开发帧级过渡节拍对齐器与适配器，动态实现视觉场景转换与音乐节拍的精准同步。  <br/>5. **新数据集与评估指标**：发布首个针对电商广告和视频平台的视频-音乐配对数据集，并设计专用评估指标验证任务性能。|
|2511.09525v1|[Spatial Audio Rendering for Real-Time Speech Translation in Virtual Meetings](https://arxiv.org/abs/2511.09525v1)|**贡献点：**  <br/>1. **提出空间音频渲染在实时翻译中的作用**：首次系统研究空间音频渲染对多语言会议中语言理解的影响，揭示其对跨语言沟通的关键作用。  <br/>2. **语言多样性选择**：选取语法、书写系统及资源可得性差异显著的希腊语、卡纳达语、普通话、乌克兰语，增强研究普适性。  <br/>3. **多维度实验设计**：通过within-subjects实验，对比四种音频条件（空间+混响、空间无混响、双耳、单耳），全面评估感知线索对理解、认知负荷及用户体验的影响。  <br/>4. **量化与定性结果**：实验证明空间音频使理解准确率提升100%，同时通过主观反馈验证其提升清晰度与参与感的效果。  <br/>5. **设计建议创新**：为会议平台的实时翻译集成提供可操作的用户界面优化方案，推动包容性跨语言通信在远程协作中的应用。  <br/><br/>**总结（100字内）**：  <br/>该研究探索空间音频渲染对多语言会议理解与体验的影响，发现其显著提升准确率及参与度，提出优化远程协作平台翻译集成的设计建议，推动跨语言沟通的技术改进。|
|2511.09037v1|[Sound impact of simple viscoelastic damping changes due to aging and the role of the double bentside on soundboard tension in a 1755 Dulcken harpsichord](https://arxiv.org/abs/2511.09037v1)|总结（100字以内）:  <br/>本研究通过FDTD和FEM模型分析1755年古钢琴声板老化对音色的影响，发现高频弦亮度下降与低频弦亮度增加的反常现象，并揭示其与频率依赖阻尼滤波效应的关系，同时探讨了弦连接位置对声学特性的影响。  <br/><br/>贡献点：  <br/>1. **方法创新**：首次将有限差分时域（FDTD）模型应用于古钢琴声板老化声学特性研究，结合实测数据与模拟实验。  <br/>2. **精准测量**：在52个弦位置处测量声板厚度并模拟冲击响应，量化老化对声学参数（如T60衰减时间）的具体影响。  <br/>3. **音色量化分析**：利用频谱质心（spectral centroid）指标，首次揭示琴弦位置与老化导致音色亮度变化的依赖关系。  <br/>4. **反常现象发现**：提出低频弦亮度提升、高频弦亮度下降的反常结果，解释为频率相关的阻尼滤波效应。  <br/>5. **结构特性探索**：通过3D有限元模型（FEM）验证 Dulcken 古钢琴将8'弦连接外壁而非内壁的结构设计对声板张力影响有限，提出其他潜在的声学解释。|
|2511.09029v1|[Non-verbal Perception of Room Acoustics using Multi Dimensional Scaling Metho](https://arxiv.org/abs/2511.09029v1)|**贡献点总结：**  <br/>本研究提出了一种新的方法，将音乐与双耳房间脉冲响应测量结合，利用MDS揭示房间声学感知的五个关键维度（混响密度、分形相关维度、粗糙度、响度、早期衰减时间），突破了传统依赖专家记忆或单向评分的局限，为客观量化主观声学体验提供了新思路。<br/><br/>**分点贡献：**  <br/>1. **提出新研究方法**：首次将实际音乐与双耳房间脉冲响应（binaural room impulse response）融合，探索主观听觉体验的多维表征。  <br/>2. **引入MDS技术**：采用多维标度（MDS）分析，从听众的感知数据中提取房间声学的感知维度，而非依赖传统专家评分或单向量表。  <br/>3. **确定关键感知维度**：通过实验验证，发现房间声学的主观感知可由五个（psycho-）acoustical参数解释（混响密度、分形相关维度、粗糙度、响度、早期衰减时间），为声学设计与评估提供量化依据。  <br/>4. **优化主观-客观关联**：突破了传统方法中单一参数或专家记忆的局限，通过音乐场景的结合更全面地反映真实听觉环境的感知复杂性。  <br/>5. **应用价值明确**：结果可应用于音乐厅、录音室等空间声学优化，提升主观体验的客观化研究和工程实践。|
|2511.08755v1|[Chord-conditioned Melody and Bass Generation](https://arxiv.org/abs/2511.08755v1)|该文本描述的是一篇音乐生成领域的论文，而非语音领域。但根据摘要内容，其贡献点可总结为以下三点：<br/><br/>1. **模型比较**：系统评估了五种基于Transformer的和弦条件生成方法（无和弦条件、独立声部生成、低音优先生成、旋律优先生成、联合生成），为音乐生成任务提供了不同策略的对比基准。  <br/>2. **理论指标设计**：提出结合音乐理论的评价体系，量化分析音高内容、音程大小和和弦音使用等音乐特征，提升生成结果的可评估性。  <br/>3. **效果验证**：实验证明和弦条件化对音乐风格的音高内容和和弦音使用特征的还原效果显著，尤其low-first模型在风格一致性上表现更优。  <br/><br/>总结（100字以内）：  <br/>该文对比了五种基于Transformer的和弦条件音乐生成方法，设计音乐理论导向的评估指标，验证了和弦条件化对风格特征的提升作用，尤其突出low-first模型的优势。|
|2511.08261v1|[Uncertainty Calibration of Multi-Label Bird Sound Classifiers](https://arxiv.org/abs/2511.08261v1)|总结：该研究系统评估了生物声学多标签分类器的校准问题，揭示校准差异及模型特性，并提出简单有效的方法提升校准性能，强调不确定性校准在实际应用中的重要性。<br/><br/>贡献点：<br/>1. 首次系统性评估生物声学领域多标签分类器的校准问题，填补该领域校准研究的空白。<br/>2. 提出使用阈值无关指标（ECE、MCS）和判别指标（cmAP）的混合评估框架，全面分析校准效果。<br/>3. 发现模型校准存在显著的数据集和类别差异，揭示低频类别反而更易校准的反直觉现象。<br/>4. 验证简单后处理校准方法（如Platt scaling）的可行性，证明小规模标注校准集即可显著改善校准效果。|
|2511.07336v6|[AcousTools: A 'Full-Stack', Python-Based, Acoustic Holography Library](https://arxiv.org/abs/2511.07336v6)||
|2511.07336v5|[AcousTools: A 'Full-Stack', Python-Based, Acoustic Holography Library](https://arxiv.org/abs/2511.07336v5)|总结（100字以内）：  <br/>本文提出AcousTools，一个基于Python的声学全息全栈解决方案，整合了建模、相位计算、场分析与硬件控制，旨在成为声学全息领域的标准库，提升研究效率与方法可比性。<br/><br/>贡献点：  <br/>1. **提出全栈框架**：定义声学全息的完整流程（建模、相位计算、场分析、硬件控制），填补现有工具的集成空白。  <br/>2. **开发AcousTools库**：首次提供覆盖所有关键步骤的Python库，支持从抽象到物理化的全流程开发与验证。  <br/>3. **推动标准化**：通过统一接口和全面功能，有望成为声学全息领域的标准代码库，降低研究门槛。  <br/>4. **方法可比性工具**：为研究人员提供框架，直观比较不同方法在全栈中的位置，促进技术迭代与协作。|
|2511.07336v4|[AcousTools: A 'Full-Stack', Python-Based, Acoustic Holography Library](https://arxiv.org/abs/2511.07336v4)|总结：  <br/>AcousTools是首个全栈声学全息软件库，集成建模、仿真、硬件控制等功能，推动新型应用开发与研究方法比较。<br/><br/>贡献点：  <br/>1. 提出首个完整的“全栈”声学全息解决方案，覆盖从建模到硬件控制的全流程（含声场分析、相位检索等）。  <br/>2. 开发Python基库AcousTools，集成声学传播仿真、力场预测、散射模式分析等核心功能。  <br/>3. 实现多应用场景支持（触觉、体积显示、药物输送等），填补现有工具碎片化问题。  <br/>4. 通过易用性语言（Python）降低研究门槛，促进科研复现与创新应用开发。  <br/>5. 提供方法比较框架，使研究者能直观理解不同技术在声学全息体系中的定位。|
|2511.07336v3|[AcousTools: A 'Full-Stack', Python-Based, Acoustic Holography Library](https://arxiv.org/abs/2511.07336v3)|贡献点总结（100字以内）：  <br/>本文提出AcousTools，首个面向声学全栈的Python库，涵盖建模、相位检索、声场分析与硬件控制，填补现有工具在全流程整合的空白，推动新型声学应用开发与研究方法比较。  <br/><br/>分点贡献：  <br/>1. **提出全栈解决方案**：首次定义声学全栈流程（从抽象到物理化），覆盖设置、传播建模、相位检索、声场分析、硬件控制等核心环节。  <br/>2. **开发AcousTools库**：构建首个集成化Python工具，支持声学全栈需求，填补领域中缺乏统一软件平台的空白。  <br/>3. **功能全面性**：展示库在模拟声学行为、预测力场和散射模式等关键任务上的能力，满足多应用场景（触觉、显示、制造等）。  <br/>4. **易用性设计**：基于Python语言，降低研究门槛，提升开发与验证效率。  <br/>5. **推动研究创新**：为研究人员提供标准化工具，促进新型应用开发及现有方法的对比与复现。  <br/>6. **方法可视化**：通过全栈框架支持技术方法的对比与分析，增强跨研究的可理解性和互操作性。|
|2511.07336v2|[AcousTools: A 'Full-Stack', Python-Based, Acoustic Holography Library](https://arxiv.org/abs/2511.07336v2)|**贡献点：**  <br/>1. **提出全栈解决方案**：首次整合声学全息的全流程（建模、相位检索、声场分析、硬件控制），填补现有工具碎片化的缺陷。  <br/>2. **开发AcousTools库**：基于Python的开源框架，支持中空声学应用的完整开发需求，提供统一的代码接口。  <br/>3. **多场景支持**：覆盖空中触觉、体积显示、生物医学等应用，增强跨领域研究的可行性。  <br/>4. **提升研究效率**：通过标准化工具加速新型应用开发与成果复现，降低技术门槛。  <br/>5. **方法比较框架**：为研究者提供直观的全栈视角，便于理解与对比不同方法的适配性。  <br/><br/>**总结（100字内）：**  <br/>本研究提出首个全栈声学全息软件框架AcousTools，整合建模、控制等全流程，支持多场景应用，推动新型技术研发与跨领域研究。|
|2511.05817v2|[TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech](https://arxiv.org/abs/2511.05817v2)|贡献点：<br/>1. 识别文本提示对设计师创意流的阻碍，提出多模态交互需求  <br/>2. 开发TalkSketch系统，融合手绘与实时语音输入的创新设计  <br/>3. 构建上下文感知的AI响应机制，实现动态交互支持  <br/>4. 重构GenAI工具定位，强调其在设计过程中的参与价值  <br/><br/>总结：TalkSketch通过多模态交互提升设计创意流程，解决文本提示局限性，展示GenAI在设计过程中的深度参与潜力。|
|2511.05171v2|[Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models](https://arxiv.org/abs/2511.05171v2)|总结（100字以内）:  <br/>本文提出通过模型合并策略提升NatureLM的指令跟随灵活性，同时保持生物声学领域知识，并在零样本分类任务中实现超过200%的相对性能提升，创闭集零样本分类新纪录。<br/><br/>贡献点:  <br/>1. **提出模型融合策略**：通过插值融合NatureLM与其基础语言模型，解决单模型在多任务指令下的性能退化问题。  <br/>2. **恢复指令跟随能力**：有效提升模型对复合指令（如同时请求通用名和科学名）的适应性，减少准确率下降。  <br/>3. **增强零样本泛化**：在未见物种的闭集零样本分类任务中实现显著性能提升（>200%相对改进），推动生物声学模型的通用性。  <br/>4. **验证效果与效率**：实验表明该策略仅需微小计算开销即可恢复关键能力，且在基准测试中达到新SOTA水平。|
|2511.00279v2|[LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279v2)||
|2510.21004v2|[Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004v2)|总结：  <br/>本研究系统评估了FOICE生成语音的检测效果，提出针对性微调策略并分析其泛化能力，揭示现有检测技术的不足，推动下一代音频深度伪造检测方法的发展。<br/><br/>贡献点：  <br/>1. **首次系统评估**：对FOICE生成语音的检测能力进行全面评估，证实主流检测器在清洁与噪声环境下均难以有效识别。  <br/>2. **针对性微调策略**：设计捕捉FOICE特有特征的微调方法，显著提升检测准确率。  <br/>3. **泛化能力分析**：研究微调后检测器对未知合成工具的适应性，发现专精FOICE与保持鲁棒性的平衡问题。|