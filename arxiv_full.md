|Source|Title|Summary|
|---|---|---|
|2506.23986v2|[StreamFlow: Streaming Flow Matching with Block-wise Guided Attention   Mask for Speech Token Decoding](http://arxiv.org/abs/2506.23986v2)|**贡献点总结**  <br/>1. 提出StreamFlow架构，实现基于扩散变压器的流式流匹配，解决传统框架的全局感受野限制。  <br/>2. 设计局部块状注意力掩码策略，通过分块处理和分层结合优化长序列生成中的外推问题。  <br/>3. 实验验证该方法在保持语音质量的同时，显著降低推理时间与第一包延迟，优于同类流式方法。  <br/><br/>**总结（100字以内）**  <br/>提出StreamFlow，结合扩散变压器与局部块状注意力机制，解决流式语音生成的长序列外推和实时性挑战，实现高质量音频合成与低延迟推理。|
|2506.23986v1|[StreamFlow: Streaming Flow Matching with Block-wise Guided Attention   Mask for Speech Token Decoding](http://arxiv.org/abs/2506.23986v1)|**贡献点：**  <br/>1. 提出**StreamFlow**框架，首次将**扩散变压器（DiT）**与**流式流匹配（Flow Matching）**结合，实现高效的流式语音生成。  <br/>2. 设计**局部块状接收场策略**，通过分块处理和**块间注意力掩码**，解决长历史依赖导致的序列外推问题。  <br/>3. 引入**分层组合机制**，跨不同DiT块调节接收场，平衡流式生成与音频质量。  <br/>4. 实验证明在保持推理效率的同时，**语音质量优于现有流式方法**，且**首包延迟仅180ms**，显著提升实时交互体验。  <br/>5. 实现**非流式方法的性能水平**，同时具备流式处理能力，解决传统方法在流式场景下的局限性。  <br/><br/>**总结（100字以内）：**  <br/>StreamFlow创新性地结合扩散变压器与流式流匹配，通过局部块状接收场和分层注意力机制解决长序列生成问题，实现在保持高质量的同时实现低延迟流式语音生成，突破传统方法的局限。|
|2506.23873v1|[Emergent musical properties of a transformer under contrastive   self-supervised learning](http://arxiv.org/abs/2506.23873v1)|总结：  <br/>该研究挑战了对比自监督学习在局部MIR任务中的局限性假设，发现结合ViT-1D和NT-Xent损失的模型在局部任务中表现优异，并揭示了Transformer在音乐特征提取与序列建模中的潜在能力。<br/><br/>贡献点：  <br/>1. **方法创新**：提出将轻量级视觉Transformer（ViT-1D）与对比自监督学习（contrastive SSL）结合，应用于局部MIR任务（如和弦估计）。  <br/>2. **性能验证**：通过实验表明，即使未专门训练，ViT-1D的序列标记在局部任务中表现优于类标记，且NT-Xent损失在序列建模中有效。  <br/>3. **特征分析**：揭示序列标记中通过权重共享提取的音乐属性（如起始点），以及不同Transformer层捕捉不同音乐维度（通过注意力机制和自相似性矩阵）。  <br/>4. **理论推进**：强调对比自监督学习在序列建模中的潜在优势，推动对Transformer在音乐理解中能力的深入解释，而非单纯追求性能优化。|
|2506.23670v1|[Efficient Interleaved Speech Modeling through Knowledge Distillation](http://arxiv.org/abs/2506.23670v1)|总结：  <br/>该论文提出通过层对齐蒸馏压缩大模型，开发出高效的小参数语音生成模型TinyWave，实现性能与规模的平衡，并开源相关资源促进研究。<br/><br/>贡献点：  <br/>1. **方法创新**：提出层对齐蒸馏（layer-aligned distillation）技术，通过匹配隐藏状态、注意力图和软化logits实现模型压缩，保持性能最小损失。  <br/>2. **模型设计**：推出TinyWave模型家族，含2B参数，支持语音到语音及混合语音-文本生成任务（如speech-only和interleaved生成）。  <br/>3. **性能验证**：在Libri-Light数据集上表现接近教师模型（1.4 normalized perplexity差距），在StoryCloze和SALMon任务上超越大小匹配的基线模型（93-97%准确率）。  <br/>4. **部署优化**：针对商用硬件优化模型，适用于实时对话系统、辅助技术及低资源场景。  <br/>5. **开源贡献**：公开模型、训练代码和评估脚本，促进语音生成领域的可复现研究。|
|2506.23582v1|[RELATE: Subjective evaluation dataset for automatic evaluation of   relevance between text and audio](http://arxiv.org/abs/2506.23582v1)|**贡献点总结**（100字以内）：  <br/>提出RELATE数据集，首度通过主观评价量化文本与音频的相关性；开发基于合成音频的评分预测模型，显著优于传统CLAPScore；验证模型泛化能力，适用于多类声音场景。<br/><br/>**分点贡献：**<br/>1. **构建首个主观评估的开放数据集**  <br/>   打造RELATE数据集，通过人工标注对文本与音频的相关性进行系统性主观评价，填补领域中缺乏高质量主观数据的空白。<br/><br/>2. **提出改进的音频相关性评分模型**  <br/>   设计一种自动预测合成音频主观评分的模型，相较传统CLAPScore在评估精度上实现显著提升，并扩展其适用范围至多种声音类别。<br/><br/>3. **验证模型通用性和评估趋势**  <br/>   通过实验验证模型在不同声音类别中的通用性，表明其性能优势具有广泛适用性，推动文本到音频相关性评估方法的发展。|
|2506.23437v1|[From Large-scale Audio Tagging to Real-Time Explainable Emergency   Vehicle Sirens Detection](http://arxiv.org/abs/2506.23437v1)|**贡献点分点总结：**  <br/>1. 提出**E2PANNs**模型：基于PANNs框架设计轻量级CNN，专为二分类EV警报检测优化，提升计算效率。  <br/>2. 构建专用数据集：开发AudioSet EV子集，用于模型微调与多数据集验证，解决数据不足问题。  <br/>3. 实验评估全面：含消融研究、跨域基准测试及边缘设备实时部署，验证模型普适性与性能。  <br/>4. 可解释性分析：利用Guided Backpropagation和ScoreCAM揭示模型特征提取机制，增强可靠性。  <br/>5. 实测性能优化：通过帧级与事件级检测指标及误报分析，确保模型在实际场景中的准确性与稳定性。  <br/>6. 应用价值突出：实现边缘端高效监控，适用于安全关键场景（如智能交通、自动驾驶），达到领域SOTA水平。  <br/><br/>**总结（100字以内）：**  <br/>本文提出轻量级E2PANNs模型，专为高效EV警报检测优化，构建专用数据集并验证其跨域与实时性能，结合可解释性分析提升可靠性，最终在边缘硬件上实现高精度、低延迟应用，推动智能交通系统发展。|
|2506.23371v1|[Investigating an Overfitting and Degeneration Phenomenon in   Self-Supervised Multi-Pitch Estimation](http://arxiv.org/abs/2506.23371v1)|总结（100字以内）:  <br/>本文提出结合音高不变与等变性质的自监督目标，改进多音高估计框架，在封闭训练中取得显著提升，同时揭示了自监督数据导致的模型退化问题，并分析其潜在原因。<br/><br/>贡献点：<br/>1. **提出混合自监督目标**：首次将音高不变性（pitch-invariant）和音高等变性（pitch-equivariant）性质融入监督学习的MPE框架，构建更具鲁棒性的训练范式。<br/>2. **验证性能提升**：在封闭训练条件下证明了该联合训练策略的有效性，显著优于传统监督学习方法。<br/>3. **发现过拟合与退化现象**：揭示了模型在监督数据上过拟合，而自监督数据上性能退化的矛盾现象。<br/>4. **分析机制与提供见解**：深入探讨该现象的成因，并提出对自监督学习与监督学习结合的理论思考，为未来研究提供方向。|
|2506.23130v1|[The Florence Price Art Song Dataset and Piano Accompaniment Generator](http://arxiv.org/abs/2506.23130v1)|**贡献点（分点）：**  <br/>1. **建立数字档案**：发布包含Florence Price 112首声乐与钢琴作品的全面数据集，涵盖MuseScore、MusicXML、MIDI及PDF格式，为学术研究与音乐再创作提供资源。  <br/>2. **风格建模创新**：利用该数据集微调符号音乐生成模型，开发专用工具生成符合Price风格的钢琴伴奏，填补了历史音乐风格数据驱动建模的空白。  <br/>3. **实验验证有效性**：通过盲听实验对比模型与基线方法，证明所生成伴奏在风格相似性上显著优于基线模型，提供可量化的评估依据。  <br/>4. **公开研究工具**：开源模型（Florence Price Piano Accompaniment Generator）及数据集，推动对历史音乐家风格的算法研究与应用。  <br/><br/>**总结（100字以内）**：  <br/>该研究发布Florence Price的声乐作品数据集并开发风格化钢琴伴奏生成模型，通过实验验证模型有效性，为音乐历史研究与AI创作提供资源与工具。|
|2506.23030v1|[VisionScores -- A system-segmented image score dataset for deep learning   tasks](http://arxiv.org/abs/2506.23030v1)|总结（100字以内）:  <br/>VisionScores 是首个系统分段的钢琴乐谱数据集，涵盖双手作品，结合图形相似性与创作模式，提供两种对比场景及标准化图像格式，还包含元数据和未分段乐谱，支持多维度分析与模型训练。<br/><br/>贡献点:<br/>1. **首个系统分段的图像评分数据集**：定义了首个按音乐系统划分的乐谱图像数据集，解决传统数据集结构单一的问题。  <br/>2. **领域特异性设计**：专为双手钢琴作品构建，深度结合音乐创作的图形特征与乐器依赖性规律。  <br/>3. **双场景对比分析**：提供两组对比样本（不同作者同曲目类型 vs 同作者不同曲目类型），利于研究作者风格与作品结构的关系。  <br/>4. **标准化图像格式**：统一灰度JPG格式与像素分辨率（128×512），提升模型训练与跨数据集比较的兼容性。  <br/>5. **多维度数据支持**：包含系统顺序、作品元数据、分段样本与未分段完整乐谱，满足多样化的分析需求。|
|2506.22972v1|[Adaptable Non-parametric Approach for Speech-based Symptom Assessment:   Isolating Private Medical Data in a Retrieval Datastore](http://arxiv.org/abs/2506.22972v1)|**贡献点：**  <br/>1. 提出NoNPSA框架：首个基于非参数方法的语音症状评估系统，解决参数模型隐私和适应性问题。  <br/>2. 隐私保护机制：通过隔离医疗数据至检索存储库，避免隐私信息泄露到模型参数中。  <br/>3. 高效数据更新：框架支持无需重新训练即可更新医疗数据，提升系统灵活性。  <br/>4. 自监督特征提取：使用通用数据集预训练的SSL模型提取通用特征，实现相似性检索。  <br/>5. 元数据优化：结合元数据过滤与标签关联，改进评估分数的准确性。  <br/>6. 实验验证：证明NoNPSA在性能上与微调SSL方法相当，同时具备更强的隐私性、更新效率和适应性。  <br/><br/>**总结：**  <br/>本研究提出首个非参数语音症状评估框架，通过隐私保护机制和高效数据更新，解决了参数模型的局限性，验证了非参数方法在医疗领域的可行性。|
|2506.22944v1|[Feasibility of spectral-element modeling of wave propagation through the   anatomy of marine mammals](http://arxiv.org/abs/2506.22944v1)|总结：  <br/>本研究首次应用3D谱元法模拟海豚头超声波传播，克服有限元法局限，为海洋生物学研究提供高效可扩展工具，推动回声定位及噪声污染等领域的探索。<br/><br/>贡献点：  <br/>1. **首次应用3D SEM**：开发了首个针对海豚头超声波传播的3D谱元法仿真模型，填补了该方法在生物声学研究中的空白。  <br/>2. **方法优势验证**：证明SEM在高频率仿真的指数收敛性和并行计算效率，优于传统FEM的线性系统求解与收敛性问题。  <br/>3. **高精度网格构建**：基于CT扫描数据创建详细六面体网格，精确捕捉声学脂肪、下颌等复杂解剖结构。  <br/>4. **波型模拟验证**：通过平面波和球面波仿真实验，验证SEM在超声时域建模中的有效性与可靠性。  <br/>5. **跨学科应用价值**：为海洋生物学研究提供新工具，助力回声定位机制、人类活动噪声影响及听觉系统研究，具有重要保育意义。|
|2506.22858v1|[Mind the Gap: Entity-Preserved Context-Aware ASR Structured   Transcriptions](http://arxiv.org/abs/2506.22858v1)|总结：  <br/>提出基于重叠上下文窗口的语义增强训练方法，提升ASR系统对命名实体和格式化的识别能力，并在跨分段边界处理中取得显著效果。<br/><br/>贡献点：  <br/>1. **提出重叠上下文扩展方法**：通过滑动5秒重叠窗口，构建40秒有效语义窗口，提升长文本中的实体识别与格式准确性。  <br/>2. **解决跨分段实体问题**：将跨越分段边界的实体完整分配至右块，确保格式正确性。  <br/>3. **引入实体标注训练数据**：嵌入实体标签的丰富数据使模型同时学习识别与类型化格式化能力。|
|2506.22810v1|[A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech   Recognition](http://arxiv.org/abs/2506.22810v1)|**总结**（100字以内）：  <br/>该研究提出一种新的自训练方法提升Whisper模型在失语症语音识别中的性能，并构建了大规模多样化数据集推动SAP挑战赛，系统在Word Error Rate和Semantic Score上取得第二名成绩。<br/><br/>**贡献点**：  <br/>1. **构建大规模数据集**：发布了一个大且多样化的英语失语症语音数据集（SAP），突破了传统数据集局限于孤立词和短句的限制，为研究提供更全面的训练材料。  <br/>2. **提出自训练方法**：设计了一种新颖的自训练策略，通过扩展训练数据并增强模型对不完整语音片段的鲁棒性，提升长时失语症语音的识别效果。  <br/>3. **推动独立系统发展**：基于SAP数据集，开发了无需依赖特定语音或文本的失语症语音识别系统，解决了此前仅适用于命令交互和语音适应的局限性。  <br/>4. **取得显著实验成果**：在SAP挑战赛中，系统在Word Error Rate和Semantic Score指标上均获得第二名，验证了方法的有效性和竞争力。|
|2506.22789v1|[WavShape: Information-Theoretic Speech Representation Learning for Fair   and Privacy-Aware Audio Processing](http://arxiv.org/abs/2506.22789v1)|**贡献点：**  <br/>1. **提出 WavShape 框架**：首次结合信息论与自监督学习，设计了一种兼顾公平性、隐私性和任务相关性的语音嵌入学习方法。  <br/>2. **创新 MI 优化策略**：通过 Donsker-Varadhan 公式实现互信息估计，构建 MI 驱动的编码器，系统性过滤敏感属性。  <br/>3. **实验证明有效性**：在三个公开数据集上验证，显著降低嵌入与敏感属性的互信息（最高 81%），同时保留 97% 以上任务相关信息。  <br/>4. **推动语音系统发展**：为构建公平、隐私保护且资源高效的语音系统提供了理论依据和技术路径。  <br/><br/>**总结（100字以内）**：  <br/>WavShape 通过信息理论方法优化语音嵌入，有效降低敏感属性泄露风险并提升公平性，同时保持任务关键信息，推动语音系统的隐私保护与资源效率发展。|
|2506.22661v1|[Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for   Music Identification](http://arxiv.org/abs/2506.22661v1)|**贡献点：**  <br/>1. **提出基于音乐信号属性与真实房间声学的自监督策略**：通过结合音频内容本身的信号特性及实际声学环境，优化神经音频指纹方法的监督机制，提升模型鲁棒性。  <br/>2. **首次系统评估度量学习方法在AFP中的适用性**：对比分析多种度量学习损失函数（如NT-Xent、三元组损失等），验证自监督三元组损失在性能上的优势。  <br/>3. **揭示多正样本对锚点训练的影响差异**：发现不同损失函数对训练过程中多正样本策略的响应存在显著差异，为模型设计提供新视角。  <br/>4. **在合成与真实数据集上实现SOTA性能**：方法在大型合成退化数据集和真实音乐场所录制的场景中均取得当前最优结果。  <br/><br/>**总结：**  <br/>本文通过优化自监督策略和系统评估度量学习方法，在音频指纹领域实现性能突破，揭示了多正样本训练对不同损失函数的影响差异，并在合成与真实场景中达成SOTA结果。|
|2506.22646v1|[Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR](http://arxiv.org/abs/2506.22646v1)|**贡献点：**  <br/>1. 提出无需显式说话人查询的流式多说话人语音识别方法，简化传统依赖目标说话人嵌入或报名音频的流程。  <br/>2. 引入动态说话人语音活动预测机制，实现对单个ASR实例的实时自适应。  <br/>3. 通过注入说话人监督生成的说话人特定内核至ASR编码器层，增强对目标说话人的建模能力。  <br/>4. 支持处理完全重叠的语音信号，提升流式场景下的鲁棒性。  <br/>5. 在离线和流式场景均取得SOTA性能，验证方法对严重语音重叠问题的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种无需显式说话人查询的流式多说话人语音识别方法，利用说话人特定内核动态适应ASR模型，有效处理完全重叠语音，实现离线和流式场景下的高性能识别。|
|2506.22628v1|[Evaluating Sound Similarity Metrics for Differentiable, Iterative   Sound-Matching](http://arxiv.org/abs/2506.22628v1)|总结：  <br/>该研究提出可微分迭代声音匹配方法，系统评估不同合成器与损失函数的组合效果，发现损失函数性能高度依赖于合成器类型，强调需开发定制化相似性指标而非通用方案。<br/><br/>贡献点：  <br/>1. **提出可微分迭代声音匹配框架**：结合传统手艺人声设计流程与机器学习技术，实现基于损失函数的自动化声音生成优化。  <br/>2. **系统评估损失函数适应性**：针对三种不同类型合成器（减法/加法/AM），测试四种新旧可微分损失函数的组合，共16组实验。  <br/>3. **多维性能评估指标**：采用参数差异、频谱距离和人工评分三重指标，发现其结果具有中度一致性。  <br/>4. **揭示合成器-损失函数依赖关系**：通过事后分析，证明损失函数效果显著受合成器类型影响，主张针对性开发相似性度量。|
|2506.22362v1|[DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding](http://arxiv.org/abs/2506.22362v1)|总结：DiffSoundStream通过语义引导的神经编码器和潜扩散模型优化，显著提升非流式语音生成效率，在50token/s下实现与标准模型100token/s相当的语音质量，并通过步长蒸馏减少采样步骤。<br/><br/>贡献点：<br/>1. 提出语义引导的神经编码器（Conditioning neural codec on semantic tokens），通过减少语义与声学token间冗余提升编码效率；<br/>2. 引入潜在扩散模型（Latent diffusion models）实现从语义和粗糙声学token的高质量波形合成；<br/>3. 在50token/s的非流式场景中达到与标准SoundStream（100token/s）等效的语音质量；<br/>4. 开发步长蒸馏方法（Step-size distillation），仅用4步扩散采样即可实现少量质量损失的高效推理。|
|2506.22311v1|[Reconstructing Intelligible Speech from the Pressure Sensor Data in   HVACs](http://arxiv.org/abs/2506.22311v1)|**贡献点总结（100字以内）**：  <br/>本研究提出WaLi方法，利用低采样率压力传感器数据重建可理解语音，突破现有热词检测局限。通过复杂值模型和噪声补偿技术，有效提升语音质量并揭示其在隐私安全中的潜在风险。<br/><br/>---<br/><br/>**分点贡献**：  <br/>1. **语音重建创新**：提出WaLi模型，首次实现从压力传感器（0.5kHz采样率）低分辨率数据中恢复可理解语音，超越传统仅能检测热词/短语的技术。  <br/>2. **复杂模型设计**：采用复杂值Conformer与CGAB模块，捕捉语音信号中音素间（inter-phoneme）及音素内（intra-phoneme）依赖关系，提升低频混叠成分的重建质量。  <br/>3. **噪声抑制技术**：针对HVAC系统引入的瞬时噪声，通过恢复低频混叠分量的清洁幅度与相位，增强语音信号的清晰度。  <br/>4. **实证效果验证**：在真实场景中验证方法有效性，达到0.5kHz~8kHz上采样下的LSD 1.24和NISQA-MOS 1.78，凸显隐私威胁的严重性。|
|2506.22237v1|[Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll   and CQT Representations](http://arxiv.org/abs/2506.22237v1)|总结：  <br/>提出基于CRNN的神经网络方法，结合频谱与时间特征，显著提升MIDI到音频的对齐准确率，与DTW结合增强鲁棒性，推动语音领域音频-MIDI同步技术发展。<br/><br/>贡献点：  <br/>1. **提出CRNN架构**：首次采用卷积循环神经网络处理未对齐的钢琴roll和频谱图，联合提取频谱与时间特征以实现音频-MIDI同步。  <br/>2. **构建增强数据集**：创建包含模拟人类时间误差的MIDI文件数据集，提升模型对真实场景的适应能力。  <br/>3. **性能提升**：在多种容限窗口下，模型对齐准确率比传统DTW提高20%。  <br/>4. **方法整合**：将DTW与CRNN结合，增强同步结果的鲁棒性和一致性。  <br/>5. **领域推动**：验证了神经网络在音频-MIDI同步任务中的有效性，为语音处理相关技术提供新思路。|
|2506.22194v1|[Cross-lingual Data Selection Using Clip-level Acoustic Similarity for   Enhancing Low-resource Automatic Speech Recognition](http://arxiv.org/abs/2506.22194v1)|**贡献点分点列出：**  <br/>1. **提出Clipping-wise Acoustic Token Distribution Similarity (CATDS)**：一种基于声学特征的细粒度数据选择方法，突破传统语言级别相似性限制。  <br/>2. **精准匹配目标语言需求**：通过识别声学相关的捐赠剪辑，实现与目标语言的更优对齐。  <br/>3. **结合SSL模型表示**：方法设计与自监督学习模型的特征表示一致，提升语义对齐效果。  <br/>4 ***挑战性样本优化*：提供更具挑战性但有价值的训练样本，改善低资源语言模型性能。  <br/>5. ***实验验证有效性*：证明CATDS在低资源ASR任务中优于传统方法，并能有效利用先前被视为有害的捐赠语言。  <br/><br/>**总结（100字以内）：**  <br/>本文提出CATDS方法，通过细粒度声学标记分布相似度优化低资源ASR的捐赠数据选择，提升模型性能，并解决传统方法忽视剪辑级差异的问题，成功利用之前被排除的捐赠语言。|
|2506.22143v1|[SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition](http://arxiv.org/abs/2506.22143v1)|总结：  <br/>本文提出SAGE数据生成方法与基于ER的泛化增强技术，结合外部语言模型并优化少样本训练，显著提升阿拉伯语和英语代码切换任务的语音识别性能，超越大型多语言模型。<br/><br/>贡献点：  <br/>1. **提出SAGE数据生成方法**：通过修改音频拼接技术，生成人工阿拉伯语-英语代码切换语料，解决数据稀缺问题。  <br/>2. **引入经验回放机制**：设计ER启发的策略，提升方言阿拉伯语与代码切换语音的泛化能力，缓解灾难性遗忘。  <br/>3. **融合域外语言模型**：整合3-gram语言模型，将阿拉伯语-英语代码切换任务的平均WER从31.7%降至26.6%。  <br/>4. **少样本微调优化**：针对代码切换任务进行少样本微调，进一步降低WER 4.9%。  <br/>5. **性能超越现有模型**：所提方法在阿拉伯语-英语代码切换任务中达到31.1% WER，优于USM和Whisper-large-v2（规模大十倍以上）5.5%和8.4%。|
|2506.22023v1|[Robust and Efficient Autoregressive Speech Synthesis with Dynamic   Chunk-wise Prediction Policy](http://arxiv.org/abs/2506.22023v1)|总结（100字以内）：  <br/>提出DCAR框架，通过动态分块机制与chunk-to-frame注意力解决长序列语音合成的稳定性与效率问题，实现质量与速度双提升，为实时语音合成提供新范式。<br/><br/>贡献点：  <br/>1. **动态分块机制**：引入chunk-wise自回归框架，有效解决传统模型在处理长序列时的帧到帧注意力不稳定性问题。  <br/>2. **chunk-to-frame注意力**：通过多标记预测训练，构建动态注意力机制，提升生成内容的连贯性与语义一致性。  <br/>3. **轻量模块设计**：采用on-policy训练的轻量模块实现快速分块预测，降低计算资源消耗并提高推理效率。  <br/>4. **自适应预测跨度**：动态调整预测范围，弱化序列长度依赖，显著减少生成延迟。  <br/>5. **实证效果突破**：在测试集上实现72.27%的语音可理解性提升和2.61倍推理速度优化，验证方法有效性。  <br/>6. **系统通用性分析**：证明DCAR可作为下一代语音合成系统的通用基础架构，具有广泛的应用前景。|
|2506.21990v1|[Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech   Transcription in the Cockpit](http://arxiv.org/abs/2506.21990v1)|总结：  <br/>本研究提出针对驾驶舱对话转录的规范化方案和LoRA高效微调方法，结合多语言数据集显著提升Whisper模型的语音识别准确率，为特定领域ASR应用提供有效改进。<br/><br/>贡献点：  <br/>1. **多语言驾驶舱对话数据集构建**：收集并手动标注了约85分钟的模拟器语音和130分钟的飞行员访谈录音，涵盖德语和英语双语环境，覆盖特定领域的复杂词汇和多语言场景。  <br/>2. **多阶段规范化方案设计**：提出基于领域特征的多种规范化方法，有效优化转录文本，显著降低Word Error Rate（WER）。  <br/>3. **高效微调技术应用**：采用Low-Rank Adaptation（LoRA）进行性能高效的模型微调，提升ASR任务在驾驶舱场景下的适应性。  <br/>4. **性能提升验证**：通过规范化与LoRA联合优化，将预训练Whisper Large模型的WER从68.49%降至26.26%，验证了方法的有效性。  <br/>5. **特定领域语音识别改进**：针对驾驶舱场景的语境特殊性（如专有名词、多语言混杂等），提出可迁移的优化策略，填补了现有ASR模型在该领域的应用空白。|
|2506.21951v1|[HighRateMOS: Sampling-Rate Aware Modeling for Speech Quality Assessment](http://arxiv.org/abs/2506.21951v1)|**贡献点：**  <br/>1. 提出**HighRateMOS**，首个非侵入式MOS模型显式考虑采样率变化。  <br/>2. 整合**四类多模态特征**：可学习采样率嵌入、Wav2vec 2.0自监督嵌入、多尺度CNN谱特征和MFCC特征。  <br/>3. 在**AudioMOS 2025 Track3**中采用集成框架，五项指标排名首位。  <br/>4. 通过实验验证**直接建模采样率**可提升语音质量预测的鲁棒性与采样率无关性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个考虑采样率的非侵入式语音质量评估模型HighRateMOS，结合多模态特征与集成方法，在多个指标中领先，并证明直接建模采样率能增强预测的鲁棒性。|
|2506.21921v1|[Explainable anomaly detection for sound spectrograms using pooling   statistics with quantile differences](http://arxiv.org/abs/2506.21921v1)|**贡献点：**<br/>1. 提出针对频谱图的特定异常检测方法，区别于传统处理原始音频信号的范式。  <br/>2. 构建基于统计评估的理论框架，为异常检测提供可解释的统计依据。  <br/>3. 强调方法的内在可解释性，满足工业场景中对透明性和人类可理解性的需求。  <br/>4. 解决智能算法在工业应用中的争议，为预测性维护与质量保障提供替代方案。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出一种面向频谱图的可解释性异常检测方法，结合统计评估与理论动机，旨在解决工业应用中对透明性与人类专家参与的需求矛盾，为预测性维护提供可靠、易理解的解决方案。|
|2506.21712v1|[Identifying Speaker Information in Feed-Forward Layers of   Self-Supervised Speech Transformers](http://arxiv.org/abs/2506.21712v1)|**贡献点：**  <br/>1. 识别自监督语音Transformer中与说话人信息相关的神经元，填补了该领域研究空白。  <br/>2. 提出通过k-means聚类和i-vectors分析神经元关联性，揭示其与音素和性别类别对应关系。  <br/>3. 证明特定神经元对说话人相关任务的性能至关重要，提出保护这些神经元的修剪策略。  <br/>4. 实验验证保护关键神经元可显著提升说话人任务性能，为模型优化提供新思路。  <br/><br/>**总结：**  <br/>本文通过分析自监督语音模型中与说话人信息相关的神经元，揭示其编码机制，并提出保护关键神经元的修剪方法，有效提升说话人任务性能。|
|2506.21478v1|[SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis   with Multi-Resolution Architecture](http://arxiv.org/abs/2506.21478v1)|贡献点总结：<br/>1. 提出SmoothSinger，采用单阶段统一框架直接优化低质量合成音频，解决两阶段流程带来的退化问题  <br/>2. 创新性设计参考引导的双分支架构，利用任意基线系统的低质量音频作为参考信号提升表达性  <br/>3. 引入并行低频上采样路径增强U-Net，更精准捕捉音高轮廓与长时频谱依赖关系  <br/>4. 改进训练对齐方式，以退化真实音频替代参考音频消除时序偏差  <br/>5. 在大规模中文歌唱语料库上验证方法有效性，取得SOTA客观与主观评价结果  <br/>6. 通过系统消融实验证实各创新模块对降低伪影、提升自然度的关键作用  <br/><br/>（共98字）|
|2506.21463v1|[Aligning Spoken Dialogue Models from User Interactions](http://arxiv.org/abs/2506.21463v1)|总结：  <br/>提出语音偏好对齐框架，构建大规模语音对话数据集，微调全双工语音模型，验证反馈提升效果，并强调动态平衡对自然对话的重要性。<br/><br/>贡献点：  <br/>1. 提出面向实时语音对话的新型偏好对齐框架，解决传统文本模型难以适应语音动态（如打断、插话）的问题。  <br/>2. 构建包含15万+偏好对的多轮语音数据集，融合AI反馈标注，涵盖语言内容与时间上下文变化。  <br/>3. 利用离线对齐方法优化全双工自回归语音到语音模型，提升实时交互能力。  <br/>4. 通过大规模实验验证反馈机制对生成事实性、安全性及上下文对齐对话的有效性。  <br/>5. 部署模型进行全场景人类评估，突破单轮对话限制，评估实际应用效果。  <br/>6. 揭示多动态因素（如语言、时间）平衡对构建自然实时对话系统的关键作用。|
|2506.21440v1|[Learnable Adaptive Time-Frequency Representation via Differentiable   Short-Time Fourier Transform](http://arxiv.org/abs/2506.21440v1)|总结：  <br/>提出可微分STFT框架，实现参数梯度优化，解决传统调参方法效率低和效果差的问题，并成功与神经网络联合优化，验证了其在语音信号处理中的有效性。<br/><br/>贡献点：  <br/>1. **提出统一的可微分STFT形式**：首次将STFT参数化为可微分模型，支持通过梯度下降等方法自动优化参数，替代传统的人工或启发式调参。  <br/>2. **降低参数调优计算复杂度**：避免传统离散搜索的高计算成本，提供连续优化路径，提升效率。  <br/>3. **与神经网络联合优化**：使STFT参数与网络权重可同步训练，增强模型整体性能和适应性。  <br/>4. **实验验证有效性**：在模拟和真实语音数据上证明方法能提升时频表示质量并优化下游任务表现。|
|2506.21386v1|[Hybrid Deep Learning and Signal Processing for Arabic Dialect   Recognition in Low-Resource Settings](http://arxiv.org/abs/2506.21386v1)|总结：  <br/>提出混合模型解决阿拉伯语方言识别问题，通过实验验证MFCC+CNN优于DWT+RNN，同时指出数据集与标签的局限性，为未来研究提供方向和基线。  <br/><br/>贡献点：  <br/>1. 开发混合模型（信号处理+深度学习）应对低资源方言识别挑战；  <br/>2. 对比评估MFCC+CNN与DWT+RNN两种架构，证明频谱特征与卷积模型的优越性；  <br/>3. 构建方言过滤后的Common Voice阿拉伯语子集进行模型训练；  <br/>4. 通过实验数据（91.2% vs. 66.5%）验证方法有效性及性能差异；  <br/>5. 识别数据集规模、区域标签重叠等限制因素，提出未来改进方向（如大数据、自监督、Transformer）。|
|2506.21298v1|[Exploring Adapter Design Tradeoffs for Low Resource Music Generation](http://arxiv.org/abs/2506.21298v1)|总结（100字以内）:<br/>本文通过分析适配器配置对音乐生成模型的影响，揭示了卷积与Transformer适配器在音乐细节和结构上的差异，确定中等规模适配器的最优平衡，并比较了Mustango与MusicGen在生成质量、效率及多样性的表现，为低资源音乐生成的高效微调提供指导。<br/><br/>贡献点：<br/>1. **系统分析适配器配置**：首次系统研究不同适配器架构、位置及规模对两个AI音乐模型（MusicGen/Mustango）在两种音乐类型（Hindustani Classical/Turkish Makam）的微调效果，明确设计选择的影响机制。<br/>2. **揭示架构特性差异**：发现卷积适配器擅长捕捉细粒度音乐细节（如装饰音），Transformer适配器更优长程依赖关系保持（如结构即兴），为适配器选择提供理论依据。<br/>3. **提出资源效率平衡点**：通过实验验证，确定40M参数的中等规模适配器在表达力与生成质量间达到最佳权衡，优化了PEFT应用的计算资源分配。<br/>4. **对比模型生成特性**：量化分析Mustango与MusicGen在生成多样性、稳定性、节奏一致性及美学表现的差异，揭示其训练效率与生成质量的权衡关系。|
|2506.21269v1|[Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management:   A Study on Speed Classification in Suzhou](http://arxiv.org/abs/2506.21269v1)|**贡献点分点总结**  <br/>1. **数据集发布**：构建并公开了 Suzhou Urban Road Acoustic Dataset（SZUR-Acoustic Dataset），附带完整的数据采集协议和标注指南，提升实验透明性与可复现性。  <br/>2. **双模态特征融合模型**：提出 BMCNN（Bimodal-Feature-Fusion Deep Convolutional Neural Network），结合 Mel 频率倒谱系数和小波包能量特征，通过跨模态注意力机制优化时频信息利用。  <br/>3. **预处理策略创新**：设计自适应降噪和归一化方法，有效抑制环境噪声干扰。  <br/>4. **性能验证**：在 SZUR-Acoustic 和 IDMT-Traffic 数据集上验证模型效果，分类准确率分别达 87.56% 和 96.28%，并进行消融实验与鲁棒性测试。  <br/>5. **实际应用价值**：方法可集成至智慧城市交通管理系统，实现实时噪声监测与速度估计，助力交通优化、污染治理与可持续城市规划。  <br/><br/>**总结（100字内）**  <br/>本研究提出双模态特征融合的 BMCNN 模型，构建公开声学数据集并设计自适应预处理策略，显著提升速度分类准确率，方法可应用于智能交通噪声监测，优化交通流与城市可持续规划。|
|2506.21174v1|[Performance improvement of spatial semantic segmentation with enriched   audio features and agent-based error correction for DCASE 2025 Challenge Task   4](http://arxiv.org/abs/2506.21174v1)|总结：  <br/>该论文提出三种改进方法：引入Spectral Roll-off和Chroma特征提升声景分类；应用代理系统减少标签误判；优化数据集增强低表现类识别，实验表明CA-SDRi提升达14.7%。<br/><br/>贡献点：  <br/>1. **多特征融合**：将Spectral Roll-off和Chroma特征与Mel-spectral嵌入结合，增强音频标记模型对混合音频中细微线索的捕捉能力。  <br/>2. **代理标签校正**：设计基于代理的系统优化S5输出，有效减少误报并提升CA-SDRi指标。  <br/>3. **数据集优化**：通过剔除无关样本和引入外部数据，针对性提升低性能类别的分类准确率。|
|2506.21167v1|[A Hierarchical Deep Learning Approach for Minority Instrument Detection](http://arxiv.org/abs/2506.21167v1)|贡献点：<br/>1. **引入新模型类和策略**：提出一类新模型用于分层音乐预测，并设计多种策略整合分层结构。<br/>2. **基于权威分类体系**：采用Hornbostel-Sachs乐器分类体系构建层次化框架，提升分类结构合理性。<br/>3. **验证方法有效性**：在MedleyDB数据集（涵盖多乐器与音乐类型）上验证分层分类方法的适用性。<br/>4. **解决粗粒度检测问题**：通过弥合细粒度识别与组别识别间的差距，实现更可靠的粗粒度乐器活动检测。<br/>5. **推动领域发展**：为音乐信息检索中的乐器标注与发现提供新思路，促进后续研究。<br/><br/>总结：  <br/>本文构建基于Hornbostel-Sachs分类的分层模型，通过整合策略提升粗粒度乐器检测可靠性，验证方法在MedleyDB数据集上的有效性，推动音乐信息检索技术发展。|
|2506.21090v1|[Post-training for Deepfake Speech Detection](http://arxiv.org/abs/2506.21090v1)|总结（100字以内）:  <br/>本文提出后训练框架，结合自监督学习与领域微调，构建大规模多语言数据集，开发AntiDeepfake模型，在深度伪造语音检测中实现超越现有SOTA的性能，并开放模型及代码。  <br/><br/>贡献点:  <br/>1. **方法创新**：首创后训练策略，将通用预训练与领域特定微调结合，弥合SSL模型在深度伪造检测中的适应性差距。  <br/>2. **数据集构建**：提出包含超56,000小时真实语音及18,000小时人工合成语音的多语言数据集（覆盖100+语言），增强模型泛化能力。  <br/>3. **性能突破**：实验表明，后训练模型在未见过的深度伪造数据上表现优异，进一步微调后显著超越现有SOTA检测器。  <br/>4. **开源贡献**：提供模型检查点与代码，推动技术复现与应用，促进语音安全领域的研究进展。|
|2506.21086v1|[PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time   Stretching](http://arxiv.org/abs/2506.21086v1)|总结：  <br/>PeakNetFP是首个基于频谱峰值的神经音频指纹系统，通过层次化特征提取与对比学习提升性能，实现高效处理时间拉伸音频，在保持高识别准确率的同时显著减少参数和数据量，为音频指纹技术提供轻量化新方向。<br/><br/>贡献点：  <br/>1. **首次提出基于频谱峰值的神经音频指纹系统**（PeakNetFP），融合传统peak-based方法与神经网络优势。  <br/>2. **引入层次化点特征提取技术**，借鉴PointNet++结构提升特征表示效率与鲁棒性。  <br/>3. **采用对比学习训练策略**，借鉴NeuralFP的深度学习框架，增强模型对时间拉伸音频的适应性。  <br/>4. **实验证明高效性**：参数量仅为NeuralFP的1/100，输入数据体积缩小11倍，且保持90%+ Top-1识别率。  <br/>5. **推动音频指纹技术发展**：成功结合轻量特性与模式识别能力，为可扩展解决方案提供新范式。|
|2506.21074v1|[CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via   Dynamic Frame Rate](http://arxiv.org/abs/2506.21074v1)|**总结（100字以内）**  <br/>提出CodecSlime，首次在神经语音编解码器中实现动态帧率，无监督且架构无关，有效提升重建质量并实现比特率灵活控制。<br/><br/>**贡献点**  <br/>1. **首个动态帧率方案**：首次在神经语音编解码器中引入动态帧率（DFR）机制，解决固定帧率（FFR）导致的时序信息密度不匹配问题。  <br/>2. **无监督架构适配性**：方法独立于具体模型架构，无需额外训练，适用于多种编码器结构（如VQ-GAN）。  <br/>3. **双创新技术**：结合ScheDFR（推理优化）和Melt-and-Cool（训练优化），分别适应动态帧率调整和模型训练。  <br/>4. **显著性能提升**：在相同模型架构和比特率下，将重建WER降低46%，其他指标表现竞争力。  <br/>5. **灵活比特率控制**：单模型支持多帧率推理，实现重建质量与比特率之间的自适应权衡。  <br/>6. **开放验证资源**：提供音频样本链接，便于社区复现和验证成果。|
|2506.20995v2|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v2)|贡献点：  <br/>1. 提出分步骤的视频到音频生成方法，每一步生成对应特定声音事件的独立音频轨道，模仿传统Foley制作流程。  <br/>2. 引入文本提示引导的生成机制，结合先前生成的音频轨道作为条件，实现更精准的语义对齐。  <br/>3. 首次将“概念否定”思想应用于生成任务设计，用于结构化音效生成并避免语义重复。  <br/>4. 构建无需专用配对数据集的训练框架，利用预训练视频-音频模型提升数据可访问性。  <br/>5. 实验证明该方法可生成多组语义不同的音频轨道，显著优于现有基线的复合音频合成质量。  <br/><br/>总结：  <br/>本文提出基于概念否定的分步视频音频生成框架，通过文本引导与预训练模型降低数据依赖，实现高质量多音轨合成。|
|2506.20995v1|[Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](http://arxiv.org/abs/2506.20995v1)|**贡献点：**  <br/>1. 提出分步骤视频-音频生成方法，模拟传统Foley流程，逐个生成与视频中特定声事件对应的音频轨道。  <br/>2. 每个生成步骤为受文本提示和先前音频的引导式合成任务，实现音频生成的连贯性和多样性。  <br/>3. 引入概念否定机制，优化生成音频的语义区分度，避免重复或冗余。  <br/>4. 构建无需专用配对数据集的训练框架，利用预训练视频-音频模型提升数据可访问性。  <br/>5. 实验验证方法在生成多段语义不同的音频轨道方面优于现有基准，显著提升复合音频合成质量。  <br/><br/>**总结：**  <br/>该论文提出一种基于分步生成与概念否定机制的视频-音频合成方法，无需专用数据集，有效提升音频多样性和合成质量。|
|2506.20945v1|[A Multi-Stage Framework for Multimodal Controllable Speech Synthesis](http://arxiv.org/abs/2506.20945v1)|**贡献点总结：**  <br/>本文提出三阶段多模态可控语音合成框架，通过监督学习与知识蒸馏优化人脸编码器泛化能力，结合文本-人脸与文本-语音数据增强文本编码器多样性，实验验证其在人脸和文本驱动合成任务中的优越性。  <br/><br/>**分点贡献：**  <br/>1. **提出三阶段框架**：设计全新的多模态可控语音合成流程，融合人脸与文本模态，突破传统单模态方法的局限。  <br/>2. **提升泛化能力**：采用监督学习与知识蒸馏技术优化人脸编码器，解决数据质量导致的稳健性与泛化性问题。  <br/>3. **增强语音多样性**：通过联合训练文本编码器在文本-人脸和文本-语音数据上，扩展生成语音的风格多样性与控制粒度。  <br/>4. **实验证明有效性**：在人脸驱动与文本驱动两类任务中均优于单模态基线方法，验证框架在生成高质量语音中的优势。|
|2506.20609v1|[Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot   Recordings](http://arxiv.org/abs/2506.20609v1)|**贡献点分点总结：**  <br/>1. **低成本替代方案**：提出基于声学分析的枪声检测与分类方法，利用常见设备（如手机）而非昂贵专业系统，显著降低检测成本。  <br/>2. **大规模声学特征研究**：构建包含3459条记录的数据库，系统分析枪口爆破、冲击波等声学特性，揭示其与枪支类型、弹药及射击方向的关系。  <br/>3. **模型对比与优化**：设计SVM基线模型和CNN深度学习架构，实现枪声检测与分类的联合任务，验证CNN在清洁数据上的优势（mAP 0.58 vs. SVM 0.39）。  <br/>4. **实际应用挑战分析**：探讨数据质量与环境噪声对模型性能的影响，揭示噪声数据下的性能下降（mAP 0.35），为系统部署提供参考。  <br/>5. **长期部署目标**：提出开发实时、低成本的部署系统，为第一响应者提供关键情报，推动技术在公共安全领域的实用化。|
|2506.20361v1|[The role of audio-visual integration in the time course of phonetic   encoding in self-supervised speech models](http://arxiv.org/abs/2506.20361v1)|贡献点：  <br/>1. 首次探讨自监督学习模型（如AV-HuBERT）是否能模拟人类多模态语音感知中的音频-视觉异步性（唇动与发声的100-300ms时间差）。  <br/>2. 提出通过线性分类器分析模型输出随时间变化的语音解码性，揭示AV-HuBERT在音素信息时间可用性上早于音频模型约20ms。  <br/>3. 指出AV-HuBERT的时间分辨率不足及特征拼接机制导致其未能充分捕捉多模态感知的时间动态，为模型改进提供理论依据。  <br/><br/>总结（100字以内）：  <br/>该研究发现AV-HuBERT因时间分辨率低和特征拼接问题，未能有效模拟人类多模态语音感知中的音频-视觉异步性，限制其建模能力。|
|2506.20288v1|[Lightweight Target-Speaker-Based Overlap Transcription for Practical   Streaming ASR](http://arxiv.org/abs/2506.20288v1)|**贡献点总结（100字以内）**  <br/>提出轻量级流式ASR系统，结合说话人无关与条件模型，通过FiLM技术与合成数据训练，有效降低重叠语音WER，保持计算负载可控，实现动态说话人跟踪与高效转录。<br/><br/>---<br/><br/>**详细贡献点分列：**  <br/>1. **轻量级扩展方案**：为流式ASR系统设计目标说话人基于的轻量级模块，显著降低处理重叠语音的计算开销。  <br/>2. **双模型协同机制**：结合说话人无关（SI）模型与说话人条件（SC）模型，根据重叠场景动态切换以优化性能。  <br/>3. **高效重叠检测**：采用冻结SI模型输出的紧凑二分类器，实现低代价但高精度的重叠语音分割。  <br/>4. **FiLM与说话人嵌入整合**：利用Feature-wise Linear Modulation (FiLM)技术将说话人嵌入融入SC模型，提升目标说话人转录准确性。  <br/>5. **合成混合数据训练**：通过合成数据训练SC模型，专精转录目标说话人，无需额外标注真实重叠数据。  <br/>6. **模块复用与灵活性**：在原有ASR系统上最小修改，支持动态说话人跟踪，提升系统适用性。  <br/>7. **实验证明有效性**：在16%重叠率的捷克电视辩论数据集上，将WER从68.0%降至35.78%，总计算负载仅增加44%。  <br/>8. **实际应用价值**：提供可扩展的解决方案，适用于广播等动态多说话人场景的连续ASR服务。|
|2506.20243v1|[CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](http://arxiv.org/abs/2506.20243v1)|**贡献点总结：**  <br/>1. **提出基于语音片段的多自监督学习（SSL）模型融合方法**：集成Wav2Vec2、HuBERT和WavLM，利用其在语音建模的互补优势。  <br/>2. **创新语音分段技术**：采用Silero-VAD分割呼吸组片段，实现细粒度时序分析并缓解过分割问题。  <br/>3. **设计可学习的加权融合机制**：平衡声学与语言特征，引入片段级流利度标记（如语速、停顿时长）。  <br/>4. **构建CNN-BiLSTM框架**：捕捉多片段间的局部与长期依赖关系，增强模型表征能力。  <br/>5. **验证显著性能提升**：在Avalinguo和Speechocean762数据集上，F1得分与皮尔逊相关系数均优于基线模型。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出基于语音片段的多SSL模型融合方法，结合CNN-BiLSTM框架，有效提升非母语者语音流利度评估性能，实验表明在多个数据集上均优于现有基线。|
|2506.20190v1|[An Exploration of ECAPA-TDNN and x-vector Speaker Representations in   Zero-shot Multi-speaker TTS](http://arxiv.org/abs/2506.20190v1)|总结（100字以内）:  <br/>本研究比较了三种说话人编码器在零说话人TTS中的表现，发现原始H/ASP编码器效果最优，ECAPA-TDNN优于x-vector，但整体表现不如H/ASP，强调了实证评估的重要性并提供对比框架。<br/><br/>贡献点:  <br/>1. **对比实验设计**：在零说话人TTS框架中系统比较了H/ASP、x-vector、ECAPA-TDNN三种编码器的性能，量化其在语音合成中的效果差异。  <br/>2. **跨领域适配研究**：首次验证了主流说话人识别模型（如ECAPA-TDNN）在零说话人TTS任务中的适配性，揭示其潜在局限性。  <br/>3. **综合评估方法**：结合主观聆听测试（关注说话人相似性）与客观cosine距离分析，提供多维度的性能评估体系。  <br/>4. **数据集应用**：基于捷克语语料进行训练与跨域测试，为非英语语音合成研究提供参考案例。  <br/>5. **框架推广价值**：提出可复用的零说话人TTS对比框架，为后续研究提供实验基础与可复现性支持。|
|2506.20001v1|[Improved Topology-Independent Distributed Adaptive Node-Specific Signal   Estimation for Wireless Acoustic Sensor Networks](http://arxiv.org/abs/2506.20001v1)|总结：  <br/>本文提出TI-DANSE+算法，通过利用邻居局部部分和与树剪枝策略，显著提升非全连接WASN中分布式信号估计的收敛速度，并降低发射功率。<br/><br/>贡献点：  <br/>1. **提出TI-DANSE+算法**：改进传统TI-DANSE算法的收敛速度，适用于非全连接无线声学传感器网络。  <br/>2. **引入局部部分和利用机制**：允许节点在每轮迭代中结合邻居传递的局部部分和，增加自由度并加速收敛。  <br/>3. **设计树剪枝策略**：进一步优化网络结构，提升收敛效率。  <br/>4. **理论与实验验证**：证明TI-DANSE+在非全连接网络中达到全连接DANSE的收敛速度，同时降低功耗。|
|2506.19875v1|[Speaker Embeddings to Improve Tracking of Intermittent and Moving   Speakers](http://arxiv.org/abs/2506.19875v1)|**贡献点：**  <br/>1. 提出基于说话人嵌入的后跟踪身份再分配方法，解决动态场景下轨迹不连续导致的身份误判问题。  <br/>2. 整合多通道音频信号与初始空间轨迹信息，通过波束成形增强目标说话人信号以提取高质量嵌入。  <br/>3. 构建了一种利用注册库进行动态身份匹配的框架，提升神经网络与传统跟踪系统的适应性。  <br/>4. 系统评估了方法在实际场景中的有效性，验证其对身份分配性能的显著提升。  <br/>5. 分析了波束成形参数和输入时长对嵌入提取的影响，为优化模型提供理论依据。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出基于说话人嵌入的后跟踪身份再分配方法，结合波束成形与多通道信号，有效解决动态场景下轨迹不连续问题，提升身份识别性能，并分析了关键参数的影响。|
|2506.19774v1|[Kling-Foley: Multimodal Diffusion Transformer for High-Quality   Video-to-Audio Generation](http://arxiv.org/abs/2506.19774v1)|**贡献点总结（100字以内）:**  <br/>提出Kling-Foley多模态视频-音频生成模型，通过扩散变压器和同步模块提升语义与时间对齐；开发通用潜在音频编码器支持多场景建模；引入立体渲染增强音频空间感；开源工业级基准Kling-Audio-Eval解决数据不足问题；实验验证其在音频-视觉生成任务中达到SOTA性能。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **多模态生成模型设计**  <br/>   - 提出Kling-Foley，集成视频、音频和文本模态，通过多模态扩散变压器建模跨模态交互。  <br/><br/>2. **强化对齐能力**  <br/>   - 引入帧级视频-音频对齐机制与音频-视觉同步模块，提升语义对齐和时序同步精度。  <br/><br/>3. **通用音频编码器**  <br/>   - 开发可处理音效、语音、唱歌、音乐等多场景的潜在音频编码器（universal latent audio codec）。  <br/><br/>4. **空间感知音频渲染**  <br/>   - 采用立体渲染方法，使合成音频具有空间感（spatial presence）。  <br/><br/>5. **开源工业级基准**  <br/>   - 针对数据缺失问题，开源Kling-Audio-Eval工业级基准，完善视频-音频生成评估体系。  <br/><br/>6. **性能突破**  <br/>   - 通过流匹配目标训练，在分布匹配、语义对齐、时序对齐和音频质量等指标上达到公共模型SOTA。|
|2506.19446v1|[Vo-Ve: An Explainable Voice-Vector for Speaker Identity Evaluation](http://arxiv.org/abs/2506.19446v1)|总结：  <br/>本文提出Vo-Ve，一种可解释的语音向量嵌入技术，通过语音属性概率捕捉说话人身份，在相似性评估中表现优异并拓展了跨任务的应用潜力。<br/><br/>贡献点：  <br/>1. **提出新型语音向量嵌入**：Vo-Ve首次引入语音属性类概率的显式表示，用于捕捉说话人身份。  <br/>2. **增强可解释性**：与传统不可解释的嵌入不同，Vo-Ve通过概率分布提供语音属性的直观解释。  <br/>3. **性能竞争力验证**：实验证明Vo-Ve在说话人相似性评估中与现有技术效果相当。  <br/>4. **跨任务应用价值**：强调其高阶可解释性对提升多种语音任务评估方案的潜力。|
|2506.19441v1|[TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to   Speech Systems](http://arxiv.org/abs/2506.19441v1)|**贡献点**  <br/>1. **提出TTSDS2指标**：改进传统TTS评估指标TTSDS，具有更高的鲁棒性，并在16种对比指标中唯一实现跨领域和语言的Spearman相关系数均超过0.50。  <br/>2. **构建大规模主观数据集**：提供包含11,000+主观评分的合成语音数据集，支持多语言和领域评估。  <br/>3. **开发持续更新的测试生成管道与基准**：设计避免数据泄漏的多语言测试数据集生成机制，并提供覆盖14种语言的动态更新基准。  <br/><br/>**总结**（100字以内）：  <br/>本文提出更鲁棒的TTSDS2评估指标，并构建包含11,000+主观评分的数据集，开发持续更新的多语言测试生成管道和基准，全面提升合成语音质量评估的准确性与实用性。|
|2506.19404v1|[Loss functions incorporating auditory spatial perception in deep   learning -- a review](http://arxiv.org/abs/2506.19404v1)|**贡献点总结：**  <br/>1. 系统综述了结合空间感知线索（如ITD、ILD）的新型binaural信号损失函数，填补传统方法在感知属性捕捉上的不足。  <br/>2. 聚焦源定位与房间响应维度，排除频谱-时序属性的通用评估，针对性提升空间音频质量建模。  <br/>3. 指出房间参数估计和特性嵌入方法在损失函数设计中的潜力，为神经网络训练提供新方向。  <br/>4. 提出未来需开发更贴合人类空间感知体验的损失函数，推动沉浸式音频技术的感知一致性优化。  <br/><br/>**总结（100字内）：** 本文系统回顾了基于空间感知线索的binaural损失函数，强调源定位与房间响应的重要性，揭示当前研究在房间声学属性上的不足，并提出未来需结合参数估计和嵌入方法构建更感知友好的损失函数以提升空间音频质量。|
|2506.19335v1|[Learning to assess subjective impressions from speech](http://arxiv.org/abs/2506.19335v1)|总结：本文提出基于个性化主观语音描述符的评估框架，构建ACR与CCR混合数据集，引入ppref评估指标，验证CCR数据对小样本训练的有效性，为语音质量评估提供新方法。<br/><br/>贡献点：<br/>1. **提出主观语音描述符（SVDs）概念**：将口语中如“cute voice”等描述性短语定义为SVDs，针对个性化语音评分任务。<br/>2. **设计个性化适配框架**：构建能够处理个体专属SVDs（如“my favorite voice”）的模型框架，适应不同语音场景。<br/>3. **构建混合标注数据集**：整合绝对分类评分（ACR）与对比分类评分（CCR）数据，为研究提供多样化标注资源。<br/>4. **引入ppref评估指标**：提出用于衡量预测评分顺序准确性的新指标，改进对比评分场景的评估方式。<br/>5. **验证CCR训练优势**：通过实验表明，采用CCR数据训练的模型在小数据量下表现优于ACR训练模型，支持个性化SVDs的有效学习路径。|
|2506.19315v1|[JCAPT: A Joint Modeling Approach for CAPT](http://arxiv.org/abs/2506.19315v1)|总结：  <br/>本研究提出首个结合语音归因、状态空间模型与提示机制的CAPT框架，通过联合建模APA和MDD任务，显著提升发音评估的可解释性与细粒度时间推理能力，并在基准测试中取得优于现有方法的性能。<br/><br/>贡献点：  <br/>1. **首次融合三要素**：提出首个将语音归因、SSM（选择性状态空间模型）建模与提示机制结合的CAPT系统，创新性地整合多技术路径。  <br/>2. **联合建模任务**：设计统一框架同时优化自动发音评估（APA）与发音错误检测与诊断（MDD）任务，实现任务间互惠提升。  <br/>3. **多模态特征结合**：引入语音特征与think token策略，增强模型对发音细节的时空关系建模能力，提升结果可解释性。  <br/>4. **实验验证优势**：在speechocean762基准上验证模型有效性，尤其在MDD任务中表现显著优于传统方法。|
|2506.19253v1|[A Robust Method for Pitch Tracking in the Frequency Following Response   using Harmonic Amplitude Summation Filterbank](http://arxiv.org/abs/2506.19253v1)|**贡献点：**  <br/>1. 提出首个基于谐波结构的FFR F0估计算法，突破传统ACF方法的局限。  <br/>2. 设计“谐波振幅求和”（HAS）技术，通过刺激感知滤波器组选择性聚合F0及谐波振幅，抑制非谐波噪声。  <br/>3. 引入“最显著峰”选择策略，替代传统“最高峰”选取方式，更准确反映FFR的周期性特性。  <br/>4. 验证算法在宽频范围（89-452 Hz）自然语音刺激下的有效性，实验结果表明其性能优于ACF（平均RMSE降低8.8%-47.4%）。  <br/><br/>**总结（100字以内）：**  <br/>该研究首次提出基于谐波结构的FFR F0估计方法，通过刺激感知滤波器组和显著峰选择策略提升精度，在16名受试者数据中验证其性能优于传统ACF，平均RMSE降低8.8%-47.4%。|
|2506.19159v1|[Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data](http://arxiv.org/abs/2506.19159v1)|总结：  <br/>本文提出了一种联合语音与文本优化方法，统一多模态表示并应用于领域自适应，显著提升ASR性能，降低多个数据集的词错误率（WER）。<br/><br/>贡献点：  <br/>1. **提出联合优化框架**：设计J-TAED模型，首次结合语音与文本输入模态进行联合训练，仅需语音输入即可完成推理。  <br/>2. **多模态表示统一**：通过联合训练实现语音与文本的语义对齐，提升模型跨模态理解能力。  <br/>3. **文本驱动的领域自适应**：扩展模型至文本领域迁移任务，无需语音数据即可缓解数据稀缺问题。  <br/>4. **有效性验证**：在LibriSpeech等数据集上显著降低WER（5.8~12.8%），并验证了在金融、命名实体等跨域任务中的泛化能力。|
|2506.19108v1|[A Fourier Explanation of AI-music Artifacts](http://arxiv.org/abs/2506.19108v1)|**贡献点：**  <br/>1. **提出系统性频率伪影的识别方法**：首次发现生成式AI音乐中解卷积模块输出存在可检测的系统性频率伪影，表现为微小但独特的谱峰。  <br/>2. **理论证明伪影来源**：数学证明此类伪影源于特定模型架构，而非训练数据或权重，为AI生成内容检测提供新理论依据。  <br/>3. **跨场景验证有效性**：通过开源模型与商业AI音乐生成器（如Suno、Udio）的实验，验证了理论在真实场景中的适用性。  <br/>4. **开发简单高效的检测准则**：基于上述发现提出一种可解释、无需复杂训练的检测方法，准确率超过99%，与深度学习方法相当。  <br/><br/>**总结（100字内）：**  <br/>该论文提出基于系统性频率伪影的AI音乐检测方法，揭示其源于模型架构而非数据，通过实验验证有效性，并开发出高准确率且可解释的检测准则，为解决版权与伦理争议提供理论支持和实用工具。|
|2506.19085v1|[Benchmarking Music Generation Models and Metrics via Human Preference   Studies](http://arxiv.org/abs/2506.19085v1)|**贡献点**  <br/>1. **大规模实验验证**：构建包含6,000首生成歌曲及15,000对音频比较的数据集，利用2,500名参与者进行主观评估，探索人类偏好与客观指标的相关性。  <br/>2. **首次模型/指标排名**：提出首个基于人类偏好的音乐生成模型及评估指标的系统性排名，揭示当前技术的相对优劣。  <br/>3. **开放数据支持研究**：公开生成音乐数据集与人类评估结果，推动语音领域主观指标研究的开放性和可复现性。  <br/><br/>**总结（100字以内）**  <br/>论文通过大规模生成音乐与主观评估实验，首次系统性地基于人类偏好对模型和指标进行排名，并开放数据集促进语音领域主观评价研究。|
|2506.19014v2|[IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection](http://arxiv.org/abs/2506.19014v2)|总结（100字以内）:  <br/>该研究提出首个包含印度英语口音的音频深度伪造数据集（IFD），填补南亚数据空白，提升检测性能，并公开数据以促进研究应用。<br/><br/>贡献点:  <br/>1. 提出IndieFake数据集（IFD），首次系统性收集南亚地区印度英语母语者语音数据，弥补现有数据集对该区域覆盖率不足的问题。  <br/>2. 构建包含27.17小时真实与伪造音频的平衡数据集，覆盖50位多样化的印度英语说话人，增强模型泛化能力。  <br/>3. 引入说话人级语音特征标注，为深度伪造检测提供更丰富的上下文信息，区别于ASVspoof21等仅提供语料数据的基准集。  <br/>4. 验证IFD在深度伪造检测任务中的有效性，其性能优于ASVspoof21（DF）并更具挑战性，推动该领域基准测试升级。  <br/>5. 打造公开可访问的研究平台，提供完整数据、文档及示例剪辑，便于学术复用与技术验证。|
|2506.19014v1|[IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection](http://arxiv.org/abs/2506.19014v1)|**贡献点总结（100字以内）**  <br/>本文提出首个聚焦南亚语境的音频深度伪造数据集IndieFake，包含27.17小时印度英语说话者的真实与伪造音频，通过多样化口音和说话者级特征提升检测性能，并验证其在跨文化场景下的有效性。  <br/><br/>**分点贡献**  <br/>1. **填补数据空白**：首次构建针对南亚语境（尤其是印度英语口音）的音频深度伪造数据集，解决现有数据集缺乏地区代表性的问题。  <br/>2. **多样化与平衡性**：包含50位印度英语说话者的多变口音和语音特征，实现真实与伪造音频的均衡分布（27.17小时样本）。  <br/>3. **说话者级特征**：提供详细的说话者属性（如年龄、性别、文化背景），而非仅依赖语音内容，增强数据的描述性。  <br/>4. **性能验证**：在对比实验中，IFD显著优于ASVspoof21 (DF)数据集，并展现出对传统ITW数据集的更高挑战性，证明其有效性。  <br/>5. **公开可访问性**：数据集将在论文接受后公开，便于学术界和工业界进一步研究与应用。|
|2506.18954v1|[SHAMaNS: Sound Localization with Hybrid Alpha-Stable Spatial Measure and   Neural Steerer](http://arxiv.org/abs/2506.18954v1)|**贡献点总结（100字以内）：**  <br/>提出结合α-稳定模型与物理信息神经网络的SSL方法，通过Neural Steerer插值方向图并优化空间度量估计，有效解决多声源场景下的残差误差，提升定位精度。<br/><br/>**分点贡献：**  <br/>1. **混合模型设计**：首次结合α-稳定分布（非高斯信号建模）与神经网络（方向图建模），利用两者的互补性提升声源定位鲁棒性。  <br/>2. **Neural Steerer提出**：设计物理信息神经网络，用于插值固定麦克风阵列的测量方向图（SVs），实现更精确的信号空间表示。  <br/>3. **误差补偿机制**：将α-稳定模型用于建模Neural Steerer的残差重建误差，增强多声源场景下目标方向估计的准确性。  <br/>4. **性能验证**：实验表明该方法在多声源情况下显著优于现有技术，验证了其有效性与优越性。|
|2506.18843v1|[USAD: Universal Speech and Audio Representation via Distillation](http://arxiv.org/abs/2506.18843v1)|**贡献点总结（100字以内）**  <br/>提出统一音频表示模型USAD，融合语音、声学和音乐数据；通过层间蒸馏整合领域特定SSL模型；单编码器实现多任务高性能，接近SOTA结果。<br/><br/>**分点贡献：**  <br/>1. **统一框架**：首次构建覆盖语音、声音、音乐的多类型音频表示模型，突破传统领域分割限制。  <br/>2. **高效蒸馏方法**：采用层间知识蒸馏技术，从领域特定模型中迁移学习，提升模型泛化能力。  <br/>3. **单编码器多任务性能**：在SUPERB和HEAR基准上，以单一编码器实现多项任务的接近SOTA表现，验证模型有效性。|
|2506.18735v1|[An Audio-centric Multi-task Learning Framework for Streaming Ads   Targeting on Spotify](http://arxiv.org/abs/2506.18735v1)|**贡献点总结（100字以内）:**  <br/>提出Cross-modal Adaptive Mixture-of-Experts (CAMoE)框架，通过模态感知任务分组、自适应损失掩码和深度交叉网络，优化多模态广告的CTR预测，实现音频广告CTR提升14.5%，视频广告CTR提升1.3%，并降低音频广告eCPC 4.8%。  <br/><br/>**分点贡献:**  <br/>1. **提出CAMoE框架**：首个针对音频中心化与多模态广告（音频、视频、展示）联合优化的CTR预测模型，解决传统模型在混合模态场景下的局限性。  <br/>2. **模态感知任务分组**：通过动态分组不同模态任务，提升模型对音频、视频等多模态数据的适配性和推理效率。  <br/>3. **自适应损失掩码机制**：优化损失函数设计，针对性地处理不同广告格式的稀疏性问题，提升整体性能。  <br/>4. **深度交叉网络（DCN）集成**：引入DCN捕捉多模态广告中的复杂特征交互，增强模型表达能力。  <br/>5. **实验验证有效性**：通过消融实验和实际部署验证，在音频、视频、展示广告中均实现显著性能提升（AUC-PR优于传统方法）。  <br/>6. **实际应用成果**：在Spotify广告平台落地，实现音频广告CTR+14.5%，视频广告CTR+1.3%，并减少音频广告eCPC 4.8%。|
|2506.18729v2|[MuseControlLite: Multifunctional Music Generation with Lightweight   Conditioners](http://arxiv.org/abs/2506.18729v2)|**贡献点分点列出：**  <br/>1. **提出轻量级微调机制**：设计MuseControlLite，通过引入旋转位置编码（rotary positional embeddings）提升文本到音乐生成的条件控制精度，显著减少模型调优成本。  <br/>2. **创新性位置编码应用**：首次发现并验证在时间相关条件（如旋律控制）中，位置编码对生成模型的性能具有关键影响，弥补了文本到音乐领域对时间属性建模的不足。  <br/>3. **性能提升**：在旋律控制任务中，控制准确率从56.6%提升至61.1%，同时参数量仅为SOTA方法的1/6.75，仅需85M可训练参数。  <br/>4. **多任务兼容性**：支持音乐属性控制、音频修复（inpainting）和音频扩展（outpainting）等多种应用场景，验证了方法的通用性。  <br/>5. **高效与可复用**：基于Stable Audio Open预训练模型，无需重新训练即可实现高效微调，降低实际部署门槛。  <br/>6. **开源与可验证性**：提供完整代码、模型权重和演示示例，促进研究社区的验证与应用。  <br/><br/>**总结（100字以内）：**  <br/>MuseControlLite通过旋转位置编码提升文本到音乐生成的控制精度，参数量减少6.75倍，支持多任务，并开源以促进应用。|
|2506.18729v1|[MuseControlLite: Multifunctional Music Generation with Lightweight   Conditioners](http://arxiv.org/abs/2506.18729v1)|**贡献点：**  <br/>1. **提出轻量化控制机制**：设计MuseControlLite，显著降低微调成本（仅85M可训练参数），同时保持高控制精度。  <br/>2. **引入时间感知位置编码**：首次将旋转位置编码（Rotary Positional Embeddings）应用于文本到音乐模型的时间条件控制，提升旋律控制准确率6.7%。  <br/>3. **支持多模态条件控制**：兼容多种时间变化的音乐属性和参考音频信号，实现全局音频属性控制、音频修复（inpainting）和扩展（outpainting）。  <br/>4. **对比实验验证效果**：在MusicGen-Large和Stable Audio Open ControlNet基础上，展示更优的可控性和更低的计算资源需求。  <br/><br/>**总结：**  <br/>MuseControlLite通过轻量化设计和旋转位置编码提升文本到音乐生成的控制精度与效率，在多种任务中优于现有模型。|
|2506.18680v1|[DuetGen: Music Driven Two-Person Dance Generation via Hierarchical   Masked Modeling](http://arxiv.org/abs/2506.18680v1)|总结（100字以内）:  <br/>DuetGen提出两阶段框架，结合VQ-VAE与masked transformers生成同步双人舞蹈，解决音乐与动作交互的复杂性，实现多风格舞蹈生成，实验验证其在动作真实性和音乐协调性上的领先性能。<br/><br/>贡献点分点列出：  <br/>1. **创新框架结构**：提出双阶段生成模型（编码-解码），通过离散Token化方法将双人舞蹈动作与音乐信号解耦，实现从音乐到舞蹈动作的生成。  <br/>2. **统一交互表示**：将双人动作视为整体进行建模，学习共享的运动Token，增强两人动作的协调性与同步性。  <br/>3. **分层学习策略**：采用粗到细的层级学习（VQ-VAE处理高层语义，masked transformer分阶段生成高低层Token），提升动作生成的细节与语义一致性。  <br/>4. **掩码建模方法**：通过随机掩码训练生成模型，使Transformer在推理中逐步填充空Token序列，实现迭代式动作生成。  <br/>5. **跨领域性能验证**：在多风格舞蹈数据集上进行广泛实验，证明其在动作真实感、音乐-舞蹈对齐度和舞伴协作度上的先进性。|
|2506.18671v3|[TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for   Harmonious Music-Driven Group Choreography](http://arxiv.org/abs/2506.18671v3)|**总结（100字以内）:**  <br/>TCDiff++提出音乐驱动的集体舞蹈生成框架，通过定位嵌入、距离一致性损失、足部适配器及序列解码器，解决多人碰撞、脚滑和长序列问题，实现高质量、连贯的舞蹈生成，实验表现最佳。<br/><br/>**贡献点分点列出:**  <br/>1. **提出TCDiff++框架**：首个音乐驱动的端到端集体舞蹈生成模型，解决长编舞生成中的关键问题。  <br/>2. **多人碰撞优化**：引入舞者定位嵌入与距离一致性损失，保持舞者间合理相对位置与间距。  <br/>3. **单人脚滑抑制**：设计交换模式嵌入与Footwork Adaptor，精准修正动作细节以减少脚滑现象。  <br/>4. **长序列生成策略**：提出长扩散采样方法，通过注入位置信息减少剧烈位置变化，提升连贯性。  <br/>5. **序列解码器增强**：集成序列解码层，优化模型对长时序数据的选择性处理能力。  <br/>6. **实验验证优势**：在长时长场景下取得SOTA性能，证明方法在高质量、协调性编舞生成中的有效性。|
|2506.18671v2|[TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for   Harmonious Music-Driven Group Choreography](http://arxiv.org/abs/2506.18671v2)|**总结（100字以内）**  <br/>TCDiff++提出三种改进方法解决音乐驱动舞蹈生成的三大问题，引入定位嵌入、交换模式嵌入及长时采样策略，实验显示其在长时场景下实现SOTA性能，生成高质量连贯舞蹈。<br/><br/>**贡献点**  <br/>1. **解决多舞者碰撞问题**  <br/>   - 提出**舞者定位嵌入（dancer positioning embedding）**，优化舞者间相对位置关系；  <br/>   - 引入**距离一致性损失（distance-consistency loss）**，约束舞者间距离在合理范围内。  <br/><br/>2. **缓解单舞者脚滑问题**  <br/>   - 设计**交换模式嵌入（swap mode embedding）**，明确舞者切换规律；  <br/>   - 开发**Footwork Adaptor**，优化原始动作以减少脚部滑动。  <br/><br/>3. **提升长时舞蹈生成质量**  <br/>   - 提出**长组扩散采样策略（long group diffusion sampling）**，通过注入位置信息降低突变；  <br/>   - 集成**Sequence Decoder层**，增强模型对长序列的分步处理能力。  <br/><br/>4. **整体性能突破**  <br/>   - 在长时长场景下实现**SOTA（state-of-the-art）**效果，确保生成舞蹈的高质量与连贯性。|
|2506.18671v1|[TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for   Harmonious Music-Driven Group Choreography](http://arxiv.org/abs/2506.18671v1)|总结：  <br/>本文提出TCDiff++框架，通过定位嵌入、距离一致性损失、交换模式嵌入、脚步适配器、扩散采样策略和序列解码层，解决了音乐驱动团体舞蹈生成中的多舞者碰撞、脚步滑动及长序列不连贯等问题，实现了长时长高质量生成。<br/><br/>贡献点：  <br/>1. **多舞者碰撞解决**：引入**舞者定位嵌入**(Dancer Positioning Embedding)与**距离一致性损失**(Distance-Consistency Loss)，有效维持舞者间相对位置与合理距离。  <br/>2. **脚步滑动控制**：设计**交换模式嵌入**(Swap Mode Embedding)与**脚步适配器**(Footwork Adaptor)，优化单舞者动作细节，减少脚步滑动现象。  <br/>3. **长序列连贯性提升**：提出**长团体扩散采样策略**(Long Group Diffusion Sampling)与**序列解码层**(Sequence Decoder)，抑制位置突变并增强长时序动作生成的可控性。  <br/>4. **端到端框架创新**：构建音乐驱动的端到端框架TCDiff++，在长时长团体编舞生成中达到SOTA性能，确保生成质量与动作一致性。|
|2506.18623v1|[Efficient and Generalizable Speaker Diarization via Structured Pruning   of Self-Supervised Models](http://arxiv.org/abs/2506.18623v1)|贡献点总结：  <br/>1. 提出结构化剪枝与知识蒸馏结合的SSL模型压缩方法，显著降低计算和内存开销；  <br/>2. 引入基于MACs的剪枝目标，探索模块化与渐进式剪枝策略；  <br/>3. 分析训练数据量对压缩效果的影响；  <br/>4. 实现模型体积减少80%且性能不变，推理速度提升4倍；  <br/>5. 在八种公共数据集上进行大规模评估，达成多数场景下的SOTA性能；  <br/>6. 展示对CHiME-6数据集的强泛化能力，无需领域适配即可媲美CHiME-7挑战第三名系统；  <br/>7. 开源全部模型与代码，助力研究复现与后续开发。|
|2506.18532v1|[End-to-End Spoken Grammatical Error Correction](http://arxiv.org/abs/2506.18532v1)|**贡献点：**<br/>1. 提出基于Whisper的端到端（E2E）SGEC框架，对比级联、半级联和E2E架构的优劣。  <br/>2. 设计自动伪标注框架，显著扩充训练数据（77→2500小时）以解决SGEC数据稀缺问题。  <br/>3. 探索ASR输出的上下文信息增强SGEC准确性。  <br/>4. 提出参考对齐新方法，消除因流畅转录错误导致的错误编辑假设。  <br/>5. 引入编辑置信度评估机制，排除低置信度修改以提升反馈可靠性。  <br/><br/>**总结：**  <br/>本研究提出E2E SGEC框架，通过伪标注、上下文利用和参考对齐等方法，有效提升口语语法纠错与反馈生成性能。|
|2506.18488v2|[AI-Generated Song Detection via Lyrics Transcripts](http://arxiv.org/abs/2506.18488v2)|**贡献点：**  <br/>1. 提出基于ASR模型转录歌词的AI生成音乐检测方法，解决传统音频检测方法在泛化性和抗干扰能力不足的问题。  <br/>2. 通过多语言、多流派歌词验证，证明方法在语言和音乐类型上表现优越，尤其结合Whisper large-v2和LLM2Vec嵌入的模型效果突出。  <br/>3. 显示方法在音频扰动等实际场景中的鲁棒性优于现有音频检测技术。  <br/>4. 填补了实际应用中仅依赖音频而非完整歌词的空白，提升检测方法的实用性。  <br/><br/>**总结：**  <br/>本文利用ASR模型转录歌词，突破传统音频检测技术的局限，在多语言和多流派场景中实现高效准确的AI生成音乐识别，并增强对音频扰动的鲁棒性。|
|2506.18488v1|[AI-Generated Song Detection via Lyrics Transcripts](http://arxiv.org/abs/2506.18488v1)|**贡献点：**  <br/>1. **识别现有方法的局限性**：指出音频检测器难以泛化到未知生成器或音频扰动场景，且依赖清洁歌词的现有方法在实际应用中受限。  <br/>2. **提出基于ASR的解决方案**：通过通用自动语音识别（ASR）模型转录歌曲歌词，弥补歌词数据不可用的缺陷，适用于纯音频场景。  <br/>3. **跨语言与多类型有效性验证**：在多语种、多音乐风格的数据集上验证模型性能，证明其在多样化场景中的鲁棒性。  <br/>4. **提升检测鲁棒性**：实验表明该方法在音频扰动和不同生成器测试中优于现有音频检测技术。  <br/>5. **代码开放与复现性**：提供开源代码库，便于研究者复现和扩展工作。  <br/><br/>**总结：**  <br/>本研究提出基于ASR的AI音乐检测方法，解决了歌词数据缺失问题，实现了跨语言/类型鲁棒检测，并开放代码促进复现。|
|2506.18406v1|[Fully Few-shot Class-incremental Audio Classification Using Multi-level   Embedding Extractor and Ridge Regression Classifier](http://arxiv.org/abs/2506.18406v1)|总结：  <br/>本文提出Fully FCAC（FFFC）问题，设计解耦模型结构，通过Transformer编码器和融合模块提取特征，并采用持续更新的岭回归分类器，在三个数据集上实现更高准确率和更低复杂度。<br/><br/>贡献点：  <br/>1. **提出全少样本增量分类问题（FFFC）**：针对基础类和增量类均存在样本稀缺的现实场景，提出更具挑战性的Fully FCAC任务。  <br/>2. **设计解耦模型架构**：将模型分为多级嵌入提取器（含音频频谱Transformer编码器与融合模块）和动态更新的岭回归分类器，分离特征提取与分类过程。  <br/>3. **优化训练策略**：基类阶段训练嵌入提取器并冻结其参数，增量阶段仅更新分类器，降低样本需求与计算成本。  <br/>4. **实验证明有效性**：在三个公开数据集上验证，准确率超过现有方法，且模型复杂度更优，并公开代码促进复现与应用。|
|2506.18402v1|[Infant Cry Emotion Recognition Using Improved ECAPA-TDNN with Multiscale   Feature Fusion and Attention Enhancement](http://arxiv.org/abs/2506.18402v1)|总结：  <br/>本研究提出改进ECAPA-TDNN框架，结合多尺度特征融合与注意力增强，提升婴儿哭声情感识别准确率至82.20%，并实现轻量化参数（1.43 MB）和低计算量（0.32 Giga FLOPs），优于基线方法。<br/><br/>贡献点：  <br/>1. **方法创新**：提出改进的ECAPA-TDNN模型，融入多尺度特征融合机制与通道注意力增强模块，有效捕捉婴儿哭声中的情感特征。  <br/>2. **性能提升**：在公共数据集上实现82.20%的识别准确率，参数量仅1.43 MB，计算量低至0.32 Giga FLOPs，显著优于现有方法。  <br/>3. **问题解决**：针对性解决婴儿哭声情感识别中的噪声干扰、多尺度特征整合不足以及小样本数据挑战。  <br/>4. **工程优化**：通过模型架构改进实现高效推理，提升实际应用中的可行性与实时性。|
|2506.18326v1|[Selecting N-lowest scores for training MOS prediction models](http://arxiv.org/abs/2506.18326v1)|**总结**：  <br/>提出N_low-MOS作为更可靠的语音质量代表值，通过取N个最低意见评分的平均，提升MOSNet在VC模型比较中的有效性。<br/><br/>**贡献点**：  <br/>1. **提出新假设**：揭示人类主观评分时更关注低质量语段，评分差异源于对低质量部分的忽视。  <br/>2. **设计新指标**：引入N_low-MOS，定义为N个最低意见评分的均值，作为更贴近真实主观质量的评估方式。  <br/>3. **实验证明有效性**：在VCC2018和BVCC数据集上验证，N_low-MOS显著提升MOSNet的LCC和SRCC指标。  <br/>4. **提升模型应用**：证明基于N_low-MOS的MOSNet能更准确地比较语音转换（VC）模型的性能。|
|2506.18312v1|[Large-Scale Training Data Attribution for Music Generative Models via   Unlearning](http://arxiv.org/abs/2506.18312v1)|**贡献点：**  <br/>1. 提出将无学习方法应用于音乐生成模型的训练数据归因（TDA），识别特定数据对输出的贡献。  <br/>2. 支持白盒归因，推动AI生成音乐中对原创艺术家的公平认可与版权追溯。  <br/>3. 验证无学习归因方法在文本到音乐扩散模型中的可行性，通过超参数搜索和定量评估分析其一致性。  <br/>4. 对比无学习归因与相似度归因方法，揭示二者在音乐生成场景下的差异。  <br/>5. 引入大规模TDA框架至音乐生成领域，为构建伦理与可问责的AI音乐创作系统提供新方向。  <br/><br/>**总结（100字内）：**  <br/>本文提出基于无学习的音乐生成模型数据归因方法，实现对创作来源的精准识别，解决AI音乐版权问题，并验证其有效性，为伦理AI音乐创作提供技术支撑。|
|2506.18307v1|[Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting](http://arxiv.org/abs/2506.18307v1)|**贡献点：**  <br/>1. 提出新型评分聚合方法，解决传统离散MOS标注的局限性。  <br/>2. 基于标注者内部连续评分与离散选择行为的假设，构建量化潜在连续分布的模型。  <br/>3. 引入潜分布峰值作为新代表值替代MOS，提升神经网络预测性能。  <br/>4. 实验证明新方法在语音质量评估中的有效性，优于传统MOSNet目标。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出基于标注者连续评分假设的新方法，量化潜在分布并替换MOS预测目标，显著提升语音质量评估模型性能，并通过实验验证其有效性。|
|2506.18296v1|[JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking   Styles](http://arxiv.org/abs/2506.18296v1)|总结：  <br/>本文提出日本偶像语音语料库（JIS），为语音生成AI研究提供特定群体的高质量数据，促进TTS和VC系统评估，推动个性化语音生成研究，并支持伦理开放使用。<br/><br/>贡献点：  <br/>1. 构建首个聚焦"日本年轻女性现场偶像"的语音语料库（JIS），提供标准化的语音数据集。  <br/>2. 通过统一的舞台姓名标识，便于招募熟悉偶像的听众进行主观评测实验。  <br/>3. 引入"听众偏好匹配"研究方向，为个性化语音生成提供新视角和数据支持。  <br/>4. 免费开放非商业研究使用，促进语音AI领域的共享与协作。  <br/>5. 配套提供日本偶像文化背景说明，确保数据的伦理使用和研究的有效性。  <br/>6. 包含基础数据分析，为实际应用提供可参考的特征指导。|
|2506.18281v1|[Blind Source Separation in Biomedical Signals Using Variational Methods](http://arxiv.org/abs/2506.18281v1)|总结：  <br/>提出基于VAE的无监督心肺声音分离方法，通过潜在空间聚类与概率重构实现准确分离，具备临床应用价值。<br/><br/>贡献点：  <br/>1. **创新方法**：首次将变分自编码器（VAE）应用于心肺重叠声音的无监督分离，无需标注数据或源特性先验知识。  <br/>2. **实际验证**：在临床模拟环境中使用数字听诊器记录真实数据，验证方法的可行性与鲁棒性。  <br/>3. **有效结果**：实现心肺声音的潜在空间聚类，并保留原始信号的关键频谱特征，分离效果显著。  <br/>4. **应用潜力**：为便携式诊断设备和智能听诊系统提供可解释的盲源分离框架，拓展了语音处理在医疗场景的应用。|
|2506.18182v1|[Human Voice is Unique](http://arxiv.org/abs/2506.18182v1)|贡献点：<br/>1. 提出首个客观计算人类语音独特性的框架，量化语音作为生物识别标识符的唯一性。<br/>2. 构建基于语音信号因果特征（非相互依赖/可推导）的统计模型，确保计算的科学性与独立性。<br/>3. 通过量化变量分析，在100亿人口中证明语音匹配概率可达1/几千至1/十亿亿级，揭示语音独特性。<br/>4. 探讨该计算结果对语音处理应用（如身份验证、人声分析）的实践指导意义，推动技术优化。<br/><br/>总结：本文首次建立客观评估语音独特性的框架，通过独立因果特征的统计分析证明语音具有极强唯一性，并为相关应用提供理论依据。|
|2506.18143v1|[AI Harmonizer: Expanding Vocal Expression with a Generative   Neurosymbolic Music AI System](http://arxiv.org/abs/2506.18143v1)|**贡献点总结**（100字以内）:  <br/>本文提出无需用户输入的AI和谐器，融合生成式AI与符号音乐模型，实现自主四声部和声生成，拓展演唱与创作应用，开源代码支持进一步研究。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **自主生成和声**：无需用户手动指定调或使用外部设备，直接从独唱旋律生成四声部音乐和声。  <br/>2. **技术整合**：结合前沿生成式AI算法（如音高检测、声音建模）与定制训练的符号音乐模型，实现更自然的和声排列。  <br/>3. **应用场景拓展**：探索AI和谐器在音乐表演和创作中的潜力，并讨论实时实现的未来方向。  <br/>4. **开源实现**：提供离线版本的系统代码，推动AI辅助演唱技术的实践与研究。|
|2506.18055v1|[Face-Voice Association for Audiovisual Active Speaker Detection in   Egocentric Recordings](http://arxiv.org/abs/2506.18055v1)|贡献点：<br/>1. 提出SL-ASD框架：首次采用跨模态面部-语音生物特征关联替代传统时间同步建模，解决第一视角录音中的遮挡、运动模糊等挑战。<br/>2. 创新Transformer编码器设计：通过动态视觉质量加权机制，有效整合面部身份信息，提升视觉特征利用效率。<br/>3. 系统级集成方案：将前端语音分割方法与跨模态关联模型结合，构建完整的音频视觉主动说话人检测系统。<br/>4. 参数优化成果：在保持竞争力的同时，显著减少可学习参数量（较传统方法减少约40%），验证轻量化模型的有效性。<br/>5. 实验验证有效性：在复杂egocentric场景中，SL-ASD性能达到或超越传统参数密集型方法，证明新方法的可行性。<br/><br/>总结：本文提出SL-ASD框架，通过跨模态面部-语音关联替代传统时间同步建模，结合Transformer编码器与语音分割方法，有效解决第一视角录音中的挑战问题，实现轻量化高性能的主动说话人检测系统。|
|2506.18035v1|[Splitformer: An improved early-exit architecture for automatic speech   recognition on edge devices](http://arxiv.org/abs/2506.18035v1)|总结：  <br/>本文提出一种结合早期退出机制与可变帧率分析的语音识别模型，通过引入并行下采样层提升性能，仅小幅增加参数而保持推理时间不变。<br/><br/>贡献点：  <br/>1. **融合两种架构优势**：将早期退出（Early-Exit）的动态计算调整能力与可变帧率（VFR）的内存效率特性相结合，解决单一架构的局限性。  <br/>2. **提出并行下采样层设计**：在模型中引入并行处理下采样输入的层，实现更高效的计算负载分配与性能优化。  <br/>3. **验证性能与效率平衡**：实验表明新方法在标准基准上大幅提升语音识别性能，同时仅小幅增加参数量且不增加推理时间。|
|2506.17886v2|[GD-Retriever: Controllable Generative Text-Music Retrieval with   Diffusion Models](http://arxiv.org/abs/2506.17886v2)|贡献点总结：  <br/>1. 提出Generative Diffusion Retriever (GDR)框架，通过扩散模型优化检索-对齐的潜在空间，提升文本-音乐检索性能。  <br/>2. 引入生成工具（如负提示、DDIM反演）实现对检索行为的可控性，增强用户交互能力。  <br/>3. 支持音频-only潜在空间检索，无需联合训练编码器。  <br/>4. 实现事后检索行为调整（post-hoc manipulation），拓展多模态检索的灵活性。  <br/><br/>（总结：GDR通过扩散模型优化检索空间，提供可控与交互能力，支持非联合训练检索，实现事后行为调整。）|
|2506.17886v1|[GD-Retriever: Controllable Generative Text-Music Retrieval with   Diffusion Models](http://arxiv.org/abs/2506.17886v1)|总结：  <br/>本研究提出GDR框架，结合生成扩散模型与检索优化，提升文本-音乐检索的可控性和交互性，支持非联合训练编码器和后处理行为调整。<br/><br/>贡献点：  <br/>1. **提出新型框架**：设计Generative Diffusion Retriever (GDR)，通过扩散模型在检索优化的潜在空间中生成查询，解决多模态检索中语义表达不明确的问题。  <br/>2. **提升可控性**：引入负提示（negative prompting）和DDIM反演等生成工具，实现对检索行为的灵活控制。  <br/>3. **优化检索性能**：在对比实验中，GDR表现优于对比模型（contrastive teacher models），并支持音频-only潜在空间中的独立检索。  <br/>4. **增强交互能力**：通过后处理技术实现对检索过程的动态调整，为用户提供更自然的交互体验。|
|2506.17818v1|[CultureMERT: Continual Pre-Training for Cross-Cultural Music   Representation Learning](http://arxiv.org/abs/2506.17818v1)|总结（100字以内）:  <br/>提出CultureMERT-95M多文化音乐基础模型，通过两阶段持续预训练策略及任务算术方法，显著提升跨文化音乐表示学习性能，同时保持对西方数据的稳定性，并公开模型促进文化感知研究。  <br/><br/>贡献点:  <br/>1. **提出多文化适应模型CultureMERT-95M**：专为增强跨文化音乐表示学习和理解设计，覆盖希腊、土耳其、印度等非西方音乐传统。  <br/>2. **两阶段持续预训练策略**：集成学习率重升温与重衰减机制，实现低计算资源下的稳定模型适应。  <br/>3. **提升非西方音乐任务性能**：在650小时多文化数据集训练下，非西方音乐自动标签任务的ROC-AUC与AP平均提升4.9%，超越现有SOTA。  <br/>4. **任务算术方法**：通过权重空间融合单一文化模型，实现与多文化训练模型相当的效果，且无西方数据性能退化。  <br/>5. **公开模型与数据**：发布CultureMERT-95M及CultureMERT-TA-95M，推动世界音乐表示学习的进一步研究与文化感知模型开发。|
|2506.17815v1|[SLAP: Siamese Language-Audio Pretraining Without Negative Samples for   Music Understanding](http://arxiv.org/abs/2506.17815v1)|总结：  <br/>该论文提出SLAP框架，通过避免负样本和应用BYOL范式解决多模态联合嵌入的内存瓶颈与模态间隙问题，实现了更高效的多模态表示学习和单GPU的大规模训练。<br/><br/>贡献点：  <br/>1. **提出无需负样本的SLAP框架**：突破传统多模态对比学习对海量负样本的依赖，降低训练成本。  <br/>2. **引入BYOL范式**：首次将Bootstrap Your Own Latent方法应用于音频-文本联合训练，提升模型扩展性。  <br/>3. **解决模态间隙问题**：通过优化使音频与文本嵌入位于更统一的语义空间，减少跨模态差异。  <br/>4. **提升任务性能**：在文本-音乐检索、零样本分类等任务中表现优于CLAP，且在多类MIR任务中具有竞争力。  <br/>5. **增强训练鲁棒性**：模型对批大小变化具有更强的稳定性，优化了检索任务的泛化能力。  <br/>6. **支持单GPU大规模训练**：通过梯度累积技术实现高效训练，突破硬件限制。|
|2506.17778v1|[Algebraic Structures in Microtonal Music](http://arxiv.org/abs/2506.17778v1)|总结：该论文探讨音乐理论中群论结构的应用，提出24音阶体系的数学分析框架，扩展了微分音音乐与谐波结构的研究。<br/><br/>贡献点：<br/>1. **24音阶体系构建**  <br/>   将传统12平均律的八度分割为24个等分音（四分音），为微分音音乐提供更精细的理论基础。<br/><br/>2. **数学化音乐动作**  <br/>   通过为24音阶分配数值，建立音乐动作的数学表达方法，推动音乐理论形式化研究。<br/><br/>3. **群论与音乐结构结合**  <br/>   探索24音阶体系下的音乐与谐波结构如何通过群论（如群作用、对称性）进行解释，拓展音乐理论的数学工具。<br/><br/>（注：摘要内容实际属于音乐理论领域，而非语音领域。）|
|2506.17690v1|[Low-resource keyword spotting using contrastively trained transformer   acoustic word embeddings](http://arxiv.org/abs/2506.17690v1)|**贡献点（分点）：**  <br/>1. 提出对比学习框架ContrastiveTransformer，用于生成声学词嵌入（AWEs）以解决极低资源环境下的关键词识别任务。  <br/>2. 首次在Luganda和Bambara语言的广播数据中应用该模型，其中Bambara为严重资源匮乏语言，验证方法的跨语言适用性。  <br/>3. 采用归一化温度缩放交叉熵（NT-Xent）损失直接优化嵌入空间，替代传统预训练模型或循环编码器的间接方法。  <br/>4. 对比实验表明，该模型在性能上超越现有主流AWE方法（含自监督模型、RLN模型和DTW基线）在两种语言中的表现。  <br/><br/>**总结（100字内）：**  <br/>提出了对比学习驱动的ContrastiveTransformer模型，直接优化声学词嵌入空间，有效提升极低资源关键词识别性能，并在Luganda和Bambara语言中验证其适用性。|
|2506.17686v1|[Enhancing Few-shot Keyword Spotting Performance through Pre-Trained   Self-supervised Speech Models](http://arxiv.org/abs/2506.17686v1)|总结：  <br/>提出结合自监督学习、注意力机制和知识蒸馏的轻量化少样本语音关键词识别方法，显著提升分类准确率并优化边缘设备部署效率。<br/><br/>贡献点：  <br/>1. **提出新型训练框架**：整合自监督学习（用于鲁棒特征提取）、降维技术和知识蒸馏，解决少样本语音关键词识别（FS-KWS）中准确率与误报率平衡的问题。  <br/>2. **改进教师模型训练**：基于Wav2Vec 2.0引入Sub-center ArcFace损失，增强类间可分性和类内紧致性，提升特征表示能力。  <br/>3. **优化学生模型设计**：采用轻量ResNet15架构，并结合注意力机制实现高效维度压缩，适配资源受限的边缘计算场景。  <br/>4. **验证实际效果**：在Google Speech Commands（GSC）数据集上实现10-shot分类准确率从33.4%提升至74.1%（1%误报率），验证方法在现实场景的适用性和性能优势。|
|2506.17499v1|[Episode-specific Fine-tuning for Metric-based Few-shot Learners with   Optimization-based Training](http://arxiv.org/abs/2506.17499v1)|1. 提出episode-specific during-inference fine-tuning新范式，首次将支持样本用于调整度量空间而非仅做相似度比对  <br/>2. 设计Rotational Division Fine-Tuning (RDFT)及其两种变体（IDFT/ADFT），通过构造伪支持-查询对实现非参数模型的微调  <br/>3. 引入基于优化的元学习框架，有效解决少样本任务中数据量不足导致的过拟合问题  <br/>4. 在ES-50、Speech Commands V2、Medley-solos-DB三个跨领域音频数据集上验证，展现显著的性能提升与泛化能力  <br/>5. 首次证明该方法对注意力机制模型具有特殊优势，推动少样本语音分类技术的改进|
|2506.17497v1|[From Generality to Mastery: Composer-Style Symbolic Music Generation via   Large-Scale Pre-training](http://arxiv.org/abs/2506.17497v1)|**贡献点：**  <br/>1. **两阶段训练框架**：首次提出基于大规模通用音乐语料预训练（REMI模型）与小规模作曲家风格微调的结合方法，解决数据稀缺下的风格建模难题。  <br/>2. **风格指示符条件训练**：设计轻量级适配模块，通过风格指示符（如作曲家标签）实现对特定风格的精准控制。  <br/>3. **音乐概念迁移机制**：验证通用音乐知识与特定作曲家风格学习的协同作用，揭示模型如何从泛化能力中构建音乐元素理解。  <br/>4. **多维度评估体系**：提出风格准确性和音乐美学的客观（如指标分析）与主观（如听觉评估）综合评价方法。  <br/>5. **性能优化结果**：实验表明方法在风格建模精度和音乐表现力上显著优于基线和消融实验。  <br/>6. **风格理解分析**：通过生成过程研究，提供模型如何从预训练到微调逐步精进风格理解的机制性洞察。  <br/><br/>**总结（100字内）：**  <br/>该研究提出两阶段训练框架，结合广泛音乐语料预训练与风格微调，设计适配模块实现精准作曲家风格控制，验证了知识迁移的有效性，并通过多维度评估证明了方法在生成质量与音乐表现上的优势。|
|2506.17409v1|[Adaptive Control Attention Network for Underwater Acoustic Localization   and Domain Adaptation](http://arxiv.org/abs/2506.17409v1)|总结（100字以内）:  <br/>提出多分支网络架构结合CNN与Conformer自注意力机制，引入AGC层提升输入特征稳定性，并通过跨域测试验证泛化能力，显著优于现有方法，为水下声源定位建立新基准。<br/><br/>贡献点分点:  <br/>1. **多分支网络架构**：设计专用网络以预测移动声源与接收器的距离，结合CNN空间特征提取与Conformer时序建模，提升复杂环境下的定位精度。  <br/>2. **自适应增益控制（AGC）层**：创新性地引入AGC模块，动态调整输入特征幅度，解决多距离、信号强度和噪声条件下的能量不一致问题。  <br/>3. **跨域泛化能力验证**：采用“单域训练、多域测试”的策略，仅需少量测试域数据即可完成微调，证明方法在环境变化下的鲁棒性。  <br/>4. **特征融合方案**：结合log-mel频谱图和GCC-PHAT时序特征，优化输入表征以捕捉更全面的声学信息。  <br/>5. **性能突破**：在真实水下信号数据集上验证，提出的方法优于现有SOTA方案，建立水下声源定位的新基准。|
|2506.17055v1|[Universal Music Representations? Evaluating Foundation Models on World   Music Corpora](http://arxiv.org/abs/2506.17055v1)|**贡献点总结：**  <br/>1. **跨文化音乐泛化评估**：系统检验了五种音频基础模型在西方、希腊、土耳其、印度等多元音乐传统中的表现差异。  <br/>2. **多方法验证**：结合探针分析、分层监督微调、少样本学习等技术，全面评估模型跨文化能力。  <br/>3. **模型表现规律**：发现大模型在非西方音乐上优于西方音乐，但文化差异越远效果越弱。  <br/>4. **SOTA结果**：在五项国际音乐理解任务中达到当前最优性能，验证基础模型在世界音乐领域的有效性。  <br/>5. **知识编码洞察**：证明基础模型已具备跨文化音乐知识，定向微调未必优于探针分析。  <br/>6. **基准框架建立**：提出可复用的评估框架和度量标准，推动对通用音乐表示的研究。  <br/><br/>**总结（100字以内）：**  <br/>本文通过多方法评估，揭示了音频基础模型在跨文化音乐理解中的表现差异，发现大模型对非西方音乐表现更优，建立首个跨文化基准框架，为未来全球音乐表示研究提供方向与度量。|
|2506.16969v2|[State-Space Models in Efficient Whispered and Multi-dialect Speech   Recognition](http://arxiv.org/abs/2506.16969v2)|总结：  <br/>该论文提出结合Mamba状态空间模型与四种自监督预训练模型的方案，用于高效解决耳语语音识别及方言变异问题，并在两个数据集上取得最佳性能，同时支持多方言场景，代码开源。<br/><br/>贡献点：  <br/>1. **方法创新**：设计基于Mamba的状态空间模型，结合Wav2Vec2、WavLM、HuBERT和Whisper四种自监督模型，应对耳语语音与方言变异的双重挑战。  <br/>2. **性能突破**：在wTIMIT和CHAINS数据集上实现当前最优的耳语语音识别效果，验证了方法的有效性。  <br/>3. **资源效率**：提出低数据量和低计算负载下的高效解决方案，减少训练成本。  <br/>4. **跨方言泛化**：模型支持新加坡、美国、爱尔兰等多方言场景，提升跨语言适应能力。  <br/>5. **开源共享**：代码公开，便于复现与实际应用，推动领域发展。|
|2506.16889v2|[ITO-Master: Inference-Time Optimization for Audio Effects Modeling of   Music Mastering Processors](http://arxiv.org/abs/2506.16889v2)|总结：  <br/>提出ITO-Master框架，通过推理时优化提升母带风格迁移的自定义和效果，验证其在不同风格下的适应性。<br/><br/>贡献点：  <br/>1. **提出ITO-Master框架**：首个集成推理时优化（ITO）的参考型母带风格迁移系统，解决传统方法固定处理限制用户微调的问题。  <br/>2. **动态参考嵌入优化**：在推理阶段优化参考嵌入，使用户可通过微调实现更精确的母带结果控制。  <br/>3. **黑盒与白盒方法探索**：系统性研究两类建模方法，验证ITO对不同音乐风格的通用性和性能提升。  <br/>4. **多维度效果验证**：结合客观评估、主观听测与CLAP嵌入的文本条件分析，全面证明ITO对风格相似性与适应性的增强效果。  <br/>5. **用户可控性提升**：提供超越初始迁移的细化调整能力，满足艺术家个性化需求。|
|2506.15981v2|[Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via   Multi-View Fusion](http://arxiv.org/abs/2506.15981v2)|总结：  <br/>本研究提出DE-detect，一种结合自动转录歌词与音频语音特征的多模态检测方法，有效解决现有检测器对歌词依赖和音频扰动的局限性，实现更高的鲁棒性和实用性，并开源代码促进应用。<br/><br/>贡献点：  <br/>1. **提出多模态检测框架**：首次融合自动转录歌词与音频中的语音特征，突破了单一模态（仅音频或仅歌词）的检测限制。  <br/>2. **解决歌词依赖与音频扰动问题**：无需依赖准确歌词输入，通过从音频中提取歌词相关信息，增强对抗音频扰动和低级噪声的鲁棒性。  <br/>3. **验证实际性能优势**：实验表明DE-detect在检测AI生成音乐任务中优于现有歌词依赖模型，并在音频扰动下保持更高准确率。  <br/>4. **开源实现促进应用**：提供可复现的代码库，便于行业和研究者在实际场景中部署和优化检测技术。|
|2506.15614v1|[TTSOps: A Closed-Loop Corpus Optimization Framework for Training   Multi-Speaker TTS Models from Dark Data](http://arxiv.org/abs/2506.15614v1)|总结（100字以内）:  <br/>TTSOps提出了一种全自动闭环框架，通过动态数据清洗和基于预测MOS的评估，有效利用低质量但信息丰富的网络数据，显著提升多说话人TTS系统的自然度和多样性。<br/><br/>贡献点分点列出:  <br/>1. **提出TTSOps框架**：首个针对"dark data"（嘈杂、未标注网络语音数据）的全自动闭环多说话人TTS构建系统，突破传统方法对高质量语料的依赖。  <br/>2. **三重核心组件**：  <br/>   - 自动化数据收集机制  <br/>   - 语句级动态清洗方法选择（根据数据质量自适应）  <br/>   - 嵌入式评价机制（利用预测MOS量化语音对模型性能的影响）  <br/>3. **闭环优化策略**：同时优化语料库和TTS模型，动态调整数据选择和清洗流程以适配目标模型特性。  <br/>4. **实验验证**：在日语文本数据集上验证，显著优于传统基于声学质量的基线方法，在合成语音自然度和说话人多样性上取得突破。|
|2506.15548v1|[Versatile Symbolic Music-for-Music Modeling via Function Alignment](http://arxiv.org/abs/2506.15548v1)|总结（100字以内）:  <br/>提出基于音乐模态自身表达的统一框架，通过参数高效方法和轻量级适配器实现多任务性能突破，并公开所有资源促进研究。<br/><br/>贡献点:  <br/>1. **统一音乐任务范式**：首次将音乐内容理解（如和弦识别）与生成（如旋律生成）任务整合到基于音乐符号序列的建模框架中；  <br/>2. **参数高效迁移方法**：利用预训练语言模型（LM）作为参考和目标序列的编码器，实现模型参数的高效复用；  <br/>3. **轻量级适配器设计**：通过简单适配器连接不同LM，有效降低计算开销并保持任务性能；  <br/>4. **多任务性能验证**：在和弦识别、旋律生成、鼓点生成等任务中均取得最优结果；  <br/>5. **开源促进研究**：公开代码、模型权重及演示，推动音乐AI领域的可复现性与协作发展。|
|2506.15530v1|[Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music   Diffusion Models](http://arxiv.org/abs/2506.15530v1)|**贡献点总结（100字以内）：**  <br/>本研究提出利用预训练文本到音乐扩散模型实现乐器编辑，通过分类器确定中间时间步以平衡内容保留与音色调整，无需额外训练且保持生成速度，显著提升了音乐创作的控制精度与效率。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **首次将文本到音乐扩散模型用于乐器编辑**：提出在不重新训练模型的前提下，通过控制生成过程实现对现有音频中乐器的修改与替换。  <br/>2. **中间时间步选择机制**：基于模型生成流程的分析，利用乐器分类器识别最佳中间时间步，实现内容与音色的可控平衡。  <br/>3. **无需额外训练且保持高效生成**：所提方法不依赖模型微调，维持原扩散模型的生成速度，同时提升对生成结果的可控性。|
|2506.15514v1|[Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper](http://arxiv.org/abs/2506.15514v1)|**贡献点**：<br/>1. **系统性评估源分离效果**：首次系统研究音乐源分离技术对自动歌词转录（ALT）性能的影响，提出最佳实践建议。<br/>2. **短文本转录优化**：设计基于拼接的改进方法，显著降低短文本任务的Word Error Rate（WER）。<br/>3. **长文本转录创新**：将源分离作为语音活动检测器，提出基于段边界识别的算法，提升长文本任务的WER表现。<br/>4. **开源最优结果**：在Jam-ALT长文本基准上，无需训练或微调即达到开源系统的最先进性能。<br/>5. **标准数据集发布**：推出MUSDB-ALT数据集，作为首个符合Jam-ALT规范且提供公开人声轨道的长文本歌词转录数据集。  <br/><br/>**总结**：  <br/>本研究系统评估源分离对ALT的影响，提出适用于短/长文本的改进方法，实现开源系统最优性能，并发布首个符合标准的长文本歌词数据集。|
|2506.15456v1|[Factorized RVQ-GAN For Disentangled Speech Tokenization](http://arxiv.org/abs/2506.15456v1)|总结：  <br/>HAC提出多级分解的语音编解码框架，通过双知识蒸馏策略实现声学、音素、词汇层级分离，生成具有自然性和语义可解释性的离散语音表示，适用于生成和理解任务。<br/><br/>贡献点：  <br/>1. **多层级语音表示**：首次将语音编解码器的瓶颈分解为三个语言层级（声学、音素、词汇），实现跨层级的分离表征。  <br/>2. **双知识蒸馏策略**：结合预训练语音编码器（HuBERT）和文本编码器（LaBSE），分别提取音素级结构和词汇级语义信息。  <br/>3. **解耦标记集验证**：实验证明HAC生成的标记集在音素与词级语义上解耦，且保持语音自然性与语义可解释性。  <br/>4. **性能优越性**：在重建质量与解耦效果上超越单级基线模型，为下游语音任务提供统一的离散表征方案。|
|2506.15107v1|[I Know You're Listening: Adaptive Voice for HRI](http://arxiv.org/abs/2506.15107v1)|总结：  <br/>该研究提出三个创新贡献：开发轻量且表达力强的机器人语音系统、设计环境自适应语音调整机制、创建针对二语学习者的发音清晰优化模式，显著提升语言教学场景下的语音表现力与可懂度。<br/><br/>贡献点：  <br/>1. **轻量化与高表达力语音系统**  <br/>   - 基于Matcha-TTS进行微调，结合emoji提示实现动态情感表达  <br/>   - 支持实时运行并适用于长时间场景（如故事讲述）  <br/>   - 通过案例研究验证其社会适应性与语音表现力  <br/><br/>2. **环境自适应语音调整方法**  <br/>   - 针对嘈杂/高能环境动态调整音高和语速  <br/>   - 环境感知优化使语音更具情境适配性  <br/><br/>3. **L2可懂度优化的语音增强方案**  <br/>   - 基于元音时长特征设计"L2清晰模式"  <br/>   - 通过数据驱动方法优化紧张元音的发音清晰度  <br/>   - 提升二语学习者对词汇的识别准确率与语音可接受性|
|2506.15029v1|[An accurate and revised version of optical character recognition-based   speech synthesis using LabVIEW](http://arxiv.org/abs/2506.15029v1)|**贡献点：**  <br/>1. **提出解决方案**：设计并实现了一种OCR-语音合成系统，弥补现有盲文和NGO音频记录在书籍获取上的不足。  <br/>2. **技术实现**：采用LabVIEW平台构建系统，提升了可靠性、成本效益和用户友好性，适配语音通信需求。  <br/>3. **性能优化**：验证了系统在准确性、可靠性及用户体验上的优势，证明其比传统方法更高效。  <br/>4. **应用场景拓展**：为盲人提供更灵活的书籍访问方式，增强信息获取能力，推动语音技术在辅助工具中的应用。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于OCR的语音合成系统，利用LabVIEW实现，解决了盲人书籍获取受限问题，提升沟通效率与用户体验。|
|2506.14877v1|[Beyond Universality: Cultural Diversity in Music and Its Implications   for Sound Design and Sonification](http://arxiv.org/abs/2506.14877v1)|**贡献点分点总结：**  <br/>1. **批判性视角**：挑战"音乐作为普遍语言"的假设，揭示文化多样性在声音设计中的被忽视作用。  <br/>2. **跨文化理论框架**：通过音乐学与民族音乐学的比较分析，建立文化语境对听觉感知与审美判断的核心影响理论。  <br/>3. **历史-当代关联**：发现传统音乐实践与现代声音设计在文化逻辑上的共性，提出跨时代方法论的借鉴价值。  <br/>4. **实践建议**：倡导将多元文化元素融入声音设计流程，推动设计方法论的包容性变革。  <br/>5. **框架重构**：提出重新评估现有声音设计/声化框架的必要性，强调文化导向的创新路径。  <br/>6. **应用价值**：以音乐为案例证明文化整合对提升声音设计全球适应性与深度体验的关键意义。  <br/><br/>**总结（100字内）：**  <br/>本文通过跨文化与历史视角，揭示音乐作为"普遍语言"的认知偏见，提出文化多样性在声音设计中的核心地位，倡导构建文化敏感的理论与实践框架，为提升全球受众的声学体验提供新方向。|
|2506.14864v1|[pycnet-audio: A Python package to support bioacoustics data processing](http://arxiv.org/abs/2506.14864v1)|**贡献点：**  <br/>1. **提出实用处理框架**：构建了基于PNW-Cnet模型的pycnet-audio工具，为被动声学监测数据提供高效、可扩展的处理流程。  <br/>2. **扩展模型应用范围**：将PNW-Cnet从仅监测北方斑 owl 的种群，扩展至检测约80种森林野生动物的叫声及多种人类与环境噪声。  <br/>3. **解决大规模数据挑战**：针对超10⁵小时的音频数据，开发自动化检测方法以替代人工标注，显著提升生态声学研究的可行性。  <br/>4. **集成多源噪声分类**：支持对环境噪声、人类活动噪声等非目标声源的识别，增强监测系统的全面性与实用性。  <br/><br/>**总结（100字以内）：**  <br/>论文开发了pycnet-audio工具，基于PNW-Cnet模型扩展用于检测80种森林野生动物叫声及环境噪声，解决了大规模声学数据的自动化处理难题，推动被动声学监测在生态研究中的应用。|
|2506.14750v1|[Exploring Speaker Diarization with Mixture of Experts](http://arxiv.org/abs/2506.14750v1)|**贡献点：**  <br/>1. 提出NSD-MS2S系统，融合记忆感知多说话人嵌入模块与序列到序列架构，提升说话人嵌入质量与标签映射效率。  <br/>2. 引入Shared and Soft Mixture of Experts (SS-MoE)模块，通过共享与软混合专家机制减少模型偏差，增强系统性能。  <br/>3. 在CHiME-6、DiPCo、Mixer 6和DIHARD-III等复杂数据集上验证，实现SOTA效果，显著提升鲁棒性与真实场景适用性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出NSD-MS2S系统，结合记忆模块和序列到序列架构，并引入SS-MoE模块优化模型，有效提升说话人分离的鲁棒性和泛化能力，在多个复杂数据集上达到SOTA水平。|
|2506.14723v1|[Adaptive Accompaniment with ReaLchords](http://arxiv.org/abs/2506.14723v1)|总结（100字以内）：  <br/>提出ReaLchords在线音乐生成模型，结合预训练与强化学习，引入新型奖励机制与未来旋律蒸馏方法，实现与用户旋律同步的和声伴奏生成，验证了其对未知输入的适应性及协作创作潜力。<br/><br/>贡献点：  <br/>1. **提出在线生成模型**：首个支持实时即兴和声伴奏生成的模型，可同步与用户旋律交互（人类或AI）。  <br/>2. **双目标强化学习框架**：采用最大似然预训练，结合新型奖励模型（评估和声与时间连贯性）和蒸馏项（从未来旋律教师模型中获取信息）。  <br/>3. **创新奖励与蒸馏机制**：  <br/>   - 奖励模型：联合优化旋律与和声的协调性（调性、节奏、时间）。  <br/>   - 蒸馏方法：提出基于未来旋律预测的知识蒸馏技术，提升模型对长程依赖的建模能力。  <br/>4. **实验验证有效性**：通过定量实验和听觉测试，证明模型对陌生输入的适应性及生成高质量伴奏的能力。  <br/>5. **应用拓展前景**：为实时即兴表演（jamming）和多模态同步创作（如文本、视觉）提供技术支持。|
|2506.14684v2|[Refining music sample identification with a self-supervised graph neural   network](http://arxiv.org/abs/2506.14684v2)|总结：本文提出轻量级模型、两阶段方法及针对短时查询的基准测试，提升ASID性能与鲁棒性。<br/><br/>贡献点：  <br/>1. **轻量高效模型架构**：基于图神经网络和对比学习框架，模型仅使用9%的参数量，性能接近现有SOTA系统（mAP 44.2%）。  <br/>2. **两阶段检索优化**：提出粗粒度相似度搜索（候选筛选）+交叉注意力分类器（过滤干扰、精排候选）的混合方法，填补了传统模型在样本匹配过滤中的空白。  <br/>3. **短时查询基准建设**：为Sample100数据集开发细粒度标注，首次针对短时音频查询场景进行系统性实验与评估。|
|2506.14684v1|[Refining music sample identification with a self-supervised graph neural   network](http://arxiv.org/abs/2506.14684v1)|总结：  <br/>本文提出轻量级图神经网络对比学习框架，显著降低模型参数量并提升样本识别性能，设计两阶段方法解决短查询场景下匹配精度问题，发布新细粒度注释数据用于实际评测。<br/><br/>贡献点：  <br/>1. **轻量级架构**：提出基于图神经网络（GNN）的对比学习框架，仅使用9%的参数量即可达到当前SOTA系统的性能（mAP 44.2%）。  <br/>2. **两阶段检索方法**：引入粗粒度相似度搜索筛选候选，再结合交叉注意力分类器精炼结果，解决现有模型对冗余/无关匹配的识别缺陷。  <br/>3. **短时查询适配**：为Sample100数据集创建新细粒度注释，并针对短时查询场景进行基准测试，推动实际应用中的样本识别研究。|
|2506.14657v1|[ASAP-FE: Energy-Efficient Feature Extraction Enabling Multi-Channel   Keyword Spotting on Edge Processors](http://arxiv.org/abs/2506.14657v1)|总结：  <br/>提出ASAP-FE框架，通过三重创新（半重叠IIR框、稀疏感知数据压缩、动态并行处理）实现多通道KWS的高效处理，显著降低计算负载与能耗，同时保持接近基线的识别精度，适用于边缘设备。<br/><br/>贡献点：  <br/>1. **提出ASAP-FE框架**：针对边缘计算环境中的多通道KWS需求，设计硬件导向的前端处理结构，兼顾效率与实时性。  <br/>2. **三重技术创新**：  <br/>   - 半重叠IIR帧处理：减少25%冗余数据，保留关键音素过渡信息。  <br/>   - 稀疏感知数据压缩：结合跳帧与步进过滤，实现额外50%数据减量。  <br/>   - 动态并行处理：引入参数化滤波器集群与优先级调度算法，降低延迟并优化能效。  <br/>3. **实验验证与优化**：在FPGA原型和45nm工艺下验证设计，支持32通道实时处理，15并行滤波器实现性能与能耗的最佳平衡，仅导致1%以内的精度损失（相较于传统全重叠基线）。|
|2506.14571v1|[The Perception of Phase Intercept Distortion and its Application in Data   Augmentation](http://arxiv.org/abs/2506.14571v1)|**贡献点总结（100字以内）**:  <br/>提出相位截断失真（频率无关相位偏移）的新概念，验证其不可感知性，揭示相位失真在语音信号中的隐蔽特性，并将其作为数据增强方法应用于音频机器学习，提升任务性能。<br/><br/>**分点贡献**:  <br/>1. **提出新概念**：定义"相位截断失真"（Phase-Intercept Distortion）为一种频率无关的相位偏移，指出其虽显著改变波形但具备不可感知性。  <br/>2. **实验验证理论**：通过人类受试实验验证该假设，证明相位截断失真对人类听觉感知无显著影响。  <br/>3. **数据增强应用**：创新性地将相位截断失真引入音频数据增强领域，为机器学习提供新型训练数据生成方法。  <br/>4. **性能提升**：开展多组实验表明，该方法可有效提升音频机器学习任务的性能，验证其应用价值。|
|2506.14504v1|[Evolving music theory for emerging musical languages](http://arxiv.org/abs/2506.14504v1)|总结：  <br/>该研究通过现象学和归纳方法，提出音高是感知构造而非客观属性，揭示了音高分裂和多稳定性现象，挑战传统理论并整合调音系统生成机制，构建基于感知变异性的新模型。<br/><br/>贡献点：  <br/>1. **提出音高感知的主观性**：论证音高是听众与听觉条件共同塑造的感知构造，而非客观物理属性。  <br/>2. **揭示音高分裂现象**：通过准谐波音分析，证明单一音高可传递多重感知音高，引发音高分裂。  <br/>3. **发现音高感知的多稳定性**：指出同一听众对音高的感知可能随时间变化，具有多稳定性特征。  <br/>4. **重构调音系统理论**：提出调音系统可能源于音的内部结构，而非外部规则。  <br/>5. **引入跨学科类比**：借鉴海岸线悖论，支持以感知变异性为核心的模型，挑战传统理论框架。|
|2506.14503v1|[An Open Research Dataset of the 1932 Cairo Congress of Arab Music](http://arxiv.org/abs/2506.14503v1)|**总结**（100字以内）:  <br/>本研究发布ORD-CC32数据集，基于1932年开罗阿拉伯音乐大会录音，包含元数据、音高与节奏模式标签、人工标注音高信息及声学特征，支持阿拉伯音乐调音、调式及区域差异的计算分析，推动跨学科研究与开放资源共享。<br/><br/>---<br/><br/>**贡献点**分点列出：  <br/>1. **数据集构建**：创建首个基于1932年开罗阿拉伯音乐大会录音的开放研究数据集（ORD-CC32），涵盖历史弥足珍贵的阿拉伯音乐多传统样本。  <br/>2. **多维标注**：整合结构化元数据、旋律模式（maqam）与节奏模式（iqa）标签、人工标注的音高信息及先进的声学特征（基于高精度音高检测技术）。  <br/>3. **研究支持**：为计算分析阿拉伯音乐调音体系、调式差异及地域演变提供统一数据资源，推动计算音乐学、音乐信息检索等领域的研究。  <br/>4. **应用案例**：通过音高直方图分析方法，首次以数据驱动方式揭示阿拉伯音乐中微音差异的区域规律。  <br/>5. **资源开放**：在Zenodo平台公开共享数据集，配套提供特征提取与元数据检索工具，促进跨学科协作与学术透明化。|
|2506.14434v1|[Unifying Streaming and Non-streaming Zipformer-based ASR](http://arxiv.org/abs/2506.14434v1)|总结：  <br/>提出统一框架，通过动态右上下文与分块注意力掩码训练单一端到端ASR模型，兼顾流式与非流式场景，实验证明有效减少词错误率并灵活控制延迟-准确率权衡。|
|2506.14427v2|[M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization   Dataset](http://arxiv.org/abs/2506.14427v2)|贡献点：<br/>1. 提出多模态数据融合的自动化数据集构建方法，通过音频与视频信息结合生成高精度伪标签<br/>2. 发布首个包含多语言、多场景、多模态的通用说话人日志数据集(M3SD)，基于真实网络视频数据<br/>3. 开源数据集及配套代码，为语音领域研究提供可复现的基准资源<br/><br/>总结：本研究通过音频-视频融合技术构建了多模态、多场景、多语言的说话人日志数据集，并开源相关资源，有效缓解了数据不足与模型泛化难题。|
|2506.14427v1|[M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization   Dataset](http://arxiv.org/abs/2506.14427v1)|**贡献点：**  <br/>1. **提出多模态多场景多语言数据集构建方法**：通过音频与视频结合生成高精度伪标签，构建大规模、多样化的M3SD数据集，解决数据资源不足问题。  <br/>2. **设计场景相关模型微调策略**：基于预训练模型，结合目标场景（如会议）的特定数据，采用Adapter与LoRA联合微调实现模型领域适应优化。  <br/>3. **开源数据集与代码**：公开M3SD数据集及配套代码，推动语音领域研究复现与技术发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过多模态伪标签生成构建大规模数据集，并提出联合微调策略实现场景自适应，有效解决数据不足与模型泛化问题，开源资源助力领域发展。|
|2506.14398v1|[A Comparative Study on Proactive and Passive Detection of Deepfake   Speech](http://arxiv.org/abs/2506.14398v1)|总结：  <br/>提出统一评估框架，分析不同模型对语音属性扰动的鲁棒性差异，并公开实验代码。<br/><br/>贡献点：  <br/>1. **提出统一评估框架**：首次设计可同时评估主动水印模型与被动检测器的联合框架，解决两类方法对比缺乏统一标准的问题。  <br/>2. **标准化训练与测试流程**：所有模型基于相同数据集和共享评估指标进行训练和测试，确保公平比较。  <br/>3. **揭示模型脆弱性差异**：系统分析两类模型在对抗语音属性扰动（如音高、节奏等）时的具体弱点，为针对性防御策略提供依据。  <br/>4. **开放可复现代码**：提供完整训练与评估代码，促进研究社区的验证与扩展。|
|2506.14396v1|[Manipulated Regions Localization For Partially Deepfake Audio: A Survey](http://arxiv.org/abs/2506.14396v1)|**贡献点：**  <br/>1. 首次系统性综述部分深度伪造音频的操纵区域定位任务，填补该领域的研究空白。  <br/>2. 梳理现有方法的分类与技术基础，明确其核心框架与分支方向。  <br/>3. 分析当前技术的局限性（如检测难度、定位精度等），指出实际应用中的挑战。  <br/>4. 探讨潜在的发展趋势，为未来研究提供理论指导与方向参考。  <br/><br/>**总结（100字以内）：**  <br/>本文首次系统综述部分深度伪造音频的操纵区域定位问题，覆盖方法分类、技术基础及挑战，揭示当前研究空白，为提升安全检测提供关键洞见与未来方向。|
|2506.14293v3|[SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative   music modeling](http://arxiv.org/abs/2506.14293v3)|**贡献点：**<br/>1. 提出首个专为生成音乐建模任务（如文本-音乐、音乐描述、歌声合成等）构建的高质量、大规模流行音乐数据集Sleeping-DISCO 9M。<br/>2. 填补了现有开放数据集在流行歌曲及知名艺术家作品覆盖上的空白，解决生成音乐模型缺乏真实音乐语料的问题。<br/>3. 通过使用实际流行音乐而非合成/重新录制的音乐，提升数据集的多样性、真实性及现实场景适用性。<br/><br/>**总结（100字以内）：**  <br/>Sleeping-DISCO 9M是首个大规模、高质量的流行音乐数据集，填补生成音乐任务的空缺，通过真实音乐构建提升多样性与现实性，推动相关模型发展。|
|2506.14293v2|[SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative   music modeling](http://arxiv.org/abs/2506.14293v2)|贡献点总结（100字以内）:  <br/>提出Sleeping-DISCO 9M，首个基于真实流行音乐和知名艺术家的大规模数据集，填补生成音乐建模任务中缺乏真实音乐风格数据的空白，提升模型实际应用效果。<br/><br/>详细贡献点:  <br/>1. **首个真实音乐数据集**：构建了包含实际流行歌曲和世界知名艺术家的Sleeping-DISCO 9M，解决此前生成音乐任务中缺乏真实音乐样本的问题。  <br/>2. **高质量数据覆盖**：涵盖文本-音乐、音乐描述、演唱合成、旋律重建和跨模型检索等生成任务，提供统一且高质量的训练资源。  <br/>3. **对比与改进**：针对过往合成/再录制数据（如GTSinger）和纯大规模音频数据（如DISCO-10M）的局限性，强调真实音乐内容与风格的重要性。  <br/>4. **推动实际应用**：通过真实音乐数据增强模型对现实音乐场景的适应性，促进生成音乐社区的实践发展。|
|2506.14293v1|[SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative   music modeling](http://arxiv.org/abs/2506.14293v1)|总结：  <br/>本文提出Sleeping-DISCO 9M，一个基于真实流行音乐与艺术家的大型数据集，解决了生成音乐任务中缺乏真实性和多样性的挑战，推动音乐生成领域发展。<br/><br/>贡献点：  <br/>1. **首个高质量真实音乐数据集**：填补了语音领域生成任务中缺乏代表流行歌曲和知名艺术家的开放数据集的空白。  <br/>2. **真实音乐内容构建**：采用实际流行音乐而非合成或重新录制的音乐，更贴近现实音乐的风味和多样性。  <br/>3. **多任务适用性**：为文本-音乐、音乐描述生成、歌唱语音合成、旋律重建及跨模型检索等生成任务提供统一的数据支持。  <br/>4. **规模优势**：构建了比现有数据集（如DISCO-10M、LAIONDISCO-12M）更大的数据集（9M），提升模型训练效率与效果。|
|2506.14226v1|[Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification](http://arxiv.org/abs/2506.14226v1)|总结：  <br/>首次提出利用零样本文本到语音系统进行测试时数据增强，验证其对短语音识别的有效性，发现合成语音长度与真实语音效果差异，并为未来研究提供方向。  <br/><br/>贡献点：  <br/>1. **首次应用ZS-TTS系统**：将零样本文本到语音系统首次引入测试时数据增强框架，用于提升短语音说话人验证性能。  <br/>2. **多模型对比验证**：系统性评估NatureSpeech 3、CosyVoice和MaskGCT三种SOTA预训练ZS-TTS模型在说话人验证中的有效性。  <br/>3. **合成与真实语音结合**：实验证明结合真实与合成语音样本可显著降低EER（10%-16%），尤其对短时长语音提升明显。  <br/>4. **揭示语音长度差异**：分析发现合成语音的长度与真实语音效果存在差异，长合成语音未达到长真实语音的降噪效果。  <br/>5. **提供研究见解**：指出ZS-TTS在说话人验证中的潜力与挑战，为后续研究提供理论依据和改进方向。|
|2506.14223v1|[Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature   Transcription](http://arxiv.org/abs/2506.14223v1)|总结：  <br/>提出基于T5架构的Fretting-Transformer模型，解决吉他MIDI转录中的弦-品歧义和演奏性问题，结合多数据集与创新预处理方法，开发新评估指标并实验证明其优于现有方法和商业工具。<br/><br/>贡献点：  <br/>1. **提出Fretting-Transformer模型**：首个采用T5架构的编码器-解码器结构，实现MIDI序列到吉他指板的自动化转录。  <br/>2. **符号翻译框架**：将音乐转录任务转化为符号翻译问题，有效解决弦-品歧义及物理演奏性信息缺失的挑战。  <br/>3. **多数据集与预处理策略**：整合DadaGP、GuitarToday和Leduc数据集，设计全新预处理与标记方法以提升数据质量。  <br/>4. **专用评估指标**：开发量化吉他指板准确性和物理可演奏性的评估体系，完善领域评价标准。  <br/>5. **性能优化技术**：引入上下文敏感处理及调音/假音条件建模，显著提升模型在复杂场景下的转录精度。  <br/>6. **行业应用验证**：实验证明在准确性与实用性上超越A*算法和Guitar Pro等基线方法及商业系统，推动自动化吉他转录技术发展。|
|2506.14190v1|[AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](http://arxiv.org/abs/2506.14190v1)|总结：  <br/>提出AsyncSwitch框架，通过异步适应与文本预训练提升代码切换ASR性能，在马来-英语数据上实现9.02% WER降低，同时增强单语表现。<br/><br/>贡献点：  <br/>1. **异步适应框架设计**：首个基于异步策略的代码切换ASR预训练框架，无需同步语音-文本配对数据，降低数据收集成本。  <br/>2. **三阶段训练流程**：  <br/>   - 阶段一：仅用代码切换文本预训练解码器自注意力和前馈层；  <br/>   - 阶段二：通过交叉注意力机制对齐编码器与解码器；  <br/>   - 阶段三：基于配对语料全模型微调，提升多语言适应性。  <br/>3. **跨语言性能提升**：在Whisper模型上验证框架有效性，显著降低马来-英语代码切换的WER，并增强对Singlish、Malay等语言的单语识别能力。|
|2506.14177v1|[Can we train ASR systems on Code-switch without real code-switch data?   Case study for Singapore's languages](http://arxiv.org/abs/2506.14177v1)|**贡献点总结：**  <br/>1. 提出词组级混合方法生成合成代码切换语料，模拟真实语言模式。  <br/>2. 通过合成数据与单语数据结合，优化大模型（Whisper/MMS/SeamlessM4T）的代码切换语音识别性能。  <br/>3. 首次建立针对东南亚低资源语言对（BM-EN、ZH-BM、TA-EN）的代码切换语音识别基准测试。  <br/>4. 实验证明该方法显著提升模型在代码切换和单语任务中的表现，且具备成本效益。  <br/><br/>**简要总结（100字内）：**  <br/>本研究提出词组级合成数据生成方法，结合单语数据优化大模型，针对东南亚低资源语言对建立新代码切换语音识别基准，验证了其在提升识别性能和降低数据成本方面的有效性。|
|2506.14153v1|[Pushing the Performance of Synthetic Speech Detection with   Kolmogorov-Arnold Networks and Self-Supervised Learning Models](http://arxiv.org/abs/2506.14153v1)|总结（100字以内）:  <br/>本文提出将Kolmogorov-Arnold网络(KAN)应用于XLSR-Conformer模型，替代传统MLP，显著提升合成语音检测性能，在ASVspoof2021数据集实现60.55%相对提升和0.70% EER，验证了KAN在SSL框架下的有效性与应用前景。  <br/><br/>贡献点:  <br/>1. **架构创新**：首次提出利用Kolmogorov-Arnold表示定理设计的KAN网络，替代传统MLP作为SSL模型的核心组件。  <br/>2. **性能提升**：在ASVspoof2021基准数据集上，通过集成KAN使合成语音检测性能在LA和DF集提升60.55%，在21LA集实现0.70% EER。  <br/>3. **方法有效性验证**：系统性评估KAN在SSL模型中的适用性，为合成语音检测提供了新的高效架构方向。|
|2506.14148v1|[Acoustic scattering AI for non-invasive object classifications: A case   study on hair assessment](http://arxiv.org/abs/2506.14148v1)|**贡献点：**  <br/>1. 提出基于声学散射的新颖非侵入性物体分类方法，用于头发评估。  <br/>2. 通过发射声学刺激并采集散射信号，实现对头发类型和湿度的AI驱动分类。  <br/>3. 对比多种分类方法（全监督学习、嵌入式分类、监督基础模型微调、自监督模型微调），证明自监督模型全参数微调策略的优越性。  <br/>4. 实验结果达到近90%分类准确率，验证方法的有效性。  <br/>5. 强调声学散射在隐私保护和非接触场景中的优势，为多行业应用提供新方向。  <br/><br/>**总结：**  <br/>本研究提出基于声学散射的非侵入性分类方法，通过自监督模型微调实现高精度头发评估，为隐私保护和非接触检测提供新思路。|
|2506.13833v1|[A Survey on World Models Grounded in Acoustic Physical Information](http://arxiv.org/abs/2506.13833v1)|总结：  <br/>该综述系统总结了基于声学物理信息的世界模型研究，涵盖理论框架、核心方法、应用案例及挑战，提出未来研究方向以实现可靠、因果且负责任的声学智能。<br/><br/>贡献点：  <br/>1. **理论构建**：阐明物理定律如何指导声学信号中物理信息的编码机制，建立声学世界模型的理论基础。  <br/>2. **方法综述**：梳理物理信息神经网络（PINNs）、生成模型及自监督多模态学习等核心方法框架。  <br/>3. **应用拓展**：总结声学世界模型在机器人、自动驾驶、医疗与金融等领域的关键应用。  <br/>4. **挑战分析**：系统讨论技术（如不确定性建模）与伦理问题，并提出未来研究路线图。  <br/>5. **方向展望**：倡导发展具身主动声学智能，推动AI系统通过声学构建“直觉物理”引擎。|
|2506.13709v1|[SpeechRefiner: Towards Perceptual Quality Refinement for Front-End   Algorithms](http://arxiv.org/abs/2506.13709v1)|总结：  <br/>本研究提出SpeechRefiner，通过Conditional Flow Matching显著提升语音感知质量，克服传统预处理技术的不足。  <br/><br/>贡献点：  <br/>1. **提出新型后处理工具 SpeechRefiner**：针对传统语音预处理（降噪、去混响、分离）的缺陷（如残余噪声、新artifacts），设计了感知质量优化模块。  <br/>2. **引入 Conditional Flow Matching 方法**：利用CFM框架实现语音质量提升，填补现有工具在感知质量评价方面的空白。  <br/>3. **多任务对比实验验证有效性**：与近期任务特定优化方法进行基准测试，证明SpeechRefiner在感知质量改进上的优势。  <br/>4. **多阶段处理流程整合**：在内部集成多前端算法的处理管道中验证其泛化能力，提升整体系统表现。  <br/>5. **提供可验证的音频演示**：通过实际音频示例直观展示改进效果，增强成果的可信度和实用性。|
|2506.13595v1|[Persistent Homology of Music Network with Three Different Distances](http://arxiv.org/abs/2506.13595v1)|总结：  <br/>本研究将持久同调应用于音乐图，提出三种基于边路径的距离定义，分析其对拓扑特征的影响，发现一维持久同调的包含关系，并通过真实音乐数据验证成果。<br/><br/>贡献点：  <br/>1. **方法创新**：首次将预定义权重的音乐图与持久同调结合，探索其在音乐数据中的拓扑分析潜力。  <br/>2. **距离定义**：提出三种基于边路径的新型距离度量，拓展了音乐图拓扑分析的多样性。  <br/>3. **拓扑影响分析**：系统研究不同距离定义对持久条码、持续图及出生/死亡边的影响机制。  <br/>4. **理论发现**：揭示一维持久同调中存在包含关系，并证明其在不同距离定义下的拓扑结构关联性。  <br/>5. **实证验证**：通过真实音乐数据验证理论发现，提升研究结果的实用性和可信度。|
|2506.13455v1|[Stereo sound event localization and detection based on PSELDnet   pretraining and BiMamba sequence modeling](http://arxiv.org/abs/2506.13455v1)|总结：  <br/>提出基于BiMamba和非对称卷积的SELD系统，在保持性能的同时降低计算复杂度。<br/><br/>贡献点：  <br/>1. **提出新型架构**：结合预训练PSELDnet与双向Mamba序列建模，构建SELD系统。  <br/>2. **替换传统模块**：以BiMamba替代Conformer模块，优化序列建模效率。  <br/>3. **引入非对称卷积**：增强对时间-频率维度间时空关系的建模能力。  <br/>4. **验证性能与效率**：在DCASE2025数据集上实现性能提升且计算复杂度降低。  <br/>5. **突出方法优势**：证明BiMamba架构在应对SELD任务挑战中的有效性。|
|2506.13414v1|[BUT System for the MLC-SLM Challenge](http://arxiv.org/abs/2506.13414v1)|贡献点：  <br/>1. 提出双说话人ASR系统，整合DiCoW（Whisper的diarization-conditioned变体）与DiariZen（基于Pyannote的diarization管道）  <br/>2. 验证DiariZen在跨领域多语言场景中的强泛化能力（无需微调）  <br/>3. 展示DiCoW通过领域适应在MLC-SLM挑战数据中实现性能提升  <br/>4. 识别训练数据中的标签不一致性（如缺失语音段、错误静默标注），提出解决策略以增强系统鲁棒性  <br/><br/>总结：  <br/>本研究构建了跨语言双说话人ASR系统，通过对比实验验证了DiariZen的泛化能力与DiCoW的领域适应效果，同时提出数据标注优化方法提升系统稳定性。|
|2506.13396v1|[Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR](http://arxiv.org/abs/2506.13396v1)|总结：  <br/>提出语言特定双向上下文集成与字符级掩码训练策略，结合两阶段解码方法，显著提升多语言连续对话ASR性能，超越高数据量基线模型。<br/><br/>贡献点：  <br/>1. **引入语言特定双向上下文**：将多语言对话中的双向上下文信息融入语音大语言模型（SLLM），提升跨语言理解能力。  <br/>2. **字符级上下文掩码策略**：在训练中随机移除部分上下文以增强模型鲁棒性，模拟真实场景中的不完整转录问题。  <br/>3. **两阶段解码框架**：采用“孤立段解码+上下文重解码”流程，通过邻近假设优化最终转录结果。  <br/>4. **数据效率验证**：在1500小时多语言数据集上实现18%相对提升，超越6000小时训练的竞赛基线模型。|
|2506.13339v1|[NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025](http://arxiv.org/abs/2506.13339v1)|贡献点：<br/>1. 开发了NTU Speechlab多语言语音识别系统并取得Interspeech 2025 MLC-SLM挑战赛第5名<br/>2. 提出语言特定提示（language-specific prompts）与模型平均（model averaging）技术<br/>3. 实现Mix Error Rate从20.2%降至10.6%的显著提升（绝对改善9.6%，相对改善48%）<br/>4. 系统性优化模型架构、数据选择与训练策略<br/>5. 为未来Speech Large Language Models提供实践性技术借鉴<br/><br/>总结（100字内）：<br/>本研究构建了NTU Speechlab多语言语音识别系统，在挑战赛中取得优异成绩，通过语言提示与模型平均技术显著提升识别准确率，为构建高效多语言语音大模型提供了有效方法和实践参考。|
|2506.13300v3|[Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models](http://arxiv.org/abs/2506.13300v3)|**贡献点：**  <br/>1. 提出适用于MLC-SLM双赛道的系统框架（ASR与SD-ASR），实现跨语言对话场景的语音处理。  <br/>2. 设计多阶段训练流程，通过课程学习、思维链数据增强和RLVR技术显式提升模型推理与自校正能力。  <br/>3. 实验结果显著优于官方基准，在Track 1（WER/CER 11.57%）和Track 2（tcpWER/tcpCER 17.67%）均取得优异性能。  <br/>4. 通过全面的消融研究验证各组件在挑战约束下的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出多阶段训练框架，结合课程学习、思维链数据增强和RLVR技术，显著提升多语言对话语音模型的ASR与SD-ASR性能，并通过消融实验验证方法有效性，成果优于官方基准。|
|2506.13300v2|[Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models](http://arxiv.org/abs/2506.13300v2)|**贡献点：**  <br/>1. 提出针对多语言对话语音语言模型挑战（MLC-SLM）的系统，覆盖ASR与SD-ASR两个赛道。  <br/>2. 设计多阶段训练框架，通过课程学习、Chain-of-Thought数据增强和RLVR技术增强模型的推理与自纠正能力。  <br/>3. 在MLC-SLM官方测试集中取得优于基线的性能：Track 1 WER/CER为11.57%，Track 2 tcpWER/tcpCER为17.67%。  <br/>4. 通过全面的消融研究验证各组件在挑战约束下的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出多语言对话语音模型的多阶段训练框架，融合课程学习、Chain-of-Thought数据增强和RLVR技术，显著提升ASR与SD-ASR性能，并通过消融实验验证各组件的有效性。|
|2506.13300v1|[Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models](http://arxiv.org/abs/2506.13300v1)|总结:  <br/>该论文提出了一种多阶段训练框架，结合课程学习、思维链数据增强和RLVR技术，显著提升多语言对话语音识别及说话人分割任务的性能，并通过消融实验验证了各组件的有效性。<br/><br/>贡献点:  <br/>1. 提出多阶段训练框架：通过显式增强推理能力与自纠错机制，优化语音语言模型在ASR任务中的表现。  <br/>2. 整合三种关键技术：  <br/>   - 课程学习：分阶段逐步提升模型能力；  <br/>   - 思维链数据增强：促进中间推理过程；  <br/>   - RLVR（奖励驱动优化）：基于可验证奖励强化自校正能力。  <br/>3. 实现SOTA性能：在MLC-SLM挑战的两个赛道均取得显著结果（WER/CER 11.57%、tcpWER/tcpCER 17.67%）。  <br/>4. 全面消融验证：在挑战约束下证明各技术组件的独立有效性。|
|2506.13295v1|[Instance-Specific Test-Time Training for Speech Editing in the Wild](http://arxiv.org/abs/2506.13295v1)|**贡献点：**<br/>1. 提出实例特定的测试时训练方法，显著提升语音编辑系统在未见声学条件下的泛化能力。<br/>2. 通过直接监督（未编辑区域真实音色特征）与间接监督（基于时长约束和音素预测的辅助损失）结合，解决带宽不连续问题。<br/>3. 实现精准的语音速率控制，利用掩码长度调整适配目标时长。<br/>4. 在真实场景基准数据集上验证方法有效性，优于现有系统在客观和主观指标上。<br/><br/>**总结：**  <br/>该论文提出一种测试时训练方法，通过融合直接与间接监督机制，有效解决语音编辑中的带宽不连续问题，并实现对语音速率的精准控制，显著提升系统在复杂声学环境下的性能。|
|2506.13279v1|[Boundary-Informed Sound Field Reconstruction](http://arxiv.org/abs/2506.13279v1)|**贡献点：**  <br/>1. **提出中间问题设定**：针对部分或不确定边界信息的情况，提出介于完全无信息与完全已知信息之间的声场重建方法。  <br/>2. **构建边界感知先验模型**：基于阻抗边界条件设计边界信息先验，将物理约束融入声场建模中。  <br/>3. **联合优化超参数**：在贝叶斯框架下，同时优化噪声方差、信号方差及阻抗边界条件等未知超参数。  <br/>4. **验证方法有效性**：通过数值实验证明，即使仅提供数百个边界点或存在1分米的边界不确定性，该方法仍能显著提升重建精度。  <br/><br/>**总结：**  <br/>本文通过结合边界信息先验与贝叶斯框架，提出了一种在部分边界信息下高精度声场重建方法，验证了其在减少测量需求和应对边界不确定性中的有效性。|
|2506.13272v1|[SONIC: Sound Optimization for Noise In Crowds](http://arxiv.org/abs/2506.13272v1)|总结：  <br/>提出了基于ARM Cortex-M7的嵌入式实时降噪系统SONIC，采用LMS算法提升语音清晰度，创新性解决传统ANC的局限性，并展示了低功耗DSP在实际场景中的有效性。<br/><br/>贡献点：  <br/>1. **提出新型嵌入式降噪系统**：开发了SONIC系统，基于STM32H753ZI微控制器实现实时噪声抑制，适用于资源受限的嵌入式场景。  <br/>2. **替代传统ANC方法**：采用自适应滤波（LMS）技术，优化语音清晰度，突破传统ANC在动态噪声处理中的局限性。  <br/>3. **微控制器信号处理优化**：系统从嵌入式硬件视角出发，分析信号处理算法的性能因素，确定最优实现方案。  <br/>4. **架构效率提升**：设计高效系统架构，充分利用MCU的计算能力，实现低功耗与实时性的平衡。  <br/>5. **低功耗DSP方案验证**：通过实验证明低功耗DSP在音频降噪中的可行性，提供对复杂AI方法的替代路径。|
|2506.13199v1|[Do Music Preferences Reflect Cultural Values? A Cross-National Analysis   Using Music Embedding and World Values Survey](http://arxiv.org/abs/2506.13199v1)|总结：  <br/>本研究通过音乐数据分析揭示国家文化价值观，发现音乐偏好与文化区域存在显著关联，为理解全球文化边界提供新视角。<br/><br/>贡献点：  <br/>1. **跨文化音乐数据集**：首次整合62国长期音乐偏好数据（涵盖西方与非西方），构建全球范围的音乐偏好图谱。  <br/>2. **多模态分析方法**：结合CLAP音频嵌入与LP-MusicCaps/GPT语义生成，实现音乐内容的量化与语义双重表征。  <br/>3. **对比聚类技术**：基于对比嵌入识别各国音乐与全球标准的差异，提出新的文化差异分析框架。  <br/>4. **文化关联验证**：通过MANOVA和卡方检验，验证音乐聚类与世界价值观调查（WVS）文化区的显著匹配性。  <br/>5. **文化信号编码**：揭示音乐偏好反映文化价值观的潜在机制，确立音乐数据作为文化研究代理的可行性。|
|2506.13053v1|[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching](http://arxiv.org/abs/2506.13053v1)|总结：  <br/>提出ZipVoice，通过轻量化设计实现高质量零样本语音合成，相较传统模型参数减少3倍、推理速度提升30倍，并开放代码和模型资源。<br/><br/>贡献点：  <br/>1. 提出Zipformer-based流匹配解码器，解决模型压缩与建模能力之间的矛盾，维持高质量语音生成；  <br/>2. 引入平均上采样初始对齐方法与Zipformer编码器，增强语音可懂度；  <br/>3. 开发流蒸馏技术，减少采样步骤并消除无分类器引导的推理开销；  <br/>4. 在100k小时多语言数据集上验证模型效果，达到SOTA水平的同时显著提升效率；  <br/>5. 开源代码、模型权重及演示样本，促进研究复现与应用。|
|2506.13001v1|[Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV](http://arxiv.org/abs/2506.13001v1)|**总结（100字以内）**:  <br/>本研究提出MIDI-RWKV模型，解决传统端到端音乐生成在协同创作中的局限性，支持个性化、多轨道、长上下文与可控的音乐填充，并在边缘设备上实现高效生成，同时验证了低样本量下的微调方法，开源模型与代码。  <br/><br/>**贡献点（分点）**:  <br/>1. **提出音乐填充新范式**：针对音乐创作的迭代特性，设计支持个性化、多轨道、长上下文和可控性的符号音乐填充方法，弥补端到端系统在人机协作中的不足。  <br/>2. **开发轻量化模型架构**：基于RWKV-7线性结构构建MIDI-RWKV模型，优化边缘设备上的高效与连贯音乐共创作能力。  <br/>3. **创新初始状态微调技术**：提出无需大量样本的低样本微调策略，实现模型个性化定制，提升适应用户体验。  <br/>4. **全面评估与开源**：通过定量化与定性指标验证模型效果，并公开代码和权重，促进研究复现与应用。|
|2506.12817v1|[Magnetoencephalography (MEG) Based Non-Invasive Chinese Speech Decoding](http://arxiv.org/abs/2506.12817v1)|**贡献点列表：**  <br/>1. 构建首个针对非侵入式中文语音BCI的文本-磁共振脑电图（text-MEG）数据集。  <br/>2. 提出多模态辅助语音解码（MASD）算法，同时整合文本和声学信息以提升解码精度。  <br/>3. 通过实验验证了text-MEG数据集及MASD算法在实际任务中的有效性。  <br/>4. 填补了非侵入式语音BCI中多模态辅助解码研究的空白，为中文语音BCI发展提供新方向。  <br/><br/>**总结（100字以内）：**  <br/>本文首次构建中文非侵入式文本-MEG数据集，并提出多模态辅助语音解码算法，验证其有效性，推动语音BCI在中文场景下的研究进展。|
|2506.12785v1|[Frequency Dynamic Convolutions for Sound Event Detection](http://arxiv.org/abs/2506.12785v1)|总结（100字以内）:  <br/>本文提出频率动态卷积（FDY conv）及其扩展模型（DFD、PFD、MDFD、TFD），通过频率自适应机制提升声事件检测性能，解决了传统2D卷积的频率依赖性不足问题，实验证明在多个数据集和应用场景中均有效。<br/><br/>贡献点分点：  <br/><br/>1. **提出Frequency Dynamic Convolution (FDY conv)**  <br/>   - 引入动态调整卷积核的机制，基于输入信号的频率成分构建最优频率响应，通过频率特定注意力权重对多基核进行自适应加权，提升SED性能（DESED数据集提升7.56%）。  <br/><br/>2. **提出Dilated FDY Conv (DFD conv)**  <br/>   - 使用不同扩张率的卷积核扩展频率轴感受野，增强频率特异性特征表示，性能提升至9.27%。  <br/><br/>3. **提出Partial FDY Conv (PFD conv)**  <br/>   - 通过结合标准2D卷积与频率自适应核，降低计算复杂度（参数减少54.4%），同时保持性能，提升7.80%。  <br/><br/>4. **提出Multi-Dilated FDY Conv (MDFD conv)**  <br/>   - 通过多扩张率卷积核解决DFD的结构限制，更全面捕捉频率依赖模式，性能达到10.98%。  <br/><br/>5. **提出TAP-FDY Conv (TFD conv)**  <br/>   - 集成时序注意力池化（TA）、速度注意力池化（VA）和平均池化（AP），兼顾静态、准静态和瞬态事件检测，参数减少30.01%且性能等同MDFD conv。  <br/><br/>6. **实验证明模型有效性**  <br/>   - 案例研究表明：  <br/>     - FDY conv增强非平稳事件检测；  <br/>     - DFD conv对宽谱特征事件更有效；  <br/>     - PFD conv提升准平稳事件检测；  <br/>     - TFD conv在瞬态信号检测中表现最佳。|
|2506.12705v1|[Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss   and Cochlear Neural Degeneration](http://arxiv.org/abs/2506.12705v1)|**贡献点总结：**  <br/>1. 提出通过计算听觉外周模型和神经图相似性指数测度（NSIM）量化听力损失与耳蜗神经退化（CND）的客观方法。  <br/>2. 验证NSIM在音素识别任务中可准确映射听力障碍者的听觉性能。  <br/>3. 证明NSIM能敏感检测CND引起的听力缺陷，作为非侵入性听觉突触病生物标志物的候选。  <br/><br/>**摘要总结（100字内）：**  <br/>本研究提出NSIM方法，通过模拟听觉外周计算模型，量化听力损失与CND，并验证其在音素识别和突触病检测中的应用价值。|
|2506.12672v1|[SC-SOT: Conditioning the Decoder on Diarized Speaker Information for   End-to-End Overlapped Speech Recognition](http://arxiv.org/abs/2506.12672v1)|总结：  <br/>本文提出SC-SOT框架，通过显式引入说话人信息提升多说话人语音识别性能，解决了SOT隐式分隔的不足，并在重叠语音场景中验证了其有效性。<br/><br/>贡献点：  <br/>1. **提出SC-SOT方法**：首次将说话人信息显式结合到基于SOT的端到端多说话人ASR系统中，解决隐式分隔不足的问题。  <br/>2. **揭示隐式分离的局限性**：发现SOT在重叠语音中依赖模糊的声学线索进行隐式说话人分离，并提出改进策略。  <br/>3. **引入说话人嵌入**：通过联合训练的端到端说话人分离模型生成嵌入，帮助解码器聚焦目标说话人的特征。  <br/>4. **融合说话人活动信息**：利用说话人活动信号指导解码器抑制非目标说话人，增强语音识别鲁棒性。  <br/>5. **无需说话人注册**：通过联合训练的模型直接生成说话人信息，避免传统方法依赖的注册步骤。  <br/>6. **实验验证有效性**：在重叠语音场景中展示SC-SOT的显著性能提升，证实其在多说话人ASR中的优越性。|
|2506.12665v1|[ANIRA: An Architecture for Neural Network Inference in Real-Time Audio   Applications](http://arxiv.org/abs/2506.12665v1)|总结：  <br/>本研究提出了跨平台推理库anira，支持多种框架并解决实时音频应用中的延迟问题，通过解耦机制和基准测试优化性能，揭示了不同模型与引擎组合的实时表现差异。<br/><br/>贡献点：  <br/>1. **跨平台高效推理库开发**：提出anira，兼容ONNX Runtime、LibTorch和TensorFlow Lite，满足实时音频应用需求。  <br/>2. **实时性优化机制**：通过解耦推理与音频回调至静态线程池，降低实时违规风险，确保信号连续性。  <br/>3. **内置延迟管理与基准测试**：集成延迟控制功能，并对多配置下的不同神经网络架构进行系统性性能评估。  <br/>4. **实证分析与统计建模**：量化分析模型架构、框架及实时性因素对运行效率的影响，为实际应用提供数据支持。  <br/>5. **模型-引擎匹配建议**：揭示特定组合的初始推理延迟问题，提出状态模型优先LibTorch、无状态模型优先ONNX的优化结论。|
|2506.12627v1|[Towards Neural Audio Codec Source Parsing](http://arxiv.org/abs/2506.12627v1)|**贡献点总结**：<br/>1. 提出NACSP框架，将音频深度伪造的源归属问题转化为对生成参数的结构化回归任务，突破传统开放集归属的局限；<br/>2. 构建首个基于语音预训练模型的全面基准，系统评估NAC参数检测性能；<br/>3. 设计HYDRA框架，利用双曲几何解耦潜在属性，通过曲率感知子空间实现多任务泛化；<br/>4. 验证HYDRA在双曲空间中的优越性，相较欧几里得空间基线模型提升检测效果。|
|2506.12500v1|[Mitigating Non-Target Speaker Bias in Guided Speaker Embedding](http://arxiv.org/abs/2506.12500v1)|总结（100字以内）:  <br/>本文提出基于目标语音活动的改进方法，解决多说话人场景中说话人嵌入在低重叠情况下的性能退化问题，提升低/高重叠场景的说话人验证和分割效果。<br/><br/>贡献点:  <br/>1. **揭示退化根源**：分析现有框架中全局统计模块对非目标语音区间的过度敏感性是导致低重叠条件下性能下降的关键原因。  <br/>2. **提出改进机制**：设计基于目标语音活动的统计方法，仅在目标活跃区间计算统计量，减少非目标噪声干扰。  <br/>3. **提升多场景性能**：方法在低重叠和高重叠场景均实现说话人验证性能提升，同时改善多数据集的说话人分割效果。  <br/>4. **优化框架鲁棒性**：通过引入目标活动指导，增强说话人嵌入在复杂语音环境下的鲁棒性和适用性。|
|2506.12440v1|[Style-based Composer Identification and Attribution of Symbolic Music   Scores: a Systematic Survey](http://arxiv.org/abs/2506.12440v1)|**总结**：  <br/>本研究首次系统综述风格化作曲家识别与作者归属的文献，揭示研究不足，提出稳健评估指标和未来研究指南，提升可靠性与音乐学有效性。<br/><br/>**贡献点**：  <br/>1. **首次系统综述**：系统梳理风格化作曲家识别与作者归属领域的58篇文献，填补该方向研究空白。  <br/>2. **批判性分析**：评估现有研究的验证协议、计算方法及数据集平衡问题，指出其局限性。  <br/>3. **指标优化建议**：强调平衡准确率和严格交叉验证的重要性，提出更科学的评估标准。  <br/>4. **特征与模型演进**：总结多样的特征表示方式及机器学习方法的发展历程。  <br/>5. **实证案例研究**：分析巴赫、乔斯金、列侬-麦卡特尼等争议作品，探讨算法在音乐归属中的实际应用与挑战。  <br/>6. **未来研究指导**：提出可操作的指南，提升研究的可信度、可重复性与音乐学价值。|
|2506.12405v1|[Methods for pitch analysis in contemporary popular music: multiple   pitches from harmonic tones in Vitalic's music](http://arxiv.org/abs/2506.12405v1)|总结：  <br/>该研究揭示当代流行音乐中，单个谐波复调音可产生多个主动感知音高，通过实验验证音乐属性与听觉感知的关系及个体差异。<br/><br/>贡献点：  <br/>1. **提出新理论观点**：首次指出当代流行音乐中多重感知音高是创作者有意为之的主动特征，而非偶然现象。  <br/>2. **实证研究方法**：设计两项听力实验（感知数量评估与手动转录），系统分析音乐信号与听觉感知的关联。  <br/>3. **发现音乐属性影响**：揭示合成谐波音通过突出上泛音和特定自相关特征，可引发更多模糊音高感知。  <br/>4. **强调个体差异**：证明听众的感知结果存在显著差异，说明听觉体验的主观性和环境条件的重要性。  <br/>5. **案例支持**：以电子艺术家Vitalic的作品为实例，展示多重音高的实际应用及文化语境相关性。|
|2506.12311v1|[Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech](http://arxiv.org/abs/2506.12311v1)|总结：  <br/>提出Phonikud，首个基于IPA的希伯来语轻量级G2P系统，开源代码与数据，并构建ILSpeech数据集作为基准，显著提升实时TTS的速度-准确性平衡。<br/><br/>贡献点：  <br/>1. **首个综合解决方案**：开发Phonikud系统，首次为现代希伯来语提供完整的IPA转录，解决传统G2P忽略重音等音素特征的不足。  <br/>2. **高效轻量设计**：通过轻量级适配器优化现有变音符号模型，实现几乎无延迟的实时文本处理。  <br/>3. **开源开放数据集**：发布ILSpeech数据集（带IPA标注的转录语音），作为希伯来语G2P与TTS的基准及训练资源。  <br/>4. **性能提升验证**：实验表明Phonikud在G2P准确性上优于现有方法，推动实时希伯来TTS模型的高效训练。  <br/>5. **促进研究复用**：公开代码、数据与模型，支持学术界和工业界进一步开发与应用。|
|2506.12008v1|[Reimagining Dance: Real-time Music Co-creation between Dancers and AI](http://arxiv.org/abs/2506.12008v1)|总结（100字以内）:  <br/>本研究提出了一种新型AI系统，通过多模态架构实现舞蹈动作与音乐的双向互动，使舞者成为音乐创作的共同参与者，拓展了AI在表演艺术中的协作角色。<br/><br/>贡献点分点列出:  <br/>1. **双向创意合作模式**：建立舞者与音乐的互动关系，突破传统单向“动作回应音乐”范式，实现“音乐响应动作”的动态共创。  <br/>2. **多模态实时生成系统**：设计可感知舞蹈动作并智能组合预录音乐片段的系统，生成连贯的音乐环境以支持即兴创作。  <br/>3. **动作-音频特征关联分析**：通过数据挖掘揭示舞蹈动作质量和音乐特征之间的隐含通信规律，为人工智能艺术干预提供理论依据。  <br/>4. **扩展AI在表演艺术的应用**：将AI定位为“响应型合作者”，赋能专业舞蹈和大众即兴艺术表达，提升创作可能性与互动性。|
|2506.11862v1|[Confidence-Based Self-Training for EMG-to-Speech: Leveraging Synthetic   EMG for Robust Modeling](http://arxiv.org/abs/2506.11862v1)|**贡献点：**  <br/>1. 提出**Confidence-based Multi-Speaker Self-training (CoM2S)** 新方法，通过合成数据与音素置信度过滤机制提升V-ETS模型性能；  <br/>2. 构建**Libri-EMG** 数据集，首个开放、时间对齐、多说话人的voiced EMG与语音配对数据集；  <br/>3. 实验验证方法有效性，显著提升音素准确率、降低语音混淆和词错误率；  <br/>4. 开源代码与数据集，为后续研究提供基准资源。  <br/><br/>**总结（100字以内）:**  <br/>该研究提出CoM2S方法和Libri-EMG数据集，利用合成数据与音素置信度过滤机制提升V-ETS模型性能，有效解决数据稀缺问题，并开源资源支持未来研究。|
|2506.11811v1|[Abstract Sound Fusion with Unconditioned Inversion Model](http://arxiv.org/abs/2506.11811v1)|**贡献点总结：**  <br/>1. 首次定义抽象音并提出声音融合的目标，即生成超越简单叠加的音频特征。  <br/>2. 开发基于DPMSolver++采样的新型SDE和ODE反演模型，通过固定模型输出消除循环依赖。  <br/>3. 无需提示条件即可实现灵活引导的可控音频合成，提升生成自由度与稳定性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出抽象音概念与声音融合方法，基于DPMSolver++构建新型反演模型，消除噪声预测循环依赖，实现无需提示的灵活可控音频合成，为生成超越简单叠加的音频提供了新途径。|
|2506.11747v1|[Enabling automatic transcription of child-centered audio recordings from   real-world environments](http://arxiv.org/abs/2506.11747v1)|**贡献点总结：**  <br/>1. 提出一种新型方法，通过筛选可可靠转录的语句，提升ASR在儿童长时语音数据中的准确性，避免全量处理带来的误差问题。  <br/>2. 验证该方法在四个英语语料库上的有效性，证明部分转录可实现显著优于全量转录的词错误率（WER）。  <br/>3. 通过统计自动转录与人工标注的词频相关性（r=0.92-0.98），验证自动转录结果的可靠性与有效性。  <br/>4. 为实现儿童中心长时音频数据的细节化自动化语言分析提供了一种实用且可扩展的解决方案。|
|2506.11703v1|[Tracking of Spatially Dynamic Room Impulse Responses Along Locally   Linearized Trajectories](http://arxiv.org/abs/2506.11703v1)|**贡献点**  <br/>1. **方法扩展**：将原始基于线性轨迹的RIR估计方法从短轨迹延伸至更长的轨迹，通过分段处理提高适用性。  <br/>2. **场景泛化**：放宽时间区间非重叠的假设，适配更复杂的房间声学环境和真实运动轨迹。  <br/>3. **真实数据验证**：在trajectoRIR数据库上进行实验，验证分段方法在控制L形轨迹的真实房间场景中的有效性。  <br/>4. **实用化改进**：通过离散点RIR测量与移动麦克风数据结合，提供更高效的多点RIR估计方案。  <br/><br/>**总结**  <br/>该论文提出一种分段处理的RIR估计方法，扩展了原始线性轨迹模型的适用范围，并通过真实数据验证其有效性，适用于复杂房间环境中的多点声学分析。|
|2506.11630v1|[Lightweight and Robust Multi-Channel End-to-End Speech Recognition with   Spherical Harmonic Transform](http://arxiv.org/abs/2506.11630v1)|总结（100字以内）:  <br/>提出SHTNet框架，通过空间声场分解、SSAFN网络及Rand-SHT训练解决多通道ASR的跨阵列泛化问题，显著降低计算量并提升系统鲁棒性。<br/><br/>贡献点分点列出:  <br/>1. **几何不变的声场分解**：利用球面谐波变换（SHT）将麦克风信号转化为几何无关的谐波系数，实现信号处理与阵列几何的解耦。  <br/>2. **多模态注意力融合网络（SSAFN）**：结合坐标感知的空间建模、改进的自注意力信道融合及频谱噪声抑制，无需传统波束成形模块。  <br/>3. **随机训练策略（Rand-SHT）**：通过随机通道选择与阵列几何重建增强模型泛化能力与鲁棒性，提升跨异构阵列（如圆形、方形、双耳阵列）的性能。  <br/>4. **高效性验证**：在Aishell-4、Alimeeting和XMOS等多数据集上实现39.26%平均CER，计算量比传统神经波束成形器减少97.1%。|
|2506.11620v1|[(SimPhon Speech Test): A Data-Driven Method for In Silico Design and   Validation of a Phonetically Balanced Speech Test](http://arxiv.org/abs/2506.11620v1)|总结：  <br/>本研究提出一种基于计算的语音测试方法SimPhon，通过模拟听力损失的感知效应优化单词对选择，解决了传统听力测试对高阈值听力损失表征不足的问题，提升了诊断效率和临床适用性。<br/><br/>贡献点：  <br/>1. **提出新型测试方法**：开发SimPhon语音测试体系，首次构建多阶段计算流程实现语音测试的虚拟设计与验证。  <br/>2. **模拟感知机制**：利用现代ASR系统替代人工听者，模拟感音神经性听力损失对语音感知的影响。  <br/>3. **数据驱动优化**：通过控制声学降质分析，识别高频音素混淆模式，指导大规模语料库的候选词对筛选。  <br/>4. **专家协同精炼**：结合模拟测试与人工校验，系统化筛选出25个优化单词对（SimPhon-25）。  <br/>5. **突破传统表征**：测试性能与标准SII无显著关联，揭示其捕捉的感知缺陷超出了单纯听阈范围。  <br/>6. **提升开发效率**：计算优化的测试集显著降低研发成本，为临床应用提供可直接试验的工具。|
|2506.11605v1|[Dissecting the Segmentation Model of End-to-End Diarization with Vector   Clustering](http://arxiv.org/abs/2506.11605v1)||
|2506.11532v1|[From Sharpness to Better Generalization for Speech Deepfake Detection](http://arxiv.org/abs/2506.11532v1)|**贡献点：**<br/>1. **提出理论框架**：首次将sharpness作为语音假声检测（SDD）泛化的理论代理，突破原有依赖性能指标（如等错误率）的评估方式。  <br/>2. **揭示sharpness特性**：发现sharpness在领域迁移时显著增加，表明模型对未见数据的敏感性与泛化能力之间的关联。  <br/>3. **方法创新**：应用Sharpness-Aware Minimization (SAM)减少sharpness，提升模型在多样化未见测试集上的性能稳定性。  <br/>4. **实证验证**：通过统计分析验证sharpness与泛化在多数测试场景下存在显著相关性，证明其理论合理性。  <br/>5. **策略价值**：提出sharpness-aware训练作为改进SDD鲁棒性的新策略，为模型设计提供理论指导。  <br/><br/>**总结（100字以内）：**  <br/>本文首次将sharpness作为SDD泛化的理论指标，通过SAM方法降低sharpness提升模型稳健性，相关性分析验证其有效性，为语音假声检测的泛化能力研究提供新方向。|
|2506.11476v1|[LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation](http://arxiv.org/abs/2506.11476v1)|总结：  <br/>本文提出了一种轻量模块化架构，有效解决文本到音频扩散模型在音乐生产中缺乏细粒度时间控制的问题，提升生成效率并保持高质量输出。<br/><br/>贡献点：  <br/>1. **轻量模块化架构设计**：显著减少参数数量，降低内存占用，兼容固定与独立控制，提升模型灵活性。  <br/>2. **音质与控制保真度**：在保持与ControlNet相当的音频质量前提下，实现更精确的时间变化控制。  <br/>3. **高效训练与部署**：通过模块化设计，支持更高效的独立控制训练与部署，降低计算资源需求。  <br/>4. **全面评估与示例验证**：提供大量客观/主观评估结果及音频示例，验证方法在音乐生成任务中的有效性。|
|2506.11403v1|[A correlation-permutation approach for speech-music encoders model   merging](http://arxiv.org/abs/2506.11403v1)|**贡献点：**<br/><br/>1. **提出相关排列方法**：设计基于特征交叉相关性的层对齐策略，解决语音与音乐编码器权重空间不一致问题。  <br/>2. **扩展至Transformer层融合**：将方法应用于Transformer结构，突破原有模型类型的限制。  <br/>3. **实现高效模型合并**：通过逐层计算排列矩阵，显著降低计算成本的同时提升模型性能。  <br/>4. **验证性能提升**：实验表明，合并模型在保留语音能力下，音乐性能提升14.83分（平均得分）。  <br/>5. **统一音频模型构建**：首次实现从独立训练的语音和音乐编码器生成高效统一的音频模型。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种基于特征相关性的模型合并方法，解决语音与音乐编码器对齐问题，实现高效统一音频模型，显著提升音乐性能（+14.83分），同时保留语音能力。|
|2506.11350v1|[GLAP: General contrastive audio-text pretraining across domains and   languages](http://arxiv.org/abs/2506.11350v1)|总结：  <br/>提出GLAP模型，扩展CLAP以支持多语言和多领域，显著提升语音检索、分类及零样本任务表现，并验证其跨50种语言的关键字识别能力。<br/><br/>贡献点：  <br/>1. **多语言与多领域扩展**：首次将CLAP方法扩展至支持多语言（非仅英语）和多领域（包括语音、音乐等）的音频-文本预训练。  <br/>2. **标准任务竞争力**：在Clotho和AudioCaps等音频-文本检索基准上实现与现有方法相当的性能。  <br/>3. **语音任务超越**：在语音检索和分类任务中显著优于现有方法，展现更强的语音理解能力。  <br/>4. **零样本基准性能**：在广泛使用的声事件零样本任务中取得优异结果，同时提升语音内容基准表现。  <br/>5. **跨语言关键能力验证**：通过50种语言的关键词识别评估，证明其强大的多语言通用性。|
|2506.11331v1|[MUDAS: Mote-scale Unsupervised Domain Adaptation in Multi-label Sound   Classification](http://arxiv.org/abs/2506.11331v1)|**总结**：  <br/>本文提出MUDAS，一种适用于多标签声纹分类的高效无监督域自适应框架，解决了传统方法在资源受限IoT设备上的局限性。<br/><br/>**贡献点**：  <br/>1. **多标签适应框架**：首次设计专门针对多标签任务的无监督域适应框架，突破现有单标签UDA算法的限制。  <br/>2. **资源高效部署**：通过选择性重新训练分类器，利用高置信度数据实现模型在设备端的低计算和内存需求适应。  <br/>3. **自适应伪标签生成**：引入类特定的阈值策略，提升伪标签可靠性并优化多标签分类准确性。  <br/>4. **多样性正则化技术**：提出多样性正则化机制，增强模型在复杂声学环境下的泛化能力。  <br/>5. **实验验证**：在SONYC-UST数据集上验证MUDAS性能，表明其在资源受限场景下的有效性优于现有方法。|
|2506.11169v1|[Advances in Small-Footprint Keyword Spotting: A Comprehensive Review of   Efficient Models and Algorithms](http://arxiv.org/abs/2506.11169v1)|**贡献点分点总结：**  <br/>1. 提出一种高效TinyML框架，专为边缘设备上的小规模关键词识别（SF-KWS）设计，满足低功耗、低内存的需求。  <br/>2. 系统性综述了七类关键技术（模型架构、学习技术、模型压缩等），为SF-KWS系统开发提供全面技术分类。  <br/>3. 通过跨领域分析自动语音识别（ASR）与spoken SF-KWS的联系，识别多项潜在研究方向。  <br/>4. 强调资源受限场景下的技术适配性，推动SF-KWS在实际应用中的可行性与优化路径探索。  <br/><br/>**总结（100字以内）：**  <br/>本文系统综述了七类技术，提出高效TinyML框架，推动SF-KWS在边缘设备上的应用，并挖掘跨领域研究方向，为未来研究提供指导。|
|2506.11160v4|[S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamless Speech-Text Alignment and Streaming Speech   Generation](http://arxiv.org/abs/2506.11160v4)|总结：  <br/>提出S2ST-Omni框架，通过任务分解和轻量级适配器解决多语言语音翻译依赖大语料和实时性问题，实验验证其性能优于现有方法。<br/><br/>贡献点：  <br/>1. 提出S2ST-Omni框架，将多语言语音到语音翻译拆分为语音-文本翻译（S2TT）和文本-语音合成（TTS）两阶段，简化任务复杂度。  <br/>2. 引入轻量级语音适配器，弥合语音与文本表征的模态差异，减少对大规模平行语料的依赖。  <br/>3. 结合Whisper和Qwen 3.0预训练模型，提升语音理解与文本生成的准确性和鲁棒性。  <br/>4. 采用流式语音生成模型，确保翻译结果的实时响应能力与连贯性。  <br/>5. 在CVSS基准测试中验证了框架的优越性，显著优于当前主流S2ST方法。|
|2506.11160v2|[S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamlessly Speech-Text Alignment and Streaming   Speech Decoder](http://arxiv.org/abs/2506.11160v2)|总结:  <br/>提出S2ST-Omni框架，通过预训练模型、轻量适配器和流式解码器解决多语言语音-语音翻译中的低延迟、小语料依赖及跨模态对齐问题，显著提升翻译质量与效率。<br/><br/>贡献点:  <br/>1. **提出S2ST-Omni框架**：首个高效、可扩展的多语言S2ST系统，兼顾高质量与低延迟。  <br/>2. **减少对并行语料依赖**：利用Whisper（语音理解）与Qwen 3.0（文本理解）预训练模型，替代传统大规模语料需求。  <br/>3. **跨模态适配技术**：设计轻量级语音适配器，弥合语音与文本表示的模态差异，提升预训练知识利用率。  <br/>4. **流式解码机制**：在TTS阶段引入流式语音解码器，实现端到端实时翻译与高并发处理能力。  <br/>5. **实验验证优势**：在CVSS基准上超越多个S2ST基线，证明框架在翻译质量与效率上的有效性。|
|2506.11160v1|[S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamlessly Speech-Text Alignment and Streaming   Speech Decoder](http://arxiv.org/abs/2506.11160v1)|总结（100字以内）:  <br/>提出S2ST-Omni框架，通过分解S2ST为S2TT和TTS任务，结合预训练模型与轻量适配器，提升翻译质量并降低对平行语料的依赖，同时采用流式解码器实现低延迟，实验验证其在CVSS基准上的优越性和实用价值。<br/><br/>贡献点:  <br/>1. **任务分解与统一框架**：首次将多语言S2ST拆分为S2TT和TTS任务，通过单一端到端模型整合，提升效率和可扩展性。  <br/>2. **减少对平行语料依赖**：利用Whisper和Qwen 3.0等大规模预训练模型，降低对稀缺平行语音语料的需求。  <br/>3. **轻量语音适配器设计**：引入轻量级语音适配器对齐语音与文本表征，有效融合多模态预训练知识。  <br/>4. **流式解码器优化**：在TTS阶段采用流式解码器，兼顾翻译质量与实时性能，实现低延迟输出。  <br/>5. **实验验证优势**：在CVSS基准上超越现有S2ST基线，证明框架在效率与效果上的双重竞争力。|
|2506.11157v1|[Improved in-car sound pick-up using multichannel Wiener filter](http://arxiv.org/abs/2506.11157v1)|**贡献点：**  <br/>1. **提出多通道维纳滤波算法**：应用于双麦克风车载系统，以提升语音质量。  <br/>2. **解决回声干扰问题**：有效减轻回声导致的notch-filtering效应，增强语音清晰度。  <br/>3. **引入新型评估指标**：采用Deep Noise Suppression Mean Opinion Score进行客观性能评估。  <br/>4. **分析头部运动影响**：探讨驾驶员/乘客头动对语音处理效果的影响并提出应对方案。  <br/>5. **对比现有方法**：证明该方法在语音增强性能上显著优于简单信号混合策略。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出基于多通道维纳滤波的双麦克风车载语音增强方案，有效抑制回声干扰和背景噪声，引入新型评估指标并分析头动影响，验证了其在语音质量提升上的优越性。|
|2506.11130v2|[A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](http://arxiv.org/abs/2506.11130v2)|**贡献点：**  <br/>1. 提出基于自精炼框架的ASR性能提升方法，仅依赖无标签语音数据。  <br/>2. 构建闭环系统：利用伪标签训练高保真TTS，再用合成语音文本对反哺ASR模型。  <br/>3. 首次将框架应用于台湾台语普通话，验证其有效性。  <br/>4. 通过混合无标签语音、文本数据和合成内容，实现Whisper-large-v2到专精模型Twister的适配。  <br/>5. 实验显示Twister在普通话和普通话-英语代码切换任务中分别降低20%和50%误率。  <br/>6. 为低资源/领域特定场景提供比伪标签自蒸馏更优的ASR改进路径。  <br/><br/>**总结（100字以内）：**  <br/>本文提出无需标注数据的自精炼框架，通过闭环训练显著提升ASR性能，验证了其在台语普通话中的有效性，并为低资源场景提供了高效改进方案。|
|2506.10855v1|[Analyzing the relationships between pretraining language, phonetic,   tonal, and speaker information in self-supervised speech models](http://arxiv.org/abs/2506.10855v1)|1. **语言多样性分析**：首次系统研究wav2vec2模型在四种不同语言（非英语）中的表示特性，突破了以往以英语为主的分析局限。  <br/>2. **多模态信息解码**：提出通过探针分类器与几何分析方法，同时探究音素、声调及说话人信息的编码机制。  <br/>3. **正交性发现**：揭示不同语言特征（音素、声调、说话人）在模型隐空间中的子空间呈现显著正交性，证明其解耦表征特性。  <br/>4. **层间模式一致性**：发现模型各层对语言匹配与非匹配语音的探测准确率模式相似，但在后期层语言匹配的音素和声调探测略有优势。  <br/>5. **跨语言鲁棒性结论**：论证wav2vec2的表示结构与预训练语言无关，为多语言语音处理模型的泛化能力提供理论支持。  <br/><br/>**总结**：该研究通过跨语言实验揭示了wav2vec2在音素、声调和说话人信息编码上的正交性与鲁棒性，拓展了模型分析的视角。|
|2506.10754v1|[BNMusic: Blending Environmental Noises into Personalized Music](http://arxiv.org/abs/2506.10754v1)|**贡献点**  <br/>1. 提出一种基于文本提示的个性化音乐生成方法，作为传统声学掩蔽技术的替代方案。  <br/>2. 设计BNMusic框架，包含两个关键阶段：音乐生成（mel-spectrogram表示）与自适应放大优化。  <br/>3. 引入跨模态生成技术，实现环境噪音与音乐在节奏和音色上的对齐，减少噪声感知。  <br/>4. 通过多数据集（MusicBench、EPIC-SOUNDS、ESC-50）的实验验证框架有效性，提升整体听觉体验。  <br/><br/>**总结**  <br/>提出BNMusic框架，利用文本生成个性化音乐并结合自适应放大技术，有效减少环境噪音的感知，提升音频体验。|
|2506.10747v1|[FairASR: Fair Audio Contrastive Learning for Automatic Speech   Recognition](http://arxiv.org/abs/2506.10747v1)|**贡献点总结**  <br/>（99字）  <br/>提出FairASR系统，通过结合梯度反转层与无监督对比损失，利用多人口统计数据集减少语音识别中的群体偏差，实现公平的跨群体泛化，同时保持整体性能竞争力。<br/><br/>**分点贡献**  <br/>1. **提出FairASR系统**：首次系统性解决语音识别（ASR）中的公平性问题，通过学习与群体身份无关的表征实现跨群体的公平泛化。  <br/>2. **构建多人口统计数据集**：采用包含多群体信息的语料库，为公平性建模提供更全面的数据基础。  <br/>3. **创新方法结合**：将梯度反转层（用于抑制歧视性特征）与无监督对比损失（用于保留可泛化的语音模式）联合应用，实现性能与公平性的平衡。  <br/>4. **实验证明有效性**：在保持整体ASR性能的同时，显著降低不同群体间的识别性能差异，验证了方法的可行性与优势。|
|2506.10698v2|[Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound   Classification](http://arxiv.org/abs/2506.10698v2)|**贡献点（分点）：**  <br/>1. **提出新模型DDE-MAE**：针对呼吸声分类中数据稀缺问题，改进传统Masked Autoencoder（MAE）模型，增强模型对有限数据的适应性。  <br/>2. **双编码器设计**：通过独立编码器分离疾病相关与疾病无关的特征，实现特征解耦以减少领域偏移（domain mismatch）。  <br/>3. **解决领域不匹配问题**：综合考虑不同电子听诊器、患者群体及录音环境的影响，提升模型的泛化能力。  <br/>4. **验证有效性**：在ICBHI数据集上达到与现有方法相当的性能，证明了该方法的实用价值。  <br/><br/>**总结（100字以内）：**  <br/>本文提出DDE-MAE模型，通过双编码器分离关键特征并解决领域不匹配问题，提升了呼吸声音分类在数据稀缺场景下的性能，验证了其有效性。|
|2506.10698v1|[Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound   Classification](http://arxiv.org/abs/2506.10698v1)|**贡献点：**  <br/>1. 提出改进的Masked Autoencoder（MAE）框架DDE-MAE，专门针对呼吸声分类中的数据稀缺与领域不匹配问题。  <br/>2. 引入双编码器结构，分别提取疾病相关与疾病无关的特征，实现特征解耦以降低领域干扰。  <br/>3. 在ICBHI标准数据集上验证了方法的有效性，取得了具有竞争力的分类性能。  <br/><br/>**总结：**  <br/>本文提出DDE-MAE模型，通过双编码器实现特征解耦，缓解呼吸声分类中的数据不足和领域不匹配问题，并在ICBHI数据集上验证了其有效性。|
|2506.10676v1|[Description and Discussion on DCASE 2025 Challenge Task 4: Spatial   Semantic Segmentation of Sound Scenes](http://arxiv.org/abs/2506.10676v1)|**贡献点（分点）：**  <br/>1. 提出**Spatial Semantic Segmentation of Sound Scenes (S5)**任务，聚焦于多通道输入信号中声音事件的检测与分离，结合空间信息以提升沉浸式通信技术。  <br/>2. 构建**DCASE2025 Task 4 Dataset**，全新录制并整理，为S5任务提供标准化数据资源。  <br/>3. 报告基于该数据集的**S5系统实验结果**，验证了任务框架的有效性，推动声学场景与事件处理技术的发展。  <br/>4. 明确S5任务在现有挑战任务基础上的改进方向，强调对检测与分离功能的针对性研究，而非完全6DoF分离。  <br/><br/>**总结（100字以内）：**  <br/>本论文提出S5任务，结合空间信息提升声音事件检测与分离技术，构建了DCASE2025的专用数据集，并展示了基于该数据集的系统实验结果，推动沉浸式通信研究。|
|2506.10653v1|[Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy   Minimisation and Speaker Codes](http://arxiv.org/abs/2506.10653v1)|总结：  <br/>该论文提出基于多假说条件熵损失和短向量说话人代码的说话人自适应方法，在少量未标注数据下显著提升了语音识别性能。<br/><br/>贡献点：  <br/>1. **新颖损失函数**：提出条件熵损失（Conditional Entropy over Complete Hypotheses），基于多假说而非单个错误率较高的伪标签，提升对初始识别错误的鲁棒性。  <br/>2. **高效说话人表征**：设计"说话人代码"（Speaker Code），用短向量表征说话人特征，减少对训练数据量的依赖。  <br/>3. **实验证明有效性**：在远场噪声增强的Common Voice数据集上，验证了方法在仅1分钟（20%相对提升）和10分钟（29%相对提升）适应数据下的显著性能改进。|
|2506.10349v1|[Joint ASR and Speaker Role Tagging with Serialized Output Training](http://arxiv.org/abs/2506.10349v1)|**贡献点总结（100字以内）**  <br/>提出基于序列化输出训练（SOT）的联合ASR与说话人角色识别方法，首次将角色标记整合到Whisper预训练模型中，实现单次解码生成角色感知转录，解决多说话人场景下的识别难题，实验验证SOT显著降低多说话人WER，为对话式AI提供统一模型方案。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：提出序列化输出训练（SOT）框架，实现ASR与说话人角色标记的联合建模。  <br/>2. **模型优化**：在Whisper中引入角色特定标记，通过微调提升角色信息与语音内容的协同生成能力。  <br/>3. **实验验证**：在真实对话数据集上对比自监督基线方法，证明SOT在多说话人场景下可降低超过10%的WER。  <br/>4. **应用价值**：构建统一模型，支持单次解码完成角色感知的精准语音转录，推动对话系统发展。|
|2506.10289v1|[RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory   Coding](http://arxiv.org/abs/2506.10289v1)|总结（100字以内）：  <br/>本研究提出RT-VC，实现零样本实时语音转换，兼顾超低延迟（比SOTA降低13.3%）与高质量合成。创新点包括基于语音特征空间分离内容与说话人特征，以及整合DDSP技术提升vocoding效率。<br/><br/>贡献点：  <br/>1. **零样本实时语音转换系统**：首次实现无需目标说话人数据的实时语音转换，突破传统方法依赖目标样本的限制。  <br/>2. **内容-说话人特征解耦**：通过语音特征空间建模，有效分离语音内容与说话人特征，提升转换结果的鲁棒性与可解释性。  <br/>3. **可微数字信号处理（DDSP）集成**：引入DDSP技术直接从语音特征生成音频，显著减少转换延迟并保持高质量输出。  <br/>4. **实时性性能突破**：在CPU环境下实现61.4 ms超低延迟，超越当前SOTA方法13.3%，推动语音转换在实际应用中的可行性。|
|2506.10225v1|[Fine-Grained control over Music Generation with Activation Steering](http://arxiv.org/abs/2506.10225v1)|**贡献点**：<br/>1. 提出基于推理时干预的细粒度音乐生成控制方法，通过调整MusicGen模型的残差流或注意力层激活实现。  <br/>2. 设计两种干预策略：利用线性探针权重进行残差流引导，或直接调控注意力层激活。  <br/>3. 将模型任务建模为回归问题，采用均方误差（MSE）优化以保留激活空间的方向信息。  <br/>4. 融合全局文本提示与局部干预机制，实现音乐生成的多维度控制（音色/风格/流派）。  <br/>5. 提供公开音频示例与demo页面验证方法有效性。  <br/><br/>**总结**：  <br/>该研究提出通过推理时干预实现音乐生成的全局与局部控制，结合回归建模提升性能，并提供可视化示例验证。|
|2506.10207v1|[FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio   Classification](http://arxiv.org/abs/2506.10207v1)|1. 提出FedMLAC：首个统一互学框架，同步解决联邦音频分类的三大挑战（数据异质性/模型异质性/数据污染）  <br/>2. 双模型架构：引入本地个性化模型与轻量级全局共享Plug-in模型，实现泛化与个性化双重目标  <br/>3. 双向知识蒸馏：通过知识迁移机制，使Plug-in模型适应客户端数据分布，提升模型鲁棒性  <br/>4. 分层剪枝聚合（LPA）：创新性地在服务器端基于参数偏差过滤污染数据，增强对抗干扰能力  <br/>5. 多基准验证：在涵盖语音与非语音任务的四个数据集上验证方法有效性，证明其广泛适用性|
|2506.10165v1|[The 2025 PNPL Competition: Speech Detection and Phoneme Classification   in the LibriBrain Dataset](http://arxiv.org/abs/2506.10165v1)|总结：  <br/>本研究发布全球最大个体MEG语音数据集LibriBrain与配套工具库pnpl，定义语音检测与音素分类两个核心任务，并通过标准化评测体系和竞赛机制推动非侵入式脑机接口在语音领域的突破性发展。<br/><br/>贡献点：  <br/>1. 构建首个大规模个体MEG语音数据集LibriBrain，填补非侵入式脑数据研究空白。  <br/>2. 开发用户友好型Python工具库pnpl，促进数据与深度学习框架的无缝集成。  <br/>3. 设计语音检测与音素分类两大基础任务，建立标准化数据划分与评估指标体系。  <br/>4. 提供基准模型、教程代码、讨论平台与公开排行榜，构建完整研究生态。  <br/>5. 首创双赛道机制（标准轨与扩展轨），鼓励算法创新与大规模计算并行发展。|
|2506.10097v1|[Description and Discussion on DCASE 2025 Challenge Task 2: First-shot   Unsupervised Anomalous Sound Detection for Machine Condition Monitoring](http://arxiv.org/abs/2506.10097v1)|贡献点总结（100字以内）:  <br/>1. 提出首个基于领域泛化的第一拍无监督异常声音检测任务框架，用于机器状态监测；  <br/>2. 实现无需机器特异性超参数调优的快速部署目标；  <br/>3. 构建包含新机器类型声音的公开评估数据集，提升实际应用挑战性。|
|2506.09984v1|[InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio   Conditions](http://arxiv.org/abs/2506.09984v1)|**贡献点总结**：  <br/>1. **多主体支持**：首次打破单实体假设，实现多概念（人类和物体）在同一视频中的协同动画生成。  <br/>2. **区域级条件绑定**：引入区域特定的时空绑定机制，将多模态条件（文本、图像、音频）与每个身份的局部区域精确对齐。  <br/>3. **显式布局推理**：通过掩码预测器自动推断布局信息，匹配去噪视频与参考图像的外观特征，提升多模态一致性。  <br/>4. **局部音频注入**：在迭代过程中将音频条件注入对应区域，实现多模态条件与布局的动态对齐，增强生成可控性。  <br/>5. **实证验证**：通过实验和消融研究证明显式布局控制在多模态场景下的有效性，优于传统隐式方法。  <br/><br/>**简要总结**（100字以内）：  <br/>本文提出一种端到端多模态人体动画框架，支持多主体协同生成，通过区域级条件绑定和显式布局推理，实现高精度、可控的多概念视频生成，验证了其在复杂交互场景下的优越性。|
|2506.09874v1|[UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow   Matching](http://arxiv.org/abs/2506.09874v1)|总结：  <br/>提出UmbraTTS模型，通过流匹配技术实现语音与环境音联合生成，结合自监督框架解决数据对齐难题，显著提升环境感知语音合成质量。<br/><br/>贡献点：  <br/>1. **首个联合生成语音与环境音频的TTS模型**：基于流匹配方法，同时合成语音和背景环境音，突破传统TTS仅生成语音的局限。  <br/>2. **细粒度背景音控制能力**：支持对背景音量的精确调节，增强语音与环境的交互性和场景适配性。  <br/>3. **自监督数据提取框架**：从未标注录音中自动生成语音、背景音频和文字转录，解决配对数据匮乏问题。  <br/>4. **环境感知音频生成**：在复杂场景中保持语音自然性和环境音的连贯性，超越现有基线模型性能。|
|2506.09804v1|[Regularizing Learnable Feature Extraction for Automatic Speech   Recognition](http://arxiv.org/abs/2506.09804v1)|总结：  <br/>本文提出两种正则化方法，改进SpecAugment在神经前端中的应用，有效提升可学习特征提取在ASR中的性能，缩小与传统特征的差距。<br/><br/>贡献点：  <br/>1. **对比分析**：揭示神经前端性能落后于传统方法的主要原因在于过拟合风险增加。  <br/>2. **音频扰动优化**：发现针对可学习特征的音频扰动方法能带来更大的性能提升。  <br/>3. **SpecAugment改进**：识别其在神经前端中的两个局限性，并提出STFT域掩码方案作为有效修正。  <br/>4. **方法整合**：结合音频扰动与STFT掩码两种正则化策略，显著缩小传统特征与可学习特征的性能差异。|
|2506.09792v2|[Incorporating Linguistic Constraints from External Knowledge Source for   Audio-Visual Target Speech Extraction](http://arxiv.org/abs/2506.09792v2)|总结：  <br/>提出基于预训练语音语言模型的辅助知识源，通过引入语言约束提升AV-TSE的语音质量和可懂度，同时保持推理效率，并验证了其在多语言和视觉受限场景中的稳健性。<br/><br/>贡献点：  <br/>1. **知识源创新**：首次将预训练的语音语言模型（PSLMs）和语言模型（PLMs）作为AV-TSE的辅助知识源，提供额外的语法规则约束。  <br/>2. **无额外计算成本**：设计无需增加推理阶段计算负担的框架，实现语音质量与可懂度的提升。  <br/>3. **多语言适应性**：验证模型在多语言环境下的有效性，展示跨语言的泛化能力。  <br/>4. **视觉受限鲁棒性**：在缺乏视觉线索的场景下仍保持性能优势，增强模型的实用性。  <br/>5. **跨模态协同**：通过语言约束与视觉信息的融合，提升音频-视觉协同的语音分离效果。|
|2506.09792v1|[Incorporating Linguistic Constraints from External Knowledge Source for   Audio-Visual Target Speech Extraction](http://arxiv.org/abs/2506.09792v1)|总结（100字以内）：<br/>该研究提出利用预训练语言模型作为辅助知识源，通过引入语言约束提升音频-视觉说话人分离效果，方法在推理阶段无额外计算成本，并在多语言及视觉线索受损场景中展现优异性能。<br/><br/>贡献点：<br/>1. 提出将预训练语音-语言模型（PSLMs）与语言模型（PLMs）的语言约束作为辅助监督信号，用于音频-视觉说话人分离任务。<br/>2. 设计无需增加推理计算负担的轻量级方法，有效提升语音质量与可理解性。<br/>3. 首次验证该方法在多语言环境和视觉线索缺失情况下的鲁棒性，展现跨场景适应能力。<br/>4. 探索语音领域与语言模型的深度融合机制，为语音增强提供新的知识注入思路。|
|2506.09709v1|[Training-Free Voice Conversion with Factorized Optimal Transport](http://arxiv.org/abs/2506.09709v1)|**总结**：  <br/>提出一种无需训练的语音转换方法Factorized MKL-VC，通过因子化最优传输图解决维度方差问题，在短时参考音频下实现高质量跨语言转换，性能超越kNN-VC并接近FACodec。<br/><br/>**贡献点**：  <br/>1. **无需训练的改进方案**：首次提出完全无需训练的kNN-VC修改方法，简化了传统语音转换的复杂流程。  <br/>2. **因子化最优传输图**：用因子化最优传输映射替代kNN回归，在WavLM嵌入子空间中实现更高效的跨语言特征转换。  <br/>3. **解决维度方差问题**：通过因子化技术均衡不同维度的方差，提升跨语言语音转换的稳定性与泛化能力。  <br/>4. **短参考音频性能突破**：在仅需5秒参考音频的情况下，显著优于kNN-VC的语音内容保留能力和鲁棒性。  <br/>5. **与SOTA方法媲美**：在跨语言语音转换任务中，性能接近当前最先进的FACodec，验证了方法的有效性。|
|2506.09653v1|[Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women](http://arxiv.org/abs/2506.09653v1)|**总结（100字以内）:**  <br/>构建SRUTI基准，通过合成语音技术解决低资源语言Bhojpuri女性数据匮乏问题，显著降低WER至4.7%，提供可扩展且非侵入性方案，促进数字包容与ASR技术公平性。<br/><br/>**贡献点分点列出:**  <br/>1. **填补研究空白**：创建首个聚焦农村Bhojpuri女性语音的基准数据集SRUTI，揭示现有ASR系统在弱势群体语音识别中的性能缺陷。  <br/>2. **创新合成方法**：提出基于短时音频（25-30秒/人）生成合成语音的方案，绕过社会文化障碍实现低成本数据扩展。  <br/>3. **性能提升验证**：实验表明合成数据可将WER降低4.7%，证实其有效提升低资源语言ASR系统的可行性。  <br/>4. **促进数字包容**：提供一种可扩展、低干预的解决方案，助力低资源语言社区（尤其是农村女性）实现数字化服务的平等接入。|
|2506.09606v1|[Unmasking real-world audio deepfakes: A data-centric approach](http://arxiv.org/abs/2506.09606v1)|总结：  <br/>该研究提出真实世界音频深度伪造数据集AI4T，通过数据集编纂、剪枝和增强等数据驱动策略显著提升检测性能，验证了数据为中心方法在实际应用中的有效性。<br/><br/>贡献点：  <br/>1. **构建首个真实世界音频深度伪造数据集**（AI4T），填补现有研究中对非科研场景数据缺失的空白。  <br/>2. **揭示真实世界深度伪造样本的挑战性**，证明其对现有高性能检测模型仍存在显著威胁。  <br/>3. **提出数据中心策略**，通过数据集优化（curation）、剪枝（pruning）和增强（augmentation）提升模型鲁棒性与泛化能力。  <br/>4. **实验证明数据驱动方法的有效性**，在In-the-Wild数据集上将EER降低55%至1.7%，在AI4T数据集上降低63%，显著优于传统模型优化方案。  <br/>5. **开源代码与数据**，推动语音深度伪造检测领域研究与应用的透明化与可复现性。|
|2506.09549v1|[A Study on Speech Assessment with Visual Cues](http://arxiv.org/abs/2506.09549v1)|**贡献点（分点列出）:**  <br/>1. **提出多模态语音质量评估框架**：首次将音频特征与视觉线索结合，用于非侵入式评估PESQ和STOI，突破了仅依赖音频信号的局限性。  <br/>2. **设计双分支特征提取架构**：采用STFT提取频谱特征，结合视觉编码器生成视觉嵌入，实现音频与视觉模态的独立处理与信息互补。  <br/>3. **引入多任务学习与注意力机制**：通过CNN-BLSTM融合多模态特征，并利用注意力机制优化特征交互，同时预测PESQ和STOI提升模型整体性能。  <br/>4. **验证噪声场景下的有效性**：在LRS3-TED数据集上引入DEMAND噪声增强，证明该框架在实际噪声环境下的鲁棒性和优越性。  <br/>5. **显著提升评估指标性能**：在噪声条件下，PESQ和STOI的LCC分别提高9.61%和11.47%，展示视觉信息对非侵入式语音评估的显著贡献。  <br/><br/>**总结（100字以内）:**  <br/>本文提出结合音频与视觉模态的多任务框架，通过双分支架构和CNN-BLSTM+注意力机制优化特征融合，显著提升非侵入式语音质量评估性能，尤其在噪声环境中的LCC分别提高9.61%和11.47%。|
|2506.09521v1|[You Are What You Say: Exploiting Linguistic Content for VoicePrivacy   Attacks](http://arxiv.org/abs/2506.09521v1)|总结：  <br/>该研究探讨了语音匿名化系统中说话人验证攻击的可行性，提出基于BERT的ASV方法，并发现语义关键词对系统可解释性的影响，建议优化数据集以提升隐私评估的公平性。<br/><br/>贡献点：  <br/>1. **方法创新**：提出将预训练语言模型BERT适配为自动说话人验证（ASV）系统，用于评估语音匿名化系统的隐私保护效果。  <br/>2. **实验结果**：在VoicePrivacy攻击者数据集上实现35%的平均等错误率（EER），部分说话人EER低至2%，验证了方法有效性。  <br/>3. **可解释性分析**：揭示系统决策与语义相似关键词的关联性，指出LibriSpeech数据集构建方式对攻击效果的影响。  <br/>4. **数据集改进建议**：呼吁重新设计VoicePrivacy数据集以消除偏差，挑战仅依赖全局EER指标评估隐私性能的做法。|
|2506.09487v1|[BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for   Long-Term Audio Generation](http://arxiv.org/abs/2506.09487v1)|总结：本文系统介绍了BemaGANv2，提出新型AMP模块与MED判别器架构，完善了音频生成模型的评估体系，并开放代码与预训练模型，促进方法复现与应用。<br/><br/>贡献点：  <br/>1. **架构创新**：在生成器中引入Anti-aliased Multi-Periodicity (AMP)模块，替代传统ResBlocks，通过Snake激活函数优化周期结构建模。  <br/>2. **判别器设计**：提出Multi-Envelope Discriminator (MED)，结合Multi-Resolution Discriminator (MRD)，增强对长程依赖和周期性特征的捕捉能力。  <br/>3. **系统性评估**：对比分析多种判别器配置（MSD+MED、MSD+MRD、MPD+MED+MRD），使用客观指标（FAD、SSIM、PLCC、MCD）和主观评价（MOS、SMOS）验证性能。  <br/>4. **教程与复现支持**：提供完整模型架构、训练方法和实现指南，提升可复现性，并开放代码和预训练模型资源。|
|2506.09448v1|[OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for   Automatic Speech Recognition with Dynamic Vocabulary](http://arxiv.org/abs/2506.09448v1)|总结：  <br/>本研究提出一种结合上下文偏置与预训练语音模型的方法，在小数据集下有效提升罕见词识别性能，实验表明该方法显著降低词错误率并优化实时效率。<br/><br/>贡献点：  <br/>1. **方法创新**：首次将现有上下文偏置（CB）技术与预训练语音基础模型（SFMs）结合，通过冻结预训练参数保留模型原有优势，实现高效微调。  <br/>2. **小数据有效性**：在少量数据条件下，显著提升CB对罕见词的识别效果，突破传统CB方法需从头训练导致的性能瓶颈。  <br/>3. **性能提升**：实验验证在LibriSpeech数据集上，该方法使偏置词错误率（B-WER）降低11.6%，整体词错误率（WER）降低0.9，并减少7.5%的实时计算量。|
|2506.09375v1|[CoLMbo: Speaker Language Model for Descriptive Profiling](http://arxiv.org/abs/2506.09375v1)|**贡献点：**  <br/>1. 提出CoLMbo模型，首次将说话人编码器与基于提示的条件机制结合，突破传统分类任务限制。  <br/>2. 实现结构化提取方言、性别、年龄等多维度说话人属性，支持生成上下文丰富的描述。  <br/>3. 引入用户自定义提示，使模型能动态适配新特征并生成个性化描述（如方言差异、年龄相关特质）。  <br/>4. 在零样本跨数据集场景中表现出色，显著提升传统说话人识别系统的泛化能力与实用性。  <br/>5. 为语音领域提供可扩展的说话人建模框架，推动高阶特征分析与场景化应用的发展。  <br/><br/>**总结：**  <br/>本文提出CoLMbo，通过融合说话人编码器与提示条件机制，实现多属性结构化提取与动态定制化描述，突破传统分类限制，在零样本场景中取得显著进展，推动说话人识别技术的升级。|
|2506.09344v1|[Ming-Omni: A Unified Multimodal Model for Perception and Generation](http://arxiv.org/abs/2506.09344v1)|**贡献点分点总结：**  <br/>1. 提出首个**多模态统一感知与生成模型**（支持图像、文本、音频、视频）；  <br/>2. 设计**专用编码器与MoE架构Ling**，通过模态特定路由器实现高效多模态输入处理与融合；  <br/>3. 集成**先进音频解码器和图像生成模块**（Ming-Lite-Uni），支持自然语音生成和高质量图像编辑；  <br/>4. 实现**无需单独模型或微调的多功能任务处理**（如上下文聊天、文本转语音）；  <br/>5. **开源代码和模型权重**，首次与GPT-4o匹敌，推动社区研究与开发。  <br/><br/>**总结（100字以内）：**  <br/>Ming-Omni是首个开源的多模态统一生成模型，通过专用编码器和MoE架构实现跨模态高效处理与生成，支持语音、图像等任务，且开源促进社区应用。|
|2506.09231v1|[Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating   Nasality](http://arxiv.org/abs/2506.09231v1)|总结：<br/>本研究提出协同训练模型，整合声源与发音信息，验证鼻音量作为真实数据，显著提升语音逆推系统的性能，改善美国英语说话人的软腭运动预测。<br/><br/>贡献点：<br/>1. 提出将声源特征（F0、周期/非周期能量）与口腔TVs融合的协同训练模型，突破传统独立估计方法。<br/>2. 验证鼻音量（nasalance）作为声学鼻音测量的可靠基准，证实其对软腭运动模式的预测有效性。<br/>3. 首次通过对比实验量化协同模型的提升效果：口腔TVs估计提升5%，鼻音量估计提升9%。<br/>4. 建立完整的SI系统框架，涵盖口鼻腔协同控制的多维度声学-发音映射建模。|
|2506.09218v1|[A Technique for Isolating Lexically-Independent Phonetic Dependencies in   Generative CNNs](http://arxiv.org/abs/2506.09218v1)|总结：  <br/>本研究提出了一种新型方法，通过窄瓶颈FC层探测卷积层的词法独立泛化能力，揭示了卷积层能动态概括语音依赖关系的特性。<br/><br/>贡献点：  <br/>1. **探究生成CNN的词法不变泛化能力**：评估了未受词汇约束的生成卷积神经网络在原始音频波形上的音位学概括潜力。  <br/>2. **分析FC瓶颈缩小的影响**：对比了全连接层从1024通道缩减至8通道后对模型性能的改变。  <br/>3. **提出新探测技术**：设计了一种基于随机特征图输入卷积块的实验方法，用于验证模型在窄FC瓶颈下的词法独立泛化特性。  <br/>4. **揭示卷积层的动态泛化机制**：证明卷积层能够超越全连接层的词法约束，动态生成语音相关的依赖关系。|
|2506.09206v1|[SimClass: A Classroom Speech Dataset Generated via Game Engine   Simulation For Automatic Speech Recognition Research](http://arxiv.org/abs/2506.09206v1)|总结：  <br/>提出基于游戏引擎的课堂噪声合成方法，构建包含噪声与语音数据的SimClass数据集，填补教育领域语音数据空白，验证其对语音识别和增强模型的实用性。<br/><br/>贡献点：  <br/>1. **提出课堂噪声合成新方法**：利用游戏引擎技术，开发可扩展的音频场景生成方案，解决传统方法难以模拟真实课堂环境的局限。  <br/>2. **构建SimClass数据集**：首次整合合成课堂噪声语料与模拟课堂语音数据，为教育领域的语音研究提供标准化数据资源。  <br/>3. **创新数据生成技术**：通过关联公开儿童语音语料与YouTube讲座视频，有效模拟真实课堂对话场景，提升数据真实性。  <br/>4. **验证数据有效性**：实验表明SimClass能高质量近似真实课堂语音，支持开发鲁棒的语音识别与增强模型，推动教育AI应用。|
|2506.09189v1|[Fractional Fourier Sound Synthesis](http://arxiv.org/abs/2506.09189v1)|总结（100字以内）：  <br/>本文创新性提出将分数阶傅里叶变换应用于声音合成，通过时间-频率连续插值和alpha域直接合成技术，拓展了传统方法的局限，为音频创作提供了新的工具和方法论，验证了其在提升声学创意方面的潜力。<br/><br/>贡献点：  <br/>1. **引入FrFT在声音合成中的创新应用**：首次将分数阶傅里叶变换（FrFT）用于音频生成，突破传统傅里叶变换的离散时间-频率分析限制。  <br/>2. **提出时间-频率连续插值框架**：通过分数阶参数实现时域与频域的平滑转换，提升信号操控的灵活性。  <br/>3. **开发alpha域直接合成方法**：构建基于FrFT的新型合成范式，生成传统手段无法实现的音色与动态特性。  <br/>4. **系统解析FrFT理论基础**：梳理其数学原理与历史演进，深化对FrFT在音频处理中潜力的理解。  <br/>5. **验证新型声学技术有效性**：通过实验证明alpha合成与alpha滤波等方法的创新性，推动音频创作边界。|
|2506.08911v1|[Implementing Keyword Spotting on the MCUX947 Microcontroller with   Integrated NPU](http://arxiv.org/abs/2506.08911v1)|**贡献点：**<br/>1. **硬件实现**：在NXP MCXN947嵌入式平台（集成NPU）部署KWS系统，实现资源受限设备上的实时语音交互。<br/>2. **模型优化**：结合MFCC特征提取与轻量级CNN分类器，通过量化感知训练（QAT）显著压缩模型体积（30.58 KB）并保持高准确率（97.06%）。<br/>3. **性能验证**：实验显示NPU实现的推理速度比CPU提升59倍，证明高效低功耗语音接口在嵌入式平台的可行性。<br/><br/>**总结（100字以内）：**  <br/>本研究在NXP MCXN947嵌入式平台上实现高效KWS系统，结合MFCC与CNN并采用量化感知训练优化模型，实现30.58 KB大小和97.06%准确率，NPU使推理速度提升59倍，验证了低功耗语音交互的可行性。|
|2506.08846v1|[Addressing Pitfalls in Auditing Practices of Automatic Speech   Recognition Technologies: A Case Study of People with Aphasia](http://arxiv.org/abs/2506.08846v1)|总结（100字以内）:  <br/>本文提出ASR审计的三大改进方向，构建更全面的审计框架，并通过实证研究揭示失语症群体的系统性性能差距，呼吁建立灵活、公平的审计实践以应对ASR技术发展。<br/><br/>贡献点:  <br/>1. **识别ASR审计三大核心问题**：提出传统方法在文本标准化、细分人群分析和单一误差度量的局限性，首次系统性指出其对弱势群体公平性的潜在影响。  <br/>2. **构建多维度审计框架**：设计兼顾标准化方法多样性、细分群体分析及生成式模型误差类型的综合评估体系，提升审计全面性。  <br/>3. **实证揭示系统性性能差距**：通过案例研究对比6种主流ASR系统对失语症群体与对照组的表现，验证框架的有效性并提供实证数据支持。  <br/>4. **提出实践建议**：倡导动态调整审计策略，确保其与快速演进的ASR技术生态兼容，推动行业标准落地。|
|2506.08570v2|[Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling   Paradigms for Text-to-Music Generation](http://arxiv.org/abs/2506.08570v2)|总结：  <br/>该研究系统比较文本到音乐生成中的两种主要建模范式，通过统一训练条件揭示其性能差异，为未来模型设计提供指导。<br/><br/>贡献点：  <br/>1. **提出系统性实证分析框架**：首次针对文本到音乐生成的建模范式进行系统的对比实验，隔离其他变量影响。  <br/>2. **统一训练条件对比**：使用相同数据集、训练配置和基础架构从头训练模型，确保实验的公平性和可比性。  <br/>3. **多维度评估体系**：涵盖生成质量、鲁棒性、可扩展性、条件对齐能力及音频修复等关键指标，全面分析范式特性。  <br/>4. **明确范式优劣势**：揭示自回归解码与条件流匹配在复杂任务中的表现差异，指出各自适用场景与局限性。  <br/>5. **提供可复现实验成果**：公开音频示例及模型，便于社区验证和进一步研究。|
|2506.08564v1|[Neighbors and relatives: How do speech embeddings reflect linguistic   connections across the world?](http://arxiv.org/abs/2506.08564v1)|**贡献点总结**  <br/>1. **提出基于语音嵌入的跨语言关系分析方法**：利用细调后的XLS-R模型生成语言嵌入，通过LDA揭示全球106种语言的系谱、词汇及地理距离关系。  <br/>2. **验证嵌入距离与传统指标的一致性**：证明语音嵌入能有效捕捉全球和局部语言类型学模式，为数据驱动研究提供新工具。  <br/>3. **解决语言变化可视化难题**：通过对比层次聚类与网络方法，强调语言演变的动态性，并探索可视化技术的改进方向。  <br/>4. **方法论优化与扩展应用**：优化语料库规模和潜在空间维度，支持低资源语言研究，实现宏观与微观语言变异的统一分析。  <br/>5. **推动跨学科整合**：规划未来研究整合社会语言学因素，深化对语言多样性多维理解。  <br/><br/>**100字内总结**：  <br/>本研究通过语音嵌入分析106种语言关系，验证其与传统指标的一致性，解决可视化挑战，并优化方法论以支持低资源语言研究，推动语言学与社会语言学的跨学科整合。|
|2506.08540v1|[Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas   and Partitas: Topological and Geometrical Explorations](http://arxiv.org/abs/2506.08540v1)|总结（100字以内）：  <br/>本文提出基于高阶网络的拓扑框架，分析巴赫小提琴奏鸣曲与帕蒂塔的音乐结构，揭示了音乐类型特有的几何拓扑特征，包括欧拉特征演变、曲率变化及对高斯-博内定理的遵循差异，区分了慢板、赋格与巴洛克舞蹈等多种音乐形式。<br/><br/>贡献点：  <br/>1. **提出高阶网络建模方法**：首次将单音、和弦、三音等音乐元素构建为顶点、边、三角形等高阶拓扑结构，突破传统图表示的二维限制。  <br/>2. **动态音乐流建模**：通过建模连续音符间的过渡，探索音乐的时间演进特性与结构动态性。  <br/>3. **音乐类型特征识别**：系统分析作品的几何和拓扑属性，发现不同音乐类型（如慢板、赋格、巴洛克舞蹈）的特征差异。  <br/>4. **量化欧拉特征与曲率**：揭示音乐发展中欧拉特征演变规律及曲率变化趋势，为音乐结构分析提供新维度。  <br/>5. **高斯-博内定理验证**：验证不同乐章类型对数学定理（如高斯-博内定理）的符合程度，强化分析的理论基础。|
|2506.07920v1|[W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](http://arxiv.org/abs/2506.07920v1)|总结（100字以内）:  <br/>提出W4S4，基于冗余小波框架的SSM新类，通过稳定对角化与高效计算，在长序列建模任务中显著优于HiPPO-based SSMs，验证了其有效性，并为下一代深度SSM模型提供基础。<br/><br/>贡献点:  <br/>1. **提出W4S4框架**：基于冗余小波帧构建新的SSM类，替代传统线性递归和卷积结构。  <br/>2. **理论与计算优势**：实现稳定对角化，支持快速核计算，无需低秩近似，提升效率与可解释性。  <br/>3. **长序列建模能力**：在延迟重建任务和分类基准中，比HiPPO-based SSMs保留更长时序信息。  <br/>4. **实验验证广泛性**：通过多种任务（如分类、长序列建模）的实验，证明其性能优越性。  <br/>5. **应用前景**：为下一代深度SSM模型提供可扩展、通用的基础架构。|
|2506.07722v2|[Towards a Unified Benchmark for Arabic Pronunciation Assessment: Quranic   Recitation as Case Study](http://arxiv.org/abs/2506.07722v2)|总结（100字以内）:  <br/>本文提出首个针对阿拉伯语发音评估的统一基准框架，通过构建MSA专用音素集和公开测试集QuranMB.v1，结合古兰经诵读案例研究，评估基线模型并揭示挑战，为阿拉伯语音识别技术的发展提供标准化基础。  <br/><br/>贡献点:  <br/>1. **首个阿拉伯语发音评估基准**：创建QuranMB.v1，作为阿拉伯语发音错误检测的标准化测试集。  <br/>2. **定制化音素集**：开发专门针对现代标准阿拉伯语（MSA）发音特点的音素集合，解决传统音素集不适用的问题。  <br/>3. **全流程框架**：提出涵盖数据处理、音素建模、基准构建的完整研究流程，推动阿拉伯语语音评估体系的完善。  <br/>4. **基线模型评估**：分析现有模型在MSA发音评估中的表现，明确当前技术优势与局限性，为后续研究提供参考。  <br/>5. **跨领域应用价值**：通过古兰经诵读的案例研究，强调该框架在宗教、教育等阿拉伯语应用场景中的适用性。|
|2506.07722v1|[Towards a Unified Benchmark for Arabic Pronunciation Assessment: Quranic   Recitation as Case Study](http://arxiv.org/abs/2506.07722v1)|**贡献点总结（100字以内）**  <br/>本文提出首个公开的MSA发音错误检测基准数据集QuranMB.v1，构建了包含数据处理、专用音素集设计及基线模型评估的全流程框架，为阿拉伯语音技术发展提供标准化工具，揭示评估挑战与前景，推动相关研究。<br/><br/>**分点贡献：**  <br/>1. **构建统一基准**：提出首个针对MSA发音错误检测的公开测试数据集QuranMB.v1，作为案例研究。  <br/>2. **全流程数据处理**：开发覆盖数据预处理、特征提取与标注的综合管道，支持发音评估任务。  <br/>3. **专用音素集设计**：创建针对MSA发音特点的专门音素集合，捕捉语言细微差异。  <br/>4. **基线模型评估**：分析多类模型性能，为MSA发音评估提供初始研究成果与挑战分析。  <br/>5. **标准化框架推动研究**：通过统一基准和流程，促进阿拉伯语言技术及发音评估领域的进一步发展。|
|2506.07659v1|[Unified Semi-Supervised Pipeline for Automatic Speech Recognition](http://arxiv.org/abs/2506.07659v1)|总结：  <br/>提出首个全流程开源半监督ASR框架，支持多语言数据集构建与伪标签生成算法TopIPL，实验证明在低/高资源语言中均显著提升识别性能。<br/><br/>贡献点：  <br/>1. **完整开源框架**：构建覆盖「未标注数据收集-伪标签生成-模型训练」的全流程半监督ASR框架  <br/>2. **跨语言可扩展性**：实现任意语言的可扩展数据集生成，利用Creative Commons许可的公共语音数据  <br/>3. **新型伪标签算法**：提出TopIPL伪标签算法，在葡萄牙语（+18-40%）、亚美尼亚语（+5-16%）、西班牙语（+2-8%）等多语言场景验证其有效性|
|2506.07646v1|[Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation](http://arxiv.org/abs/2506.07646v1)|**贡献点总结：**  <br/>本文提出了一种结合预训练ASR模型与字典先验知识的音素和语调标注方法，构建了日语TTS数据集，解决了传统方法依赖单一模态的局限性，并在客观和主观评估中展现出优越性能。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **多模态标注方法创新**：首次提出融合音频与文本信息的联合标注框架，能同时生成音素（phonemic）和语调（prosodic）标签，提升TTS数据集的标注精度和丰富度。  <br/>2. **预训练ASR模型优化**：基于大规模预训练ASR模型，通过条件化真实文本转录，实现对词级字符和标注标签的联合输出，增强模型对语音与文本关联的理解。  <br/>3. **字典驱动的纠错机制**：引入字典先验知识作为解码策略，有效修正音素标注错误，提高标注结果的可靠性。  <br/>4. **数据集构建与评估**：验证了所提方法在日语TTS数据集中的有效性，客观指标优于纯文本/音频方法，主观语音自然度接近人工标注水平。|
|2506.07634v2|[SongBloom: Coherent Song Generation via Interleaved Autoregressive   Sketching and Diffusion Refinement](http://arxiv.org/abs/2506.07634v2)|**贡献点总结：**<br/>1. 提出SongBloom框架，融合自回归草图生成与扩散模型优化，解决歌曲生成中全局连贯性与局部保真度的平衡问题。<br/>2. 引入交织生成范式，通过渐进式扩展与细节细化过程，整合语义和声学上下文引导创作。<br/>3. 实验证明其性能优于现有方法，接近商业音乐平台水平，提供公开的音频样本、代码和模型权重。|
|2506.07634v1|[SongBloom: Coherent Song Generation via Interleaved Autoregressive   Sketching and Diffusion Refinement](http://arxiv.org/abs/2506.07634v1)|总结：  <br/>SongBloom提出了一种结合自回归草图和扩散模型的框架，通过逐步扩展和整合语义声学上下文，实现了高质量歌曲生成，并超越现有方法，接近商业平台水平。<br/><br/>贡献点：  <br/>1. **提出新型框架**：设计了SongBloom，采用自回归草图生成与扩散模型精炼的交织范式，解决传统方法在全局连贯性与局部保真度的平衡问题。  <br/>2. **融合模型优势**：结合扩散模型的高保真性和语言模型的可扩展性，提升生成歌曲的质量与效率。  <br/>3. **分阶段生成机制**：从短到长逐步扩展音乐草图，从粗到细粒度细化细节，增强结构连贯性与内容完整性。  <br/>4. **上下文整合策略**：通过整合先验语义和声学上下文，指导生成过程以确保音乐性与歌词匹配度。  <br/>5. **性能验证**：在主观和客观指标上超越现有方法，生成质量可比商业音乐生成平台。|
|2506.07536v1|[Bayesian Learning for Domain-Invariant Speaker Verification and   Anti-Spoofing](http://arxiv.org/abs/2506.07536v1)|**贡献点总结（100字以内）**  <br/>本研究提出贝叶斯加权RFN（BWRFN）方法，通过引入权重不确定性建模，解决固定权重RFN在域不匹配下的局限，显著提升ASV和反欺骗性能。<br/><br/>**分点贡献：**  <br/>1. **提出BWRFN方法**：首次结合变分推断与RFN，通过建模权重的后验分布，动态调整频率分量的权重，增强模型对域不匹配的鲁棒性。  <br/>2. **引入权重不确定性**：针对域偏移导致的权重不确定性，采用贝叶斯框架量化频率权重的不确定性，改进传统固定权重策略。  <br/>3. **提升实验效果**：在跨数据集ASV、跨TTS反欺骗及反欺骗鲁棒ASV任务中，BWRFN显著优于WRFN和RFN，验证其有效性。  <br/>4. **域适应优化**：通过时间与通道轴的特征统计归一化，降低特征图的域依赖性，推动语音认证系统的实际应用。|
|2506.07526v1|[Generative Voice Bursts during Phone Call](http://arxiv.org/abs/2506.07526v1)|总结：  <br/>本文提出了一种基于生成式AI的语音突发传输方法，解决通话中紧急消息传递问题，具有上下文感知和优先级推断能力，可跨行业应用。<br/><br/>贡献点：  <br/>1. **提出新型语音传输机制**：设计Generative Voice Bursts系统，允许在通话中接收紧急短音频消息，突破传统呼叫等待的局限。  <br/>2. **融合上下文感知技术**：通过位置、健康数据、图像、环境噪音等多源信息生成语音内容，提升消息的相关性和紧急性。  <br/>3. **引入多模态推理机制**：结合语音、文本和优先级推断，使高优先级消息可绕过常规通话等待限制，保障紧急通信时效性。  <br/>4. **应用生成式AI模型**：采用GPT Neo等文本生成模型，通过语音合成技术实现自动化消息生成与可配置分发（间隔G秒，次数N次）。  <br/>5. **跨行业应用潜力**：方法可应用于电信、移动设备制造和应急通信平台，具备实际落地价值和广泛影响力。|
|2506.07515v1|[Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for   Multi-Talker Speech Recognition](http://arxiv.org/abs/2506.07515v1)|总结：  <br/>提出无需辅助信息的多说话人语音识别框架，结合SD-CTC与SOT实现26%的错误率降低，性能接近依赖辅助信息的前沿方法。<br/><br/>贡献点：  <br/>1. 提出Speaker-Distinguishable CTC（SD-CTC），扩展CTC模型以联合分配token和说话人标签，解决说话人分配失败问题。  <br/>2. 将SD-CTC整合至Serialized Output Training（SOT）框架，仅通过重叠语音和转录实现说话人区分，无需额外辅助信息。  <br/>3. 实验证明多任务学习（SD-CTC + SOT）显著提升SOT模型性能，减少26%的错误率，与依赖辅助信息的SOTA方法效果相当。|
|2506.07494v2|[Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A   Proposal for Offline Speech Recognition and IoT Integration](http://arxiv.org/abs/2506.07494v2)|**贡献点：**  <br/>1. **离线语音识别集成**：将离线关键词唤醒（KWS）技术应用于资源受限的家用设备，实现本地化语音指令理解，减少对云端服务的依赖。  <br/>2. **分布式物联网架构**：设计本地化、去中心化的物联网网络，提升系统鲁棒性与可扩展性，降低通信延迟和能耗。  <br/><br/>**总结：**  <br/>本文提出基于离线语音识别和本地化物联网的智能家居方案，解决云端依赖导致的延迟、能耗和单点故障问题，实现低延迟语音控制与更高的系统稳定性及能源效率。|
|2506.07494v1|[Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A   Proposal for Offline Speech Recognition and IoT Integration](http://arxiv.org/abs/2506.07494v1)|总结：  <br/>本文提出基于离线语音识别和本地物联网的智能家居方案，通过本地化关键词检测和去中心化网络架构，解决了云端部署的能耗、延迟和单点故障问题，实现高效、可持续的语音控制。<br/><br/>贡献点：  <br/>1. **本地化语音识别整合**：提出将离线关键词检测（KWS）技术嵌入资源受限的家用设备，使其独立识别用户指令，减少对互联网的依赖。  <br/>2. **去中心化物联网架构**：设计本地物联网网络，通过分布式架构提升系统鲁棒性、可扩展性，并降低通信延迟和能耗。  <br/>3. **端到端低延迟控制**：实现全屋无网络依赖的低延迟语音交互，增强用户体验和能源效率。  <br/>4. **可持续系统优化**：通过离线处理与本地网络减少数据传输需求，提升整体能源可持续性。|
|2506.07473v3|[An introduction to pitch strength in contemporary popular music analysis   and production](http://arxiv.org/abs/2506.07473v3)|**贡献点：**<br/>1. 提出音高强度（pitch strength）作为低级感知参数在当代流行音乐中的关键作用，可能提升生成AI模型在音乐制作中的适用性。  <br/>2. 揭示音高强度在歌曲内的显著动态变化，既存在于整体结构也适用于局部细节分析。  <br/>3. 阐明音高强度对处理多声部不协和（polyphonic dissonance）的贡献，为音乐生成的音色控制提供依据。  <br/>4. 探讨音高强度与感知丰富性的关联，指出其可能通过高次谐波的感知显性化实现更复杂的听觉表达。  <br/><br/>**总结（100字以内）：**  <br/>该研究从信号与感知角度分析音高强度的多维度作用，揭示其在音乐结构、多声部处理及听觉丰富性中的关键角色，提出通过低级感知参数优化生成AI模型，使其更贴近音乐制作需求。|
|2506.07473v1|[An introduction to pitch strength in contemporary popular music analysis   and production](http://arxiv.org/abs/2506.07473v1)|**贡献点：**  <br/>1. 提出音高强度（pitch strength）作为当代流行音乐中关键的低级感知参数，可能提升生成式AI模型在音乐生产中的适用性。  <br/>2. 通过信号和感知分析，证明音高强度在歌曲内部及跨歌曲间存在显著变化，为音乐结构建模提供依据。  <br/>3. 揭示音高强度对音乐中小尺度（如旋律）和大尺度（如段落）结构的共同贡献。  <br/>4. 证明音高强度在处理多声部不协和音时的作用，增强AI模型对复杂音乐音色的控制能力。  <br/>5. 指出音高强度可能涉及上泛音在感知丰富度视角下的可听性，拓展对音乐感知机制的理解。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过分析音高强度的多维特性，揭示其在音乐结构、多声部处理及感知丰富度中的关键作用，为生成式AI音乐模型与实际生产的结合提供理论支持。|
|2506.07358v1|[Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream   Multi-Modal Learning Framework](http://arxiv.org/abs/2506.07358v1)|**贡献点：**<br/>1. 提出单流多模态学习框架替代传统双流分离模型，减少冗余神经层，提升模型效率；  <br/>2. 设计协作音频-视觉学习块，实现跨层持续融合，高效捕捉多模态特征；  <br/>3. 引入多模态分类模块，增强分类器对模态内容的依赖性与对跨模态不匹配的鲁棒性；  <br/>4. 在DF-TIMIT、FakeAVCeleb和DFDC数据集上验证，参数量仅0.48M，相较SOTA方法显著轻量化，且对单/多模态及未见过的Deepfake均表现优异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出轻量级单流多模态网络框架，通过协作学习块与分类模块提升音频-视觉Deepfake检测效率与鲁棒性，相较主流方法参数减少78.6%，在多种数据集和Deepfake类型中均取得优异性能。|
|2506.07294v1|[Towards Generalized Source Tracing for Codec-Based Deepfake Speech](http://arxiv.org/abs/2506.07294v1)|**贡献点：**  <br/>1. **揭示问题**：指出基于CoSG数据训练的源追溯模型易过拟合非语音区域且泛化能力差。  <br/>2. **提出方法**：设计SASTNet，联合使用Whisper（语义编码）和Wav2vec2+AudioMAE（声学编码）实现多模态特征融合。  <br/>3. **验证效果**：在CodecFake+数据集上取得SOTA性能，证明其在真实CoSG生成语音中的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出SASTNet，通过整合语义与声学特征编码解决深伪语音源追溯问题，在真实数据上实现最佳性能，推动语音伪造检测技术发展。|
|2506.07237v2|[Multi-Distillation from Speech and Music Representation Models](http://arxiv.org/abs/2506.07237v2)|总结（100字以内）:  <br/>该论文提出跨域蒸馏框架，将语音与音乐模型统一为轻量模型，并通过实验证明其在多样化任务和小样本场景下的有效性。|
|2506.07237v1|[Multi-Distillation from Speech and Music Representation Models](http://arxiv.org/abs/2506.07237v1)|贡献点总结（100字以内）: <br/>提出跨领域蒸馏框架，统一语音与音乐模型并减小规模，验证其在多样化任务中的有效性及少样本场景下的优越性，证明跨领域方法在数据有限场景的必要性。<br/><br/>分点贡献：<br/>1. **跨领域模型统一**：首次将语音（HuBERT）与音乐（MERT）模型整合为单一模型，解决现实音频混合场景的处理问题。<br/>2. **模型压缩创新**：通过多导师蒸馏显著降低模型体积，实现性能与效率的平衡。<br/>3. **领域平衡策略**：探索多种方法协调语音与音乐领域的知识迁移，提升跨域泛化能力。<br/>4. **任务兼容性验证**：在多样化任务中证明模型性能可媲美领域专用模型，展示跨域蒸馏的广泛适用性。<br/>5. **少样本学习优势**：在标签数据稀缺场景下，模型表现优于专用模型，凸显通用模型的实用性。|
|2506.07233v1|[Reducing Object Hallucination in Large Audio-Language Models via   Audio-Aware Decoding](http://arxiv.org/abs/2506.07233v1)|总结：  <br/>提出Audio-Aware Decoding（AAD）方法，通过对比解码减少LALMs音频幻觉，实验证明在多个数据集上显著提升性能，并通过消融研究验证方法有效性。<br/><br/>贡献点：  <br/>1. 提出**Audio-Aware Decoding（AAD）**，一种轻量级推理策略，利用对比解码对比音频上下文与无上下文的token预测logits，有效抑制音频幻觉问题。  <br/>2. 理论上通过对比解码机制，**增强音频存在时概率提升的token**，提升模型对音频信息的敏感性与准确性。  <br/>3. 在**object hallucination数据集**上实验证明，AAD将F1分数提升0.046至0.428，显著优于现有方法。  <br/>4. 在**Clotho-AQA等通用音频QA数据集**中验证，AAD使准确率提升5.4%-10.3%。  <br/>5. 开展**全面的消融研究**，系统分析AAD各组件对性能的影响，提供方法优化依据。|
|2506.07207v1|[Methods for pitch analysis in contemporary popular music: Vitalic's use   of tones that do not operate on the principle of acoustic resonance](http://arxiv.org/abs/2506.07207v1)|总结：  <br/>该论文通过分析Vitalic音乐中不谐和音调的创作手法，提出其在音乐领域中的独特贡献，并探讨类似特性在当代流行音乐中的普遍性，为声音结构研究提供新视角。<br/><br/>贡献点：  <br/>1. **解析音乐中的非谐和音调结构**：以Vitalic的《No Fun》为例，首次系统分析电子音乐中通过单不谐和音生成多旋律的合成技术。  <br/>2. **提出双音高同时感知的理论框架**：研究不谐和音调如何在音乐中形成两个或多个独立音高，突破传统声学共振的音高生成模式。  <br/>3. **跨案例验证通用性**：通过扩展分析其他音乐作品的相似现象，证明不谐和音调的多音高特性在当代流行音乐中的广泛存在。  <br/>4. **结合音乐创作启发声音研究**：将电子音乐中的音色设计方法关联到语音领域的声学分析，为非传统语音信号处理提供参考。  <br/><br/>（注：原文内容未明确指向语音领域，但若按“语音领域”要求解读，贡献点可能需假设其与声音结构、音高感知或语音合成技术相关联。）|
|2506.07199v1|[Audio synthesizer inversion in symmetric parameter spaces with   approximately equivariant flow matching](http://arxiv.org/abs/2506.07199v1)|贡献点总结：  <br/>提出音频合成参数逆向问题源于内在对称性，尤其是排列不变性，设计条件生成模型及排列等变连续归一化流解决该问题，并提出自适应发现对称性的放松策略，实验验证在真实合成器中优于传统方法。<br/><br/>分点贡献：  <br/>1. **揭示对称性根源**：首次明确音频合成器的信号-参数逆向问题主要由其内在对称性（如排列不变性）引起。  <br/>2. **对比实验分析**：证明传统回归方法在排列对称性下性能不佳，即使采用对称性破缺策略或不变损失函数仍存在局限。  <br/>3. **条件生成模型**：将等效解视为概率分布模式，通过条件生成模型显著提升音频重建效果。  <br/>4. **排列等变流**：引入排列等变的连续归一化流，进一步优化隐式参数分布的建模，提升性能。  <br/>5. **自适应对称性学习**：提出放松等变策略，从数据中自适应识别关键对称性以适应复杂合成器结构。  <br/>6. **实际应用验证**：在真实合成器Surge XT上验证方法有效性，证明其优于回归与生成模型基准。|
|2506.07149v1|[Technical Report: A Practical Guide to Kaldi ASR Optimization](http://arxiv.org/abs/2506.07149v1)|总结（100字以内）:  <br/>本文提出三种创新优化策略：声学模型中Conformer与多流TDNN-F结合、动态超参数调整、以及基于贝叶斯优化和n-gram剪枝的语言模型管理，显著提升Kaldi ASR系统的准确性、鲁棒性和可扩展性。<br/><br/>贡献点：<br/>1. **声学模型增强**  <br/>   - 设计集成多流TDNN-F结构的自定义Conformer模块，提升特征提取能力与时间建模性能。<br/><br/>2. **超参数优化**  <br/>   - 引入动态超参数调整技术，结合先进数据增强方法，增强模型泛化能力并减少过拟合。<br/><br/>3. **语言模型效率提升**  <br/>   - 提出基于贝叶斯优化的语言模型管理策略，结合n-gram剪枝技术，在保持相关性的同时提升计算效率。|
|2506.07118v1|[RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression   Diagnosis](http://arxiv.org/abs/2506.07118v1)|**总结（100字以内）:**  <br/>提出脑启发的RBA-FE模型，结合改进的分层网络与ARSLIF神经元，提升抑郁症诊断的噪声鲁棒性与特征提取精度，实验验证在多个数据集上的优越性能。<br/><br/>**贡献点分点列出:**  <br/>1. **提出RBA-FE模型**：设计了一种基于改进分层网络的脑启发音频特征提取器，专门针对抑郁症诊断任务。  <br/>2. **多维度声学特征提取**：从原始音频中提取六个声学特征，同时捕捉空间特性和时间依赖性，增强特征表示能力。  <br/>3. **改进神经元模型ARSLIF**：引入自适应速率光滑泄漏积分-火（ARSLIF）神经元，模拟大脑信号选择性调节机制，提升对环境噪声的鲁棒性。  <br/>4. **实验验证有效性**：在MODMA、AVEC2014和DAIC-WOZ数据集上验证模型性能，达到或超越现有方法的准确率（0.8750-0.8974）。  <br/>5. **增强模型可解释性**：通过对比实验表明，ARSLIF模型可揭示抑郁症音频中的异常放电模式，提供脑机制驱动的解释性分析。|
|2506.07078v1|[E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech   Foundation Models](http://arxiv.org/abs/2506.07078v1)|总结：  <br/>该论文提出E-BATS框架，通过三个创新组件解决语音模型在域移场景下的适应问题，实现效率与性能的平衡，并在多个数据集上验证了其有效性。<br/><br/>贡献点：  <br/>1. **首次提出无需反向传播的语音TTA框架**（E-BATS），专为语音基础模型设计，解决现有方法在资源受限场景下的内存问题。  <br/>2. **引入轻量级提示适应机制**，通过前向传播实现特征对齐，降低计算开销并保持适应效果。  <br/>3. **设计多尺度损失函数**，同时捕捉全局（语句级）和局部（token级）声学域移特征。  <br/>4. **开发测试时指数移动平均机制**，提升跨语句适应的稳定性。  <br/>5. **实验验证有效性**，在16种声学条件下的4个噪声数据集上，相比无反向传播基线提升4.1%-13.5%准确率，且内存消耗降低2.0-6.4倍。  <br/>6. **推动实际应用**，为构建高效、鲁棒的语音处理系统提供可扩展的解决方案。|
|2506.07073v1|[Insights on Harmonic Tones from a Generative Music Experiment](http://arxiv.org/abs/2506.07073v1)|贡献点：  <br/>1. **提出studio-lab作为跨学科协作框架**：构建研究者、音乐制作人与AI模型的协同实验模式，推动音乐生成技术与音乐实践的深度融合。  <br/>2. **揭示AI生成复音和声的多音高表达能力**：发现音乐制作人利用AI生成的复音和声传达多个音高，表明模型具备解析和生成复杂音乐结构的潜力。  <br/>3. **证明AI的多声部旋律生成机制**：通过实验验证AI能基于单音复音序列生成结构化、连贯的多声部旋律，突破传统单音生成的局限性。  <br/>4. **引发对人类音乐感知理论的再思考**：挑战长期存在的“人类能否感知和声为独立音高”的争议，为音乐认知研究与AI生成能力评估提供新视角。  <br/><br/>总结：  <br/>本研究通过跨学科实验揭示AI在音乐生成中的复杂能力，推动音乐创作与声学理论的交叉发展，为感知机制和生成技术提供了新证据。|
|2506.07036v2|[In This Environment, As That Speaker: A Text-Driven Framework for   Multi-Attribute Speech Conversion](http://arxiv.org/abs/2506.07036v2)|总结：  <br/>提出TES-VC框架，实现音色与环境声学的独立可控语音转换，通过合成数据与潜在扩散模型解耦特征，并引入基于检索的音色控制模块，在保持内容的同时提升生成质量与应用潜力。<br/><br/>贡献点：  <br/>1. **独立控制音色与环境**：首次实现文本驱动下说话人音色和环境声学的解耦控制，允许分别调整目标语音的音色特征和环境特征。  <br/>2. **端到端文本处理**：通过同时处理文本输入，生成与描述匹配的语音，保持源内容不变且避免属性干扰。  <br/>3. **潜在扩散建模**：采用潜在扩散模型训练合成数据，有效分离语音和环境特征，提升生成稳定性。  <br/>4. **无配对数据的音色控制**：引入基于检索的音色控制模块（RBTC），仅需抽象描述即可精确调整音色，无需配对数据。  <br/>5. **高内容保留与可控制性**：实验验证方法在保持语音内容完整性的同时，显著提升音色与环境的可控性，具备广泛应用前景。|
|2506.07036v1|["In This Environment, As That Speaker": A Text-Driven Framework for   Multi-Attribute Speech Conversion](http://arxiv.org/abs/2506.07036v1)|总结：  <br/>提出TES-VC框架，实现文本驱动下独立控制说话人音色与环境音效，通过合成数据与潜扩散模型解耦特征，引入检索式音色控制模块，提升生成语音的准确性和内容保留能力，验证其在语音转换任务中的高效性与应用潜力。<br/><br/>贡献点：  <br/>1. **提出TES-VC框架**：首次实现文本驱动的语音转换，支持独立控制说话人音色与环境音效的生成。  <br/>2. **解耦特征处理**：利用合成数据与潜在扩散模型分离语音和环境特征，消除属性间干扰。  <br/>3. **无需配对数据的音色控制**：引入检索式音色控制（RBTC）模块，通过抽象描述精准操控音色。  <br/>4. **内容保留能力**：在生成目标语音时保持源内容不变，确保语义一致性。  <br/>5. **实验验证有效性**：通过对比实验证明方法在音色、环境和内容保留上的优势，展示广泛应用前景。|
|2506.06888v1|[Automatic Speech Recognition of African American English: Lexical and   Contextual Effects](http://arxiv.org/abs/2506.06888v1)|总结（100字以内）:  <br/>该研究分析了AAE中的CCR和ING减少对ASR性能的影响，比较了端到端系统有无LM对词汇邻近效应和上下文预测性的敏感度差异，为改进ASR在AAE场景下的表现提供了新洞见。<br/><br/>贡献点:  <br/>1. **识别关键AAE变量**：首次系统分析了非裔美国人英语中的辅音簇缩减（CCR）和ING减少对自动语音识别（ASR）准确率的具体影响。  <br/>2. **量化语音识别误差**：通过实验验证CCR和ING减少对词错误率（WER）存在显著但微小的影响，明确了其在ASR中的干扰程度。  <br/>3. **对比LM作用差异**：揭示端到端ASR系统在无外部语言模型（LM）时更易受词汇邻近效应影响，而对上下文预测性依赖更低。  <br/>4. **方法创新**：提出结合wav2vec 2.0与蒙特利尔强制对齐器（MFA）发音扩展的标注流程，提升AAE语音特征检测的可靠性。  <br/>5. **实际应用指导**：为优化ASR在AAE等方言场景下的性能提供了理论依据和系统设计参考。|
|2506.06834v1|[Rhythm Features for Speaker Identification](http://arxiv.org/abs/2506.06834v1)|**贡献点：**  <br/>1. **提出基于节奏的说话人识别新方法**：首次将语音信号中的节奏信息（时间结构）作为高阶特征，通过深度学习模型进行文本无关的说话人身份识别。  <br/>2. **验证节奏特征的有效性**：实验证明节奏信息对说话人识别具有显著作用，支持其作为潜在的语音身份特征的重要性。  <br/>3. **揭示实际应用挑战**：指出非特定语境下（如自发性语音）语音的高内在变异性可能削弱节奏特征的识别效果，为后续研究提供方向。  <br/><br/>**总结（100字内）：**  <br/>本文提出基于节奏特征的文本无关说话人识别方法，验证其有效性并揭示自发性语音中节奏特征的局限性，为语音身份识别提供了新视角和改进方向。|
|2506.06772v1|[SynHate: Detecting Hate Speech in Synthetic Deepfake Audio](http://arxiv.org/abs/2506.06772v1)|总结：  <br/>本研究提出首个多语言合成音频仇恨言论检测数据集SynHate，涵盖37种语言，采用新颖的四分类框架，并评估主流自监督模型性能，推动跨语言、文化敏感的对抗合成仇恨言论解决方案。<br/><br/>贡献点：  <br/>1. **首个多语言数据集**：构建SynHate，支持37种语言，涵盖全球和印度的多样性仇恨言论模式。  <br/>2. **创新四分类框架**：提出Real-normal、Real-hate、Fake-normal、Fake-hate四类标签，提升仇恨言论检测的细粒度。  <br/>3. **跨语言模型评估**：系统评估Whisper-small/medium、XLS-R、AST、mHuBERT等五种自监督模型，揭示语言差异对性能的影响。  <br/>4. **性能对比与发现**：验证Whisper-small在整体表现上最优，同时指出跨数据集泛化能力的不足。  <br/>5. **开源促进研究**：发布数据集及基线代码，推动构建更鲁棒、文化敏感的多语言合成音频检测解决方案。|
|2506.06756v1|[Can Quantized Audio Language Models Perform Zero-Shot Spoofing   Detection?](http://arxiv.org/abs/2506.06756v1)|**分点贡献：**  <br/>1. **系统性评估**：首次系统研究五种大音频语言模型（GAMA、LTU-AS、MERaLiON、Qwen-Audio、SALMONN）在零样本音频欺骗检测任务中的性能表现。  <br/>2. **量化鲁棒性分析**：全面分析FP32、FP16、INT8不同精度量化对模型性能的影响，揭示量化与任务复杂度之间的关系。  <br/>3. **预测偏差发现**：指出所有模型在量化后均存在严重的预测偏差，导致实际性能等同于随机分类，暴露模型架构的局限性。  <br/>4. **效率与准确性的权衡**：证明FP16量化在显著降低内存和计算需求的同时，对检测准确率影响可忽略，为实际部署提供关键参考。  <br/>5. **INT8的负面影响**：揭示INT8量化会导致平衡准确率显著下降，强调精度降低对模型鲁棒性的潜在危害。  <br/>6. **实践指导与优化方向**：基于研究结果，提出FP16量化作为最优折中方案，为模型部署和未来改进提供实证依据。  <br/><br/>**总结（100字内）：**  <br/>本研究系统评估五种大音频语言模型在零样本欺骗检测中的性能，揭示量化对模型鲁棒性的关键影响，发现FP16能在保持性能的同时显著降低资源消耗，而INT8则加剧偏差，为高效部署提供理论支持和实践建议。|
|2506.06732v1|[Neural Spectral Band Generation for Audio Coding](http://arxiv.org/abs/2506.06732v1)|**贡献点：**  <br/>1. **提出非盲音频带宽扩展新范式**：首次将非盲BWE（Non-blind BWE）与深度神经网络（DNN）结合，突破传统SBR方法依赖粗糙特征提取的局限性。  <br/>2. **优化编码流程协作机制**：通过在音频编码管道的前端和末端分别部署DNN侧信息提取与带宽扩展模块，实现更高效的信号处理协同。  <br/>3. **提升多类型音频信号处理能力**：利用DNN对低频信号进行更精确的分析，改进对各类音频信号（如复杂音色、噪声等）的高频频段重构效果。  <br/>4. **降低对原始信号先验信息的依赖**：仅基于低频信号直接估计高频内容，减少对传统SBR中高层特征的依赖，增强方法通用性。  <br/>5. **解决盲BWE的性能瓶颈**：通过非盲框架优化DNN模型，避免盲BWE因忽略原始信号信息导致的重构质量下降问题。  <br/><br/>**总结：**  <br/>本文提出一种基于DNN的非盲音频带宽扩展方法，通过优化编码流程协作机制，提升多类型音频信号的高频频段重构性能，解决传统SBR与盲BWE的局限性。|
|2506.06675v1|[Accurate analysis of the pitch pulse-based magnitude/phase structure of   natural vowels and assessment of three lightweight time/frequency voicing   restoration methods](http://arxiv.org/abs/2506.06675v1)|总结：  <br/>本文提出新型算法与三种模型实现方案，解决 whispered speech 到自然语音的转换问题，通过分析元音差异和对比听觉测试验证方法有效性。<br/><br/>贡献点：  <br/>1. **提出新型 pitch 脉冲分割算法**：用于表征自然语音中 voiced 区域的谐波相位/幅度结构，揭示持续元音与共articulated 元音的关键差异。  <br/>2. **设计三种合成语音实现方案**：包括频域重构、联合时频域重构，以及基于生理机制的单脉冲滤波方法，覆盖不同信号处理路径。  <br/>3. **系统性对比与验证**：通过客观示例和主观听觉测试（词上下文场景）对三种方法进行性能评估，为技术落地提供实证支持。|
|2506.06603v1|[CAtCh: Cognitive Assessment through Cookie Thief](http://arxiv.org/abs/2506.06603v1)|总结：  <br/>该研究首次将ADRD预测模型扩展应用于认知障碍（CI）预测，对比了多模态与单模态方法的性能差异，证实声学特征优于语言学特征，并提出了可解释声学特征在情感和语调分析中的优势，同时公开了研究代码。<br/><br/>贡献点：  <br/>1. **模型扩展应用**：首次将基于自发性语音的ADRD预测算法应用于更广泛的认知障碍（CI）识别，揭示CI作为ADRD前兆的潜在价值。  <br/>2. **多模态方法优势验证**：实验证明多模态情感分析方法在CI预测任务中优于单模态方法，为多模态研究提供实证支持。  <br/>3. **声学与语言学性能对比**：发现声学特征（如情感、语调）在预测CI中表现优于语言学特征（如BERT），探索声学模态的潜力。  <br/>4. **可解释特征的显著效果**：特定可解释声学特征（与情感和语调相关）在CI预测中显著优于BERT等语言学方法，推动特征选择研究。  <br/>5. **开放代码与可复现性**：提供完整代码库，促进研究复现和社区协作，提升科学验证的透明度。|
|2506.06566v1|[AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech   Recognition](http://arxiv.org/abs/2506.06566v1)|总结：  <br/>提出轻量化的失语症语音识别框架AS-ASR，结合混合训练与GPT-4增强方法，显著提升识别性能并保持标准语音识别能力，适用于边缘设备部署。<br/><br/>贡献点：  <br/>1. **提出AS-ASR框架**：基于Whisper-tiny构建，专为边缘设备低资源部署设计，实现轻量化与高效性。  <br/>2. **混合训练策略**：通过动态调整标准与失语症语音的训练比例，提升模型对失语症数据的泛化能力。  <br/>3. **GPT-4参考增强方法**：利用大规模语言模型优化失语症转录的监督质量，降低噪声干扰。  <br/>4. **实验验证与效果**：在多种数据混合配置和评估场景下，验证模型性能，实现失语症语音WER降低超30%。  <br/>5. **可扩展解决方案**：为实际应用提供通用、高效的失语症语音识别方案，兼顾标准与失语症语音处理。|
|2506.06537v1|[Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by   Connecting Pretrained Models](http://arxiv.org/abs/2506.06537v1)|总结：  <br/>提出零样本音频视觉分割框架，通过多模态预训练模型集成实现无需标注的精准分割，验证了跨模态融合在细粒度任务中的有效性。<br/><br/>贡献点：  <br/>1. 提出首个无需AVS专用标注的零样本框架，突破传统依赖像素级标注的局限；  <br/>2. 首创整合音频、视觉与文本多模态表示的方法，有效弥合模态间语义鸿沟；  <br/>3. 系统探索多预训练模型连接策略，为零样本AVS提供可复用的架构设计；  <br/>4. 在多数据集验证框架效果，达到当前最优的零样本分割性能。|
|2506.06252v1|[Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems](http://arxiv.org/abs/2506.06252v1)|总结（100字以内）:  <br/>本文提出高效的提示偏置技术，结合多任务学习框架，通过提示偏置模型和实体过滤机制提升ASR对罕见实体的识别准确率，实验证明在领域数据集上显著降低实体词错误率，且方法轻量无结构变更。<br/><br/>贡献点分点列出:  <br/>1. **提出统一的多任务学习框架**：将prompt-based biasing与实体识别任务整合，提升ASR对罕见实体的识别效果。  <br/>2. **设计双组件结构**：包含（a）提示偏置模型（判断何时关注实体）和（b）实体过滤机制（高效剔除无关实体）的创新架构。  <br/>3. **显著性能提升**：在自建领域数据集上，针对小/大实体列表分别实现30.7%和18.0%的Entity Word Error Rate下降。  <br/>4. **轻量化与高效性**：无需改变模型结构，保持简单性与轻量性，适用于实际部署。  <br/>5. **应用场景明确**：针对语音识别中罕见和领域特定实体的识别难题，提供针对性解决方案。|
|2506.06190v1|[NAT: Neural Acoustic Transfer for Interactive Scenes in Real Time](http://arxiv.org/abs/2506.06190v1)|总结：  <br/>提出Neural Acoustic Transfer方法，通过隐式神经表示实现动态声学场景的实时声场预测，并结合高效BEM生成训练数据，显著提升交互应用中的声学建模效率与精度。<br/><br/>贡献点：  <br/>1. **提出新型声学传输模型**：Neural Acoustic Transfer通过隐式神经表示编码声学传输及其动态变化，支持实时声场预测。  <br/>2. **高效训练数据生成方法**：开发快速蒙特卡洛边界元法（BEM）近似，适用于光滑Neumann条件场景。  <br/>3. **高精度BEM实现**：推出GPU加速的标准BEM版本，满足高精度声学模拟需求。  <br/>4. **动态环境建模能力**：解决物体位置、材料、尺寸变化导致的声学分布波动问题，实现高效且精准的声辐射空间建模。  <br/>5. **实验验证与应用拓展**：在多样化场景中验证方法的高精度与低延迟（30秒音频在几毫秒内完成），适用于VR、AR及先进音频生产等交互应用。|
|2506.06096v1|[Label-Context-Dependent Internal Language Model Estimation for CTC](http://arxiv.org/abs/2506.06096v1)|贡献点：<br/>1. 首次系统分析CTC的隐式上下文依赖性，揭示其通过现代编码器学习上下文相关内部语言模型（ILM）的能力<br/>2. 提出基于知识蒸馏的新型ILM估计方法，包含理论推导与优化框架<br/>3. 开发两种KD正则化技术，增强模型鲁棒性与泛化能力<br/>4. 建立跨域评估体系，验证上下文相关ILM在Librispeech和TED-LIUM数据集上的有效性<br/>5. 实现超过13%的相对WERR提升，证明标签级KD加平滑方法在ILM估计中的优越性<br/><br/>总结：本文揭示CTC的隐式上下文依赖性，提出基于知识蒸馏的新型ILM估计方法及正则化技术，在跨域评估中实现显著性能提升。|
|2506.06071v1|[CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for   Fair Speech Emotion Recognition](http://arxiv.org/abs/2506.06071v1)|总结：  <br/>提出CO-VADA方法，通过语音转换生成多样化样本，无需修改模型结构或依赖人口统计信息，提升语音情感识别系统的公平性。<br/><br/>贡献点：  <br/>1. **首创新方法**：开发了首个不依赖模型架构修改或人口统计信息的语音情感识别（SER）去偏差框架。  <br/>2. **虚假关联处理**：通过语音转换技术消除说话者特征与情感标签间的错误关联，增强模型对情感相关特征的聚焦。  <br/>3. **样本增强策略**：生成反映非主导说话者模式的样本，引入多样性以改善模型公平性。  <br/>4. **通用性与可扩展性**：兼容多种SER模型和语音转换工具，提供灵活、可扩展的解决方案。|
|2506.05984v1|[Audio-Aware Large Language Models as Judges for Speaking Styles](http://arxiv.org/abs/2506.05984v1)|**贡献点总结：**  <br/>1. 提出利用音频感知大语言模型（ALLMs）作为自动评判者，评估语音语言模型（SLMs）生成演讲的多维度说话风格。  <br/>2. 首次对比GPT-4o-audio与Gemini-2.5-pro在语音风格评估中的表现，验证其与人类评价的可比性。  <br/>3. 通过实验证实ALLMs具备评估SLMs输出内容的可行性，为语音AI的评估方法提供新思路。  <br/>4. 揭示当前SLMs在语音风格控制和自然对话生成方面存在局限性，明确技术改进方向。  <br/><br/>**摘要总结（100字以内）：**  <br/>本研究验证了ALLM作为自动评审者评估SLM演讲风格的可行性，发现Gemini与人类一致性相近，同时指出SLMs在语音控制和对话生成仍有提升空间。|
|2506.05899v1|[WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS   Prediction](http://arxiv.org/abs/2506.05899v1)|**贡献点**：  <br/>1. 提出WhisQ多模态架构，首次在文本到音乐系统中同时解决整体音乐质量（OMQ）和文本提示对齐（TA）的双重评估问题。  <br/>2. 集成Whisper Base（音频编码）与Qwen 3（文本编码），利用序列结构保持细粒度跨模态建模能力。  <br/>3. 设计双路径预测机制：OMQ通过池化音频嵌入预测，TA通过音频-文本双向序列共注意力建模。  <br/>4. 引入Sinkhorn最优传输损失，强化共享嵌入空间中的语义对齐，提升跨模态一致性。  <br/>5. 通过消融实验验证了最优传输正则化对性能的关键作用（+10% SRCC），证明显式跨模态对齐的重要性。  <br/><br/>**总结**（100字以内）：  <br/>本研究提出WhisQ架构，结合音频与文本编码，通过共注意力和最优传输优化实现双重评估，显著提升OMQ和TA预测性能，并验证了跨模态对齐对文本到音乐系统的重要性。|
|2506.05891v1|[WAKE: Watermarking Audio with Key Enrichment](http://arxiv.org/abs/2506.05891v1)|**贡献点总结（100字以内）：**  <br/>提出首个密钥可控音频水印框架WAKE，解决未经授权访问、多次嵌入解码及可变长度嵌入问题，提升音频质量和检测准确性。  <br/><br/>**分点贡献：**  <br/>1. **首次引入密钥控制机制**：WAKE通过特定密钥嵌入与恢复水印，显著增强安全性，防止错误密钥解码。  <br/>2. **解决多嵌入覆盖问题**：支持多次嵌入后水印的稳定解码，避免传统方法中的覆盖干扰。  <br/>3. **支持可变长度水印嵌入**：灵活适应不同长度的水印需求，扩展应用范围。  <br/>4. **性能优越**：在水印音频质量与检测精度上超越现有模型，验证了方法的有效性。|
|2506.05851v1|[DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection](http://arxiv.org/abs/2506.05851v1)|总结：  <br/>本文识别了音频-视频DeepFake检测中的数据集和评估问题，提出DeepSpeak v1数据集与新评估协议，并设计SIMBA方法作为简洁基线，分析音频捷径问题并提出缓解策略，最终优化FakeAVCeleb数据集的评估方案以推动领域发展。<br/><br/>贡献点：  <br/>1. 指出现有音频-视频DeepFake检测数据集存在不可复现性和关键缺陷（如FakeAVCeleb的沉默捷径问题）；  <br/>2. 提出DeepSpeak v1数据集，作为更可靠且全面的基准数据资源；  <br/>3. 首次设计并评估一种通用的多模态DeepFake检测评估协议，使用SOTA模型验证有效性；  <br/>4. 引入SImple Multimodal BAseline（SIMBA）方法，提供简明高效的基线模型以支持设计探索；  <br/>5. 深入分析音频捷径问题，提出针对性的缓解策略；  <br/>6. 改进FakeAVCeleb数据集的评估框架，增强其在实际应用中的可靠性。|
|2506.05802v2|[TADA: Training-free Attribution and Out-of-Domain Detection of Audio   Deepfakes](http://arxiv.org/abs/2506.05802v2)|**贡献点：**  <br/>1. **提出训练无关的kNN方法**：首次基于k-Nearest Neighbors（kNN）实现音频深度伪造模型溯源，无需专门训练数据。  <br/>2. **绿色AI与自监督学习结合**：利用预训练自监督学习模型，显著降低计算资源消耗，提升方法的环保性和实用性。  <br/>3. **高精度源识别能力**：在五类音频深度伪造数据集上实现0.93 F1-score，验证了方法的可靠性；同时具备跨域检测能力（OOD），对未见过的模型识别准确率达0.84 F1-score。  <br/>4. **多维分析与解释性研究**：通过多维度视角深入分析结果，挖掘模型溯源的关键特征，为后续研究提供理论支持。  <br/>5. **开源共享促进复现**：代码与数据协议已开源，便于学术界和工业界验证与应用。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出一种基于kNN的无训练音频深度伪造溯源方法，结合自监督学习实现绿色AI。在多个数据集上取得高精度识别结果，并具备跨域检测能力，通过开放代码库推动技术复现与应用。|
|2506.05802v1|[TADA: Training-free Attribution and Out-of-Domain Detection of Audio   Deepfakes](http://arxiv.org/abs/2506.05802v1)|**贡献点总结：**  <br/>1. 提出训练无关的绿色AI方法（基于kNN）用于音频伪造溯源，无需依赖生成模型训练数据。  <br/>2. 利用预训练自监督学习模型，实现跨数据集的高精度（0.93 F1）生成器样本聚类。  <br/>3. 首次在音频领域展示强出域检测能力（0.84 F1），可识别未见过的伪造模型。  <br/>4. 通过多维分析提供深入的理论与实证见解，增强方法的可靠性与泛化性。  <br/>5. 开源代码与数据协议，推动研究复现和透明性。  <br/><br/>**摘要总结（100字内）：**  <br/>本文提出一种训练无关、绿色AI的音频伪造溯源方法，基于kNN和预训练自监督模型，在多数据集上实现高精度聚类与出域检测，并开源代码促进研究可复现性。|
|2506.05593v1|[Improving Neural Diarization through Speaker Attribute Attractors and   Local Dependency Modeling](http://arxiv.org/abs/2506.05593v1)|**贡献点：**  <br/>1. **提出多阶段中间表示建模**：通过分阶段捕捉详细的“说话人属性”（如情感、语速等），而非直接建模说话人，提升说话人分割与识别的细粒度表征能力。  <br/>2. **引入Conformer架构替代Transformer**：利用卷积增强的Transformer模型，更高效建模局部语音特征，增强对多说话人场景的适应性。  <br/>3. **验证方法有效性**：在CALLHOME数据集上开展实验，证明所提方法在讲话人辨识任务中的性能提升。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过多阶段中间表示建模和Conformer架构改进，提出更精细的说话人属性表征方法，并在CALLHOME数据集验证了其有效性，提升了端到端说话人辨识性能。|
|2506.05140v1|[AudioLens: A Closer Look at Auditory Attribute Perception of Large   Audio-Language Models](http://arxiv.org/abs/2506.05140v1)|**贡献点：**<br/>1. 首次系统分析LALMs的听觉属性感知机制，揭示其内部处理过程。  <br/>2. 通过词汇投影追踪属性信息在模型层与token位置的动态变化规律。  <br/>3. 发现属性信息随层深度增加而衰减，且早层属性识别与模型准确性正相关。  <br/>4. 指出LALMs更依赖听觉输入查询而非在属性提及位置隐状态聚合信息。  <br/>5. 提出基于上述发现的模型增强方法，为性能优化提供新思路。  <br/><br/>**总结（100字以内）：**  <br/>该研究首次深入解析大音频-语言模型对听觉属性的内部处理机制，揭示属性信息在模型中的分布规律及依赖关系，并提出针对性优化方法，为提升模型性能和理解其行为提供理论依据与实践方向。|
|2506.05121v1|[The NTNU System at the S&I Challenge 2025 SLA Open Track](http://arxiv.org/abs/2506.05121v1)|**贡献点**：  <br/>1. 提出融合wav2vec 2.0与Phi-4 MLLM的跨模态评估系统，结合声学特征与语义理解。  <br/>2. 设计得分融合策略，有效弥补BERT依赖ASR转录（缺失语调/发音）和W2V缺乏语义解释的缺陷。  <br/>3. 在Speak & Improve Challenge 2025官方测试集上实现RMSE 0.375，取得第二名，优于基线模型（0.444）和第三名（0.384）。  <br/><br/>**总结**：  <br/>提出集成声学与语义分析的跨模态语音评估系统，通过得分融合策略突破BERT与W2V的局限，在竞赛中获得第二名。|
|2506.05104v1|[Survey on the Evaluation of Generative Models in Music](http://arxiv.org/abs/2506.05104v1)|总结：  <br/>本文提供跨学科综述，系统总结生成音乐系统的评估方法、指标及挑战，涵盖主观/客观、定性/定量、实证/计算等多维度分析，并从音乐学、工程与人机交互视角探讨其优劣势。<br/><br/>贡献点：<br/>1. **系统性综述**：全面梳理生成音乐系统的评估目标、方法与指标，填补领域内评估体系研究的空白。<br/>2. **多维度分类**：整合主观与客观、定性与定量、实证与计算等不同评估范式，明确其适用场景与局限。<br/>3. **跨学科视角**：从音乐学、工程、人机交互（HCI）三方面分析评估方法的优劣，推动理论与实践的融合。<br/>4. **方法论框架**：建立涵盖系统输出与模型可用性的评估框架，为后续研究提供指导和参考。|
|2506.04981v1|[Better Semi-supervised Learning for Multi-domain ASR Through Incremental   Retraining and Data Filtering](http://arxiv.org/abs/2506.04981v1)|总结：  <br/>提出一种基于增量半监督学习的ASR领域适配方法，结合多模型共识与命名实体识别（NER）进行伪标签筛选，在多领域数据集上显著提升模型性能并降低计算成本。<br/><br/>贡献点：  <br/>1. **首个增量半监督学习框架**：通过整合少量领域内标注数据与相关领域辅助数据，实现比无辅助数据的微调方法更高的性能提升（4%）。  <br/>2. **多模型共识与NER联合筛选方法**：提出结合多模型共识与NER的伪标签选择机制，相较于随机筛选，显著延缓性能饱和，提升更稳定。  <br/>3. **多领域实验验证**：在Wow和Fisher两个主流多领域数据集上，验证方法优于传统单步微调策略，且在实际任务中表现优于其他基准方法。  <br/>4. **高效筛选策略比较**：共识过滤在性能提升（22.3%/24.8%）与计算成本之间达到最佳平衡，NER则以更低成本提供竞争力的替代方案。|
|2506.04915v1|[A Practitioner's Guide to Building ASR Models for Low-Resource   Languages: A Case Study on Scottish Gaelic](http://arxiv.org/abs/2506.04915v1)|总结：  <br/>本文提出结合HMM与自监督模型的新方法，挑战传统依赖细调的信念，通过持续预训练和半监督训练实现低资源语言ASR的显著性能提升，尤其在苏格兰盖尔语上达到32%的WER降低。<br/><br/>贡献点：  <br/>1. **挑战传统方法信念**：论证在低资源语言ASR中，单纯依赖多语言端到端模型的微调并非最优方案。  <br/>2. **提出混合模型框架**：创新性地融合隐马尔可夫模型（HMM）与自监督模型，结合两者优势提升性能。  <br/>3. **优化数据利用策略**：通过持续自监督预训练与半监督训练，更高效地利用有限的语音和文本数据。  <br/>4. **实验证明有效性**：在苏格兰盖尔语等低资源语言上验证方法效果，实现相较于微调Whisper模型的32%相对WER降低。|
|2506.04890v1|[Multivariate Probabilistic Assessment of Speech Quality](http://arxiv.org/abs/2506.04890v1)|总结（100字以内）:  <br/>该论文提出基于多变量高斯分布的语音质量评估模型，利用Cholesky分解和扩展的概率仿射变换联合建模四个维度，既保持点估计精度又提供不确定性及相关性估计，推动语音质量诊断的精细化。<br/><br/>贡献点：  <br/>1. **引入多变量框架**：将传统单变量MOS估计扩展至多变量联合建模，同时考虑噪音、色彩失真、不连续性和响度四个维度。  <br/>2. **无约束协方差建模**：采用Cholesky分解预测维度间协方差，避免强假设限制，提升模型灵活性。  <br/>3. **扩展概率变换方法**：将概率仿射变换推广至多变量场景，增强对复杂语音质量特征的建模能力。  <br/>4. **提供全面评估信息**：在保持与SOTA方法同等点估计精度的同时，首次实现多维度的不确定性与相关性联合分析。  <br/>5. **推动实际应用**：通过多维诊断能力，辅助精准定位语音质量问题，指导针对性优化策略。|
|2506.04852v1|[Improving AI-generated music with user-guided training](http://arxiv.org/abs/2506.04852v1)|总结：  <br/>本文提出基于用户反馈的遗传算法优化框架，通过整合用户评分作为损失函数，提升音乐生成模型的个性化表现，实验证明该方法在两次迭代中显著提高用户满意度。<br/><br/>贡献点：  <br/>1. **引入人类计算框架**：首次将用户交互评分与遗传算法结合，实现音乐生成模型的动态自适应优化。  <br/>2. **创新损失函数设计**：将用户主观评分直接作为模型微调的损失函数，打破传统固定数据集训练的局限。  <br/>3. **迭代性能评估方法**：提出通过用户评分平均增长率量化模型改进效果，验证方法有效性（首次迭代+0.2，第二次+0.39）。  <br/>4. **解决音乐个性化难题**：针对音乐高度主观性需求，设计可响应用户偏好的交互式生成机制，弥补图像生成模型的不足。|
|2506.04779v1|[MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark](http://arxiv.org/abs/2506.04779v1)|总结（100字以内）:  <br/>提出MMSU基准，涵盖47项任务及多语言现象，系统评估SpeechLLM模型，揭示性能短板，推动语音理解与人机交互技术发展。<br/><br/>贡献点:  <br/>1. **提出MMSU基准**：首个针对自然语音理解和推理的综合评估基准，包含5,000个音频-问题-答案三元组，覆盖多模态任务。  <br/>2. **多模态语言学理论整合**：系统纳入语音学、韵律、修辞、句法、语义及语用特征，提升基准的理论基础与评估维度。  <br/>3. **模型性能评估与分析**：对14种先进SpeechLLM进行全面测试，揭示现有模型在细粒度感知和复杂推理上的不足。  <br/>4. **明确优化方向**：通过评估结果提出未来研究的关键方向，指导语音理解模型的改进与创新。  <br/>5. **开放资源支持**：提供基准数据集与评估代码，方便研究者复现与扩展，促进领域发展。|
|2506.04714v1|[IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech   translation](http://arxiv.org/abs/2506.04714v1)|总结：  <br/>本研究针对低资源Bhojpuri-Hindi语言对，通过超参数优化与数据增强方法提升SeamlessM4T模型的翻译性能，并分析跨语言信号与翻译错误对BLEU分数的影响。<br/><br/>贡献点：  <br/>1. **系统提交**：参与IWSLT 2025语音翻译共享任务，提交IIITH-BUT系统用于Bhojpuri-Hindi语言对。  <br/>2. **超参数优化**：系统研究学习率调度、更新步数、预热步数、标签平滑和批量大小等超参数对模型性能的影响。  <br/>3. **数据增强技术**：应用速度扰动和SpecAugment方法缓解数据稀缺问题，并验证其对翻译质量的提升效果。  <br/>4. **跨语言联合训练**：通过联合训练Marathi和Bhojpuri语音数据，探索跨语言信号对模型的辅助作用。  <br/>5. **错误分析**：分析翻译假说，识别影响BLEU分数的不同类型错误，为模型改进提供依据。|
|2506.04652v1|[EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label   Speech Emotion Recognition](http://arxiv.org/abs/2506.04652v1)|**贡献点（分点）：**  <br/>1. **填补研究空白**：首次系统性探索多标签语音情感识别（SER）中的去偏方法有效性与鲁棒性，解决性别偏见问题的现有研究不足。  <br/>2. **提出综合框架**：构建EMO-Debias，对13种去偏技术（包括预处理、正则化、对抗学习、有偏学习者、分布鲁棒优化等）进行大规模对比实验。  <br/>3. **实证评估**：使用WavLM和XLSR编码器，在人工与真实情感数据集上测试性别不平衡场景下的方法性能。  <br/>4. **量化权衡分析**：明确揭示公平性与准确性的平衡关系，筛选出同时降低性别性能差距且不牺牲整体模型效果的策略。  <br/>5. **提供实践指导**：总结可操作的去偏方法选择建议，并强调数据分布对结果的显著影响。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出EMO-Debias框架，系统评估13种去偏技术在多标签SER中的性能，揭示性别偏见的权衡关系，为去除性别偏差提供实用策略并强调数据分布的重要性。|
|2506.04518v2|[Towards Efficient Speech-Text Jointly Decoding within One Speech   Language Model](http://arxiv.org/abs/2506.04518v2)|总结：  <br/>本文提出了一种新型早停交替解码策略（ESI），在保持最优对齐效果的同时提升推理效率，并构建了高质量语音问答数据集以优化性能。<br/><br/>贡献点：  <br/>1. **系统比较联合解码策略**：在统一实验框架下，对比了主流的交替与并行生成方法，明确其性能、效率及对齐质量差异。  <br/>2. **发现交替解码优势**：证实交替生成在语音-文本对齐上表现最佳，但存在长序列导致的推理速度瓶颈。  <br/>3. **提出ESI优化方案**：设计早停交替机制，显著提升解码速度且保持略优性能。  <br/>4. **构建高质量语音问答数据集**：通过数据优化进一步提升语音问答任务的准确性。|
|2506.04518v1|[Towards Efficient Speech-Text Jointly Decoding within One Speech   Language Model](http://arxiv.org/abs/2506.04518v1)|贡献点总结（100字以内）:  <br/>本研究提出改进的ESI解码模式，在保持interleaved方法对齐优势的同时提升推理效率，并构建高质量语音QA数据集，显著提升了语音问答性能。<br/><br/>分点贡献:  <br/>1. **系统性对比研究**：首次在统一模型、tokenizer和训练数据下，系统比较了主流的语音-文本联合解码策略（interleaved与parallel generation）的性能、效率及对齐质量。  <br/>2. **高效解码方案**：提出ESI（Early-Stop Interleaved）方法，在缩短推理时间的同时保持甚至略微提升解码性能，解决长序列长度导致的效率瓶颈。  <br/>3. **高质量数据集构建**：设计并发布专门优化的语音问答（QA）数据集，用于提升语音QA任务的性能，推动相关研究进展。|
|2506.04495v1|[French Listening Tests for the Assessment of Intelligibility, Quality,   and Identity of Body-Conducted Speech Enhancement](http://arxiv.org/abs/2506.04495v1)|**贡献点：**  <br/>1. **系统评估EBEN模型**：首次在体传导传感器（ forehead accelerometer, rigid in-ear, throat mic）上通过主观实验验证EBEN的语音增强效果，涵盖可懂度、语音质量及说话人身份保持三方面。  <br/>2. **揭示性能差异**：发现EBEN对女性说话人喉部麦克风录音的说话人识别能力有小幅下降，表明模型效果与录音方式存在交互影响。  <br/>3. **建立相关性**：证明STOI与感知质量在体传导语音中存在关联，为客观评估提供依据。  <br/>4. **验证ECAPA2-TDNN有效性**：展示ECAPA2-TDNN在说话人验证中的表现与识别任务一致，增强其在真实场景中的可信度。  <br/>5. **提出指标局限性**：指出现有评估指标无法可靠预测EBEN对可懂度的增强效果，为未来研究提供方向。  <br/><br/>**总结（100字内）：**  <br/>该研究通过多模态主观测试评估EBEN在体传导语音中的效果，揭示其对语音质量与可懂度的提升及对女性识别的潜在影响，建立STOI与感知质量的相关性，并验证ECAPA2-TDNN的可靠性，强调现有指标预测能力的不足。|
|2506.04492v1|[Bringing Interpretability to Neural Audio Codecs](http://arxiv.org/abs/2506.04492v1)|总结:  <br/>本文提出两步方法解析语音编码器令牌，通过分析阶段揭示语音属性编码机制，合成阶段构建AnCoGen网络实现属性直接提取，提升模型可解释性。<br/><br/>贡献点:  <br/>1. 提出两阶段框架（分析+合成）系统研究语音信息在神经音频编解码器令牌中的编码机制  <br/>2. 首次量化分析语音属性（内容、身份、音高）在编码器输出中的分布与关联性  <br/>3. 开发AnCoGen网络实现对现有编解码器的后解释，直接从令牌中提取可解释的语音属性  <br/>4. 通过对比声学单位与语义单位的编码特性，揭示其在可解释性方面的差异  <br/>5. 建立可解释性导向的音频编解码器分析范式，为语音处理模型的可调试性研究提供新思路|
|2506.04392v2|[Phi-Omni-ST: A multimodal language model for direct speech-to-speech   translation](http://arxiv.org/abs/2506.04392v2)|总结：  <br/>本文提出Phi-Omni-ST，一种基于多模态模型的直接语音到语音翻译框架，通过音频Transformer与流式声码器的结合实现高效语音生成，实验表明其性能优于现有模型并接近SOTA水平。<br/><br/>贡献点：  <br/>1. 提出Phi-Omni-ST模型，首次实现基于语言模型的**直接语音到语音翻译**（ST），而非传统文本生成路径。  <br/>2. 引入**音频Transformer头**，通过**延迟音频标记预测**机制，实现语音生成与文本解码的协同对齐。  <br/>3. 集成**流式声码器**，支持实时波形合成，提升语音翻译的生成效率与流畅性。  <br/>4. 在CVSS-C数据集上验证了模型的优越性，显著超越同数据集下的基线模型，且在**扩大训练规模**后达到当前SOTA水平。|
|2506.04392v1|[Phi-Omni-ST: A multimodal language model for direct speech-to-speech   translation](http://arxiv.org/abs/2506.04392v1)|**贡献点：**  <br/>1. 提出Phi-Omni-ST模型，首次实现**直接语音到语音翻译**（ST）的多模态语言模型，无需中间文本转换。  <br/>2. 引入**音频Transformer头**与**流式vocoder**的组合架构，通过**音频标记延迟预测**实现高效语音生成与波形合成。  <br/>3. 在CVSS-C数据集上验证模型性能，**显著超越现有基线模型**，并实现端到端高保真翻译。  <br/>4. 通过扩展训练数据与模型规模，**达到当前SOTA性能水平**，证明模型的可扩展性与竞争力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Phi-Omni-ST模型，结合音频Transformer与流式vocoder实现端到端语音到语音翻译，超越基线模型并逼近SOTA性能，为高效高质量的语音翻译提供了新方案。|
|2506.04391v1|[Benchmarking Time-localized Explanations for Audio Classification Models](http://arxiv.org/abs/2506.04391v1)|总结：  <br/>本文提出了一种用于音频分类模型时间定位解释的基准框架，通过目标事件时间注释替代真实标签，系统化优化比较多种后处理解释方法，并验证了解释在发现虚假相关性中的应用价值。<br/><br/>贡献点：  <br/>1. 提出时间局部化音频分类解释的基准方案，利用目标事件时间注释作为替代真实标签的评估标准。  <br/>2. 系统化优化并对比多种模型无关的后处理解释方法，实现接近完美的解释效果。  <br/>3. 展示解释在揭示音频数据中虚假相关性方面的实用价值，增强模型可解释性研究的实际意义。|
|2506.04376v1|[Domain Adaptation Method and Modality Gap Impact in Audio-Text Models   for Prototypical Sound Classification](http://arxiv.org/abs/2506.04376v1)|**贡献点分点总结**：  <br/>1. 提出一种无需模型重训练的背景声音贡献量化方法，有效提升零样本环境声音分类性能。  <br/>2. 首次分析背景声音对模型性能的影响，并揭示其主要由SNR（信噪比）水平决定，而非背景类型。  <br/>3. 开发可跨不同背景和SNR条件的领域自适应技术，增强分类鲁棒性与泛化能力。  <br/>4. 系统研究音频-文本嵌入的模态间隙问题，证明缩小该差距可显著提升分类效果。  <br/>5. 验证方法在主流原型模型中的适用性，展现其可扩展性与对多样化环境的兼容性。  <br/><br/>**总结**（100字以内）：  <br/>本文提出背景声音贡献量化与领域自适应方法，解决零样本环境声音分类中背景干扰问题，优化音频-文本模型性能，并通过模态间隙分析提升分类准确率，验证方法的通用性与鲁棒性。|
|2506.04364v1|[Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot   Accent Robustness in Low-Resource ASR](http://arxiv.org/abs/2506.04364v1)|总结：  <br/>该论文研究了低资源下ASR训练数据变量对未见口音鲁棒性的影响，发现增加说话人数比增加单人时长更有效，并提出在新语言开发中应优先提升说话人数量。<br/><br/>贡献点：  <br/>1. **系统研究训练数据变量的影响**：首次全面分析说话人数、单人时长和口音多样性三个关键因素对ASR系统鲁棒性（尤其是未见口音）的系统性影响。  <br/>2. **说话人数优先于单人时长**：在固定训练小时数下，增加说话人数（降低单人贡献量）比延长单人时长更能提升ASR对未见口音的适应能力。  <br/>3. **说话人数与时长扩展的协同效应**：发现更多说话人数量可增强ASR性能随训练时长扩展的收益，揭示数据规模与多样性之间的交互作用。  <br/>4. **口音多样性作用有限**：在控制说话人数时，优先选择口音差异大的说话人对性能提升作用较小，挑战了传统数据多样性优化的假设。  <br/>5. **实践指导建议**：提出针对新语言开发的ASR训练策略，建议优先扩大说话人数量而非过度依赖单人数据质量或时长，具有实际应用价值。|
|2506.04214v1|[Sounding that Object: Interactive Object-Aware Image to Audio Generation](http://arxiv.org/abs/2506.04214v1)|**贡献点总结**（100字以内）:  <br/>提出交互式对象感知音频生成模型，整合对象中心学习与扩散模型，通过多模态注意力实现图像-声音关联，理论验证注意力机制逼近分割掩码，实验验证性能优于基线。<br/><br/>**分点贡献**:<br/>1. **模型创新**: 提出"交互式对象感知音频生成"方法，将声音生成与用户选定的视觉对象直接关联，解决复杂场景中多对象、多声源的音频生成难题。<br/>2. **技术整合**: 将对象中心学习融合至条件潜变量扩散模型，引入多模态注意力机制实现图像区域与对应声音的跨模态映射。<br/>3. **交互机制**: 在测试阶段利用图像分割技术，允许用户以对象级别进行交互式音频生成，提升生成的精准度和可控性。<br/>4. **理论支撑**: 理论证明注意力机制在功能上可逼近实际分割掩码，确保生成音频与视觉对象的空间一致性。<br/>5. **实验验证**: 通过定量与定性评估显示，模型在对象-声音对齐任务中优于现有基线方法，验证了方法的有效性。|
|2506.04076v1|[Acoustically Precise Hesitation Tagging Is Essential for End-to-End   Verbatim Transcription Systems](http://arxiv.org/abs/2506.04076v1)|总结（100字以内）:  <br/>该研究提出通过LoRA微调Whisper模型改进verbatim L2语音转录，比较三种标注方案，发现精确填充符标注能显著提升ASR准确率，获得优于传统方法的WER性能。<br/><br/>贡献点分点列出:  <br/>1. 提出无需外部音频数据的LoRA微调方法，优化Whisper模型在verbatim L2语音转录中的表现  <br/>2. 设计并对比三种标注方案（Pure/Rich/Extra），揭示标注细粒度对ASR性能的影响  <br/>3. 首次证明基于Gemini 2.0 Flash推断的acoustically precise填充符标注（Extra）的有效性  <br/>4. 实验结果表明"Extra"方案可使Whisper Large V3 Turbo的WER降低11.3%（从6.2%至5.5%）  <br/>5. 强调显式填充词标注在处理口语化、不流畅语音（如hesitations）中的关键作用|
|2506.04073v1|[A Statistics-Driven Differentiable Approach for Sound Texture Synthesis   and Analysis](http://arxiv.org/abs/2506.04073v1)|总结：  <br/>提出TexStat损失函数及TexEnv合成器，构建DDSP-inspired的TexDSP模型，实现纹理声音的高效生成与评估，并开源代码促进研究应用。<br/><br/>贡献点：  <br/>1. **提出TexStat损失函数**：专门设计用于纹理声音分析与合成，具备感知意义、时间不变性和抗噪鲁棒性，无需依赖时序结构。  <br/>2. **开发TexEnv合成器**：轻量级、可微分的生成方法，通过滤波噪声叠加幅度包络实现纹理音频合成。  <br/>3. **构建TexDSP生成模型**：整合TexStat和TexEnv，作为DDSP-inspired模型，专为纹理声音的生成任务优化。  <br/>4. **引入综合评估方案**：将TexStat与FAD结合，形成更全面的纹理声音合成模型评估指标。  <br/>5. **开源工具与代码**：提供PyTorch实现，确保高效性、可配置性，支持生成任务与感知评估应用。|
|2506.04037v1|[The mutual exclusivity bias of bilingual visually grounded speech models](http://arxiv.org/abs/2506.04037v1)|总结：  <br/>该研究验证了双语VGS模型中ME偏差的减弱现象，揭示了视觉嵌入方差变化与跨语言混淆的关系，并提出了ME偏差存在的新理论视角。<br/><br/>贡献点：  <br/>1. **验证双语ME偏差现象**：首次发现双语VGS模型（英语+法语/荷兰语）相比单语模型表现出更弱的ME偏差，且存在语言特例。  <br/>2. **解释混淆机制**：通过分析视觉嵌入的方差差异，揭示双语模型对熟悉数据的低方差导致新旧概念混淆增加，为ME偏差提供计算解释。  <br/>3. **探究ME偏差根源**：提出新的理论视角，解释VGS模型中ME偏差的形成原因，深化对语言学习机制的理解。  <br/>4. **多语言实验框架**：构建跨语言（英法荷）的实验范式，拓展了ME策略在多语言环境下的研究边界。|
|2506.04013v1|[Towards Better Disentanglement in Non-Autoregressive Zero-Shot   Expressive Voice Conversion](http://arxiv.org/abs/2506.04013v1)|**贡献点（分点）:**  <br/>1. **框架改进**：基于条件变分自编码器（CVAE）提出自监督非自回归语音转换框架，提升风格迁移效果。  <br/>2. **内容表征优化**：采用多语言离散语音单元降低源音色泄漏，结合增强的相似性损失和混合风格层归一化方法。  <br/>3. **风格嵌入增强**：通过局部F0信息的交叉注意力机制和全局音高/能量特征提取，提升表达性迁移能力。  <br/>4. **实验验证**：模型在情感与说话人相似性任务中显著优于基线，验证了其在风格适应和源风格泄漏抑制上的有效性。  <br/><br/>**总结（100字以内）:**  <br/>本研究通过CVAE框架改进和多层级风格嵌入设计，有效解决了语音转换中的源音色泄漏问题，并增强了情感与表达属性的迁移能力，实验表明其在风格适应性上优于现有方法。|
|2506.03959v1|[From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder   Framework for Auditory Perception and Cochlear Implant Simulation](http://arxiv.org/abs/2506.03959v1)|贡献点总结（100字以内）:  <br/>提出NeuroVoc框架，实现模型无关的声码器，通过模块化设计支持听觉模型替换，验证其在模拟耳蜗植入用户听觉感知中的有效性，展示NH与EH模型的差异，并证明声码语音在噪声中的可懂度与临床数据一致。<br/><br/>分点贡献：  <br/>1. **提出NeuroVoc框架**：首次构建模型无关的声码器系统，基于逆傅里叶变换从神经活动模式重建声波，通用性强。  <br/>2. **模块化架构设计**：允许灵活替换或修改底层听觉模型（如NH与EH），提升框架的适应性与扩展性。  <br/>3. **消除特定实现依赖**：无需为不同语音编码策略定制声码器，简化模拟听觉感知的流程。  <br/>4. **直接对比听觉模型**：通过NH与EH模型的声码语音测试，明确其在谐波结构保留上的差异。  <br/>5. **验证感知可懂度**：采用Digits-in-Noise实验，证明声码语音在噪声环境中的可懂度与临床数据高度吻合。  <br/>6. **量化性能提升**：通过SRT（信号-噪声比）数据，显示NH与EH声码语音分别比标准测试提升2.4 dB和7.1 dB。  <br/>7. **揭示CI用户表现差异**：准确反映耳蜗植入用户在噪声中的听觉障碍特性，为研究提供可靠工具。|
|2506.03917v1|[Sound Field Reconstruction Using Physics-Informed Boundary Integral   Networks](http://arxiv.org/abs/2506.03917v1)|总结：  <br/>本文提出了一种基于边界积分方程的声场重建方法，利用浅层神经网络高效预测声压分布，通过均方误差训练提升精度，并在实验中验证其优于现有物理信息数据驱动技术。<br/><br/>贡献点：  <br/>1. **提出新型模型**：首次引入边界积分网络（Boundary Integral Network）用于声场重建，基于Kirchhoff-Helmholtz边界积分方程建模，将物理规律直接嵌入网络结构。  <br/>2. **浅层网络优化**：采用浅层神经网络替代复杂结构，降低计算成本，同时实现边界与内部声压的高精度预测。  <br/>3. **误差驱动训练**：通过最小化测量麦克风位置的均方误差进行模型训练，提升重建结果与实际数据的匹配度。  <br/>4. **性能验证**：实验表明该方法在声场重建任务中优于现有物理信息神经网络（PINN）等数据驱动技术。|
|2506.03832v1|[Brain-tuned Speech Models Better Reflect Speech Processing Stages in the   Brain](http://arxiv.org/abs/2506.03832v1)|**贡献点：**  <br/>1. 揭示预训练模型与人类语音处理层级结构的差异：中层语义丰富，晚层语义贫乏。  <br/>2. 提出脑微调方法，验证其提升语音模型语义理解的有效性。  <br/>3. 发现脑微调后模型的晚层显著优于预训练模型，与语义语言区域高度对齐。  <br/>4. 通过层析分析证明模型早期层专精声学特征，晚层擅长复杂高级任务。  <br/>5. 证明脑微调模型具有清晰的层级化处理机制，可作为研究人类语音处理的更优模型生物。  <br/><br/>**总结：**  <br/>本研究发现脑微调显著改进语音模型的语义对齐，揭示模型从声学到语义的分层处理特性，使其更贴近人类语音处理机制。|
|2506.02499v2|[DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds](http://arxiv.org/abs/2506.02499v2)|**贡献点：**  <br/>1. **提出首个针对电影音频的非语音声音数据集（DnR-nonverbal）**：包含笑声、尖叫等情绪化非语音声源，弥补现有CASS数据集仅含阅读式语音的不足。  <br/>2. **定义新的语音主干（speech stem）标准**：将非语音声源（如笑声、尖叫）纳入语音主干，更贴近实际影视音频场景。  <br/>3. **揭示当前CASS模型的局限性**：通过实验验证模型对情绪化非语音声源的分割错误，并证明新数据集能有效解决该问题。  <br/>4. **公开数据集以促进研究**：提供数据集下载链接，支持合成与真实电影音频的进一步研究与评估。  <br/><br/>**总结（100字内）：**  <br/>本研究提出DnR-nonverbal数据集，专为处理电影音频中的非语音声源（如笑声、尖叫），弥补现有数据集的不足，并通过实验验证其对提升情感化语音分割性能的有效性，数据集已开放获取。|
|2506.01483v2|[Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction](http://arxiv.org/abs/2506.01483v2)|**贡献点分点总结：**  <br/>1. 提出基于说话人间相对线索（如时间顺序、年龄、音高）的新型目标说话人区分与语音分离方法。  <br/>2. 分类处理连续线索（通过相对差异）与离散线索（保留原始类别），提升特征利用效率。  <br/>3. 相对线索方法相比固定属性分类更具灵活性，支持文本引导数据集的便捷扩展。  <br/>4. 实验验证结合全部相对线索优于随机子集，性别和时间顺序在多语言及混响条件下表现最稳健。  <br/>5. 其他线索（如音高、响度、距离）在复杂场景中显著提升性能。  <br/>6. 通过微调预训练WavLM Base+CNN编码器，显著优于仅使用Conv1d编码器的基线模型。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出基于说话人相对线索的语音分离方法，结合连续与离散特征分类，提升灵活性与可扩展性。实验表明，综合所有相对线索可显著增强性能，尤其在多语言及复杂场景中，同时利用深度学习模型优化进一步提高效果。|
|2506.00975v2|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v2)|总结：  <br/>本研究提出Next-Token-Pair Prediction（NTPP）框架，首次利用解码器-only架构实现说话人无关的双通道对话学习，并在回合转换预测、响应连贯性和自然性上取得显著提升，同时降低推理延迟，提升实时应用效率。<br/><br/>贡献点：  <br/>1. **提出NTPP新范式**：创新性地设计Next-Token-Pair Prediction方法，首次将双通道语音数据用于解码器-only架构的对话学习。  <br/>2. **说话人无关对话建模**：突破传统方法依赖说话人信息的限制，实现通用双通道对话理解与生成。  <br/>3. **提升对话能力**：在turn-taking预测、响应连贯性与自然性等关键指标上取得显著性能改进。  <br/>4. **优化实时性**：相比现有方法，NTPP大幅降低推理延迟，增强实际部署的可行性。|
|2506.00861v2|[Leveraging AM and FM Rhythm Spectrograms for Dementia Classification and   Assessment](http://arxiv.org/abs/2506.00861v2)|总结：  <br/>本研究提出RFA衍生的节奏频谱图作为新特征，结合手工艺特征和数据驱动融合方法，显著提升痴呆语音分类和回归任务的性能。<br/><br/>贡献点：  <br/>1. 提出RFA衍生的节奏频谱图（RFA rhythm spectrograms）作为捕捉痴呆语音长期时域调制的新型特征。  <br/>2. 设计两种方法：(1) 从节奏频谱图手动提取特征，(2) 融合RFA频谱图、ViT听觉表征与BERT语言嵌入。  <br/>3. 手动特征在分类任务中比eGeMAPs提升14.2%，在回归任务中表现与基线相当。  <br/>4. 融合方法在分类任务中优于Mel频谱图13.1%，在回归任务中与基线表现接近。  <br/>5. 验证了RFA特征在语音分析中的有效性，为痴呆检测提供了新的技术路径。|
|2506.00628v2|[LID Models are Actually Accent Classifiers: Implications and Solutions   for LID on Accented Speech](http://arxiv.org/abs/2506.00628v2)|**贡献点总结（100字以内）:**  <br/>本文通过分析发现LID模型在处理口音语音时存在分类错误模式，揭示其依赖短语音特征而非语言特征，并提出通过输入分块提升模型鲁棒性，同时无需单语ASR系统即可有效改善口音语音识别性能。  <br/><br/>**分点贡献列述:**  <br/>1. **识别错误模式**：提出LID系统常将第二语言（L2）口音误判为母语或相关语言，揭示其在口音语音中的关键性能下降原因。  <br/>2. **分析模型特性**：证明SOTA模型对短语音片段排列具有不变性，表明其依赖短语音特征（如音系特征）而非语言特征进行分类。  <br/>3. **提出改进方法**：设计一种结合序列信息的模型框架，无需依赖单语ASR系统，有效缓解口音-语言混淆，显著提升口音语音识别性能，同时保持标准LID性能。|
|2506.00506v2|[Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond](http://arxiv.org/abs/2506.00506v2)|总结：  <br/>本研究提出基于wav2vec 2.0的语音质量评估系统，在VoiceMOS 2024挑战中取得优异成绩，并创新性地采用两阶段微调方法应对数据限制，验证了其在噪声与降噪语音评估中的有效性。<br/><br/>贡献点：  <br/>1. 构建基于wav2vec 2.0的语音质量评估系统，获VoiceMOS 2024挑战Track 3前三名，其中BAK指标预测最优，OVRL次优，SIG第三优。  <br/>2. 提出两阶段细粒度微调策略，解决挑战中对训练数据的严格限制问题，提升模型适应性。  <br/>3. 在VoiceMOS 2024数据集及CHiME 7 - UDASE数据集上验证系统性能，展示其跨数据集泛化能力。  <br/>4. 探索ITU-T P.835标准（SIG、BAK、OVRL）在实际降噪语音质量评估中的应用与优化。|
|2505.22266v2|[FGAS: Fixed Decoder Network-Based Audio Steganography with Adversarial   Perturbation Generation](http://arxiv.org/abs/2505.22266v2)|总结（100字以内）:  <br/>提出FGAS框架，通过固定解码器与对抗扰动生成增强音频隐写能力，显著提升生成音频质量与抗隐写分析性能，降低对复杂训练和大模型的依赖。<br/><br/>贡献点：<br/>1. **提出FGAS方法**：首次将固定解码器网络与对抗扰动生成结合，通过共享解码器结构和权重实现隐写信息提取，摆脱了对大预训练模型的依赖。<br/>2. **轻量化解码器设计**：构建轻量级固定解码器，兼顾隐藏信息的可靠提取与系统效率，简化了实现复杂度。<br/>3. **对抗扰动生成策略**：开发音频对抗扰动生成（APG）技术，优化扰动以保持生成音频在感知和统计上与原始信号高度相似，增强抗隐写分析能力。<br/>4. **性能提升验证**：实验表明FGAS在PSNR指标上较SOTA方法提升超10 dB，且在不同负载下抵抗隐写分析的分类错误率更高，验证了其有效性。|
|2505.20529v3|[Training Articulatory Inversion Models for Interspeaker Consistency](http://arxiv.org/abs/2505.20529v3)|**贡献点总结**  <br/>1. 提出基于最小对集的创新评估方法，提取跨说话者一致的发音目标。  <br/>2. 验证SSL模型在单/多说话人数据上是否能生成统一的发音模板。  <br/>3. 设计仅依赖语音数据的训练方法，提升跨说话者一致性。  <br/>（共99字）|
|2505.19644v2|[STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set   Source Tracing and Attribution](http://arxiv.org/abs/2505.19644v2)|**总结**：  <br/>提出STOPA数据集，系统化覆盖8个AM、6个VM及多参数设置，提升深伪语音来源追踪的准确性和可靠性，促进检测与模型透明度。<br/><br/>**贡献点**：  <br/>1. **构建系统化数据集**：创建STOPA，包含系统变化和丰富元数据，填补了深伪语音检测领域专用数据集的空白。  <br/>2. **多模型与参数覆盖**：涵盖8个声学模型、6个声码器模型及多样参数配置，支持更全面的来源追踪研究。  <br/>3. **多合成器多样性**：70万样本来自13种不同合成器，增强数据集的代表性与泛化能力。  <br/>4. **提升归属可靠性**：通过系统化控制生成因素，显著提高来源归属的准确性，助力法医学分析与模型透明度研究。|
|2505.19577v3|[MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous   Decoding](http://arxiv.org/abs/2505.19577v3)|**总结**：该研究提出一种高效的关键词识别框架MFA-KWS，结合流式CTC-Transducer与多头异步解码，优化解码策略与分数融合方法，提升性能与效率，尤其在噪声环境和任意关键词任务中表现优异，适合端侧部署。<br/><br/>**贡献点**：  <br/>1. 提出MFA-KWS框架：融合流式CTC-Transducer与多头异步解码（MFA），实现关键词识别的效率与精度优化。  <br/>2. 改进解码机制：采用关键词特定的电话同步解码替换传统RNN-T，并引入Token-and-Duration Transducer提升性能。  <br/>3. 优化分数融合策略：设计一致性基于的CDC-Last方法，显著优于单帧基线策略，在关键词检测中取得最佳效果。  <br/>4. 实现显著速度提升：在多种数据集上比帧同步基线快47%-63%，降低计算开销。  <br/>5. 优异的鲁棒性与适用性：在噪声环境中保持稳定性能，支持固定和任意关键词任务，适用于端侧实时部署。|
|2505.19493v2|[Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation](http://arxiv.org/abs/2505.19493v2)|**分点贡献：**  <br/>1. **提出两阶段算法**：结合声源方向线索与多通道信号处理，优化传统多通道AEC框架。  <br/>2. **轻量级DNN方向预测**：首次使用轻量模型预测声源方向，降低计算开销。  <br/>3. **多模态信号融合**：将预测方向信息、多通道麦克风信号与单通道远端信号联合输入AEC网络，提升恢复效果。  <br/>4. **环境适应性增强**：算法在不同声学场景下表现优异，验证了其鲁棒性和泛化能力。  <br/><br/>**总结：**  <br/>该研究提出一种基于声源方向线索的两阶段多通道AEC算法，通过轻量级DNN预测方向并融合多模态信号，在复杂环境中实现更高效的回声消除。|
|2505.17076v3|[Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English](http://arxiv.org/abs/2505.17076v3)|**贡献点：**  <br/>1. **揭示帧率对语音分词的影响差异**：首次系统分析不同帧率对汉语和英语语音分词的差异化影响，强调语言类型差异的重要性。  <br/>2. **建立帧率与语音特征的关联性**：发现帧率变化与发音密度、语言特有声学特征之间的相互作用机制。  <br/>3. **提供优化帧率选择的理论依据**：提出基于语言特性的帧率优化策略，助力自动语音识别等任务的性能提升。  <br/>4. **跨语言实证研究**：通过实验验证帧率对两种典型语言的分词效果，为多语言语音处理提供参考。  <br/><br/>**总结（100字以内）：**  <br/>本研究揭示了不同帧率对汉语和英语语音分词的影响差异，分析了帧率与语音特征间的关联，为优化语音分词器性能提供了理论依据和跨语言实证支持。|
|2505.16044v2|[Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom   Severity Estimation](http://arxiv.org/abs/2505.16044v2)|**贡献点：**  <br/>1. 提出将精神分裂症评估从单一分类任务转向症状严重程度的估计，提升诊断的精细化程度。  <br/>2. 开发多模态融合框架，整合语音、视频和文本数据，增强模型的全面性和临床适用性。  <br/>3. 构建单模态模型与多模态框架的协同机制，显著提高模型的准确性与鲁棒性。  <br/>4. 通过捕捉更详细的症状特征，为个性化治疗提供数据支持。  <br/>5. 提出一种可扩展且客观的语音领域评估工具，具备实际医疗应用潜力。  <br/><br/>**总结（100字以内）**  <br/>本文创新性地采用多模态方法整合语音、视频和文本数据，通过症状严重度估计替代传统二分类任务，开发协同模型提升诊断精度与鲁棒性，为精神分裂症的个性化评估和治疗提供客观、可扩展的解决方案。|
|2505.15965v2|[Analyzing the Impact of Accent on English Speech: Acoustic and   Articulatory Perspectives](http://arxiv.org/abs/2505.15965v2)|**贡献点总结：**  <br/>1. 提出非母语口音英语的特征差异（简化协调模式、更高平均音调）  <br/>2. 开发基于eigenspectra和声管变量的高效量化方法（无需音素转录）  <br/>3. 揭示口音对语音可懂度的影响机制，推动研究方向创新  <br/>4. 为构建包容性强、适应多样语言群体的语音处理系统提供理论支持  <br/><br/>**摘要总结（100字以内）：**  <br/>该研究通过发音与声学分析揭示非母语口音英语的特征差异，提出无需音素转录的量化方法，为理解口音对语音可懂度的影响及开发包容性语音系统提供新视角。|
|2505.14561v2|[SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised   Speaker Verification](http://arxiv.org/abs/2505.14561v2)|**贡献点：**  <br/>1. 提出Self-Supervised Positive Sampling (SSPS)方法，改进传统SSL框架中锚-正对生成策略，解决录音条件导致的通道信息冗余问题。  <br/>2. 利用聚类分配与正样本嵌入记忆队列，在潜在空间中实现跨录音条件的同身份正样本匹配，增强模型对语音特征的学习能力。  <br/>3. 在SimCLR和DINO两种SSL框架中验证SSPS有效性，显著降低EER（2.57%和2.53%），超越VoxCeleb1-O的SOTA方法。  <br/>4. 明确对比两种模型增强效果：SimCLR-SSPS通过减少说话人内部方差实现58% EER降低，DINO-SSPS性能表现相近。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出SSPS方法，解决SSL在说话人验证中的录音条件信息冗余问题，通过聚类与记忆队列技术优化正样本匹配，显著提升SimCLR和DINO模型性能，在VoxCeleb1-O数据集上超越现有SOTA方法。|
|2505.13017v3|[Optimal Scalogram for Computational Complexity Reduction in Acoustic   Recognition Using Deep Learning](http://arxiv.org/abs/2505.13017v3)|总结（100字以内）:  <br/>本文提出通过优化小波核长度和scalogram步长降低CWT计算复杂度的方法，实验验证在保持语音识别性能的同时显著提升效率，为非平稳音频处理提供更经济的解决方案。<br/><br/>贡献点分点列出:  <br/>1. **提出CWT优化方案**  <br/>   - 首次通过调整小波核长度和输出scalogram的步长参数，系统性降低CWT的计算复杂度，解决其因高计算成本而被STFT替代的问题。<br/><br/>2. **验证性能与效率平衡**  <br/>   - 实验表明，优化后的CWT在保持模型鲁棒性及识别准确率的同时，显著减少计算资源消耗，证明其可行性。<br/><br/>3. **推动非平稳音频处理**  <br/>   - 为语音识别领域提供了一种兼顾时频分析精度与计算效率的替代方法，拓展了CWT在实际应用中的潜力。|
|2505.09661v2|[Introducing voice timbre attribute detection](http://arxiv.org/abs/2505.09661v2)|总结（100字以内）:  <br/>提出语音音色属性检测（vTAD）任务及基于说话人嵌入的框架，在VCTK-RVA数据集上对比验证了ECAPA-TDNN和FACodec编码器在seen/unseen场景下的性能差异，并公开了数据集与代码以促进研究。<br/><br/>贡献点分点列出:  <br/>1. **提出新任务**：定义语音音色属性检测（vTAD）任务，通过感官属性量化语音信号的音色感知特性。  <br/>2. **设计感知框架**：构建基于说话人嵌入的框架，用于对比两段语音在特定音色描述符中的强度差异。  <br/>3. **发布数据集**：提供VCTK-RVA数据集，支持音色属性研究与模型评估。  <br/>4. **对比编码器性能**：实验证明ECAPA-TDNN在seen场景表现更优，FACodec在unseen场景具有更强泛化能力，为编码器选择提供依据。  <br/>5. **开源代码**：开放vTAD任务的实现代码，推动领域研究复现与扩展。|
|2505.09382v2|[The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](http://arxiv.org/abs/2505.09382v2)|总结（100字以内）:  <br/>本研究提出VtaD 2025挑战，通过引入感官描述符和比较声音强度的框架，推动语音音色属性研究，促进学术交流。  <br/><br/>**贡献点分点列出**:  <br/>1. **构建首个针对语音音色属性的比较研究基准**：VtaD 2025首次系统化定义语音音色属性研究框架，推动基于对比性分析的语音质量评估。  <br/>2. **提出标准化感官描述符体系**：引入"bright, coarse, soft, magnetic"等具体描述符，建立统一的语音属性分类标准，提升跨研究的可比性。  <br/>3. **创新强度对比解释模型**：设计基于特定维度内声音强度对比的解释方法，揭示人类感知音色差异的底层机制。  <br/>4. **制定明确时间线与学术组织机制**：为挑战赛设定5月启动、10月在中国镇江的NCMMSC2025会议中发布成果的规划，促进研究落地与交流。|
|2505.08752v2|[Three Tone Networks and a Tessellation](http://arxiv.org/abs/2505.08752v2)|总结：  <br/>该论文提出将Eulerian tonnetz通过二分图与实射影平面几何配置相结合，揭示调性关系的结构特性，并拓展至五声音乐与十二音音乐的调网构建，为理解历史声部进行提供几何框架。<br/><br/>贡献点：  <br/>1. **理论模型构建**：首次将Eulerian tonnetz与实射影平面的十二点十二线配置建立对应，证明Levi图对调性结构的唯一确定性。  <br/>2. **几何特性解析**：通过该配置直观展示调网中的四主六韵（hexacycles）和三主八韵（octacycles）等关键特征，深化对十九世纪声部进行的理解。  <br/>3. **扩展应用**：提出将类似调网方法应用于五声音乐及十二音音乐，拓宽了传统调性理论的适用范围。|
|2505.04113v2|[Advancing Zero-shot Text-to-Speech Intelligibility across Diverse   Domains via Preference Alignment](http://arxiv.org/abs/2505.04113v2)|**贡献点分点总结：**  <br/>1. 提出**INTP数据集**，专门针对零样本TTS的可懂度问题，扩展了DPO框架以适配多类TTS架构。  <br/>2. 通过**偏好对齐技术**，生成超出预训练分布的挑战性语音数据（如绕口令、语码转换），提升模型表现。  <br/>3. 验证**INTP的弱到强泛化能力**，显著改善CosyVoice 2等先进模型的语音可懂度。  <br/>4. 展示**迭代对齐策略**的潜力，进一步优化模型性能。  <br/>5. 提供**公开音频样本**，便于实验复现与评估。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出INTP数据集及扩展DPO框架，通过偏好对齐技术解决零样本TTS在复杂场景下的可懂度问题，并验证模型的泛化能力与迭代优化潜力，显著提升自然度、相似性及音频质量。|
|2505.04082v2|[Aliasing Reduction in Neural Amp Modeling by Smoothing Activations](http://arxiv.org/abs/2505.04082v2)|总结：  <br/>本文提出新型激活函数与Aliasing-to-Signal Ratio (ASR)指标，有效降低神经放大器模型的混叠伪影，同时保持高建模精度。<br/><br/>贡献点：  <br/>1. 提出新型激活函数，针对性解决神经网络在音频建模中因非线性导致的混叠问题；  <br/>2. 引入Aliasing-to-Signal Ratio (ASR)作为定量评估混叠程度的高精度指标；  <br/>3. 对比分析多种预存与现代激活函数（含不同stretch factor），验证其对混叠与误差的影响；  <br/>4. 发现平滑曲线激活函数可显著降低ASR，同时不显著增加传统Error-to-Signal Ratio (ESR)；  <br/>5. 验证了在保持高建模精度的前提下，减少混叠伪影的可行性，推动神经音频模型的实用性发展。|
|2505.03071v4|[The Search for Squawk: Agile Modeling in Bioacoustics](http://arxiv.org/abs/2505.03071v4)|总结:  <br/>该论文提出了一种高效、可扩展的生物声学识别系统，通过预训练嵌入、索引搜索和预计算主动学习技术，显著降低数据需求并提升分类效率，成功应用于珊瑚礁健康、幼鸟声识别和岛屿鸟类监测等新型生态研究场景。<br/><br/>贡献点:  <br/>1. **系统架构创新**：设计了一种通用、可扩展且数据高效的系统，可在1小时内开发针对新生物声学问题的识别器。  <br/>2. **预训练嵌入技术**：采用高度泛化的声学嵌入（预训练于鸟类鸣叫分类），减少对标注数据的依赖。  <br/>3. **索引音频搜索**：通过索引技术高效构建分类器训练数据集，提升数据准备效率。  <br/>4. **预计算主动学习**：利用预计算嵌入加速主动学习循环，实现分类器质量的迭代优化。  <br/>5. **多场景验证**：在珊瑚礁健康分析、幼鸟声识别和岛屿鸟类监测三大生态学案例中验证系统有效性。  <br/>6. **模拟实验方法**：通过结构化模拟实验系统探索设计决策，建立生物声学研究的最佳实践框架。|
|2504.12880v3|[Can Masked Autoencoders Also Listen to Birds?](http://arxiv.org/abs/2504.12880v3)|**贡献点：**  <br/>1. 提出适应细粒度音频领域的定制化自监督学习框架（Bird-MAE），通过调整训练流程（预训练、微调与冻结特征利用）解决通用模型在鸟类声音分类中的性能不足。  <br/>2. 首次在BirdSet数据集上实现多标签分类的SOTA性能，展示参数高效原型探测方法（prototypical probing）对冻结表示的显著提升，优于线性探测37%（MAP）。  <br/>3. 验证原型探测在低资源场景下的有效性，将冻结表示与微调性能差距缩小至3.3%（平均）。  <br/>4. 在自建的BirdSet少样本基准上证明Bird-MAE具有强泛化能力，凸显定制化自监督学习对细粒度音频任务的价值。  <br/><br/>**总结：**  <br/>本研究通过优化训练流程和提出参数高效原型探测方法，显著提升通用MAE模型在细粒度鸟类声音分类中的性能，实现SOTA并验证其少样本鲁棒性。|
|2504.10746v2|[Hearing Anywhere in Any Environment](http://arxiv.org/abs/2504.10746v2)|总结（100字以内）:  <br/>本文提出xRIR框架，结合几何特征提取与RIR编码，构建ACOUSTICROOMS数据集，实现跨房间声学环境重建与真实场景验证，显著提升泛化能力与模拟真实性。<br/><br/>贡献点:  <br/>1. **提出统一模型**：开发xRIR框架，解决现有方法对单一环境的依赖，实现跨房间的声学环境泛化重建。  <br/>2. **融合多模态特征**：结合全景深度图像提取几何信息，与少量参考RIR样本提取声学特征，增强模型跨环境适应能力。  <br/>3. **构建大规模数据集**：创建ACOUSTICROOMS数据集，包含260个房间超30万条高保真RIR模拟，用于评估与验证。  <br/>4. **验证真实场景性能**：通过sim-to-real迁移测试四类真实环境，证明模型实际应用效果与数据集的仿真真实性。  <br/>5. **超越基线方法**：实验结果表明，xRIR在RIR预测任务上显著优于传统神经方法与现有基线。|