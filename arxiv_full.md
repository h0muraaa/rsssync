|Source|Title|Summary|
|---|---|---|
|2506.05140v1|[AudioLens: A Closer Look at Auditory Attribute Perception of Large   Audio-Language Models](http://arxiv.org/abs/2506.05140v1)|**贡献点：**<br/>1. 首次系统分析LALMs的听觉属性感知机制，揭示其内部处理过程。  <br/>2. 通过词汇投影追踪属性信息在模型层与token位置的动态变化规律。  <br/>3. 发现属性信息随层深度增加而衰减，且早层属性识别与模型准确性正相关。  <br/>4. 指出LALMs更依赖听觉输入查询而非在属性提及位置隐状态聚合信息。  <br/>5. 提出基于上述发现的模型增强方法，为性能优化提供新思路。  <br/><br/>**总结（100字以内）：**  <br/>该研究首次深入解析大音频-语言模型对听觉属性的内部处理机制，揭示属性信息在模型中的分布规律及依赖关系，并提出针对性优化方法，为提升模型性能和理解其行为提供理论依据与实践方向。|
|2506.05121v1|[The NTNU System at the S&I Challenge 2025 SLA Open Track](http://arxiv.org/abs/2506.05121v1)|**贡献点**：  <br/>1. 提出融合wav2vec 2.0与Phi-4 MLLM的跨模态评估系统，结合声学特征与语义理解。  <br/>2. 设计得分融合策略，有效弥补BERT依赖ASR转录（缺失语调/发音）和W2V缺乏语义解释的缺陷。  <br/>3. 在Speak & Improve Challenge 2025官方测试集上实现RMSE 0.375，取得第二名，优于基线模型（0.444）和第三名（0.384）。  <br/><br/>**总结**：  <br/>提出集成声学与语义分析的跨模态语音评估系统，通过得分融合策略突破BERT与W2V的局限，在竞赛中获得第二名。|
|2506.05104v1|[Survey on the Evaluation of Generative Models in Music](http://arxiv.org/abs/2506.05104v1)|总结：  <br/>本文提供跨学科综述，系统总结生成音乐系统的评估方法、指标及挑战，涵盖主观/客观、定性/定量、实证/计算等多维度分析，并从音乐学、工程与人机交互视角探讨其优劣势。<br/><br/>贡献点：<br/>1. **系统性综述**：全面梳理生成音乐系统的评估目标、方法与指标，填补领域内评估体系研究的空白。<br/>2. **多维度分类**：整合主观与客观、定性与定量、实证与计算等不同评估范式，明确其适用场景与局限。<br/>3. **跨学科视角**：从音乐学、工程、人机交互（HCI）三方面分析评估方法的优劣，推动理论与实践的融合。<br/>4. **方法论框架**：建立涵盖系统输出与模型可用性的评估框架，为后续研究提供指导和参考。|
|2505.22266v2|[FGAS: Fixed Decoder Network-Based Audio Steganography with Adversarial   Perturbation Generation](http://arxiv.org/abs/2505.22266v2)|总结（100字以内）:  <br/>提出FGAS框架，通过固定解码器与对抗扰动生成增强音频隐写能力，显著提升生成音频质量与抗隐写分析性能，降低对复杂训练和大模型的依赖。<br/><br/>贡献点：<br/>1. **提出FGAS方法**：首次将固定解码器网络与对抗扰动生成结合，通过共享解码器结构和权重实现隐写信息提取，摆脱了对大预训练模型的依赖。<br/>2. **轻量化解码器设计**：构建轻量级固定解码器，兼顾隐藏信息的可靠提取与系统效率，简化了实现复杂度。<br/>3. **对抗扰动生成策略**：开发音频对抗扰动生成（APG）技术，优化扰动以保持生成音频在感知和统计上与原始信号高度相似，增强抗隐写分析能力。<br/>4. **性能提升验证**：实验表明FGAS在PSNR指标上较SOTA方法提升超10 dB，且在不同负载下抵抗隐写分析的分类错误率更高，验证了其有效性。|
|2504.12880v3|[Can Masked Autoencoders Also Listen to Birds?](http://arxiv.org/abs/2504.12880v3)|**贡献点：**  <br/>1. 提出适应细粒度音频领域的定制化自监督学习框架（Bird-MAE），通过调整训练流程（预训练、微调与冻结特征利用）解决通用模型在鸟类声音分类中的性能不足。  <br/>2. 首次在BirdSet数据集上实现多标签分类的SOTA性能，展示参数高效原型探测方法（prototypical probing）对冻结表示的显著提升，优于线性探测37%（MAP）。  <br/>3. 验证原型探测在低资源场景下的有效性，将冻结表示与微调性能差距缩小至3.3%（平均）。  <br/>4. 在自建的BirdSet少样本基准上证明Bird-MAE具有强泛化能力，凸显定制化自监督学习对细粒度音频任务的价值。  <br/><br/>**总结：**  <br/>本研究通过优化训练流程和提出参数高效原型探测方法，显著提升通用MAE模型在细粒度鸟类声音分类中的性能，实现SOTA并验证其少样本鲁棒性。|
|2506.04981v1|[Better Semi-supervised Learning for Multi-domain ASR Through Incremental   Retraining and Data Filtering](http://arxiv.org/abs/2506.04981v1)|总结：  <br/>提出一种基于增量半监督学习的ASR领域适配方法，结合多模型共识与命名实体识别（NER）进行伪标签筛选，在多领域数据集上显著提升模型性能并降低计算成本。<br/><br/>贡献点：  <br/>1. **首个增量半监督学习框架**：通过整合少量领域内标注数据与相关领域辅助数据，实现比无辅助数据的微调方法更高的性能提升（4%）。  <br/>2. **多模型共识与NER联合筛选方法**：提出结合多模型共识与NER的伪标签选择机制，相较于随机筛选，显著延缓性能饱和，提升更稳定。  <br/>3. **多领域实验验证**：在Wow和Fisher两个主流多领域数据集上，验证方法优于传统单步微调策略，且在实际任务中表现优于其他基准方法。  <br/>4. **高效筛选策略比较**：共识过滤在性能提升（22.3%/24.8%）与计算成本之间达到最佳平衡，NER则以更低成本提供竞争力的替代方案。|
|2505.19644v2|[STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set   Source Tracing and Attribution](http://arxiv.org/abs/2505.19644v2)|**总结**：  <br/>提出STOPA数据集，系统化覆盖8个AM、6个VM及多参数设置，提升深伪语音来源追踪的准确性和可靠性，促进检测与模型透明度。<br/><br/>**贡献点**：  <br/>1. **构建系统化数据集**：创建STOPA，包含系统变化和丰富元数据，填补了深伪语音检测领域专用数据集的空白。  <br/>2. **多模型与参数覆盖**：涵盖8个声学模型、6个声码器模型及多样参数配置，支持更全面的来源追踪研究。  <br/>3. **多合成器多样性**：70万样本来自13种不同合成器，增强数据集的代表性与泛化能力。  <br/>4. **提升归属可靠性**：通过系统化控制生成因素，显著提高来源归属的准确性，助力法医学分析与模型透明度研究。|
|2506.04915v1|[A Practitioner's Guide to Building ASR Models for Low-Resource   Languages: A Case Study on Scottish Gaelic](http://arxiv.org/abs/2506.04915v1)|总结：  <br/>本文提出结合HMM与自监督模型的新方法，挑战传统依赖细调的信念，通过持续预训练和半监督训练实现低资源语言ASR的显著性能提升，尤其在苏格兰盖尔语上达到32%的WER降低。<br/><br/>贡献点：  <br/>1. **挑战传统方法信念**：论证在低资源语言ASR中，单纯依赖多语言端到端模型的微调并非最优方案。  <br/>2. **提出混合模型框架**：创新性地融合隐马尔可夫模型（HMM）与自监督模型，结合两者优势提升性能。  <br/>3. **优化数据利用策略**：通过持续自监督预训练与半监督训练，更高效地利用有限的语音和文本数据。  <br/>4. **实验证明有效性**：在苏格兰盖尔语等低资源语言上验证方法效果，实现相较于微调Whisper模型的32%相对WER降低。|
|2506.04890v1|[Multivariate Probabilistic Assessment of Speech Quality](http://arxiv.org/abs/2506.04890v1)|总结（100字以内）:  <br/>该论文提出基于多变量高斯分布的语音质量评估模型，利用Cholesky分解和扩展的概率仿射变换联合建模四个维度，既保持点估计精度又提供不确定性及相关性估计，推动语音质量诊断的精细化。<br/><br/>贡献点：  <br/>1. **引入多变量框架**：将传统单变量MOS估计扩展至多变量联合建模，同时考虑噪音、色彩失真、不连续性和响度四个维度。  <br/>2. **无约束协方差建模**：采用Cholesky分解预测维度间协方差，避免强假设限制，提升模型灵活性。  <br/>3. **扩展概率变换方法**：将概率仿射变换推广至多变量场景，增强对复杂语音质量特征的建模能力。  <br/>4. **提供全面评估信息**：在保持与SOTA方法同等点估计精度的同时，首次实现多维度的不确定性与相关性联合分析。  <br/>5. **推动实际应用**：通过多维诊断能力，辅助精准定位语音质量问题，指导针对性优化策略。|
|2506.00975v2|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v2)|总结：  <br/>本研究提出Next-Token-Pair Prediction（NTPP）框架，首次利用解码器-only架构实现说话人无关的双通道对话学习，并在回合转换预测、响应连贯性和自然性上取得显著提升，同时降低推理延迟，提升实时应用效率。<br/><br/>贡献点：  <br/>1. **提出NTPP新范式**：创新性地设计Next-Token-Pair Prediction方法，首次将双通道语音数据用于解码器-only架构的对话学习。  <br/>2. **说话人无关对话建模**：突破传统方法依赖说话人信息的限制，实现通用双通道对话理解与生成。  <br/>3. **提升对话能力**：在turn-taking预测、响应连贯性与自然性等关键指标上取得显著性能改进。  <br/>4. **优化实时性**：相比现有方法，NTPP大幅降低推理延迟，增强实际部署的可行性。|
|2506.04852v1|[Improving AI-generated music with user-guided training](http://arxiv.org/abs/2506.04852v1)|总结：  <br/>本文提出基于用户反馈的遗传算法优化框架，通过整合用户评分作为损失函数，提升音乐生成模型的个性化表现，实验证明该方法在两次迭代中显著提高用户满意度。<br/><br/>贡献点：  <br/>1. **引入人类计算框架**：首次将用户交互评分与遗传算法结合，实现音乐生成模型的动态自适应优化。  <br/>2. **创新损失函数设计**：将用户主观评分直接作为模型微调的损失函数，打破传统固定数据集训练的局限。  <br/>3. **迭代性能评估方法**：提出通过用户评分平均增长率量化模型改进效果，验证方法有效性（首次迭代+0.2，第二次+0.39）。  <br/>4. **解决音乐个性化难题**：针对音乐高度主观性需求，设计可响应用户偏好的交互式生成机制，弥补图像生成模型的不足。|
|2506.04779v1|[MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark](http://arxiv.org/abs/2506.04779v1)|总结（100字以内）:  <br/>提出MMSU基准，涵盖47项任务及多语言现象，系统评估SpeechLLM模型，揭示性能短板，推动语音理解与人机交互技术发展。<br/><br/>贡献点:  <br/>1. **提出MMSU基准**：首个针对自然语音理解和推理的综合评估基准，包含5,000个音频-问题-答案三元组，覆盖多模态任务。  <br/>2. **多模态语言学理论整合**：系统纳入语音学、韵律、修辞、句法、语义及语用特征，提升基准的理论基础与评估维度。  <br/>3. **模型性能评估与分析**：对14种先进SpeechLLM进行全面测试，揭示现有模型在细粒度感知和复杂推理上的不足。  <br/>4. **明确优化方向**：通过评估结果提出未来研究的关键方向，指导语音理解模型的改进与创新。  <br/>5. **开放资源支持**：提供基准数据集与评估代码，方便研究者复现与扩展，促进领域发展。|
|2506.04714v1|[IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech   translation](http://arxiv.org/abs/2506.04714v1)|总结：  <br/>本研究针对低资源Bhojpuri-Hindi语言对，通过超参数优化与数据增强方法提升SeamlessM4T模型的翻译性能，并分析跨语言信号与翻译错误对BLEU分数的影响。<br/><br/>贡献点：  <br/>1. **系统提交**：参与IWSLT 2025语音翻译共享任务，提交IIITH-BUT系统用于Bhojpuri-Hindi语言对。  <br/>2. **超参数优化**：系统研究学习率调度、更新步数、预热步数、标签平滑和批量大小等超参数对模型性能的影响。  <br/>3. **数据增强技术**：应用速度扰动和SpecAugment方法缓解数据稀缺问题，并验证其对翻译质量的提升效果。  <br/>4. **跨语言联合训练**：通过联合训练Marathi和Bhojpuri语音数据，探索跨语言信号对模型的辅助作用。  <br/>5. **错误分析**：分析翻译假说，识别影响BLEU分数的不同类型错误，为模型改进提供依据。|
|2506.04652v1|[EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label   Speech Emotion Recognition](http://arxiv.org/abs/2506.04652v1)|**贡献点（分点）：**  <br/>1. **填补研究空白**：首次系统性探索多标签语音情感识别（SER）中的去偏方法有效性与鲁棒性，解决性别偏见问题的现有研究不足。  <br/>2. **提出综合框架**：构建EMO-Debias，对13种去偏技术（包括预处理、正则化、对抗学习、有偏学习者、分布鲁棒优化等）进行大规模对比实验。  <br/>3. **实证评估**：使用WavLM和XLSR编码器，在人工与真实情感数据集上测试性别不平衡场景下的方法性能。  <br/>4. **量化权衡分析**：明确揭示公平性与准确性的平衡关系，筛选出同时降低性别性能差距且不牺牲整体模型效果的策略。  <br/>5. **提供实践指导**：总结可操作的去偏方法选择建议，并强调数据分布对结果的显著影响。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出EMO-Debias框架，系统评估13种去偏技术在多标签SER中的性能，揭示性别偏见的权衡关系，为去除性别偏差提供实用策略并强调数据分布的重要性。|
|2506.04518v1|[Towards Efficient Speech-Text Jointly Decoding within One Speech   Language Model](http://arxiv.org/abs/2506.04518v1)|贡献点总结（100字以内）:  <br/>本研究提出改进的ESI解码模式，在保持interleaved方法对齐优势的同时提升推理效率，并构建高质量语音QA数据集，显著提升了语音问答性能。<br/><br/>分点贡献:  <br/>1. **系统性对比研究**：首次在统一模型、tokenizer和训练数据下，系统比较了主流的语音-文本联合解码策略（interleaved与parallel generation）的性能、效率及对齐质量。  <br/>2. **高效解码方案**：提出ESI（Early-Stop Interleaved）方法，在缩短推理时间的同时保持甚至略微提升解码性能，解决长序列长度导致的效率瓶颈。  <br/>3. **高质量数据集构建**：设计并发布专门优化的语音问答（QA）数据集，用于提升语音QA任务的性能，推动相关研究进展。|
|2506.04495v1|[French Listening Tests for the Assessment of Intelligibility, Quality,   and Identity of Body-Conducted Speech Enhancement](http://arxiv.org/abs/2506.04495v1)|**贡献点：**  <br/>1. **系统评估EBEN模型**：首次在体传导传感器（ forehead accelerometer, rigid in-ear, throat mic）上通过主观实验验证EBEN的语音增强效果，涵盖可懂度、语音质量及说话人身份保持三方面。  <br/>2. **揭示性能差异**：发现EBEN对女性说话人喉部麦克风录音的说话人识别能力有小幅下降，表明模型效果与录音方式存在交互影响。  <br/>3. **建立相关性**：证明STOI与感知质量在体传导语音中存在关联，为客观评估提供依据。  <br/>4. **验证ECAPA2-TDNN有效性**：展示ECAPA2-TDNN在说话人验证中的表现与识别任务一致，增强其在真实场景中的可信度。  <br/>5. **提出指标局限性**：指出现有评估指标无法可靠预测EBEN对可懂度的增强效果，为未来研究提供方向。  <br/><br/>**总结（100字内）：**  <br/>该研究通过多模态主观测试评估EBEN在体传导语音中的效果，揭示其对语音质量与可懂度的提升及对女性识别的潜在影响，建立STOI与感知质量的相关性，并验证ECAPA2-TDNN的可靠性，强调现有指标预测能力的不足。|
|2506.04492v1|[Bringing Interpretability to Neural Audio Codecs](http://arxiv.org/abs/2506.04492v1)|总结:  <br/>本文提出两步方法解析语音编码器令牌，通过分析阶段揭示语音属性编码机制，合成阶段构建AnCoGen网络实现属性直接提取，提升模型可解释性。<br/><br/>贡献点:  <br/>1. 提出两阶段框架（分析+合成）系统研究语音信息在神经音频编解码器令牌中的编码机制  <br/>2. 首次量化分析语音属性（内容、身份、音高）在编码器输出中的分布与关联性  <br/>3. 开发AnCoGen网络实现对现有编解码器的后解释，直接从令牌中提取可解释的语音属性  <br/>4. 通过对比声学单位与语义单位的编码特性，揭示其在可解释性方面的差异  <br/>5. 建立可解释性导向的音频编解码器分析范式，为语音处理模型的可调试性研究提供新思路|
|2504.10746v2|[Hearing Anywhere in Any Environment](http://arxiv.org/abs/2504.10746v2)|总结（100字以内）:  <br/>本文提出xRIR框架，结合几何特征提取与RIR编码，构建ACOUSTICROOMS数据集，实现跨房间声学环境重建与真实场景验证，显著提升泛化能力与模拟真实性。<br/><br/>贡献点:  <br/>1. **提出统一模型**：开发xRIR框架，解决现有方法对单一环境的依赖，实现跨房间的声学环境泛化重建。  <br/>2. **融合多模态特征**：结合全景深度图像提取几何信息，与少量参考RIR样本提取声学特征，增强模型跨环境适应能力。  <br/>3. **构建大规模数据集**：创建ACOUSTICROOMS数据集，包含260个房间超30万条高保真RIR模拟，用于评估与验证。  <br/>4. **验证真实场景性能**：通过sim-to-real迁移测试四类真实环境，证明模型实际应用效果与数据集的仿真真实性。  <br/>5. **超越基线方法**：实验结果表明，xRIR在RIR预测任务上显著优于传统神经方法与现有基线。|
|2506.04392v1|[Phi-Omni-ST: A multimodal language model for direct speech-to-speech   translation](http://arxiv.org/abs/2506.04392v1)|**贡献点：**  <br/>1. 提出Phi-Omni-ST模型，首次实现**直接语音到语音翻译**（ST）的多模态语言模型，无需中间文本转换。  <br/>2. 引入**音频Transformer头**与**流式vocoder**的组合架构，通过**音频标记延迟预测**实现高效语音生成与波形合成。  <br/>3. 在CVSS-C数据集上验证模型性能，**显著超越现有基线模型**，并实现端到端高保真翻译。  <br/>4. 通过扩展训练数据与模型规模，**达到当前SOTA性能水平**，证明模型的可扩展性与竞争力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Phi-Omni-ST模型，结合音频Transformer与流式vocoder实现端到端语音到语音翻译，超越基线模型并逼近SOTA性能，为高效高质量的语音翻译提供了新方案。|
|2506.04391v1|[Benchmarking Time-localized Explanations for Audio Classification Models](http://arxiv.org/abs/2506.04391v1)|总结：  <br/>本文提出了一种用于音频分类模型时间定位解释的基准框架，通过目标事件时间注释替代真实标签，系统化优化比较多种后处理解释方法，并验证了解释在发现虚假相关性中的应用价值。<br/><br/>贡献点：  <br/>1. 提出时间局部化音频分类解释的基准方案，利用目标事件时间注释作为替代真实标签的评估标准。  <br/>2. 系统化优化并对比多种模型无关的后处理解释方法，实现接近完美的解释效果。  <br/>3. 展示解释在揭示音频数据中虚假相关性方面的实用价值，增强模型可解释性研究的实际意义。|
|2506.04376v1|[Domain Adaptation Method and Modality Gap Impact in Audio-Text Models   for Prototypical Sound Classification](http://arxiv.org/abs/2506.04376v1)|**贡献点分点总结**：  <br/>1. 提出一种无需模型重训练的背景声音贡献量化方法，有效提升零样本环境声音分类性能。  <br/>2. 首次分析背景声音对模型性能的影响，并揭示其主要由SNR（信噪比）水平决定，而非背景类型。  <br/>3. 开发可跨不同背景和SNR条件的领域自适应技术，增强分类鲁棒性与泛化能力。  <br/>4. 系统研究音频-文本嵌入的模态间隙问题，证明缩小该差距可显著提升分类效果。  <br/>5. 验证方法在主流原型模型中的适用性，展现其可扩展性与对多样化环境的兼容性。  <br/><br/>**总结**（100字以内）：  <br/>本文提出背景声音贡献量化与领域自适应方法，解决零样本环境声音分类中背景干扰问题，优化音频-文本模型性能，并通过模态间隙分析提升分类准确率，验证方法的通用性与鲁棒性。|
|2506.01483v2|[Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction](http://arxiv.org/abs/2506.01483v2)|**贡献点分点总结：**  <br/>1. 提出基于说话人间相对线索（如时间顺序、年龄、音高）的新型目标说话人区分与语音分离方法。  <br/>2. 分类处理连续线索（通过相对差异）与离散线索（保留原始类别），提升特征利用效率。  <br/>3. 相对线索方法相比固定属性分类更具灵活性，支持文本引导数据集的便捷扩展。  <br/>4. 实验验证结合全部相对线索优于随机子集，性别和时间顺序在多语言及混响条件下表现最稳健。  <br/>5. 其他线索（如音高、响度、距离）在复杂场景中显著提升性能。  <br/>6. 通过微调预训练WavLM Base+CNN编码器，显著优于仅使用Conv1d编码器的基线模型。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出基于说话人相对线索的语音分离方法，结合连续与离散特征分类，提升灵活性与可扩展性。实验表明，综合所有相对线索可显著增强性能，尤其在多语言及复杂场景中，同时利用深度学习模型优化进一步提高效果。|
|2506.04364v1|[Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot   Accent Robustness in Low-Resource ASR](http://arxiv.org/abs/2506.04364v1)|总结：  <br/>该论文研究了低资源下ASR训练数据变量对未见口音鲁棒性的影响，发现增加说话人数比增加单人时长更有效，并提出在新语言开发中应优先提升说话人数量。<br/><br/>贡献点：  <br/>1. **系统研究训练数据变量的影响**：首次全面分析说话人数、单人时长和口音多样性三个关键因素对ASR系统鲁棒性（尤其是未见口音）的系统性影响。  <br/>2. **说话人数优先于单人时长**：在固定训练小时数下，增加说话人数（降低单人贡献量）比延长单人时长更能提升ASR对未见口音的适应能力。  <br/>3. **说话人数与时长扩展的协同效应**：发现更多说话人数量可增强ASR性能随训练时长扩展的收益，揭示数据规模与多样性之间的交互作用。  <br/>4. **口音多样性作用有限**：在控制说话人数时，优先选择口音差异大的说话人对性能提升作用较小，挑战了传统数据多样性优化的假设。  <br/>5. **实践指导建议**：提出针对新语言开发的ASR训练策略，建议优先扩大说话人数量而非过度依赖单人数据质量或时长，具有实际应用价值。|
|2506.04214v1|[Sounding that Object: Interactive Object-Aware Image to Audio Generation](http://arxiv.org/abs/2506.04214v1)|**贡献点总结**（100字以内）:  <br/>提出交互式对象感知音频生成模型，整合对象中心学习与扩散模型，通过多模态注意力实现图像-声音关联，理论验证注意力机制逼近分割掩码，实验验证性能优于基线。<br/><br/>**分点贡献**:<br/>1. **模型创新**: 提出"交互式对象感知音频生成"方法，将声音生成与用户选定的视觉对象直接关联，解决复杂场景中多对象、多声源的音频生成难题。<br/>2. **技术整合**: 将对象中心学习融合至条件潜变量扩散模型，引入多模态注意力机制实现图像区域与对应声音的跨模态映射。<br/>3. **交互机制**: 在测试阶段利用图像分割技术，允许用户以对象级别进行交互式音频生成，提升生成的精准度和可控性。<br/>4. **理论支撑**: 理论证明注意力机制在功能上可逼近实际分割掩码，确保生成音频与视觉对象的空间一致性。<br/>5. **实验验证**: 通过定量与定性评估显示，模型在对象-声音对齐任务中优于现有基线方法，验证了方法的有效性。|
|2506.04076v1|[Acoustically Precise Hesitation Tagging Is Essential for End-to-End   Verbatim Transcription Systems](http://arxiv.org/abs/2506.04076v1)|总结（100字以内）:  <br/>该研究提出通过LoRA微调Whisper模型改进verbatim L2语音转录，比较三种标注方案，发现精确填充符标注能显著提升ASR准确率，获得优于传统方法的WER性能。<br/><br/>贡献点分点列出:  <br/>1. 提出无需外部音频数据的LoRA微调方法，优化Whisper模型在verbatim L2语音转录中的表现  <br/>2. 设计并对比三种标注方案（Pure/Rich/Extra），揭示标注细粒度对ASR性能的影响  <br/>3. 首次证明基于Gemini 2.0 Flash推断的acoustically precise填充符标注（Extra）的有效性  <br/>4. 实验结果表明"Extra"方案可使Whisper Large V3 Turbo的WER降低11.3%（从6.2%至5.5%）  <br/>5. 强调显式填充词标注在处理口语化、不流畅语音（如hesitations）中的关键作用|
|2506.04073v1|[A Statistics-Driven Differentiable Approach for Sound Texture Synthesis   and Analysis](http://arxiv.org/abs/2506.04073v1)|总结：  <br/>提出TexStat损失函数及TexEnv合成器，构建DDSP-inspired的TexDSP模型，实现纹理声音的高效生成与评估，并开源代码促进研究应用。<br/><br/>贡献点：  <br/>1. **提出TexStat损失函数**：专门设计用于纹理声音分析与合成，具备感知意义、时间不变性和抗噪鲁棒性，无需依赖时序结构。  <br/>2. **开发TexEnv合成器**：轻量级、可微分的生成方法，通过滤波噪声叠加幅度包络实现纹理音频合成。  <br/>3. **构建TexDSP生成模型**：整合TexStat和TexEnv，作为DDSP-inspired模型，专为纹理声音的生成任务优化。  <br/>4. **引入综合评估方案**：将TexStat与FAD结合，形成更全面的纹理声音合成模型评估指标。  <br/>5. **开源工具与代码**：提供PyTorch实现，确保高效性、可配置性，支持生成任务与感知评估应用。|
|2505.15965v2|[Analyzing the Impact of Accent on English Speech: Acoustic and   Articulatory Perspectives](http://arxiv.org/abs/2505.15965v2)|**贡献点总结：**  <br/>1. 提出非母语口音英语的特征差异（简化协调模式、更高平均音调）  <br/>2. 开发基于eigenspectra和声管变量的高效量化方法（无需音素转录）  <br/>3. 揭示口音对语音可懂度的影响机制，推动研究方向创新  <br/>4. 为构建包容性强、适应多样语言群体的语音处理系统提供理论支持  <br/><br/>**摘要总结（100字以内）：**  <br/>该研究通过发音与声学分析揭示非母语口音英语的特征差异，提出无需音素转录的量化方法，为理解口音对语音可懂度的影响及开发包容性语音系统提供新视角。|
|2505.16044v2|[Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom   Severity Estimation](http://arxiv.org/abs/2505.16044v2)|**贡献点：**  <br/>1. 提出将精神分裂症评估从单一分类任务转向症状严重程度的估计，提升诊断的精细化程度。  <br/>2. 开发多模态融合框架，整合语音、视频和文本数据，增强模型的全面性和临床适用性。  <br/>3. 构建单模态模型与多模态框架的协同机制，显著提高模型的准确性与鲁棒性。  <br/>4. 通过捕捉更详细的症状特征，为个性化治疗提供数据支持。  <br/>5. 提出一种可扩展且客观的语音领域评估工具，具备实际医疗应用潜力。  <br/><br/>**总结（100字以内）**  <br/>本文创新性地采用多模态方法整合语音、视频和文本数据，通过症状严重度估计替代传统二分类任务，开发协同模型提升诊断精度与鲁棒性，为精神分裂症的个性化评估和治疗提供客观、可扩展的解决方案。|
|2506.04037v1|[The mutual exclusivity bias of bilingual visually grounded speech models](http://arxiv.org/abs/2506.04037v1)|总结：  <br/>该研究验证了双语VGS模型中ME偏差的减弱现象，揭示了视觉嵌入方差变化与跨语言混淆的关系，并提出了ME偏差存在的新理论视角。<br/><br/>贡献点：  <br/>1. **验证双语ME偏差现象**：首次发现双语VGS模型（英语+法语/荷兰语）相比单语模型表现出更弱的ME偏差，且存在语言特例。  <br/>2. **解释混淆机制**：通过分析视觉嵌入的方差差异，揭示双语模型对熟悉数据的低方差导致新旧概念混淆增加，为ME偏差提供计算解释。  <br/>3. **探究ME偏差根源**：提出新的理论视角，解释VGS模型中ME偏差的形成原因，深化对语言学习机制的理解。  <br/>4. **多语言实验框架**：构建跨语言（英法荷）的实验范式，拓展了ME策略在多语言环境下的研究边界。|
|2506.04013v1|[Towards Better Disentanglement in Non-Autoregressive Zero-Shot   Expressive Voice Conversion](http://arxiv.org/abs/2506.04013v1)|**贡献点（分点）:**  <br/>1. **框架改进**：基于条件变分自编码器（CVAE）提出自监督非自回归语音转换框架，提升风格迁移效果。  <br/>2. **内容表征优化**：采用多语言离散语音单元降低源音色泄漏，结合增强的相似性损失和混合风格层归一化方法。  <br/>3. **风格嵌入增强**：通过局部F0信息的交叉注意力机制和全局音高/能量特征提取，提升表达性迁移能力。  <br/>4. **实验验证**：模型在情感与说话人相似性任务中显著优于基线，验证了其在风格适应和源风格泄漏抑制上的有效性。  <br/><br/>**总结（100字以内）:**  <br/>本研究通过CVAE框架改进和多层级风格嵌入设计，有效解决了语音转换中的源音色泄漏问题，并增强了情感与表达属性的迁移能力，实验表明其在风格适应性上优于现有方法。|
|2506.03959v1|[From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder   Framework for Auditory Perception and Cochlear Implant Simulation](http://arxiv.org/abs/2506.03959v1)|贡献点总结（100字以内）:  <br/>提出NeuroVoc框架，实现模型无关的声码器，通过模块化设计支持听觉模型替换，验证其在模拟耳蜗植入用户听觉感知中的有效性，展示NH与EH模型的差异，并证明声码语音在噪声中的可懂度与临床数据一致。<br/><br/>分点贡献：  <br/>1. **提出NeuroVoc框架**：首次构建模型无关的声码器系统，基于逆傅里叶变换从神经活动模式重建声波，通用性强。  <br/>2. **模块化架构设计**：允许灵活替换或修改底层听觉模型（如NH与EH），提升框架的适应性与扩展性。  <br/>3. **消除特定实现依赖**：无需为不同语音编码策略定制声码器，简化模拟听觉感知的流程。  <br/>4. **直接对比听觉模型**：通过NH与EH模型的声码语音测试，明确其在谐波结构保留上的差异。  <br/>5. **验证感知可懂度**：采用Digits-in-Noise实验，证明声码语音在噪声环境中的可懂度与临床数据高度吻合。  <br/>6. **量化性能提升**：通过SRT（信号-噪声比）数据，显示NH与EH声码语音分别比标准测试提升2.4 dB和7.1 dB。  <br/>7. **揭示CI用户表现差异**：准确反映耳蜗植入用户在噪声中的听觉障碍特性，为研究提供可靠工具。|
|2506.03917v1|[Sound Field Reconstruction Using Physics-Informed Boundary Integral   Networks](http://arxiv.org/abs/2506.03917v1)|总结：  <br/>本文提出了一种基于边界积分方程的声场重建方法，利用浅层神经网络高效预测声压分布，通过均方误差训练提升精度，并在实验中验证其优于现有物理信息数据驱动技术。<br/><br/>贡献点：  <br/>1. **提出新型模型**：首次引入边界积分网络（Boundary Integral Network）用于声场重建，基于Kirchhoff-Helmholtz边界积分方程建模，将物理规律直接嵌入网络结构。  <br/>2. **浅层网络优化**：采用浅层神经网络替代复杂结构，降低计算成本，同时实现边界与内部声压的高精度预测。  <br/>3. **误差驱动训练**：通过最小化测量麦克风位置的均方误差进行模型训练，提升重建结果与实际数据的匹配度。  <br/>4. **性能验证**：实验表明该方法在声场重建任务中优于现有物理信息神经网络（PINN）等数据驱动技术。|
|2506.03832v1|[Brain-tuned Speech Models Better Reflect Speech Processing Stages in the   Brain](http://arxiv.org/abs/2506.03832v1)|**贡献点：**  <br/>1. 揭示预训练模型与人类语音处理层级结构的差异：中层语义丰富，晚层语义贫乏。  <br/>2. 提出脑微调方法，验证其提升语音模型语义理解的有效性。  <br/>3. 发现脑微调后模型的晚层显著优于预训练模型，与语义语言区域高度对齐。  <br/>4. 通过层析分析证明模型早期层专精声学特征，晚层擅长复杂高级任务。  <br/>5. 证明脑微调模型具有清晰的层级化处理机制，可作为研究人类语音处理的更优模型生物。  <br/><br/>**总结：**  <br/>本研究发现脑微调显著改进语音模型的语义对齐，揭示模型从声学到语义的分层处理特性，使其更贴近人类语音处理机制。|
|2505.04113v2|[Advancing Zero-shot Text-to-Speech Intelligibility across Diverse   Domains via Preference Alignment](http://arxiv.org/abs/2505.04113v2)|**贡献点分点总结：**  <br/>1. 提出**INTP数据集**，专门针对零样本TTS的可懂度问题，扩展了DPO框架以适配多类TTS架构。  <br/>2. 通过**偏好对齐技术**，生成超出预训练分布的挑战性语音数据（如绕口令、语码转换），提升模型表现。  <br/>3. 验证**INTP的弱到强泛化能力**，显著改善CosyVoice 2等先进模型的语音可懂度。  <br/>4. 展示**迭代对齐策略**的潜力，进一步优化模型性能。  <br/>5. 提供**公开音频样本**，便于实验复现与评估。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出INTP数据集及扩展DPO框架，通过偏好对齐技术解决零样本TTS在复杂场景下的可懂度问题，并验证模型的泛化能力与迭代优化潜力，显著提升自然度、相似性及音频质量。|
|2506.06252v1|[Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems](http://arxiv.org/abs/2506.06252v1)|总结（100字以内）:  <br/>本文提出高效的提示偏置技术，结合多任务学习框架，通过提示偏置模型和实体过滤机制提升ASR对罕见实体的识别准确率，实验证明在领域数据集上显著降低实体词错误率，且方法轻量无结构变更。<br/><br/>贡献点分点列出:  <br/>1. **提出统一的多任务学习框架**：将prompt-based biasing与实体识别任务整合，提升ASR对罕见实体的识别效果。  <br/>2. **设计双组件结构**：包含（a）提示偏置模型（判断何时关注实体）和（b）实体过滤机制（高效剔除无关实体）的创新架构。  <br/>3. **显著性能提升**：在自建领域数据集上，针对小/大实体列表分别实现30.7%和18.0%的Entity Word Error Rate下降。  <br/>4. **轻量化与高效性**：无需改变模型结构，保持简单性与轻量性，适用于实际部署。  <br/>5. **应用场景明确**：针对语音识别中罕见和领域特定实体的识别难题，提供针对性解决方案。|
|2506.06190v1|[NAT: Neural Acoustic Transfer for Interactive Scenes in Real Time](http://arxiv.org/abs/2506.06190v1)|总结：  <br/>提出Neural Acoustic Transfer方法，通过隐式神经表示实现动态声学场景的实时声场预测，并结合高效BEM生成训练数据，显著提升交互应用中的声学建模效率与精度。<br/><br/>贡献点：  <br/>1. **提出新型声学传输模型**：Neural Acoustic Transfer通过隐式神经表示编码声学传输及其动态变化，支持实时声场预测。  <br/>2. **高效训练数据生成方法**：开发快速蒙特卡洛边界元法（BEM）近似，适用于光滑Neumann条件场景。  <br/>3. **高精度BEM实现**：推出GPU加速的标准BEM版本，满足高精度声学模拟需求。  <br/>4. **动态环境建模能力**：解决物体位置、材料、尺寸变化导致的声学分布波动问题，实现高效且精准的声辐射空间建模。  <br/>5. **实验验证与应用拓展**：在多样化场景中验证方法的高精度与低延迟（30秒音频在几毫秒内完成），适用于VR、AR及先进音频生产等交互应用。|
|2506.06096v1|[Label-Context-Dependent Internal Language Model Estimation for CTC](http://arxiv.org/abs/2506.06096v1)|贡献点：<br/>1. 首次系统分析CTC的隐式上下文依赖性，揭示其通过现代编码器学习上下文相关内部语言模型（ILM）的能力<br/>2. 提出基于知识蒸馏的新型ILM估计方法，包含理论推导与优化框架<br/>3. 开发两种KD正则化技术，增强模型鲁棒性与泛化能力<br/>4. 建立跨域评估体系，验证上下文相关ILM在Librispeech和TED-LIUM数据集上的有效性<br/>5. 实现超过13%的相对WERR提升，证明标签级KD加平滑方法在ILM估计中的优越性<br/><br/>总结：本文揭示CTC的隐式上下文依赖性，提出基于知识蒸馏的新型ILM估计方法及正则化技术，在跨域评估中实现显著性能提升。|
|2506.06071v1|[CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for   Fair Speech Emotion Recognition](http://arxiv.org/abs/2506.06071v1)|总结：  <br/>提出CO-VADA方法，通过语音转换生成多样化样本，无需修改模型结构或依赖人口统计信息，提升语音情感识别系统的公平性。<br/><br/>贡献点：  <br/>1. **首创新方法**：开发了首个不依赖模型架构修改或人口统计信息的语音情感识别（SER）去偏差框架。  <br/>2. **虚假关联处理**：通过语音转换技术消除说话者特征与情感标签间的错误关联，增强模型对情感相关特征的聚焦。  <br/>3. **样本增强策略**：生成反映非主导说话者模式的样本，引入多样性以改善模型公平性。  <br/>4. **通用性与可扩展性**：兼容多种SER模型和语音转换工具，提供灵活、可扩展的解决方案。|
|2506.00506v2|[Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond](http://arxiv.org/abs/2506.00506v2)|总结：  <br/>本研究提出基于wav2vec 2.0的语音质量评估系统，在VoiceMOS 2024挑战中取得优异成绩，并创新性地采用两阶段微调方法应对数据限制，验证了其在噪声与降噪语音评估中的有效性。<br/><br/>贡献点：  <br/>1. 构建基于wav2vec 2.0的语音质量评估系统，获VoiceMOS 2024挑战Track 3前三名，其中BAK指标预测最优，OVRL次优，SIG第三优。  <br/>2. 提出两阶段细粒度微调策略，解决挑战中对训练数据的严格限制问题，提升模型适应性。  <br/>3. 在VoiceMOS 2024数据集及CHiME 7 - UDASE数据集上验证系统性能，展示其跨数据集泛化能力。  <br/>4. 探索ITU-T P.835标准（SIG、BAK、OVRL）在实际降噪语音质量评估中的应用与优化。|
|2506.05984v1|[Audio-Aware Large Language Models as Judges for Speaking Styles](http://arxiv.org/abs/2506.05984v1)|**贡献点总结：**  <br/>1. 提出利用音频感知大语言模型（ALLMs）作为自动评判者，评估语音语言模型（SLMs）生成演讲的多维度说话风格。  <br/>2. 首次对比GPT-4o-audio与Gemini-2.5-pro在语音风格评估中的表现，验证其与人类评价的可比性。  <br/>3. 通过实验证实ALLMs具备评估SLMs输出内容的可行性，为语音AI的评估方法提供新思路。  <br/>4. 揭示当前SLMs在语音风格控制和自然对话生成方面存在局限性，明确技术改进方向。  <br/><br/>**摘要总结（100字以内）：**  <br/>本研究验证了ALLM作为自动评审者评估SLM演讲风格的可行性，发现Gemini与人类一致性相近，同时指出SLMs在语音控制和对话生成仍有提升空间。|
|2506.05899v1|[WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS   Prediction](http://arxiv.org/abs/2506.05899v1)|**贡献点**：  <br/>1. 提出WhisQ多模态架构，首次在文本到音乐系统中同时解决整体音乐质量（OMQ）和文本提示对齐（TA）的双重评估问题。  <br/>2. 集成Whisper Base（音频编码）与Qwen 3（文本编码），利用序列结构保持细粒度跨模态建模能力。  <br/>3. 设计双路径预测机制：OMQ通过池化音频嵌入预测，TA通过音频-文本双向序列共注意力建模。  <br/>4. 引入Sinkhorn最优传输损失，强化共享嵌入空间中的语义对齐，提升跨模态一致性。  <br/>5. 通过消融实验验证了最优传输正则化对性能的关键作用（+10% SRCC），证明显式跨模态对齐的重要性。  <br/><br/>**总结**（100字以内）：  <br/>本研究提出WhisQ架构，结合音频与文本编码，通过共注意力和最优传输优化实现双重评估，显著提升OMQ和TA预测性能，并验证了跨模态对齐对文本到音乐系统的重要性。|
|2506.05891v1|[WAKE: Watermarking Audio with Key Enrichment](http://arxiv.org/abs/2506.05891v1)|**贡献点总结（100字以内）：**  <br/>提出首个密钥可控音频水印框架WAKE，解决未经授权访问、多次嵌入解码及可变长度嵌入问题，提升音频质量和检测准确性。  <br/><br/>**分点贡献：**  <br/>1. **首次引入密钥控制机制**：WAKE通过特定密钥嵌入与恢复水印，显著增强安全性，防止错误密钥解码。  <br/>2. **解决多嵌入覆盖问题**：支持多次嵌入后水印的稳定解码，避免传统方法中的覆盖干扰。  <br/>3. **支持可变长度水印嵌入**：灵活适应不同长度的水印需求，扩展应用范围。  <br/>4. **性能优越**：在水印音频质量与检测精度上超越现有模型，验证了方法的有效性。|
|2506.05851v1|[DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection](http://arxiv.org/abs/2506.05851v1)|总结：  <br/>本文识别了音频-视频DeepFake检测中的数据集和评估问题，提出DeepSpeak v1数据集与新评估协议，并设计SIMBA方法作为简洁基线，分析音频捷径问题并提出缓解策略，最终优化FakeAVCeleb数据集的评估方案以推动领域发展。<br/><br/>贡献点：  <br/>1. 指出现有音频-视频DeepFake检测数据集存在不可复现性和关键缺陷（如FakeAVCeleb的沉默捷径问题）；  <br/>2. 提出DeepSpeak v1数据集，作为更可靠且全面的基准数据资源；  <br/>3. 首次设计并评估一种通用的多模态DeepFake检测评估协议，使用SOTA模型验证有效性；  <br/>4. 引入SImple Multimodal BAseline（SIMBA）方法，提供简明高效的基线模型以支持设计探索；  <br/>5. 深入分析音频捷径问题，提出针对性的缓解策略；  <br/>6. 改进FakeAVCeleb数据集的评估框架，增强其在实际应用中的可靠性。|
|2505.19493v2|[Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation](http://arxiv.org/abs/2505.19493v2)|**分点贡献：**  <br/>1. **提出两阶段算法**：结合声源方向线索与多通道信号处理，优化传统多通道AEC框架。  <br/>2. **轻量级DNN方向预测**：首次使用轻量模型预测声源方向，降低计算开销。  <br/>3. **多模态信号融合**：将预测方向信息、多通道麦克风信号与单通道远端信号联合输入AEC网络，提升恢复效果。  <br/>4. **环境适应性增强**：算法在不同声学场景下表现优异，验证了其鲁棒性和泛化能力。  <br/><br/>**总结：**  <br/>该研究提出一种基于声源方向线索的两阶段多通道AEC算法，通过轻量级DNN预测方向并融合多模态信号，在复杂环境中实现更高效的回声消除。|
|2506.05802v1|[TADA: Training-free Attribution and Out-of-Domain Detection of Audio   Deepfakes](http://arxiv.org/abs/2506.05802v1)|**贡献点总结：**  <br/>1. 提出训练无关的绿色AI方法（基于kNN）用于音频伪造溯源，无需依赖生成模型训练数据。  <br/>2. 利用预训练自监督学习模型，实现跨数据集的高精度（0.93 F1）生成器样本聚类。  <br/>3. 首次在音频领域展示强出域检测能力（0.84 F1），可识别未见过的伪造模型。  <br/>4. 通过多维分析提供深入的理论与实证见解，增强方法的可靠性与泛化性。  <br/>5. 开源代码与数据协议，推动研究复现和透明性。  <br/><br/>**摘要总结（100字内）：**  <br/>本文提出一种训练无关、绿色AI的音频伪造溯源方法，基于kNN和预训练自监督模型，在多数据集上实现高精度聚类与出域检测，并开源代码促进研究可复现性。|
|2506.05593v1|[Improving Neural Diarization through Speaker Attribute Attractors and   Local Dependency Modeling](http://arxiv.org/abs/2506.05593v1)|**贡献点：**  <br/>1. **提出多阶段中间表示建模**：通过分阶段捕捉详细的“说话人属性”（如情感、语速等），而非直接建模说话人，提升说话人分割与识别的细粒度表征能力。  <br/>2. **引入Conformer架构替代Transformer**：利用卷积增强的Transformer模型，更高效建模局部语音特征，增强对多说话人场景的适应性。  <br/>3. **验证方法有效性**：在CALLHOME数据集上开展实验，证明所提方法在讲话人辨识任务中的性能提升。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过多阶段中间表示建模和Conformer架构改进，提出更精细的说话人属性表征方法，并在CALLHOME数据集验证了其有效性，提升了端到端说话人辨识性能。|