|Source|Title|Summary|
|---|---|---|
|2511.05350v1|[Perceptually Aligning Representations of Music via Noise-Augmented   Autoencoders](http://arxiv.org/abs/2511.05350v1)||
|2511.05349v1|[Passive Acoustic Monitoring of Noisy Coral Reefs](http://arxiv.org/abs/2511.05349v1)||
|2511.05171v1|[Model Merging Improves Zero-Shot Generalization in Bioacoustic   Foundation Models](http://arxiv.org/abs/2511.05171v1)||
|2511.05143v1|[Synthesizing speech with selected perceptual voice qualities - A case   study with creaky voice](http://arxiv.org/abs/2511.05143v1)||
|2511.04623v1|[PromptSep: Generative Audio Separation via Multimodal Prompting](http://arxiv.org/abs/2511.04623v1)|总结：  <br/>提出PromptSep框架，结合条件扩散模型与语音模仿，解决语言查询音频分离局限，实现音频提取与移除的SOTA性能。<br/><br/>贡献点：  <br/>1. 提出PromptSep框架，扩展语言查询音频源分离（LASS）至通用声学分离任务，支持音频提取与声音移除。  <br/>2. 引入条件扩散模型与详细数据模拟，提升模型在复杂音频场景下的分离质量。  <br/>3. 将语音模仿作为新增的直观条件模态，通过Sketch2Sound数据增强策略优化模型表现。  <br/>4. 在多个基准测试中验证PromptSep在声音移除和语音引导源分离的领先性能，同时保持语言查询任务的竞争力。|
|2511.04533v1|[CardioPHON: Quality assessment and self-supervised pretraining for   screening of cardiac function based on phonocardiogram recordings](http://arxiv.org/abs/2511.04533v1)|**贡献点：**  <br/>1. 提出CardioPHON，首个集成心音质量评估与分类的工具，实现对心血管疾病异常心功能的自动化筛查。  <br/>2. 采用自监督预训练框架，融合六组小/中型心音数据，提升模型在低质量数据下的泛化能力。  <br/>3. 多模态模型（结合音频与人口统计特征）在2022 PhysioNet挑战赛中取得官方排行榜最优成绩。  <br/>4. 单模态模型基于纯心音记录，在单模态方法中排名首位（总排名4），超越多模态模型表现。  <br/>5. 首次公开释放预训练模型，推动心血管诊断领域高效、可迁移的人工智能模型开发。  <br/><br/>**总结：**  <br/>本文提出CardioPHON模型，通过自监督预训练与多模态融合，实现心音质量评估与分类，在2022 PhysioNet挑战赛中表现优异，为心血管疾病远程监测提供高效AI解决方案。|
|2511.04376v1|[MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion   Transformers](http://arxiv.org/abs/2511.04376v1)|**贡献点总结（分点）：**  <br/>1. 提出首个具备零样本能力的文本到音乐编辑模型 MusRec。  <br/>2. 支持对真实世界音乐进行多样化编辑任务。  <br/>3. 在音乐内容保留、结构一致性与编辑保真度方面表现优于现有方法。  <br/>4. 利用修正流和扩散Transformer的最新技术实现高效音乐编辑。  <br/><br/>**总结（100字以内）：**  <br/>MusRec 是首个零样本文本到音乐编辑模型，能高效处理真实音乐的多种编辑任务，显著优于现有方法，在内容保留与结构一致性方面表现优异，为可控音乐编辑提供了坚实基础。|
|2511.04139v1|[CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource   Cantonese](http://arxiv.org/abs/2511.04139v1)|**贡献点：**<br/><br/>1. 提出CantoASR框架，结合ASR与LALM进行协作式错误修正，提升低资源粤语ASR性能。<br/>2. 引入强制对齐技术，优化声学特征提取过程。<br/>3. 使用LoRA微调的Whisper模型，增强对粤语声调的识别能力。<br/>4. 采用指令微调的Qwen-Audio模型，实现对韵律特征的意识性修正。<br/>5. 在自发粤语数据上验证，显著优于Whisper-Large-V3，表明声学与LALM结合的可扩展性。  <br/><br/>**总结：**  <br/>CantoASR通过集成ASR与LALM，结合声学特征提取与指令微调模型，有效提升低资源粤语ASR性能，展示其在声调和方言识别中的潜力。|
|2511.03942v1|[MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music   Generation](http://arxiv.org/abs/2511.03942v1)|总结：  <br/>提出MIDI-LLM，通过扩展词汇和两阶段训练实现高质量文本到MIDI生成，并利用vLLM加速推理，优于现有模型且具备实时演示。<br/><br/>贡献点：  <br/>1. **首个文本到多轨道MIDI音乐生成模型**：将文本LLM扩展为支持MIDI令牌，直接生成结构化多音轨音乐。  <br/>2. **两阶段训练优化**：设计分阶段训练策略，提升生成质量与对文本提示的控制能力。  <br/>3. **参数结构保留与推理加速**：保持原始LLM参数结构，集成vLLM库实现高效推理速度。  <br/>4. **性能对比优势**：在生成质量、文本控制和推理效率上超越近期Text2Midi模型。  <br/>5. **实践验证与公开演示**：提供可交互演示，验证模型的实际应用价值与效果。|
|2511.03423v1|[Seeing What You Say: Expressive Image Generation from Speech](http://arxiv.org/abs/2511.03423v1)|总结：  <br/>本研究提出首个端到端语音到图像模型VoxStudio，并发布情感语音图像数据集VoxEmoset，解决了情感一致性和语言歧义等关键挑战。<br/><br/>贡献点：  <br/>1. 提出VoxStudio，首个统一且端到端的语音到图像模型，直接从语音生成表达性图像。  <br/>2. 引入语音信息瓶颈（SIB）模块，将语音压缩为语义标记，保留语调和情感细节。  <br/>3. 发布大规模情感语音-图像数据集VoxEmoset，通过先进TTS生成丰富表达语音。  <br/>4. 在多个基准测试中验证方法可行性，揭示情感一致性与语言歧义等关键挑战。|
|2511.03361v1|[Open Source State-Of-the-Art Solution for Romanian Speech Recognition](http://arxiv.org/abs/2511.03361v1)|**贡献点：**  <br/>1. 首次将NVIDIA FastConformer架构应用于罗马尼亚语ASR系统，提升模型性能。  <br/>2. 构建大规模弱监督语料库（超2600小时语音），优化训练数据效率。  <br/>3. 引入混合解码器（CTC + TDT分支）并系统评估多种解码策略（贪心、ALSD、CTC束搜索结合6-gram语言模型）。  <br/>4. 实现罗马尼亚语ASR领域最先进性能，较此前系统WER降低27%。  <br/>5. 在提升准确性的同时，确保解码效率，适应低延迟应用场景需求。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个基于FastConformer的罗马尼亚语ASR系统，利用大规模弱监督数据与混合解码策略，显著提升识别性能（WER降低27%），并兼顾高效解码，推动低延迟语音应用发展。|
|2511.03337v1|[audio2chart: End to End Audio Transcription into playable Guitar Hero   charts](http://arxiv.org/abs/2511.03337v1)|总结：  <br/>提出audio2chart框架，实现从音频自动生成Guitar Hero风格图表，通过序列预测模型结合音频条件提升准确率，开源代码与预训练模型支持研究复现与应用。<br/><br/>贡献点：  <br/>1. **提出audio2chart框架**：首次将音频转换为Guitar Hero风格的离散乐谱标记，通过序列预测模型实现端到端的自动图表生成。  <br/>2. **验证音频条件有效性**：证明引入音频条件能显著提升音符预测准确率，展示其在神经乐谱生成中的可行性与效果。  <br/>3. **开源实现与预训练模型**：提供完整的训练/推理代码库（GitHub）及预训练模型（Hugging Face），支持研究复现和实际应用。|
|2511.03310v1|[TASU: Text-Only Alignment for Speech Understanding](http://arxiv.org/abs/2511.03310v1)|**贡献点分点总结：**  <br/>1. 提出TASU方法，无需音频-文本配对数据，实现跨模态对齐；  <br/>2. 首次验证TASU在零样本语音识别任务中的有效性；  <br/>3. 将TASU作为课程学习的预训练阶段，提升模型泛化能力；  <br/>4. 扩展TASU的零样本泛化能力至多种语音理解任务；  <br/>5. 在MMSU基准上超越GLM-4-Voice等主流Speech LLMs，展现高效性和可扩展性。  <br/><br/>**总结（100字以内）：**  <br/>提出TASU，通过文本数据实现无需音频-文本配对的跨模态对齐，显著提升零样本语音识别及多任务泛化能力，在MMSU基准上优于GLM-4-Voice等模型，为Speech LLMs提供高效、可扩展的对齐范式。|
|2511.03244v1|[Why Not Put a Microphone Near the Loudspeaker? A New Paradigm for   Acoustic Echo Cancellation](http://arxiv.org/abs/2511.03244v1)|**贡献点：**<br/><br/>1. 提出双麦克风配置，利用辅助参考麦克风捕捉非线性失真信号。  <br/>2. 引入基于维纳滤波的预处理模块，估计时频掩码以抑制近端语音。  <br/>3. 将净化后的参考信号用于线性AEC，再通过深度神经网络进行残余回声和噪声抑制。  <br/>4. 在匹配和不匹配数据集上均表现出优于基线方法的性能，验证了方法的鲁棒性和实用性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种双麦克风AEC方案，结合维纳滤波和深度学习，有效抑制非线性回声和噪声，在实际场景中表现优异。|
|2511.03089v1|[A Computational Approach to Analyzing Disrupted Language in   Schizophrenia: Integrating Surprisal and Coherence Measures](http://arxiv.org/abs/2511.03089v1)|**贡献点：**<br/><br/>1. 探讨了精神分裂症患者与健康对照组在语言生成中的** surprisal（惊讶度）**和** semantic coherence（语义连贯性）**差异。  <br/>2. 利用计算模型分析了语言中的**认知异常表现**，为精神分裂症症状的客观评估提供新视角。  <br/>3. 揭示了语言障碍与**症状严重程度变化**之间的关系，增强了对精神分裂症的诊断和预后理解。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过计算模型分析精神分裂症患者的语言惊讶度和语义连贯性，揭示其与症状严重程度的关系，为精神分裂症的客观评估和诊断提供新依据。|
|2511.03086v1|[Speech-Based Prioritization for Schizophrenia Intervention](http://arxiv.org/abs/2511.03086v1)|**贡献点总结：**<br/><br/>1. 提出一种基于语音的模型，用于 schizophrenia 症状严重程度的成对比较。  <br/>2. 利用发声学和发音特征进行分析。  <br/>3. 通过 Bradley-Terry 模型生成严重程度排名。  <br/>4. 在排名指标上优于传统的回归模型。  <br/>5. 为临床分诊和优先处理提供更有效的解决方案。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出一种基于语音的模型，利用发声学和发音特征对精神分裂症症状严重程度进行成对比较，并结合 Bradley-Terry 模型生成排名，提升临床分诊效率。|
|2511.03084v1|[Quantifying Articulatory Coordination as a Biomarker for Schizophrenia](http://arxiv.org/abs/2511.03084v1)|**贡献点：**  <br/>1. 提出首个基于发音特征的可解释框架，通过特征图谱差分图（eigenspectra difference plots）量化语音器官协调性。  <br/>2. 引入加权指数衰减（WSED）方法，有效区分精神分裂症患者的复杂与简单协调模式，且分离结果具有统计显著性。  <br/>3. 揭示WSED得分与临床量表（BPRS）总体严重程度及正负症状平衡的关联性，体现症状模式对协调性的动态影响。  <br/>4. 为精神分裂症提供透明、与症状严重程度敏感相关的生物标志物，推动语音分析在临床诊断中的应用。  <br/><br/>**总结（100字内）：**  <br/>本文开发了一种可解释的语音分析方法，结合特征图谱差分与加权指数衰减，量化精神分裂症患者语音器官协调性，揭示症状严重度与正负症状平衡的关联，为临床提供新的生物标志物及评估工具。|
|2511.02726v1|[Perceived Femininity in Singing Voice: Analysis and Prediction](http://arxiv.org/abs/2511.02726v1)|**贡献点：**<br/><br/>1. 首次系统研究歌唱语音中的感知女性化特征（PSVF），填补了语音领域研究空白。  <br/>2. 设计基于刺激的调查问卷，收集128名参与者数据，分析不同群体间PSVF的差异。  <br/>3. 提出基于x-vector模型的自动PSVF预测工具，为音乐内容分析提供新方法。  <br/>4. 推动对音乐中性别刻板印象的深入理解，超越二元性别分类框架。  <br/><br/>**总结（100字以内）：**  <br/>本研究首次探讨歌唱语音中的感知女性化特征，提出调查方法与自动预测模型，分析性别差异，推动音乐内容分析中性别刻板印象的研究。|
|2511.02717v1|[An unscented Kalman filter method for real time input-parameter-state   estimation](http://arxiv.org/abs/2511.02717v1)|总结：  <br/>本文提出一种新型无迹卡尔曼滤波器，分两阶段估计未知输入与系统参数，并通过扰动分析证明其唯一识别性，实现输出-only实时联合估计，优于传统方法。<br/><br/>贡献点：  <br/>1. **提出新型UKF方法**：设计了一种用于输入参数状态估计的无迹卡尔曼滤波器，首次实现动态状态、系统参数及未知输入的联合估计。  <br/>2. **分两阶段估计机制**：在每个时间步中，通过预测阶段和校正阶段分别利用动态状态和测量数据对输入进行两次估计，提升精度。  <br/>3. **理论可识别性证明**：基于扰动分析，证明系统在存在零或非零已知输入条件下可被唯一识别，为方法有效性提供理论依据。  <br/>4. **输出-only实时性优势**：相较传统输出-only参数识别方法，首次实现所有关键变量（状态、参数、输入）的实时联合估计，增强系统辨识能力。|
|2511.02379v1|[H-Infinity Filter Enhanced CNN-LSTM for Arrhythmia Detection from Heart   Sound Recordings](http://arxiv.org/abs/2511.02379v1)|**贡献点：**  <br/>1. 提出新型CNN-H-Infinity-LSTM架构，融合控制理论的H-Infinity滤波器机制，提升心律失常检测的鲁棒性与泛化能力。  <br/>2. 通过引入可训练参数优化模型结构，解决传统深度学习模型在小样本和噪声数据下的性能瓶颈。  <br/>3. 在PhysioNet CinC Challenge 2016数据集上验证模型效果，实现测试准确率99.42%和F1分数98.85%，优于现有方法。  <br/><br/>**总结（100字以内）：**  <br/>本文提出结合H-Infinity滤波器的CNN-H-Infinity-LSTM架构，通过优化参数设计提升心律失常检测的鲁棒性与泛化能力，在基准数据集上取得99.42%准确率的优异结果。|
|2511.02278v1|[Multiplexing Neural Audio Watermarks](http://arxiv.org/abs/2511.02278v1)|**贡献点总结（分点）：**  <br/>1. **提出五种神经音频水印复用策略**（并行、顺序、频分、时分、PA-TFM），系统分析其在对抗不同攻击类型（如压缩、神经重建）中的互补性。  <br/>2. **开发训练无关的PA-TFM方法**，通过动态调整时频结构提升水印的抗攻击鲁棒性，无需额外训练。  <br/>3. **引入两种基于最新语音处理技术的新型神经重建攻击**，扩展攻击测试的深度和广度，增强实验的挑战性。  <br/>4. **在LibriSpeech数据集上进行全面评估**，覆盖11种攻击方法，验证复用技术的实际有效性。  <br/>5. **实验证明复用方法（尤其是PA-TFM）显著优于单水印基线**，为语音水印的抗攻击性提供更具突破性的解决方案。  <br/><br/>**总结（100字以内）：**  <br/>本文提出五种神经音频水印复用方法，重点开发训练无关的PA-TFM策略，引入新攻击测试，并在大规模数据集上验证其有效性，显著提升了语音水印的抗攻击能力。|
|2511.02270v1|[Augmenting Open-Vocabulary Dysarthric Speech Assessment with Human   Perceptual Supervision](http://arxiv.org/abs/2511.02270v1)|**贡献点：**<br/><br/>1. 提出利用语音合成评估中的人类感知标注作为跨领域知识，用于失语症语音评估。  <br/>2. 验证了此类标注在自监督学习预训练模型中的有效性，显著提升模型性能。  <br/>3. 展示了感知评分在不同领域间知识迁移的潜力，为失语症建模提供了新思路。  <br/><br/>**总结（100字以内）：**  <br/>该研究利用语音合成评估的人类感知标注作为失语症语音评估的跨域知识，提升了自监督学习模型的性能，验证了感知评分在跨领域迁移中的价值。|
|2511.02252v1|[From the perspective of perceptual speech quality: The robustness of   frequency bands to noise](http://arxiv.org/abs/2511.02252v1)|**贡献点：**<br/><br/>1. 提出基于MUSHRA的新型方法，用于研究频率带对噪声的鲁棒性，以感知语音质量为衡量标准。  <br/>2. 将语音信号分割为32个频率带，并在不同信噪比下引入现实噪声进行测试。  <br/>3. 通过人类评分计算各频率带的噪声鲁棒性指数，揭示中频区域对噪声最不鲁棒。  <br/>4. 为提升语音质量的研究提供了新的方向，强调应关注语音信号的中频部分。  <br/><br/>**总结（100字以内）：**  <br/>本文提出一种基于MUSHRA的方法，分析各频率带对噪声的鲁棒性，发现中频区域感知语音质量较差，为改善语音质量提供了新的研究方向。|
|2511.02104v1|[Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech:   A Linguistically Motivated Approach](http://arxiv.org/abs/2511.02104v1)|总结（100字以内）:  <br/>论文提出一种基于语言学的半自动TTS声调评估框架，结合离散与连续声调度量，提供客观可解释的指标，揭示传统方法难以检测的模型弱点，助力提升下一代TTS系统声调自然性。<br/><br/>贡献点：  <br/>1. **提出语言学驱动的两层架构框架**：首次构建模仿人类声调组织结构的半自动评估体系，为TTS声调分析提供新范式。  <br/>2. **融合离散与连续声调度量**：同时量化评估事件定位（如重音、停顿）和提示实现（如语调变化），弥补传统单一维度评价的不足。  <br/>3. **基于定量语言标准的多维对比**：通过声学维度对比人类与合成语音，明确可解释的客观评价指标，提升对声调自然性的解析能力。  <br/>4. **揭示模型特定弱点**：在传统感知测试无法发现的领域（如跨说话者/提示变异性），识别TTS系统声调层面的缺陷。  <br/>5. **推动声调自然性优化路径**：为TTS系统的诊断、基准测试及改进提供理论依据，助力下一代语音技术发展。|
|2511.01773v1|[ADNAC: Audio Denoiser using Neural Audio Codec](http://arxiv.org/abs/2511.01773v1)|**贡献点：**  <br/>1. 提出将先进神经音频编码器DAC用于音乐降噪的可行性方案。  <br/>2. 构建大规模定制合成数据集，克服传统架构的局限。  <br/>3. 引入多目标损失函数，结合时域、频域和信号级保真度。  <br/>4. 实现高保真、生成式音频修复的初步验证。  <br/><br/>**总结（100字以内）:**  <br/>本文提出利用DAC音频编码器进行音乐降噪的PoC，通过自建数据集和多目标损失函数提升性能，实现高保真音频修复。|
|2511.01663v1|[The Ghost in the Keys: A Disklavier Demo for Human-AI Musical   Co-Creativity](http://arxiv.org/abs/2511.01663v1)|**贡献点**：  <br/>1. **提出具身交互新范式**：设计Aria-Duet系统，将生成模型与音乐家通过物理钢琴实现实时协作，打破传统文本提示的异步流程，还原乐器演奏的即兴性与响应性。  <br/>2. **构建低延迟技术架构**：开发支持手部动作信号触发和声学输出的框架，实现人机音乐创作的无缝衔接与流畅交互。  <br/>3. **音乐学视角的实证分析**：验证生成模型在保持风格语义（stylistic semantics）和开发音乐短语（phrasal ideas）上的能力，证明AI可参与高级音乐对话。  <br/>4. **推动人机共创实践**：展示具身系统在音乐领域应用的潜力，为人类与AI的协同创作提供新路径。  <br/><br/>**总结（100字以内）**：  <br/>该研究提出Aria-Duet系统，通过物理钢琴实现人机实时音乐协作，克服文本提示的局限性，验证生成模型在风格保持和音乐创作中的能力，并为人类-AI共创提供新范式。|
|2511.01652v1|[Leveraging Language Information for Target Language Extraction](http://arxiv.org/abs/2511.01652v1)|**贡献点总结：**  <br/>1. 提出首个用于目标语言提取的多语言公开数据集。  <br/>2. 构建了首个基于语音预训练模型语言知识的端到端框架。  <br/>3. 在英语和德语混合语音中，分别提高了SI-SNR 1.22 dB和1.12 dB。|
|2511.01372v1|[AudioNet: Supervised Deep Hashing for Retrieval of Similar Audio Events](http://arxiv.org/abs/2511.01372v1)|**贡献点：**  <br/>1. 提出监督深度哈希方法AudioNet，实现高效音频事件检索与二进制哈希编码生成。  <br/>2. 设计新型损失函数（加权对比+加权成对损失+哈希码平衡），提升检索效率。  <br/>3. 引入离散梯度传播技术，首次将梯度优化应用于离散哈希码训练。  <br/>4. 在不平衡数据集上验证方法鲁棒性，展现优异检索性能。  <br/>5. 通过系统实验与分析，建立该领域深度音频嵌入检索的基线，推动后续研究。  <br/><br/>**总结（100字以内）:**  <br/>本研究提出AudioNet，通过改进损失函数与离散梯度传播，实现高效音频事件检索，突破哈希码离散优化难题，并在不平衡数据集上验证性能，为深度音频嵌入检索建立新基线。|
|2510.18938v2|[StutterZero and StutterFormer: End-to-End Speech Conversion for   Stuttering Transcription and Correction](http://arxiv.org/abs/2510.18938v2)|**贡献点总结：**<br/><br/>1. 提出首个端到端的波形到波形模型StutterZero和StutterFormer，用于直接将结巴语音转换为流畅语音。  <br/>2. 两种模型分别采用卷积-BiLSTM与双流Transformer架构，联合预测语音和文本。  <br/>3. 在合成数据上训练，在FluencyBank上评估，显著提升WER和BERTScore指标。  <br/>4. 验证了端到端结巴语音转换的可行性，为无障碍AI与语音治疗提供了新方法。|
|2510.15865v2|[Sound Clouds: Exploring ambient intelligence in public spaces to elicit   deep human experience of awe, wonder, and beauty](http://arxiv.org/abs/2510.15865v2)|**贡献点：**  <br/>1. 提出设计和实现引发人类情感共鸣（如敬畏、惊奇、美感）的沉浸式人工智能艺术体验构想。  <br/>2. 开发了名为 Sound Clouds 的互动艺术装置，基于参与者与人高度球体的交互生成实时音乐。  <br/>3. 作为对未来环境智能（AmI）的启发性探索，强调其激发可能性而非限制功能的潜力。|
|2509.20799v2|[AuthGlass: Enhancing Voice Authentication on Smart Glasses via Air-Bone   Acoustic Features](http://arxiv.org/abs/2509.20799v2)|**贡献点：**<br/><br/>1. 提出AuthGlass，结合空气传导与骨传导语音特征提升语音认证的准确性和活体检测能力。  <br/>2. 构建了包含14个空气传导麦克风和2个骨传导单元的智能眼镜原型，实现多模态特征采集。  <br/>3. 通过42名参与者实验验证，融合声场与振动特征显著增强认证的鲁棒性和抗攻击能力。  <br/>4. 实验表明AuthGlass在多种实际场景下仍保持较高准确率，具备良好的适用性和可扩展性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出AuthGlass，结合空气与骨传导语音特征提升智能眼镜语音认证的安全性与鲁棒性，验证了其在真实场景中的有效性，为实际部署提供了可靠方案。|
|2509.20410v4|[Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex   Speech Interaction](http://arxiv.org/abs/2509.20410v4)|**贡献点：**<br/><br/>1. 提出Phoenix-VAD，基于LLM实现流式语义端点检测，支持全双工交互。  <br/>2. 利用LLM的语义理解能力和滑动窗口训练策略，提升检测可靠性。  <br/>3. 在语义完整和不完整的语音场景中均表现出色，性能优异且具有竞争力。  <br/>4. 实现语义端点检测模块与对话模型的独立优化，增强系统的灵活性和可靠性。  <br/><br/>**总结（100字以内）：**  <br/>Phoenix-VAD是首个基于LLM的流式语义端点检测模型，支持全双工交互，通过滑动窗口训练策略提升检测性能，并实现模块独立优化，为下一代人机交互提供更可靠和灵活的语音处理方案。|
|2509.19999v2|[MultiSoundGen: Video-to-Audio Generation for Multi-Event Scenarios via   SlowFast Contrastive Audio-Visual Pretraining and Direct Preference   Optimization](http://arxiv.org/abs/2509.19999v2)|**总结（100字以内）**：  <br/>提出MultiSoundGen框架，通过SF-CAVP和AVP-RPO方法解决多事件场景下语义-动态特征对齐及偏好优化难题，实现SOTA性能。<br/><br/>**贡献点**：  <br/>1. **SF-CAVP模型设计**：提出SlowFast Contrastive AVP，采用统一双流架构，显式对齐音频-视觉数据的核心语义表示与快速动态特征，解决多事件场景的复杂语义及动态特征对齐问题。  <br/>2. **AVP-RPO方法创新**：首次将直接偏好优化（DPO）引入V2A任务，基于SF-CAVP构建奖励模型，量化并优先语义-时序匹配，同时提升音频质量。  <br/>3. **综合性能验证**：在多事件场景下，通过实验验证MultiSoundGen在分布匹配、音频质量、语义对齐和时序同步方面均达到SOTA水平。（注：原文强调贡献为两项技术突破，故此总结将实验结果视为第三点补充）|
|2509.14912v2|[Back to Ear: Perceptually Driven High Fidelity Music Reconstruction](http://arxiv.org/abs/2509.14912v2)|**总结（100字以内）:**  <br/>本文提出εar-VAE，通过K-weighting感知滤波器、双相位损失设计及频谱监督优化，显著提升音频重建性能，尤其在高频和谐波与空间特性方面表现突出。<br/><br/>**贡献点:**  <br/>1. **感知对齐**：引入K-weighting感知滤波器，在损失计算前对齐VAE目标与听觉感知，提升相位准确性。  <br/>2. **相位优化**：设计两种新型相位损失——相关性损失（增强立体声一致性）和基于瞬时频率与群延迟的相位损失（提升相位精度）。  <br/>3. **频谱监督**：提出分层次频谱监督方法，幅度全由Mid/Side/Left/Right四通道监督，相位仅通过LR通道监督，强化空间特征重建。|