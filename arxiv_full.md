|Source|Title|Summary|
|---|---|---|
|2511.11124v1|[AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124v1)|总结：  <br/>提出AV-Dialog框架，结合音频与视觉线索实现鲁棒对话，解决嘈杂环境下的说话者追踪与轮次预测问题，提升对话质量。<br/><br/>贡献点：<br/>1. **首个多模态对话框架**：首次融合音频与视觉信息，实现说话者感知的对话交互，提升环境鲁棒性。<br/>2. **核心技术创新**：结合声学标记化与多任务多阶段训练策略，突破单模态模型局限。<br/>3. **多场景数据训练**：基于单模态、合成与真实音频-视觉数据集进行联合训练，增强模型泛化能力。<br/>4. **性能提升**：在干扰环境下显著降低转录错误，提升轮次预测精度与人类评估的对话质量。<br/>5. **应用价值**：为真实世界噪声场景下的对话代理提供新范式，推动语音交互向自然流畅发展。|
|2511.11104v1|[CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation](https://arxiv.org/abs/2511.11104v1)|**总结（100字以内）**：  <br/>CLARITY提出双信号优化框架，解决TTS中口音与语言偏见，通过本地化文本和检索增强口音提示，提升多方言准确性和公平性，保持语音质量。  <br/><br/>**贡献点**：  <br/>1. **问题识别**：首次系统揭示TTS中的口音偏见（默认主导口音）与语言偏见（忽略方言文化特征）的耦合关系。  <br/>2. **方法创新**：提出CLARITY框架（背骨无关），通过双信号优化策略解决偏见：  <br/>   - **上下文语言适应**：将输入文本本地化至目标方言。  <br/>   - **检索增强口音提示（RAAP）**：生成口音一致的语音提示。  <br/>3. **效果验证**：在12种英语口音上验证，显著提升口音准确性、公平性，同时保持高质量感知语音。|
|2511.10697v1|[Graph Neural Field with Spatial-Correlation Augmentation for HRTF Personalization](https://arxiv.org/abs/2511.10697v1)|总结：提出基于图神经网络的空间相关性增强HRTF个性化框架，实现高效、高质量的未见主体HRTF生成。<br/><br/>贡献点：<br/>1. **提出GraphNF-SCA框架**：首次将图神经网络与空间相关性建模结合，用于生成个性化HRTF，解决传统HRTF测量耗时且依赖个体数据的问题。<br/>2. **三模块协同设计**：包含HRTF-P（编解码结构提取通用与目标特征）、HRTF-U（建模HRTF间空间相关性）和微调阶段，形成完整个性化生成流程。<br/>3. **空间一致性优化**：通过空间相关性建模提升预测HRTF的空间一致性，显著优于传统逐位置估计方法。<br/>4. **跨位置特征融合**：利用GNN的图结构隐式捕捉HRTF的跨位置依赖关系，无需显式标注位置信息即可实现高质量渲染。<br/>5. **SOTA性能验证**：实验结果在多个数据集上达到当前最优水平，证明了框架的有效性与实用性。|
|2511.10482v1|[Proceedings of The third international workshop on eXplainable AI for the Arts (XAIxArts)](https://arxiv.org/abs/2511.10482v1)|**总结（100字以内）：**  <br/>第三次XAIxArts工作坊推动跨学科合作，探讨可解释AI在艺术领域的应用，促进AI、数字艺术与人机交互领域的知识交流，为艺术创作与技术融合提供新思路。<br/><br/>---<br/><br/>**贡献点分点：**  <br/>1. **跨学科社区构建**：汇聚HCI、AI、数字艺术等领域的研究人员，促进多学科对话与合作。  <br/>2. **研究方向聚焦**：明确XAI在艺术领域的角色，推动可解释性技术与艺术创作、体验设计的结合。  <br/>3. **学术平台搭建**：作为ACM C&C 2025子活动，提供学术交流与实践探索的平台，强化领域影响力。|
|2511.09802v1|[Investigation of Feature Selection and Pooling Methods for Environmental Sound Classification](https://arxiv.org/abs/2511.09802v1)|总结：  <br/>提出SSRP及其变体（SSRP-B、SSRP-T）用于环境声音分类，验证其在轻量CNN中的高效性与高准确性，特别是在资源受限场景下优于PCA和传统CNN方法。<br/><br/>贡献点：  <br/>1. **提出新型稀疏池化方法**：设计Sparse Salient Region Pooling（SSRP）及其变体SSRP-Basic和SSRP-Top-K，探索其在环境声音分类中的应用。  <br/>2. **对比实验验证有效性**：通过在ESC-50数据集上的实验，证明SSRP-T在准确率（80.69%）上显著优于基线CNN（66.75%）和PCA模型（37.60%）。  <br/>3. **参数优化与适应性分析**：评估不同超参数设置下的SSRP性能，展示其在调参后对计算成本与精度的平衡能力。  <br/>4. **资源约束场景的解决方案**：强调SSRP在轻量级模型中的优势，为低功耗或边缘设备的环境声音分类任务提供高效且高性能的方法。|
|2511.09682v1|[Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models](https://arxiv.org/abs/2511.09682v1)|总结：  <br/>本研究首次探讨音频推理模型（ARMs）的抗 jailbreak 攻击安全性，提出 Rebellion 方法通过抵消表征漂移提升鲁棒性，在保障良性任务性能的同时显著优化准确率与安全性的平衡。<br/><br/>贡献点：  <br/>1. **首次系统分析 ARM 的安全性**：揭示标准推理训练（RT）在对抗常规音频 jailbreak 攻击时有效，但无法抵御新颖的高级攻击，因表征漂移导致模型生成有害响应。  <br/>2. **提出 Rebellion 方法**：构建一种鲁棒 RT 框架，专门训练模型抵御最坏情况下的表征漂移，增强其对抗 jailbreak 攻击的能力。  <br/>3. **验证效果与性能平衡**：在 Qwen2-Audio 上证明 Rebellion 能有效防御高级攻击，同时不降低对良性任务的性能表现。  <br/>4. **优化准确率-安全权衡**：相比标准 RT，Rebellion 显著改善准确率与安全性的平衡对比，提升模型的实用性。|
|2511.09585v2|[Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation](https://arxiv.org/abs/2511.09585v2)|总结：本文提出VeM模型，通过分层视频解析和跨模态注意力机制解决视频音乐生成中的语义-时间对齐和节奏同步问题，创新性构建严格同步数据集并设计专用评估指标，显著提升生成音轨的高质量与同步性能。<br/><br/>贡献点：<br/>1. 提出Video Echoed in Music (VeM)框架，首次将多模态信息（视频+音乐）整合到潜在扩散模型中，实现语义、时间与节奏三重精准对齐。<br/>2. 设计分层视频解析机制作为"音乐指挥"，通过多级视频信息协调生成具有丰富语义的背景音乐。<br/>3. 开发语义引导的跨模态注意力机制(SG-CAtt)，结合模态编码器与位置/持续时间编码，确保时间连贯性。<br/>4. 引入帧级转换节拍对齐器与适配器(TB-As)，动态实现视觉场景转换与音乐节拍的精确同步。<br/>5. 构建首个严格要求过渡节拍同步的视频-音乐配对数据集，来自电商广告和视频平台，推动任务标准化。<br/>6. 提出针对性评估指标，量化衡量视频音乐生成的语义相关性与节奏精准度。|
|2511.09585v1|[Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation](https://arxiv.org/abs/2511.09585v1)|总结：  <br/>该论文提出VeM方法，通过分层视频解析、跨模态注意力机制和帧级节拍对齐技术，解决视频-音乐生成中的语义对齐与节奏同步问题，并构建了首个符合电商广告严格节拍要求的视频-音乐配对数据集。<br/><br/>贡献点：  <br/>1. **提出VeM模型**：首次构建基于潜变量扩散的视频-音乐生成框架，实现语义、时间和节奏的多维度对齐。  <br/>2. **分层视频解析**：设计多级视频解析模块，作为音乐“指挥”协调跨模态多级信息融合。  <br/>3. **SG-CAtt机制**：引入故事板引导的跨模态注意力机制，结合位置与持续时间编码确保语义与时间一致性。  <br/>4. **TB-As对齐器**：开发帧级过渡节拍对齐器与适配器，动态实现视觉场景转换与音乐节拍的精准同步。  <br/>5. **新数据集与评估指标**：发布首个针对电商广告和视频平台的视频-音乐配对数据集，并设计专用评估指标验证任务性能。|
|2511.09525v1|[Spatial Audio Rendering for Real-Time Speech Translation in Virtual Meetings](https://arxiv.org/abs/2511.09525v1)|**贡献点：**  <br/>1. **提出空间音频渲染在实时翻译中的作用**：首次系统研究空间音频渲染对多语言会议中语言理解的影响，揭示其对跨语言沟通的关键作用。  <br/>2. **语言多样性选择**：选取语法、书写系统及资源可得性差异显著的希腊语、卡纳达语、普通话、乌克兰语，增强研究普适性。  <br/>3. **多维度实验设计**：通过within-subjects实验，对比四种音频条件（空间+混响、空间无混响、双耳、单耳），全面评估感知线索对理解、认知负荷及用户体验的影响。  <br/>4. **量化与定性结果**：实验证明空间音频使理解准确率提升100%，同时通过主观反馈验证其提升清晰度与参与感的效果。  <br/>5. **设计建议创新**：为会议平台的实时翻译集成提供可操作的用户界面优化方案，推动包容性跨语言通信在远程协作中的应用。  <br/><br/>**总结（100字内）**：  <br/>该研究探索空间音频渲染对多语言会议理解与体验的影响，发现其显著提升准确率及参与度，提出优化远程协作平台翻译集成的设计建议，推动跨语言沟通的技术改进。|
|2511.09037v1|[Sound impact of simple viscoelastic damping changes due to aging and the role of the double bentside on soundboard tension in a 1755 Dulcken harpsichord](https://arxiv.org/abs/2511.09037v1)|总结（100字以内）:  <br/>本研究通过FDTD和FEM模型分析1755年古钢琴声板老化对音色的影响，发现高频弦亮度下降与低频弦亮度增加的反常现象，并揭示其与频率依赖阻尼滤波效应的关系，同时探讨了弦连接位置对声学特性的影响。  <br/><br/>贡献点：  <br/>1. **方法创新**：首次将有限差分时域（FDTD）模型应用于古钢琴声板老化声学特性研究，结合实测数据与模拟实验。  <br/>2. **精准测量**：在52个弦位置处测量声板厚度并模拟冲击响应，量化老化对声学参数（如T60衰减时间）的具体影响。  <br/>3. **音色量化分析**：利用频谱质心（spectral centroid）指标，首次揭示琴弦位置与老化导致音色亮度变化的依赖关系。  <br/>4. **反常现象发现**：提出低频弦亮度提升、高频弦亮度下降的反常结果，解释为频率相关的阻尼滤波效应。  <br/>5. **结构特性探索**：通过3D有限元模型（FEM）验证 Dulcken 古钢琴将8'弦连接外壁而非内壁的结构设计对声板张力影响有限，提出其他潜在的声学解释。|
|2511.09029v1|[Non-verbal Perception of Room Acoustics using Multi Dimensional Scaling Metho](https://arxiv.org/abs/2511.09029v1)|**贡献点总结：**  <br/>本研究提出了一种新的方法，将音乐与双耳房间脉冲响应测量结合，利用MDS揭示房间声学感知的五个关键维度（混响密度、分形相关维度、粗糙度、响度、早期衰减时间），突破了传统依赖专家记忆或单向评分的局限，为客观量化主观声学体验提供了新思路。<br/><br/>**分点贡献：**  <br/>1. **提出新研究方法**：首次将实际音乐与双耳房间脉冲响应（binaural room impulse response）融合，探索主观听觉体验的多维表征。  <br/>2. **引入MDS技术**：采用多维标度（MDS）分析，从听众的感知数据中提取房间声学的感知维度，而非依赖传统专家评分或单向量表。  <br/>3. **确定关键感知维度**：通过实验验证，发现房间声学的主观感知可由五个（psycho-）acoustical参数解释（混响密度、分形相关维度、粗糙度、响度、早期衰减时间），为声学设计与评估提供量化依据。  <br/>4. **优化主观-客观关联**：突破了传统方法中单一参数或专家记忆的局限，通过音乐场景的结合更全面地反映真实听觉环境的感知复杂性。  <br/>5. **应用价值明确**：结果可应用于音乐厅、录音室等空间声学优化，提升主观体验的客观化研究和工程实践。|
|2511.08755v1|[Chord-conditioned Melody and Bass Generation](https://arxiv.org/abs/2511.08755v1)|该文本描述的是一篇音乐生成领域的论文，而非语音领域。但根据摘要内容，其贡献点可总结为以下三点：<br/><br/>1. **模型比较**：系统评估了五种基于Transformer的和弦条件生成方法（无和弦条件、独立声部生成、低音优先生成、旋律优先生成、联合生成），为音乐生成任务提供了不同策略的对比基准。  <br/>2. **理论指标设计**：提出结合音乐理论的评价体系，量化分析音高内容、音程大小和和弦音使用等音乐特征，提升生成结果的可评估性。  <br/>3. **效果验证**：实验证明和弦条件化对音乐风格的音高内容和和弦音使用特征的还原效果显著，尤其low-first模型在风格一致性上表现更优。  <br/><br/>总结（100字以内）：  <br/>该文对比了五种基于Transformer的和弦条件音乐生成方法，设计音乐理论导向的评估指标，验证了和弦条件化对风格特征的提升作用，尤其突出low-first模型的优势。|
|2511.08261v1|[Uncertainty Calibration of Multi-Label Bird Sound Classifiers](https://arxiv.org/abs/2511.08261v1)|总结：该研究系统评估了生物声学多标签分类器的校准问题，揭示校准差异及模型特性，并提出简单有效的方法提升校准性能，强调不确定性校准在实际应用中的重要性。<br/><br/>贡献点：<br/>1. 首次系统性评估生物声学领域多标签分类器的校准问题，填补该领域校准研究的空白。<br/>2. 提出使用阈值无关指标（ECE、MCS）和判别指标（cmAP）的混合评估框架，全面分析校准效果。<br/>3. 发现模型校准存在显著的数据集和类别差异，揭示低频类别反而更易校准的反直觉现象。<br/>4. 验证简单后处理校准方法（如Platt scaling）的可行性，证明小规模标注校准集即可显著改善校准效果。|
|2511.07336v2|[AcousTools: A 'Full-Stack', Python-Based, Acoustic Holography Library](https://arxiv.org/abs/2511.07336v2)|**贡献点：**  <br/>1. **提出全栈解决方案**：首次整合声学全息的全流程（建模、相位检索、声场分析、硬件控制），填补现有工具碎片化的缺陷。  <br/>2. **开发AcousTools库**：基于Python的开源框架，支持中空声学应用的完整开发需求，提供统一的代码接口。  <br/>3. **多场景支持**：覆盖空中触觉、体积显示、生物医学等应用，增强跨领域研究的可行性。  <br/>4. **提升研究效率**：通过标准化工具加速新型应用开发与成果复现，降低技术门槛。  <br/>5. **方法比较框架**：为研究者提供直观的全栈视角，便于理解与对比不同方法的适配性。  <br/><br/>**总结（100字内）：**  <br/>本研究提出首个全栈声学全息软件框架AcousTools，整合建模、控制等全流程，支持多场景应用，推动新型技术研发与跨领域研究。|
|2510.21004v2|[Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004v2)|总结：  <br/>本研究系统评估了FOICE生成语音的检测效果，提出针对性微调策略并分析其泛化能力，揭示现有检测技术的不足，推动下一代音频深度伪造检测方法的发展。<br/><br/>贡献点：  <br/>1. **首次系统评估**：对FOICE生成语音的检测能力进行全面评估，证实主流检测器在清洁与噪声环境下均难以有效识别。  <br/>2. **针对性微调策略**：设计捕捉FOICE特有特征的微调方法，显著提升检测准确率。  <br/>3. **泛化能力分析**：研究微调后检测器对未知合成工具的适应性，发现专精FOICE与保持鲁棒性的平衡问题。|
|2510.12858v2|[A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858v2)|**贡献点：**<br/>1. **揭示现有评估系统的局限性**：指出当前基于ASR的系统依赖偏倚数据集、无法提供有效反馈，且过度强调词识别而非音质评估。  <br/>2. **提出知识驱动的评估框架**：倡导基于Quran文本不变性与Tajweed规则的规则化声学建模，替代数据驱动的统计模式。  <br/>3. **强调规则与发音点（Makhraj）的重要性**：主张以标准发音原则和发音部位为核心，而非依赖有偏数据训练的模型。  <br/>4. **设计混合系统解决方案**：建议结合语言学专家知识与音频处理技术，开发公平、可靠的评估工具，提升教育有效性。  <br/><br/>**总结（100字以内）**：  <br/>论文指出现有Quranic recitation评估系统依赖偏倚数据且缺乏音质分析，提出基于规则和发音原理的知识驱动框架，结合语言学与音频技术，开发公平有效的自动化评估工具。|