|Source|Title|Summary|
|---|---|---|
|2506.05140v1|[AudioLens: A Closer Look at Auditory Attribute Perception of Large   Audio-Language Models](http://arxiv.org/abs/2506.05140v1)|**贡献点：**<br/>1. 首次系统分析LALMs的听觉属性感知机制，揭示其内部处理过程。  <br/>2. 通过词汇投影追踪属性信息在模型层与token位置的动态变化规律。  <br/>3. 发现属性信息随层深度增加而衰减，且早层属性识别与模型准确性正相关。  <br/>4. 指出LALMs更依赖听觉输入查询而非在属性提及位置隐状态聚合信息。  <br/>5. 提出基于上述发现的模型增强方法，为性能优化提供新思路。  <br/><br/>**总结（100字以内）：**  <br/>该研究首次深入解析大音频-语言模型对听觉属性的内部处理机制，揭示属性信息在模型中的分布规律及依赖关系，并提出针对性优化方法，为提升模型性能和理解其行为提供理论依据与实践方向。|
|2506.05121v1|[The NTNU System at the S&I Challenge 2025 SLA Open Track](http://arxiv.org/abs/2506.05121v1)|**贡献点**：  <br/>1. 提出融合wav2vec 2.0与Phi-4 MLLM的跨模态评估系统，结合声学特征与语义理解。  <br/>2. 设计得分融合策略，有效弥补BERT依赖ASR转录（缺失语调/发音）和W2V缺乏语义解释的缺陷。  <br/>3. 在Speak & Improve Challenge 2025官方测试集上实现RMSE 0.375，取得第二名，优于基线模型（0.444）和第三名（0.384）。  <br/><br/>**总结**：  <br/>提出集成声学与语义分析的跨模态语音评估系统，通过得分融合策略突破BERT与W2V的局限，在竞赛中获得第二名。|
|2506.05104v1|[Survey on the Evaluation of Generative Models in Music](http://arxiv.org/abs/2506.05104v1)|总结：  <br/>本文提供跨学科综述，系统总结生成音乐系统的评估方法、指标及挑战，涵盖主观/客观、定性/定量、实证/计算等多维度分析，并从音乐学、工程与人机交互视角探讨其优劣势。<br/><br/>贡献点：<br/>1. **系统性综述**：全面梳理生成音乐系统的评估目标、方法与指标，填补领域内评估体系研究的空白。<br/>2. **多维度分类**：整合主观与客观、定性与定量、实证与计算等不同评估范式，明确其适用场景与局限。<br/>3. **跨学科视角**：从音乐学、工程、人机交互（HCI）三方面分析评估方法的优劣，推动理论与实践的融合。<br/>4. **方法论框架**：建立涵盖系统输出与模型可用性的评估框架，为后续研究提供指导和参考。|
|2505.22266v2|[FGAS: Fixed Decoder Network-Based Audio Steganography with Adversarial   Perturbation Generation](http://arxiv.org/abs/2505.22266v2)|总结（100字以内）:  <br/>提出FGAS框架，通过固定解码器与对抗扰动生成增强音频隐写能力，显著提升生成音频质量与抗隐写分析性能，降低对复杂训练和大模型的依赖。<br/><br/>贡献点：<br/>1. **提出FGAS方法**：首次将固定解码器网络与对抗扰动生成结合，通过共享解码器结构和权重实现隐写信息提取，摆脱了对大预训练模型的依赖。<br/>2. **轻量化解码器设计**：构建轻量级固定解码器，兼顾隐藏信息的可靠提取与系统效率，简化了实现复杂度。<br/>3. **对抗扰动生成策略**：开发音频对抗扰动生成（APG）技术，优化扰动以保持生成音频在感知和统计上与原始信号高度相似，增强抗隐写分析能力。<br/>4. **性能提升验证**：实验表明FGAS在PSNR指标上较SOTA方法提升超10 dB，且在不同负载下抵抗隐写分析的分类错误率更高，验证了其有效性。|
|2504.12880v3|[Can Masked Autoencoders Also Listen to Birds?](http://arxiv.org/abs/2504.12880v3)|**贡献点：**  <br/>1. 提出适应细粒度音频领域的定制化自监督学习框架（Bird-MAE），通过调整训练流程（预训练、微调与冻结特征利用）解决通用模型在鸟类声音分类中的性能不足。  <br/>2. 首次在BirdSet数据集上实现多标签分类的SOTA性能，展示参数高效原型探测方法（prototypical probing）对冻结表示的显著提升，优于线性探测37%（MAP）。  <br/>3. 验证原型探测在低资源场景下的有效性，将冻结表示与微调性能差距缩小至3.3%（平均）。  <br/>4. 在自建的BirdSet少样本基准上证明Bird-MAE具有强泛化能力，凸显定制化自监督学习对细粒度音频任务的价值。  <br/><br/>**总结：**  <br/>本研究通过优化训练流程和提出参数高效原型探测方法，显著提升通用MAE模型在细粒度鸟类声音分类中的性能，实现SOTA并验证其少样本鲁棒性。|
|2506.04981v1|[Better Semi-supervised Learning for Multi-domain ASR Through Incremental   Retraining and Data Filtering](http://arxiv.org/abs/2506.04981v1)|总结：  <br/>提出一种基于增量半监督学习的ASR领域适配方法，结合多模型共识与命名实体识别（NER）进行伪标签筛选，在多领域数据集上显著提升模型性能并降低计算成本。<br/><br/>贡献点：  <br/>1. **首个增量半监督学习框架**：通过整合少量领域内标注数据与相关领域辅助数据，实现比无辅助数据的微调方法更高的性能提升（4%）。  <br/>2. **多模型共识与NER联合筛选方法**：提出结合多模型共识与NER的伪标签选择机制，相较于随机筛选，显著延缓性能饱和，提升更稳定。  <br/>3. **多领域实验验证**：在Wow和Fisher两个主流多领域数据集上，验证方法优于传统单步微调策略，且在实际任务中表现优于其他基准方法。  <br/>4. **高效筛选策略比较**：共识过滤在性能提升（22.3%/24.8%）与计算成本之间达到最佳平衡，NER则以更低成本提供竞争力的替代方案。|
|2505.19644v2|[STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set   Source Tracing and Attribution](http://arxiv.org/abs/2505.19644v2)|**总结**：  <br/>提出STOPA数据集，系统化覆盖8个AM、6个VM及多参数设置，提升深伪语音来源追踪的准确性和可靠性，促进检测与模型透明度。<br/><br/>**贡献点**：  <br/>1. **构建系统化数据集**：创建STOPA，包含系统变化和丰富元数据，填补了深伪语音检测领域专用数据集的空白。  <br/>2. **多模型与参数覆盖**：涵盖8个声学模型、6个声码器模型及多样参数配置，支持更全面的来源追踪研究。  <br/>3. **多合成器多样性**：70万样本来自13种不同合成器，增强数据集的代表性与泛化能力。  <br/>4. **提升归属可靠性**：通过系统化控制生成因素，显著提高来源归属的准确性，助力法医学分析与模型透明度研究。|
|2506.04915v1|[A Practitioner's Guide to Building ASR Models for Low-Resource   Languages: A Case Study on Scottish Gaelic](http://arxiv.org/abs/2506.04915v1)|总结：  <br/>本文提出结合HMM与自监督模型的新方法，挑战传统依赖细调的信念，通过持续预训练和半监督训练实现低资源语言ASR的显著性能提升，尤其在苏格兰盖尔语上达到32%的WER降低。<br/><br/>贡献点：  <br/>1. **挑战传统方法信念**：论证在低资源语言ASR中，单纯依赖多语言端到端模型的微调并非最优方案。  <br/>2. **提出混合模型框架**：创新性地融合隐马尔可夫模型（HMM）与自监督模型，结合两者优势提升性能。  <br/>3. **优化数据利用策略**：通过持续自监督预训练与半监督训练，更高效地利用有限的语音和文本数据。  <br/>4. **实验证明有效性**：在苏格兰盖尔语等低资源语言上验证方法效果，实现相较于微调Whisper模型的32%相对WER降低。|
|2506.04890v1|[Multivariate Probabilistic Assessment of Speech Quality](http://arxiv.org/abs/2506.04890v1)|总结（100字以内）:  <br/>该论文提出基于多变量高斯分布的语音质量评估模型，利用Cholesky分解和扩展的概率仿射变换联合建模四个维度，既保持点估计精度又提供不确定性及相关性估计，推动语音质量诊断的精细化。<br/><br/>贡献点：  <br/>1. **引入多变量框架**：将传统单变量MOS估计扩展至多变量联合建模，同时考虑噪音、色彩失真、不连续性和响度四个维度。  <br/>2. **无约束协方差建模**：采用Cholesky分解预测维度间协方差，避免强假设限制，提升模型灵活性。  <br/>3. **扩展概率变换方法**：将概率仿射变换推广至多变量场景，增强对复杂语音质量特征的建模能力。  <br/>4. **提供全面评估信息**：在保持与SOTA方法同等点估计精度的同时，首次实现多维度的不确定性与相关性联合分析。  <br/>5. **推动实际应用**：通过多维诊断能力，辅助精准定位语音质量问题，指导针对性优化策略。|
|2506.00975v2|[NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction](http://arxiv.org/abs/2506.00975v2)|总结：  <br/>本研究提出Next-Token-Pair Prediction（NTPP）框架，首次利用解码器-only架构实现说话人无关的双通道对话学习，并在回合转换预测、响应连贯性和自然性上取得显著提升，同时降低推理延迟，提升实时应用效率。<br/><br/>贡献点：  <br/>1. **提出NTPP新范式**：创新性地设计Next-Token-Pair Prediction方法，首次将双通道语音数据用于解码器-only架构的对话学习。  <br/>2. **说话人无关对话建模**：突破传统方法依赖说话人信息的限制，实现通用双通道对话理解与生成。  <br/>3. **提升对话能力**：在turn-taking预测、响应连贯性与自然性等关键指标上取得显著性能改进。  <br/>4. **优化实时性**：相比现有方法，NTPP大幅降低推理延迟，增强实际部署的可行性。|
|2506.04852v1|[Improving AI-generated music with user-guided training](http://arxiv.org/abs/2506.04852v1)|总结：  <br/>本文提出基于用户反馈的遗传算法优化框架，通过整合用户评分作为损失函数，提升音乐生成模型的个性化表现，实验证明该方法在两次迭代中显著提高用户满意度。<br/><br/>贡献点：  <br/>1. **引入人类计算框架**：首次将用户交互评分与遗传算法结合，实现音乐生成模型的动态自适应优化。  <br/>2. **创新损失函数设计**：将用户主观评分直接作为模型微调的损失函数，打破传统固定数据集训练的局限。  <br/>3. **迭代性能评估方法**：提出通过用户评分平均增长率量化模型改进效果，验证方法有效性（首次迭代+0.2，第二次+0.39）。  <br/>4. **解决音乐个性化难题**：针对音乐高度主观性需求，设计可响应用户偏好的交互式生成机制，弥补图像生成模型的不足。|
|2506.04779v1|[MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark](http://arxiv.org/abs/2506.04779v1)|总结（100字以内）:  <br/>提出MMSU基准，涵盖47项任务及多语言现象，系统评估SpeechLLM模型，揭示性能短板，推动语音理解与人机交互技术发展。<br/><br/>贡献点:  <br/>1. **提出MMSU基准**：首个针对自然语音理解和推理的综合评估基准，包含5,000个音频-问题-答案三元组，覆盖多模态任务。  <br/>2. **多模态语言学理论整合**：系统纳入语音学、韵律、修辞、句法、语义及语用特征，提升基准的理论基础与评估维度。  <br/>3. **模型性能评估与分析**：对14种先进SpeechLLM进行全面测试，揭示现有模型在细粒度感知和复杂推理上的不足。  <br/>4. **明确优化方向**：通过评估结果提出未来研究的关键方向，指导语音理解模型的改进与创新。  <br/>5. **开放资源支持**：提供基准数据集与评估代码，方便研究者复现与扩展，促进领域发展。|
|2506.04714v1|[IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech   translation](http://arxiv.org/abs/2506.04714v1)|总结：  <br/>本研究针对低资源Bhojpuri-Hindi语言对，通过超参数优化与数据增强方法提升SeamlessM4T模型的翻译性能，并分析跨语言信号与翻译错误对BLEU分数的影响。<br/><br/>贡献点：  <br/>1. **系统提交**：参与IWSLT 2025语音翻译共享任务，提交IIITH-BUT系统用于Bhojpuri-Hindi语言对。  <br/>2. **超参数优化**：系统研究学习率调度、更新步数、预热步数、标签平滑和批量大小等超参数对模型性能的影响。  <br/>3. **数据增强技术**：应用速度扰动和SpecAugment方法缓解数据稀缺问题，并验证其对翻译质量的提升效果。  <br/>4. **跨语言联合训练**：通过联合训练Marathi和Bhojpuri语音数据，探索跨语言信号对模型的辅助作用。  <br/>5. **错误分析**：分析翻译假说，识别影响BLEU分数的不同类型错误，为模型改进提供依据。|
|2506.04652v1|[EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label   Speech Emotion Recognition](http://arxiv.org/abs/2506.04652v1)|**贡献点（分点）：**  <br/>1. **填补研究空白**：首次系统性探索多标签语音情感识别（SER）中的去偏方法有效性与鲁棒性，解决性别偏见问题的现有研究不足。  <br/>2. **提出综合框架**：构建EMO-Debias，对13种去偏技术（包括预处理、正则化、对抗学习、有偏学习者、分布鲁棒优化等）进行大规模对比实验。  <br/>3. **实证评估**：使用WavLM和XLSR编码器，在人工与真实情感数据集上测试性别不平衡场景下的方法性能。  <br/>4. **量化权衡分析**：明确揭示公平性与准确性的平衡关系，筛选出同时降低性别性能差距且不牺牲整体模型效果的策略。  <br/>5. **提供实践指导**：总结可操作的去偏方法选择建议，并强调数据分布对结果的显著影响。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出EMO-Debias框架，系统评估13种去偏技术在多标签SER中的性能，揭示性别偏见的权衡关系，为去除性别偏差提供实用策略并强调数据分布的重要性。|
|2506.04518v1|[Towards Efficient Speech-Text Jointly Decoding within One Speech   Language Model](http://arxiv.org/abs/2506.04518v1)|贡献点总结（100字以内）:  <br/>本研究提出改进的ESI解码模式，在保持interleaved方法对齐优势的同时提升推理效率，并构建高质量语音QA数据集，显著提升了语音问答性能。<br/><br/>分点贡献:  <br/>1. **系统性对比研究**：首次在统一模型、tokenizer和训练数据下，系统比较了主流的语音-文本联合解码策略（interleaved与parallel generation）的性能、效率及对齐质量。  <br/>2. **高效解码方案**：提出ESI（Early-Stop Interleaved）方法，在缩短推理时间的同时保持甚至略微提升解码性能，解决长序列长度导致的效率瓶颈。  <br/>3. **高质量数据集构建**：设计并发布专门优化的语音问答（QA）数据集，用于提升语音QA任务的性能，推动相关研究进展。|
|2506.04495v1|[French Listening Tests for the Assessment of Intelligibility, Quality,   and Identity of Body-Conducted Speech Enhancement](http://arxiv.org/abs/2506.04495v1)|**贡献点：**  <br/>1. **系统评估EBEN模型**：首次在体传导传感器（ forehead accelerometer, rigid in-ear, throat mic）上通过主观实验验证EBEN的语音增强效果，涵盖可懂度、语音质量及说话人身份保持三方面。  <br/>2. **揭示性能差异**：发现EBEN对女性说话人喉部麦克风录音的说话人识别能力有小幅下降，表明模型效果与录音方式存在交互影响。  <br/>3. **建立相关性**：证明STOI与感知质量在体传导语音中存在关联，为客观评估提供依据。  <br/>4. **验证ECAPA2-TDNN有效性**：展示ECAPA2-TDNN在说话人验证中的表现与识别任务一致，增强其在真实场景中的可信度。  <br/>5. **提出指标局限性**：指出现有评估指标无法可靠预测EBEN对可懂度的增强效果，为未来研究提供方向。  <br/><br/>**总结（100字内）：**  <br/>该研究通过多模态主观测试评估EBEN在体传导语音中的效果，揭示其对语音质量与可懂度的提升及对女性识别的潜在影响，建立STOI与感知质量的相关性，并验证ECAPA2-TDNN的可靠性，强调现有指标预测能力的不足。|
|2506.04492v1|[Bringing Interpretability to Neural Audio Codecs](http://arxiv.org/abs/2506.04492v1)|总结:  <br/>本文提出两步方法解析语音编码器令牌，通过分析阶段揭示语音属性编码机制，合成阶段构建AnCoGen网络实现属性直接提取，提升模型可解释性。<br/><br/>贡献点:  <br/>1. 提出两阶段框架（分析+合成）系统研究语音信息在神经音频编解码器令牌中的编码机制  <br/>2. 首次量化分析语音属性（内容、身份、音高）在编码器输出中的分布与关联性  <br/>3. 开发AnCoGen网络实现对现有编解码器的后解释，直接从令牌中提取可解释的语音属性  <br/>4. 通过对比声学单位与语义单位的编码特性，揭示其在可解释性方面的差异  <br/>5. 建立可解释性导向的音频编解码器分析范式，为语音处理模型的可调试性研究提供新思路|
|2504.10746v2|[Hearing Anywhere in Any Environment](http://arxiv.org/abs/2504.10746v2)|总结（100字以内）:  <br/>本文提出xRIR框架，结合几何特征提取与RIR编码，构建ACOUSTICROOMS数据集，实现跨房间声学环境重建与真实场景验证，显著提升泛化能力与模拟真实性。<br/><br/>贡献点:  <br/>1. **提出统一模型**：开发xRIR框架，解决现有方法对单一环境的依赖，实现跨房间的声学环境泛化重建。  <br/>2. **融合多模态特征**：结合全景深度图像提取几何信息，与少量参考RIR样本提取声学特征，增强模型跨环境适应能力。  <br/>3. **构建大规模数据集**：创建ACOUSTICROOMS数据集，包含260个房间超30万条高保真RIR模拟，用于评估与验证。  <br/>4. **验证真实场景性能**：通过sim-to-real迁移测试四类真实环境，证明模型实际应用效果与数据集的仿真真实性。  <br/>5. **超越基线方法**：实验结果表明，xRIR在RIR预测任务上显著优于传统神经方法与现有基线。|
|2506.04392v1|[Phi-Omni-ST: A multimodal language model for direct speech-to-speech   translation](http://arxiv.org/abs/2506.04392v1)|**贡献点：**  <br/>1. 提出Phi-Omni-ST模型，首次实现**直接语音到语音翻译**（ST）的多模态语言模型，无需中间文本转换。  <br/>2. 引入**音频Transformer头**与**流式vocoder**的组合架构，通过**音频标记延迟预测**实现高效语音生成与波形合成。  <br/>3. 在CVSS-C数据集上验证模型性能，**显著超越现有基线模型**，并实现端到端高保真翻译。  <br/>4. 通过扩展训练数据与模型规模，**达到当前SOTA性能水平**，证明模型的可扩展性与竞争力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Phi-Omni-ST模型，结合音频Transformer与流式vocoder实现端到端语音到语音翻译，超越基线模型并逼近SOTA性能，为高效高质量的语音翻译提供了新方案。|
|2506.04391v1|[Benchmarking Time-localized Explanations for Audio Classification Models](http://arxiv.org/abs/2506.04391v1)|总结：  <br/>本文提出了一种用于音频分类模型时间定位解释的基准框架，通过目标事件时间注释替代真实标签，系统化优化比较多种后处理解释方法，并验证了解释在发现虚假相关性中的应用价值。<br/><br/>贡献点：  <br/>1. 提出时间局部化音频分类解释的基准方案，利用目标事件时间注释作为替代真实标签的评估标准。  <br/>2. 系统化优化并对比多种模型无关的后处理解释方法，实现接近完美的解释效果。  <br/>3. 展示解释在揭示音频数据中虚假相关性方面的实用价值，增强模型可解释性研究的实际意义。|
|2506.04376v1|[Domain Adaptation Method and Modality Gap Impact in Audio-Text Models   for Prototypical Sound Classification](http://arxiv.org/abs/2506.04376v1)|**贡献点分点总结**：  <br/>1. 提出一种无需模型重训练的背景声音贡献量化方法，有效提升零样本环境声音分类性能。  <br/>2. 首次分析背景声音对模型性能的影响，并揭示其主要由SNR（信噪比）水平决定，而非背景类型。  <br/>3. 开发可跨不同背景和SNR条件的领域自适应技术，增强分类鲁棒性与泛化能力。  <br/>4. 系统研究音频-文本嵌入的模态间隙问题，证明缩小该差距可显著提升分类效果。  <br/>5. 验证方法在主流原型模型中的适用性，展现其可扩展性与对多样化环境的兼容性。  <br/><br/>**总结**（100字以内）：  <br/>本文提出背景声音贡献量化与领域自适应方法，解决零样本环境声音分类中背景干扰问题，优化音频-文本模型性能，并通过模态间隙分析提升分类准确率，验证方法的通用性与鲁棒性。|
|2506.01483v2|[Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction](http://arxiv.org/abs/2506.01483v2)|**贡献点分点总结：**  <br/>1. 提出基于说话人间相对线索（如时间顺序、年龄、音高）的新型目标说话人区分与语音分离方法。  <br/>2. 分类处理连续线索（通过相对差异）与离散线索（保留原始类别），提升特征利用效率。  <br/>3. 相对线索方法相比固定属性分类更具灵活性，支持文本引导数据集的便捷扩展。  <br/>4. 实验验证结合全部相对线索优于随机子集，性别和时间顺序在多语言及混响条件下表现最稳健。  <br/>5. 其他线索（如音高、响度、距离）在复杂场景中显著提升性能。  <br/>6. 通过微调预训练WavLM Base+CNN编码器，显著优于仅使用Conv1d编码器的基线模型。  <br/><br/>**总结（100字以内）：**  <br/>该研究提出基于说话人相对线索的语音分离方法，结合连续与离散特征分类，提升灵活性与可扩展性。实验表明，综合所有相对线索可显著增强性能，尤其在多语言及复杂场景中，同时利用深度学习模型优化进一步提高效果。|
|2506.04364v1|[Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot   Accent Robustness in Low-Resource ASR](http://arxiv.org/abs/2506.04364v1)|总结：  <br/>该论文研究了低资源下ASR训练数据变量对未见口音鲁棒性的影响，发现增加说话人数比增加单人时长更有效，并提出在新语言开发中应优先提升说话人数量。<br/><br/>贡献点：  <br/>1. **系统研究训练数据变量的影响**：首次全面分析说话人数、单人时长和口音多样性三个关键因素对ASR系统鲁棒性（尤其是未见口音）的系统性影响。  <br/>2. **说话人数优先于单人时长**：在固定训练小时数下，增加说话人数（降低单人贡献量）比延长单人时长更能提升ASR对未见口音的适应能力。  <br/>3. **说话人数与时长扩展的协同效应**：发现更多说话人数量可增强ASR性能随训练时长扩展的收益，揭示数据规模与多样性之间的交互作用。  <br/>4. **口音多样性作用有限**：在控制说话人数时，优先选择口音差异大的说话人对性能提升作用较小，挑战了传统数据多样性优化的假设。  <br/>5. **实践指导建议**：提出针对新语言开发的ASR训练策略，建议优先扩大说话人数量而非过度依赖单人数据质量或时长，具有实际应用价值。|
|2506.04214v1|[Sounding that Object: Interactive Object-Aware Image to Audio Generation](http://arxiv.org/abs/2506.04214v1)|**贡献点总结**（100字以内）:  <br/>提出交互式对象感知音频生成模型，整合对象中心学习与扩散模型，通过多模态注意力实现图像-声音关联，理论验证注意力机制逼近分割掩码，实验验证性能优于基线。<br/><br/>**分点贡献**:<br/>1. **模型创新**: 提出"交互式对象感知音频生成"方法，将声音生成与用户选定的视觉对象直接关联，解决复杂场景中多对象、多声源的音频生成难题。<br/>2. **技术整合**: 将对象中心学习融合至条件潜变量扩散模型，引入多模态注意力机制实现图像区域与对应声音的跨模态映射。<br/>3. **交互机制**: 在测试阶段利用图像分割技术，允许用户以对象级别进行交互式音频生成，提升生成的精准度和可控性。<br/>4. **理论支撑**: 理论证明注意力机制在功能上可逼近实际分割掩码，确保生成音频与视觉对象的空间一致性。<br/>5. **实验验证**: 通过定量与定性评估显示，模型在对象-声音对齐任务中优于现有基线方法，验证了方法的有效性。|
|2506.04076v1|[Acoustically Precise Hesitation Tagging Is Essential for End-to-End   Verbatim Transcription Systems](http://arxiv.org/abs/2506.04076v1)|总结（100字以内）:  <br/>该研究提出通过LoRA微调Whisper模型改进verbatim L2语音转录，比较三种标注方案，发现精确填充符标注能显著提升ASR准确率，获得优于传统方法的WER性能。<br/><br/>贡献点分点列出:  <br/>1. 提出无需外部音频数据的LoRA微调方法，优化Whisper模型在verbatim L2语音转录中的表现  <br/>2. 设计并对比三种标注方案（Pure/Rich/Extra），揭示标注细粒度对ASR性能的影响  <br/>3. 首次证明基于Gemini 2.0 Flash推断的acoustically precise填充符标注（Extra）的有效性  <br/>4. 实验结果表明"Extra"方案可使Whisper Large V3 Turbo的WER降低11.3%（从6.2%至5.5%）  <br/>5. 强调显式填充词标注在处理口语化、不流畅语音（如hesitations）中的关键作用|
|2506.04073v1|[A Statistics-Driven Differentiable Approach for Sound Texture Synthesis   and Analysis](http://arxiv.org/abs/2506.04073v1)|总结：  <br/>提出TexStat损失函数及TexEnv合成器，构建DDSP-inspired的TexDSP模型，实现纹理声音的高效生成与评估，并开源代码促进研究应用。<br/><br/>贡献点：  <br/>1. **提出TexStat损失函数**：专门设计用于纹理声音分析与合成，具备感知意义、时间不变性和抗噪鲁棒性，无需依赖时序结构。  <br/>2. **开发TexEnv合成器**：轻量级、可微分的生成方法，通过滤波噪声叠加幅度包络实现纹理音频合成。  <br/>3. **构建TexDSP生成模型**：整合TexStat和TexEnv，作为DDSP-inspired模型，专为纹理声音的生成任务优化。  <br/>4. **引入综合评估方案**：将TexStat与FAD结合，形成更全面的纹理声音合成模型评估指标。  <br/>5. **开源工具与代码**：提供PyTorch实现，确保高效性、可配置性，支持生成任务与感知评估应用。|
|2505.15965v2|[Analyzing the Impact of Accent on English Speech: Acoustic and   Articulatory Perspectives](http://arxiv.org/abs/2505.15965v2)|**贡献点总结：**  <br/>1. 提出非母语口音英语的特征差异（简化协调模式、更高平均音调）  <br/>2. 开发基于eigenspectra和声管变量的高效量化方法（无需音素转录）  <br/>3. 揭示口音对语音可懂度的影响机制，推动研究方向创新  <br/>4. 为构建包容性强、适应多样语言群体的语音处理系统提供理论支持  <br/><br/>**摘要总结（100字以内）：**  <br/>该研究通过发音与声学分析揭示非母语口音英语的特征差异，提出无需音素转录的量化方法，为理解口音对语音可懂度的影响及开发包容性语音系统提供新视角。|
|2505.16044v2|[Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom   Severity Estimation](http://arxiv.org/abs/2505.16044v2)|**贡献点：**  <br/>1. 提出将精神分裂症评估从单一分类任务转向症状严重程度的估计，提升诊断的精细化程度。  <br/>2. 开发多模态融合框架，整合语音、视频和文本数据，增强模型的全面性和临床适用性。  <br/>3. 构建单模态模型与多模态框架的协同机制，显著提高模型的准确性与鲁棒性。  <br/>4. 通过捕捉更详细的症状特征，为个性化治疗提供数据支持。  <br/>5. 提出一种可扩展且客观的语音领域评估工具，具备实际医疗应用潜力。  <br/><br/>**总结（100字以内）**  <br/>本文创新性地采用多模态方法整合语音、视频和文本数据，通过症状严重度估计替代传统二分类任务，开发协同模型提升诊断精度与鲁棒性，为精神分裂症的个性化评估和治疗提供客观、可扩展的解决方案。|
|2506.04037v1|[The mutual exclusivity bias of bilingual visually grounded speech models](http://arxiv.org/abs/2506.04037v1)|总结：  <br/>该研究验证了双语VGS模型中ME偏差的减弱现象，揭示了视觉嵌入方差变化与跨语言混淆的关系，并提出了ME偏差存在的新理论视角。<br/><br/>贡献点：  <br/>1. **验证双语ME偏差现象**：首次发现双语VGS模型（英语+法语/荷兰语）相比单语模型表现出更弱的ME偏差，且存在语言特例。  <br/>2. **解释混淆机制**：通过分析视觉嵌入的方差差异，揭示双语模型对熟悉数据的低方差导致新旧概念混淆增加，为ME偏差提供计算解释。  <br/>3. **探究ME偏差根源**：提出新的理论视角，解释VGS模型中ME偏差的形成原因，深化对语言学习机制的理解。  <br/>4. **多语言实验框架**：构建跨语言（英法荷）的实验范式，拓展了ME策略在多语言环境下的研究边界。|
|2506.04013v1|[Towards Better Disentanglement in Non-Autoregressive Zero-Shot   Expressive Voice Conversion](http://arxiv.org/abs/2506.04013v1)|**贡献点（分点）:**  <br/>1. **框架改进**：基于条件变分自编码器（CVAE）提出自监督非自回归语音转换框架，提升风格迁移效果。  <br/>2. **内容表征优化**：采用多语言离散语音单元降低源音色泄漏，结合增强的相似性损失和混合风格层归一化方法。  <br/>3. **风格嵌入增强**：通过局部F0信息的交叉注意力机制和全局音高/能量特征提取，提升表达性迁移能力。  <br/>4. **实验验证**：模型在情感与说话人相似性任务中显著优于基线，验证了其在风格适应和源风格泄漏抑制上的有效性。  <br/><br/>**总结（100字以内）:**  <br/>本研究通过CVAE框架改进和多层级风格嵌入设计，有效解决了语音转换中的源音色泄漏问题，并增强了情感与表达属性的迁移能力，实验表明其在风格适应性上优于现有方法。|
|2506.03959v1|[From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder   Framework for Auditory Perception and Cochlear Implant Simulation](http://arxiv.org/abs/2506.03959v1)|贡献点总结（100字以内）:  <br/>提出NeuroVoc框架，实现模型无关的声码器，通过模块化设计支持听觉模型替换，验证其在模拟耳蜗植入用户听觉感知中的有效性，展示NH与EH模型的差异，并证明声码语音在噪声中的可懂度与临床数据一致。<br/><br/>分点贡献：  <br/>1. **提出NeuroVoc框架**：首次构建模型无关的声码器系统，基于逆傅里叶变换从神经活动模式重建声波，通用性强。  <br/>2. **模块化架构设计**：允许灵活替换或修改底层听觉模型（如NH与EH），提升框架的适应性与扩展性。  <br/>3. **消除特定实现依赖**：无需为不同语音编码策略定制声码器，简化模拟听觉感知的流程。  <br/>4. **直接对比听觉模型**：通过NH与EH模型的声码语音测试，明确其在谐波结构保留上的差异。  <br/>5. **验证感知可懂度**：采用Digits-in-Noise实验，证明声码语音在噪声环境中的可懂度与临床数据高度吻合。  <br/>6. **量化性能提升**：通过SRT（信号-噪声比）数据，显示NH与EH声码语音分别比标准测试提升2.4 dB和7.1 dB。  <br/>7. **揭示CI用户表现差异**：准确反映耳蜗植入用户在噪声中的听觉障碍特性，为研究提供可靠工具。|
|2506.03917v1|[Sound Field Reconstruction Using Physics-Informed Boundary Integral   Networks](http://arxiv.org/abs/2506.03917v1)|总结：  <br/>本文提出了一种基于边界积分方程的声场重建方法，利用浅层神经网络高效预测声压分布，通过均方误差训练提升精度，并在实验中验证其优于现有物理信息数据驱动技术。<br/><br/>贡献点：  <br/>1. **提出新型模型**：首次引入边界积分网络（Boundary Integral Network）用于声场重建，基于Kirchhoff-Helmholtz边界积分方程建模，将物理规律直接嵌入网络结构。  <br/>2. **浅层网络优化**：采用浅层神经网络替代复杂结构，降低计算成本，同时实现边界与内部声压的高精度预测。  <br/>3. **误差驱动训练**：通过最小化测量麦克风位置的均方误差进行模型训练，提升重建结果与实际数据的匹配度。  <br/>4. **性能验证**：实验表明该方法在声场重建任务中优于现有物理信息神经网络（PINN）等数据驱动技术。|
|2506.03832v1|[Brain-tuned Speech Models Better Reflect Speech Processing Stages in the   Brain](http://arxiv.org/abs/2506.03832v1)|**贡献点：**  <br/>1. 揭示预训练模型与人类语音处理层级结构的差异：中层语义丰富，晚层语义贫乏。  <br/>2. 提出脑微调方法，验证其提升语音模型语义理解的有效性。  <br/>3. 发现脑微调后模型的晚层显著优于预训练模型，与语义语言区域高度对齐。  <br/>4. 通过层析分析证明模型早期层专精声学特征，晚层擅长复杂高级任务。  <br/>5. 证明脑微调模型具有清晰的层级化处理机制，可作为研究人类语音处理的更优模型生物。  <br/><br/>**总结：**  <br/>本研究发现脑微调显著改进语音模型的语义对齐，揭示模型从声学到语义的分层处理特性，使其更贴近人类语音处理机制。|
|2505.04113v2|[Advancing Zero-shot Text-to-Speech Intelligibility across Diverse   Domains via Preference Alignment](http://arxiv.org/abs/2505.04113v2)|**贡献点分点总结：**  <br/>1. 提出**INTP数据集**，专门针对零样本TTS的可懂度问题，扩展了DPO框架以适配多类TTS架构。  <br/>2. 通过**偏好对齐技术**，生成超出预训练分布的挑战性语音数据（如绕口令、语码转换），提升模型表现。  <br/>3. 验证**INTP的弱到强泛化能力**，显著改善CosyVoice 2等先进模型的语音可懂度。  <br/>4. 展示**迭代对齐策略**的潜力，进一步优化模型性能。  <br/>5. 提供**公开音频样本**，便于实验复现与评估。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出INTP数据集及扩展DPO框架，通过偏好对齐技术解决零样本TTS在复杂场景下的可懂度问题，并验证模型的泛化能力与迭代优化潜力，显著提升自然度、相似性及音频质量。|
|2506.06252v1|[Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems](http://arxiv.org/abs/2506.06252v1)|总结（100字以内）:  <br/>本文提出高效的提示偏置技术，结合多任务学习框架，通过提示偏置模型和实体过滤机制提升ASR对罕见实体的识别准确率，实验证明在领域数据集上显著降低实体词错误率，且方法轻量无结构变更。<br/><br/>贡献点分点列出:  <br/>1. **提出统一的多任务学习框架**：将prompt-based biasing与实体识别任务整合，提升ASR对罕见实体的识别效果。  <br/>2. **设计双组件结构**：包含（a）提示偏置模型（判断何时关注实体）和（b）实体过滤机制（高效剔除无关实体）的创新架构。  <br/>3. **显著性能提升**：在自建领域数据集上，针对小/大实体列表分别实现30.7%和18.0%的Entity Word Error Rate下降。  <br/>4. **轻量化与高效性**：无需改变模型结构，保持简单性与轻量性，适用于实际部署。  <br/>5. **应用场景明确**：针对语音识别中罕见和领域特定实体的识别难题，提供针对性解决方案。|
|2506.06190v1|[NAT: Neural Acoustic Transfer for Interactive Scenes in Real Time](http://arxiv.org/abs/2506.06190v1)|总结：  <br/>提出Neural Acoustic Transfer方法，通过隐式神经表示实现动态声学场景的实时声场预测，并结合高效BEM生成训练数据，显著提升交互应用中的声学建模效率与精度。<br/><br/>贡献点：  <br/>1. **提出新型声学传输模型**：Neural Acoustic Transfer通过隐式神经表示编码声学传输及其动态变化，支持实时声场预测。  <br/>2. **高效训练数据生成方法**：开发快速蒙特卡洛边界元法（BEM）近似，适用于光滑Neumann条件场景。  <br/>3. **高精度BEM实现**：推出GPU加速的标准BEM版本，满足高精度声学模拟需求。  <br/>4. **动态环境建模能力**：解决物体位置、材料、尺寸变化导致的声学分布波动问题，实现高效且精准的声辐射空间建模。  <br/>5. **实验验证与应用拓展**：在多样化场景中验证方法的高精度与低延迟（30秒音频在几毫秒内完成），适用于VR、AR及先进音频生产等交互应用。|
|2506.06096v1|[Label-Context-Dependent Internal Language Model Estimation for CTC](http://arxiv.org/abs/2506.06096v1)|贡献点：<br/>1. 首次系统分析CTC的隐式上下文依赖性，揭示其通过现代编码器学习上下文相关内部语言模型（ILM）的能力<br/>2. 提出基于知识蒸馏的新型ILM估计方法，包含理论推导与优化框架<br/>3. 开发两种KD正则化技术，增强模型鲁棒性与泛化能力<br/>4. 建立跨域评估体系，验证上下文相关ILM在Librispeech和TED-LIUM数据集上的有效性<br/>5. 实现超过13%的相对WERR提升，证明标签级KD加平滑方法在ILM估计中的优越性<br/><br/>总结：本文揭示CTC的隐式上下文依赖性，提出基于知识蒸馏的新型ILM估计方法及正则化技术，在跨域评估中实现显著性能提升。|
|2506.06071v1|[CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for   Fair Speech Emotion Recognition](http://arxiv.org/abs/2506.06071v1)|总结：  <br/>提出CO-VADA方法，通过语音转换生成多样化样本，无需修改模型结构或依赖人口统计信息，提升语音情感识别系统的公平性。<br/><br/>贡献点：  <br/>1. **首创新方法**：开发了首个不依赖模型架构修改或人口统计信息的语音情感识别（SER）去偏差框架。  <br/>2. **虚假关联处理**：通过语音转换技术消除说话者特征与情感标签间的错误关联，增强模型对情感相关特征的聚焦。  <br/>3. **样本增强策略**：生成反映非主导说话者模式的样本，引入多样性以改善模型公平性。  <br/>4. **通用性与可扩展性**：兼容多种SER模型和语音转换工具，提供灵活、可扩展的解决方案。|
|2506.00506v2|[Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond](http://arxiv.org/abs/2506.00506v2)|总结：  <br/>本研究提出基于wav2vec 2.0的语音质量评估系统，在VoiceMOS 2024挑战中取得优异成绩，并创新性地采用两阶段微调方法应对数据限制，验证了其在噪声与降噪语音评估中的有效性。<br/><br/>贡献点：  <br/>1. 构建基于wav2vec 2.0的语音质量评估系统，获VoiceMOS 2024挑战Track 3前三名，其中BAK指标预测最优，OVRL次优，SIG第三优。  <br/>2. 提出两阶段细粒度微调策略，解决挑战中对训练数据的严格限制问题，提升模型适应性。  <br/>3. 在VoiceMOS 2024数据集及CHiME 7 - UDASE数据集上验证系统性能，展示其跨数据集泛化能力。  <br/>4. 探索ITU-T P.835标准（SIG、BAK、OVRL）在实际降噪语音质量评估中的应用与优化。|
|2506.05984v1|[Audio-Aware Large Language Models as Judges for Speaking Styles](http://arxiv.org/abs/2506.05984v1)|**贡献点总结：**  <br/>1. 提出利用音频感知大语言模型（ALLMs）作为自动评判者，评估语音语言模型（SLMs）生成演讲的多维度说话风格。  <br/>2. 首次对比GPT-4o-audio与Gemini-2.5-pro在语音风格评估中的表现，验证其与人类评价的可比性。  <br/>3. 通过实验证实ALLMs具备评估SLMs输出内容的可行性，为语音AI的评估方法提供新思路。  <br/>4. 揭示当前SLMs在语音风格控制和自然对话生成方面存在局限性，明确技术改进方向。  <br/><br/>**摘要总结（100字以内）：**  <br/>本研究验证了ALLM作为自动评审者评估SLM演讲风格的可行性，发现Gemini与人类一致性相近，同时指出SLMs在语音控制和对话生成仍有提升空间。|
|2506.05899v1|[WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS   Prediction](http://arxiv.org/abs/2506.05899v1)|**贡献点**：  <br/>1. 提出WhisQ多模态架构，首次在文本到音乐系统中同时解决整体音乐质量（OMQ）和文本提示对齐（TA）的双重评估问题。  <br/>2. 集成Whisper Base（音频编码）与Qwen 3（文本编码），利用序列结构保持细粒度跨模态建模能力。  <br/>3. 设计双路径预测机制：OMQ通过池化音频嵌入预测，TA通过音频-文本双向序列共注意力建模。  <br/>4. 引入Sinkhorn最优传输损失，强化共享嵌入空间中的语义对齐，提升跨模态一致性。  <br/>5. 通过消融实验验证了最优传输正则化对性能的关键作用（+10% SRCC），证明显式跨模态对齐的重要性。  <br/><br/>**总结**（100字以内）：  <br/>本研究提出WhisQ架构，结合音频与文本编码，通过共注意力和最优传输优化实现双重评估，显著提升OMQ和TA预测性能，并验证了跨模态对齐对文本到音乐系统的重要性。|
|2506.05891v1|[WAKE: Watermarking Audio with Key Enrichment](http://arxiv.org/abs/2506.05891v1)|**贡献点总结（100字以内）：**  <br/>提出首个密钥可控音频水印框架WAKE，解决未经授权访问、多次嵌入解码及可变长度嵌入问题，提升音频质量和检测准确性。  <br/><br/>**分点贡献：**  <br/>1. **首次引入密钥控制机制**：WAKE通过特定密钥嵌入与恢复水印，显著增强安全性，防止错误密钥解码。  <br/>2. **解决多嵌入覆盖问题**：支持多次嵌入后水印的稳定解码，避免传统方法中的覆盖干扰。  <br/>3. **支持可变长度水印嵌入**：灵活适应不同长度的水印需求，扩展应用范围。  <br/>4. **性能优越**：在水印音频质量与检测精度上超越现有模型，验证了方法的有效性。|
|2506.05851v1|[DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection](http://arxiv.org/abs/2506.05851v1)|总结：  <br/>本文识别了音频-视频DeepFake检测中的数据集和评估问题，提出DeepSpeak v1数据集与新评估协议，并设计SIMBA方法作为简洁基线，分析音频捷径问题并提出缓解策略，最终优化FakeAVCeleb数据集的评估方案以推动领域发展。<br/><br/>贡献点：  <br/>1. 指出现有音频-视频DeepFake检测数据集存在不可复现性和关键缺陷（如FakeAVCeleb的沉默捷径问题）；  <br/>2. 提出DeepSpeak v1数据集，作为更可靠且全面的基准数据资源；  <br/>3. 首次设计并评估一种通用的多模态DeepFake检测评估协议，使用SOTA模型验证有效性；  <br/>4. 引入SImple Multimodal BAseline（SIMBA）方法，提供简明高效的基线模型以支持设计探索；  <br/>5. 深入分析音频捷径问题，提出针对性的缓解策略；  <br/>6. 改进FakeAVCeleb数据集的评估框架，增强其在实际应用中的可靠性。|
|2505.19493v2|[Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation](http://arxiv.org/abs/2505.19493v2)|**分点贡献：**  <br/>1. **提出两阶段算法**：结合声源方向线索与多通道信号处理，优化传统多通道AEC框架。  <br/>2. **轻量级DNN方向预测**：首次使用轻量模型预测声源方向，降低计算开销。  <br/>3. **多模态信号融合**：将预测方向信息、多通道麦克风信号与单通道远端信号联合输入AEC网络，提升恢复效果。  <br/>4. **环境适应性增强**：算法在不同声学场景下表现优异，验证了其鲁棒性和泛化能力。  <br/><br/>**总结：**  <br/>该研究提出一种基于声源方向线索的两阶段多通道AEC算法，通过轻量级DNN预测方向并融合多模态信号，在复杂环境中实现更高效的回声消除。|
|2506.05802v1|[TADA: Training-free Attribution and Out-of-Domain Detection of Audio   Deepfakes](http://arxiv.org/abs/2506.05802v1)|**贡献点总结：**  <br/>1. 提出训练无关的绿色AI方法（基于kNN）用于音频伪造溯源，无需依赖生成模型训练数据。  <br/>2. 利用预训练自监督学习模型，实现跨数据集的高精度（0.93 F1）生成器样本聚类。  <br/>3. 首次在音频领域展示强出域检测能力（0.84 F1），可识别未见过的伪造模型。  <br/>4. 通过多维分析提供深入的理论与实证见解，增强方法的可靠性与泛化性。  <br/>5. 开源代码与数据协议，推动研究复现和透明性。  <br/><br/>**摘要总结（100字内）：**  <br/>本文提出一种训练无关、绿色AI的音频伪造溯源方法，基于kNN和预训练自监督模型，在多数据集上实现高精度聚类与出域检测，并开源代码促进研究可复现性。|
|2506.05593v1|[Improving Neural Diarization through Speaker Attribute Attractors and   Local Dependency Modeling](http://arxiv.org/abs/2506.05593v1)|**贡献点：**  <br/>1. **提出多阶段中间表示建模**：通过分阶段捕捉详细的“说话人属性”（如情感、语速等），而非直接建模说话人，提升说话人分割与识别的细粒度表征能力。  <br/>2. **引入Conformer架构替代Transformer**：利用卷积增强的Transformer模型，更高效建模局部语音特征，增强对多说话人场景的适应性。  <br/>3. **验证方法有效性**：在CALLHOME数据集上开展实验，证明所提方法在讲话人辨识任务中的性能提升。  <br/><br/>**总结（100字以内）：**  <br/>该研究通过多阶段中间表示建模和Conformer架构改进，提出更精细的说话人属性表征方法，并在CALLHOME数据集验证了其有效性，提升了端到端说话人辨识性能。|
|2506.07646v1|[Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation](http://arxiv.org/abs/2506.07646v1)|**贡献点总结：**  <br/>本文提出了一种结合预训练ASR模型与字典先验知识的音素和语调标注方法，构建了日语TTS数据集，解决了传统方法依赖单一模态的局限性，并在客观和主观评估中展现出优越性能。<br/><br/>---<br/><br/>**分点贡献：**  <br/>1. **多模态标注方法创新**：首次提出融合音频与文本信息的联合标注框架，能同时生成音素（phonemic）和语调（prosodic）标签，提升TTS数据集的标注精度和丰富度。  <br/>2. **预训练ASR模型优化**：基于大规模预训练ASR模型，通过条件化真实文本转录，实现对词级字符和标注标签的联合输出，增强模型对语音与文本关联的理解。  <br/>3. **字典驱动的纠错机制**：引入字典先验知识作为解码策略，有效修正音素标注错误，提高标注结果的可靠性。  <br/>4. **数据集构建与评估**：验证了所提方法在日语TTS数据集中的有效性，客观指标优于纯文本/音频方法，主观语音自然度接近人工标注水平。|
|2506.07920v1|[W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](http://arxiv.org/abs/2506.07920v1)|总结（100字以内）:  <br/>提出W4S4，基于冗余小波框架的SSM新类，通过稳定对角化与高效计算，在长序列建模任务中显著优于HiPPO-based SSMs，验证了其有效性，并为下一代深度SSM模型提供基础。<br/><br/>贡献点:  <br/>1. **提出W4S4框架**：基于冗余小波帧构建新的SSM类，替代传统线性递归和卷积结构。  <br/>2. **理论与计算优势**：实现稳定对角化，支持快速核计算，无需低秩近似，提升效率与可解释性。  <br/>3. **长序列建模能力**：在延迟重建任务和分类基准中，比HiPPO-based SSMs保留更长时序信息。  <br/>4. **实验验证广泛性**：通过多种任务（如分类、长序列建模）的实验，证明其性能优越性。  <br/>5. **应用前景**：为下一代深度SSM模型提供可扩展、通用的基础架构。|
|2505.08752v2|[Three Tone Networks and a Tessellation](http://arxiv.org/abs/2505.08752v2)|总结：  <br/>该论文提出将Eulerian tonnetz通过二分图与实射影平面几何配置相结合，揭示调性关系的结构特性，并拓展至五声音乐与十二音音乐的调网构建，为理解历史声部进行提供几何框架。<br/><br/>贡献点：  <br/>1. **理论模型构建**：首次将Eulerian tonnetz与实射影平面的十二点十二线配置建立对应，证明Levi图对调性结构的唯一确定性。  <br/>2. **几何特性解析**：通过该配置直观展示调网中的四主六韵（hexacycles）和三主八韵（octacycles）等关键特征，深化对十九世纪声部进行的理解。  <br/>3. **扩展应用**：提出将类似调网方法应用于五声音乐及十二音音乐，拓宽了传统调性理论的适用范围。|
|2506.07722v1|[Towards a Unified Benchmark for Arabic Pronunciation Assessment: Quranic   Recitation as Case Study](http://arxiv.org/abs/2506.07722v1)|**贡献点总结（100字以内）**  <br/>本文提出首个公开的MSA发音错误检测基准数据集QuranMB.v1，构建了包含数据处理、专用音素集设计及基线模型评估的全流程框架，为阿拉伯语音技术发展提供标准化工具，揭示评估挑战与前景，推动相关研究。<br/><br/>**分点贡献：**  <br/>1. **构建统一基准**：提出首个针对MSA发音错误检测的公开测试数据集QuranMB.v1，作为案例研究。  <br/>2. **全流程数据处理**：开发覆盖数据预处理、特征提取与标注的综合管道，支持发音评估任务。  <br/>3. **专用音素集设计**：创建针对MSA发音特点的专门音素集合，捕捉语言细微差异。  <br/>4. **基线模型评估**：分析多类模型性能，为MSA发音评估提供初始研究成果与挑战分析。  <br/>5. **标准化框架推动研究**：通过统一基准和流程，促进阿拉伯语言技术及发音评估领域的进一步发展。|
|2506.07659v1|[Unified Semi-Supervised Pipeline for Automatic Speech Recognition](http://arxiv.org/abs/2506.07659v1)|总结：  <br/>提出首个全流程开源半监督ASR框架，支持多语言数据集构建与伪标签生成算法TopIPL，实验证明在低/高资源语言中均显著提升识别性能。<br/><br/>贡献点：  <br/>1. **完整开源框架**：构建覆盖「未标注数据收集-伪标签生成-模型训练」的全流程半监督ASR框架  <br/>2. **跨语言可扩展性**：实现任意语言的可扩展数据集生成，利用Creative Commons许可的公共语音数据  <br/>3. **新型伪标签算法**：提出TopIPL伪标签算法，在葡萄牙语（+18-40%）、亚美尼亚语（+5-16%）、西班牙语（+2-8%）等多语言场景验证其有效性|
|2505.20529v3|[Training Articulatory Inversion Models for Interspeaker Consistency](http://arxiv.org/abs/2505.20529v3)|**贡献点总结**  <br/>1. 提出基于最小对集的创新评估方法，提取跨说话者一致的发音目标。  <br/>2. 验证SSL模型在单/多说话人数据上是否能生成统一的发音模板。  <br/>3. 设计仅依赖语音数据的训练方法，提升跨说话者一致性。  <br/>（共99字）|
|2506.07634v1|[SongBloom: Coherent Song Generation via Interleaved Autoregressive   Sketching and Diffusion Refinement](http://arxiv.org/abs/2506.07634v1)|总结：  <br/>SongBloom提出了一种结合自回归草图和扩散模型的框架，通过逐步扩展和整合语义声学上下文，实现了高质量歌曲生成，并超越现有方法，接近商业平台水平。<br/><br/>贡献点：  <br/>1. **提出新型框架**：设计了SongBloom，采用自回归草图生成与扩散模型精炼的交织范式，解决传统方法在全局连贯性与局部保真度的平衡问题。  <br/>2. **融合模型优势**：结合扩散模型的高保真性和语言模型的可扩展性，提升生成歌曲的质量与效率。  <br/>3. **分阶段生成机制**：从短到长逐步扩展音乐草图，从粗到细粒度细化细节，增强结构连贯性与内容完整性。  <br/>4. **上下文整合策略**：通过整合先验语义和声学上下文，指导生成过程以确保音乐性与歌词匹配度。  <br/>5. **性能验证**：在主观和客观指标上超越现有方法，生成质量可比商业音乐生成平台。|
|2506.07536v1|[Bayesian Learning for Domain-Invariant Speaker Verification and   Anti-Spoofing](http://arxiv.org/abs/2506.07536v1)|**贡献点总结（100字以内）**  <br/>本研究提出贝叶斯加权RFN（BWRFN）方法，通过引入权重不确定性建模，解决固定权重RFN在域不匹配下的局限，显著提升ASV和反欺骗性能。<br/><br/>**分点贡献：**  <br/>1. **提出BWRFN方法**：首次结合变分推断与RFN，通过建模权重的后验分布，动态调整频率分量的权重，增强模型对域不匹配的鲁棒性。  <br/>2. **引入权重不确定性**：针对域偏移导致的权重不确定性，采用贝叶斯框架量化频率权重的不确定性，改进传统固定权重策略。  <br/>3. **提升实验效果**：在跨数据集ASV、跨TTS反欺骗及反欺骗鲁棒ASV任务中，BWRFN显著优于WRFN和RFN，验证其有效性。  <br/>4. **域适应优化**：通过时间与通道轴的特征统计归一化，降低特征图的域依赖性，推动语音认证系统的实际应用。|
|2506.07526v1|[Generative Voice Bursts during Phone Call](http://arxiv.org/abs/2506.07526v1)|总结：  <br/>本文提出了一种基于生成式AI的语音突发传输方法，解决通话中紧急消息传递问题，具有上下文感知和优先级推断能力，可跨行业应用。<br/><br/>贡献点：  <br/>1. **提出新型语音传输机制**：设计Generative Voice Bursts系统，允许在通话中接收紧急短音频消息，突破传统呼叫等待的局限。  <br/>2. **融合上下文感知技术**：通过位置、健康数据、图像、环境噪音等多源信息生成语音内容，提升消息的相关性和紧急性。  <br/>3. **引入多模态推理机制**：结合语音、文本和优先级推断，使高优先级消息可绕过常规通话等待限制，保障紧急通信时效性。  <br/>4. **应用生成式AI模型**：采用GPT Neo等文本生成模型，通过语音合成技术实现自动化消息生成与可配置分发（间隔G秒，次数N次）。  <br/>5. **跨行业应用潜力**：方法可应用于电信、移动设备制造和应急通信平台，具备实际落地价值和广泛影响力。|
|2506.07515v1|[Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for   Multi-Talker Speech Recognition](http://arxiv.org/abs/2506.07515v1)|总结：  <br/>提出无需辅助信息的多说话人语音识别框架，结合SD-CTC与SOT实现26%的错误率降低，性能接近依赖辅助信息的前沿方法。<br/><br/>贡献点：  <br/>1. 提出Speaker-Distinguishable CTC（SD-CTC），扩展CTC模型以联合分配token和说话人标签，解决说话人分配失败问题。  <br/>2. 将SD-CTC整合至Serialized Output Training（SOT）框架，仅通过重叠语音和转录实现说话人区分，无需额外辅助信息。  <br/>3. 实验证明多任务学习（SD-CTC + SOT）显著提升SOT模型性能，减少26%的错误率，与依赖辅助信息的SOTA方法效果相当。|
|2506.07494v1|[Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A   Proposal for Offline Speech Recognition and IoT Integration](http://arxiv.org/abs/2506.07494v1)|总结：  <br/>本文提出基于离线语音识别和本地物联网的智能家居方案，通过本地化关键词检测和去中心化网络架构，解决了云端部署的能耗、延迟和单点故障问题，实现高效、可持续的语音控制。<br/><br/>贡献点：  <br/>1. **本地化语音识别整合**：提出将离线关键词检测（KWS）技术嵌入资源受限的家用设备，使其独立识别用户指令，减少对互联网的依赖。  <br/>2. **去中心化物联网架构**：设计本地物联网网络，通过分布式架构提升系统鲁棒性、可扩展性，并降低通信延迟和能耗。  <br/>3. **端到端低延迟控制**：实现全屋无网络依赖的低延迟语音交互，增强用户体验和能源效率。  <br/>4. **可持续系统优化**：通过离线处理与本地网络减少数据传输需求，提升整体能源可持续性。|
|2506.07473v1|[An introduction to pitch strength in contemporary popular music analysis   and production](http://arxiv.org/abs/2506.07473v1)|**贡献点：**  <br/>1. 提出音高强度（pitch strength）作为当代流行音乐中关键的低级感知参数，可能提升生成式AI模型在音乐生产中的适用性。  <br/>2. 通过信号和感知分析，证明音高强度在歌曲内部及跨歌曲间存在显著变化，为音乐结构建模提供依据。  <br/>3. 揭示音高强度对音乐中小尺度（如旋律）和大尺度（如段落）结构的共同贡献。  <br/>4. 证明音高强度在处理多声部不协和音时的作用，增强AI模型对复杂音乐音色的控制能力。  <br/>5. 指出音高强度可能涉及上泛音在感知丰富度视角下的可听性，拓展对音乐感知机制的理解。  <br/><br/>**总结（100字以内）：**  <br/>本研究通过分析音高强度的多维特性，揭示其在音乐结构、多声部处理及感知丰富度中的关键作用，为生成式AI音乐模型与实际生产的结合提供理论支持。|
|2506.02499v2|[DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds](http://arxiv.org/abs/2506.02499v2)|**贡献点：**  <br/>1. **提出首个针对电影音频的非语音声音数据集（DnR-nonverbal）**：包含笑声、尖叫等情绪化非语音声源，弥补现有CASS数据集仅含阅读式语音的不足。  <br/>2. **定义新的语音主干（speech stem）标准**：将非语音声源（如笑声、尖叫）纳入语音主干，更贴近实际影视音频场景。  <br/>3. **揭示当前CASS模型的局限性**：通过实验验证模型对情绪化非语音声源的分割错误，并证明新数据集能有效解决该问题。  <br/>4. **公开数据集以促进研究**：提供数据集下载链接，支持合成与真实电影音频的进一步研究与评估。  <br/><br/>**总结（100字内）：**  <br/>本研究提出DnR-nonverbal数据集，专为处理电影音频中的非语音声源（如笑声、尖叫），弥补现有数据集的不足，并通过实验验证其对提升情感化语音分割性能的有效性，数据集已开放获取。|
|2506.07358v1|[Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream   Multi-Modal Learning Framework](http://arxiv.org/abs/2506.07358v1)|**贡献点：**<br/>1. 提出单流多模态学习框架替代传统双流分离模型，减少冗余神经层，提升模型效率；  <br/>2. 设计协作音频-视觉学习块，实现跨层持续融合，高效捕捉多模态特征；  <br/>3. 引入多模态分类模块，增强分类器对模态内容的依赖性与对跨模态不匹配的鲁棒性；  <br/>4. 在DF-TIMIT、FakeAVCeleb和DFDC数据集上验证，参数量仅0.48M，相较SOTA方法显著轻量化，且对单/多模态及未见过的Deepfake均表现优异。  <br/><br/>**总结（100字以内）：**  <br/>本文提出轻量级单流多模态网络框架，通过协作学习块与分类模块提升音频-视觉Deepfake检测效率与鲁棒性，相较主流方法参数减少78.6%，在多种数据集和Deepfake类型中均取得优异性能。|
|2506.07294v1|[Towards Generalized Source Tracing for Codec-Based Deepfake Speech](http://arxiv.org/abs/2506.07294v1)|**贡献点：**  <br/>1. **揭示问题**：指出基于CoSG数据训练的源追溯模型易过拟合非语音区域且泛化能力差。  <br/>2. **提出方法**：设计SASTNet，联合使用Whisper（语义编码）和Wav2vec2+AudioMAE（声学编码）实现多模态特征融合。  <br/>3. **验证效果**：在CodecFake+数据集上取得SOTA性能，证明其在真实CoSG生成语音中的有效性。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出SASTNet，通过整合语义与声学特征编码解决深伪语音源追溯问题，在真实数据上实现最佳性能，推动语音伪造检测技术发展。|
|2506.07237v1|[Multi-Distillation from Speech and Music Representation Models](http://arxiv.org/abs/2506.07237v1)|贡献点总结（100字以内）: <br/>提出跨领域蒸馏框架，统一语音与音乐模型并减小规模，验证其在多样化任务中的有效性及少样本场景下的优越性，证明跨领域方法在数据有限场景的必要性。<br/><br/>分点贡献：<br/>1. **跨领域模型统一**：首次将语音（HuBERT）与音乐（MERT）模型整合为单一模型，解决现实音频混合场景的处理问题。<br/>2. **模型压缩创新**：通过多导师蒸馏显著降低模型体积，实现性能与效率的平衡。<br/>3. **领域平衡策略**：探索多种方法协调语音与音乐领域的知识迁移，提升跨域泛化能力。<br/>4. **任务兼容性验证**：在多样化任务中证明模型性能可媲美领域专用模型，展示跨域蒸馏的广泛适用性。<br/>5. **少样本学习优势**：在标签数据稀缺场景下，模型表现优于专用模型，凸显通用模型的实用性。|
|2506.07233v1|[Reducing Object Hallucination in Large Audio-Language Models via   Audio-Aware Decoding](http://arxiv.org/abs/2506.07233v1)|总结：  <br/>提出Audio-Aware Decoding（AAD）方法，通过对比解码减少LALMs音频幻觉，实验证明在多个数据集上显著提升性能，并通过消融研究验证方法有效性。<br/><br/>贡献点：  <br/>1. 提出**Audio-Aware Decoding（AAD）**，一种轻量级推理策略，利用对比解码对比音频上下文与无上下文的token预测logits，有效抑制音频幻觉问题。  <br/>2. 理论上通过对比解码机制，**增强音频存在时概率提升的token**，提升模型对音频信息的敏感性与准确性。  <br/>3. 在**object hallucination数据集**上实验证明，AAD将F1分数提升0.046至0.428，显著优于现有方法。  <br/>4. 在**Clotho-AQA等通用音频QA数据集**中验证，AAD使准确率提升5.4%-10.3%。  <br/>5. 开展**全面的消融研究**，系统分析AAD各组件对性能的影响，提供方法优化依据。|
|2506.07207v1|[Methods for pitch analysis in contemporary popular music: Vitalic's use   of tones that do not operate on the principle of acoustic resonance](http://arxiv.org/abs/2506.07207v1)|总结：  <br/>该论文通过分析Vitalic音乐中不谐和音调的创作手法，提出其在音乐领域中的独特贡献，并探讨类似特性在当代流行音乐中的普遍性，为声音结构研究提供新视角。<br/><br/>贡献点：  <br/>1. **解析音乐中的非谐和音调结构**：以Vitalic的《No Fun》为例，首次系统分析电子音乐中通过单不谐和音生成多旋律的合成技术。  <br/>2. **提出双音高同时感知的理论框架**：研究不谐和音调如何在音乐中形成两个或多个独立音高，突破传统声学共振的音高生成模式。  <br/>3. **跨案例验证通用性**：通过扩展分析其他音乐作品的相似现象，证明不谐和音调的多音高特性在当代流行音乐中的广泛存在。  <br/>4. **结合音乐创作启发声音研究**：将电子音乐中的音色设计方法关联到语音领域的声学分析，为非传统语音信号处理提供参考。  <br/><br/>（注：原文内容未明确指向语音领域，但若按“语音领域”要求解读，贡献点可能需假设其与声音结构、音高感知或语音合成技术相关联。）|
|2506.07199v1|[Audio synthesizer inversion in symmetric parameter spaces with   approximately equivariant flow matching](http://arxiv.org/abs/2506.07199v1)|贡献点总结：  <br/>提出音频合成参数逆向问题源于内在对称性，尤其是排列不变性，设计条件生成模型及排列等变连续归一化流解决该问题，并提出自适应发现对称性的放松策略，实验验证在真实合成器中优于传统方法。<br/><br/>分点贡献：  <br/>1. **揭示对称性根源**：首次明确音频合成器的信号-参数逆向问题主要由其内在对称性（如排列不变性）引起。  <br/>2. **对比实验分析**：证明传统回归方法在排列对称性下性能不佳，即使采用对称性破缺策略或不变损失函数仍存在局限。  <br/>3. **条件生成模型**：将等效解视为概率分布模式，通过条件生成模型显著提升音频重建效果。  <br/>4. **排列等变流**：引入排列等变的连续归一化流，进一步优化隐式参数分布的建模，提升性能。  <br/>5. **自适应对称性学习**：提出放松等变策略，从数据中自适应识别关键对称性以适应复杂合成器结构。  <br/>6. **实际应用验证**：在真实合成器Surge XT上验证方法有效性，证明其优于回归与生成模型基准。|
|2506.07149v1|[Technical Report: A Practical Guide to Kaldi ASR Optimization](http://arxiv.org/abs/2506.07149v1)|总结（100字以内）:  <br/>本文提出三种创新优化策略：声学模型中Conformer与多流TDNN-F结合、动态超参数调整、以及基于贝叶斯优化和n-gram剪枝的语言模型管理，显著提升Kaldi ASR系统的准确性、鲁棒性和可扩展性。<br/><br/>贡献点：<br/>1. **声学模型增强**  <br/>   - 设计集成多流TDNN-F结构的自定义Conformer模块，提升特征提取能力与时间建模性能。<br/><br/>2. **超参数优化**  <br/>   - 引入动态超参数调整技术，结合先进数据增强方法，增强模型泛化能力并减少过拟合。<br/><br/>3. **语言模型效率提升**  <br/>   - 提出基于贝叶斯优化的语言模型管理策略，结合n-gram剪枝技术，在保持相关性的同时提升计算效率。|
|2506.07118v1|[RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression   Diagnosis](http://arxiv.org/abs/2506.07118v1)|**总结（100字以内）:**  <br/>提出脑启发的RBA-FE模型，结合改进的分层网络与ARSLIF神经元，提升抑郁症诊断的噪声鲁棒性与特征提取精度，实验验证在多个数据集上的优越性能。<br/><br/>**贡献点分点列出:**  <br/>1. **提出RBA-FE模型**：设计了一种基于改进分层网络的脑启发音频特征提取器，专门针对抑郁症诊断任务。  <br/>2. **多维度声学特征提取**：从原始音频中提取六个声学特征，同时捕捉空间特性和时间依赖性，增强特征表示能力。  <br/>3. **改进神经元模型ARSLIF**：引入自适应速率光滑泄漏积分-火（ARSLIF）神经元，模拟大脑信号选择性调节机制，提升对环境噪声的鲁棒性。  <br/>4. **实验验证有效性**：在MODMA、AVEC2014和DAIC-WOZ数据集上验证模型性能，达到或超越现有方法的准确率（0.8750-0.8974）。  <br/>5. **增强模型可解释性**：通过对比实验表明，ARSLIF模型可揭示抑郁症音频中的异常放电模式，提供脑机制驱动的解释性分析。|
|2506.07078v1|[E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech   Foundation Models](http://arxiv.org/abs/2506.07078v1)|总结：  <br/>该论文提出E-BATS框架，通过三个创新组件解决语音模型在域移场景下的适应问题，实现效率与性能的平衡，并在多个数据集上验证了其有效性。<br/><br/>贡献点：  <br/>1. **首次提出无需反向传播的语音TTA框架**（E-BATS），专为语音基础模型设计，解决现有方法在资源受限场景下的内存问题。  <br/>2. **引入轻量级提示适应机制**，通过前向传播实现特征对齐，降低计算开销并保持适应效果。  <br/>3. **设计多尺度损失函数**，同时捕捉全局（语句级）和局部（token级）声学域移特征。  <br/>4. **开发测试时指数移动平均机制**，提升跨语句适应的稳定性。  <br/>5. **实验验证有效性**，在16种声学条件下的4个噪声数据集上，相比无反向传播基线提升4.1%-13.5%准确率，且内存消耗降低2.0-6.4倍。  <br/>6. **推动实际应用**，为构建高效、鲁棒的语音处理系统提供可扩展的解决方案。|
|2506.07073v1|[Insights on Harmonic Tones from a Generative Music Experiment](http://arxiv.org/abs/2506.07073v1)|贡献点：  <br/>1. **提出studio-lab作为跨学科协作框架**：构建研究者、音乐制作人与AI模型的协同实验模式，推动音乐生成技术与音乐实践的深度融合。  <br/>2. **揭示AI生成复音和声的多音高表达能力**：发现音乐制作人利用AI生成的复音和声传达多个音高，表明模型具备解析和生成复杂音乐结构的潜力。  <br/>3. **证明AI的多声部旋律生成机制**：通过实验验证AI能基于单音复音序列生成结构化、连贯的多声部旋律，突破传统单音生成的局限性。  <br/>4. **引发对人类音乐感知理论的再思考**：挑战长期存在的“人类能否感知和声为独立音高”的争议，为音乐认知研究与AI生成能力评估提供新视角。  <br/><br/>总结：  <br/>本研究通过跨学科实验揭示AI在音乐生成中的复杂能力，推动音乐创作与声学理论的交叉发展，为感知机制和生成技术提供了新证据。|
|2506.07036v1|["In This Environment, As That Speaker": A Text-Driven Framework for   Multi-Attribute Speech Conversion](http://arxiv.org/abs/2506.07036v1)|总结：  <br/>提出TES-VC框架，实现文本驱动下独立控制说话人音色与环境音效，通过合成数据与潜扩散模型解耦特征，引入检索式音色控制模块，提升生成语音的准确性和内容保留能力，验证其在语音转换任务中的高效性与应用潜力。<br/><br/>贡献点：  <br/>1. **提出TES-VC框架**：首次实现文本驱动的语音转换，支持独立控制说话人音色与环境音效的生成。  <br/>2. **解耦特征处理**：利用合成数据与潜在扩散模型分离语音和环境特征，消除属性间干扰。  <br/>3. **无需配对数据的音色控制**：引入检索式音色控制（RBTC）模块，通过抽象描述精准操控音色。  <br/>4. **内容保留能力**：在生成目标语音时保持源内容不变，确保语义一致性。  <br/>5. **实验验证有效性**：通过对比实验证明方法在音色、环境和内容保留上的优势，展示广泛应用前景。|
|2506.06888v1|[Automatic Speech Recognition of African American English: Lexical and   Contextual Effects](http://arxiv.org/abs/2506.06888v1)|总结（100字以内）:  <br/>该研究分析了AAE中的CCR和ING减少对ASR性能的影响，比较了端到端系统有无LM对词汇邻近效应和上下文预测性的敏感度差异，为改进ASR在AAE场景下的表现提供了新洞见。<br/><br/>贡献点:  <br/>1. **识别关键AAE变量**：首次系统分析了非裔美国人英语中的辅音簇缩减（CCR）和ING减少对自动语音识别（ASR）准确率的具体影响。  <br/>2. **量化语音识别误差**：通过实验验证CCR和ING减少对词错误率（WER）存在显著但微小的影响，明确了其在ASR中的干扰程度。  <br/>3. **对比LM作用差异**：揭示端到端ASR系统在无外部语言模型（LM）时更易受词汇邻近效应影响，而对上下文预测性依赖更低。  <br/>4. **方法创新**：提出结合wav2vec 2.0与蒙特利尔强制对齐器（MFA）发音扩展的标注流程，提升AAE语音特征检测的可靠性。  <br/>5. **实际应用指导**：为优化ASR在AAE等方言场景下的性能提供了理论依据和系统设计参考。|
|2506.06834v1|[Rhythm Features for Speaker Identification](http://arxiv.org/abs/2506.06834v1)|**贡献点：**  <br/>1. **提出基于节奏的说话人识别新方法**：首次将语音信号中的节奏信息（时间结构）作为高阶特征，通过深度学习模型进行文本无关的说话人身份识别。  <br/>2. **验证节奏特征的有效性**：实验证明节奏信息对说话人识别具有显著作用，支持其作为潜在的语音身份特征的重要性。  <br/>3. **揭示实际应用挑战**：指出非特定语境下（如自发性语音）语音的高内在变异性可能削弱节奏特征的识别效果，为后续研究提供方向。  <br/><br/>**总结（100字内）：**  <br/>本文提出基于节奏特征的文本无关说话人识别方法，验证其有效性并揭示自发性语音中节奏特征的局限性，为语音身份识别提供了新视角和改进方向。|
|2506.06772v1|[SynHate: Detecting Hate Speech in Synthetic Deepfake Audio](http://arxiv.org/abs/2506.06772v1)|总结：  <br/>本研究提出首个多语言合成音频仇恨言论检测数据集SynHate，涵盖37种语言，采用新颖的四分类框架，并评估主流自监督模型性能，推动跨语言、文化敏感的对抗合成仇恨言论解决方案。<br/><br/>贡献点：  <br/>1. **首个多语言数据集**：构建SynHate，支持37种语言，涵盖全球和印度的多样性仇恨言论模式。  <br/>2. **创新四分类框架**：提出Real-normal、Real-hate、Fake-normal、Fake-hate四类标签，提升仇恨言论检测的细粒度。  <br/>3. **跨语言模型评估**：系统评估Whisper-small/medium、XLS-R、AST、mHuBERT等五种自监督模型，揭示语言差异对性能的影响。  <br/>4. **性能对比与发现**：验证Whisper-small在整体表现上最优，同时指出跨数据集泛化能力的不足。  <br/>5. **开源促进研究**：发布数据集及基线代码，推动构建更鲁棒、文化敏感的多语言合成音频检测解决方案。|
|2506.06756v1|[Can Quantized Audio Language Models Perform Zero-Shot Spoofing   Detection?](http://arxiv.org/abs/2506.06756v1)|**分点贡献：**  <br/>1. **系统性评估**：首次系统研究五种大音频语言模型（GAMA、LTU-AS、MERaLiON、Qwen-Audio、SALMONN）在零样本音频欺骗检测任务中的性能表现。  <br/>2. **量化鲁棒性分析**：全面分析FP32、FP16、INT8不同精度量化对模型性能的影响，揭示量化与任务复杂度之间的关系。  <br/>3. **预测偏差发现**：指出所有模型在量化后均存在严重的预测偏差，导致实际性能等同于随机分类，暴露模型架构的局限性。  <br/>4. **效率与准确性的权衡**：证明FP16量化在显著降低内存和计算需求的同时，对检测准确率影响可忽略，为实际部署提供关键参考。  <br/>5. **INT8的负面影响**：揭示INT8量化会导致平衡准确率显著下降，强调精度降低对模型鲁棒性的潜在危害。  <br/>6. **实践指导与优化方向**：基于研究结果，提出FP16量化作为最优折中方案，为模型部署和未来改进提供实证依据。  <br/><br/>**总结（100字内）：**  <br/>本研究系统评估五种大音频语言模型在零样本欺骗检测中的性能，揭示量化对模型鲁棒性的关键影响，发现FP16能在保持性能的同时显著降低资源消耗，而INT8则加剧偏差，为高效部署提供理论支持和实践建议。|
|2506.06732v1|[Neural Spectral Band Generation for Audio Coding](http://arxiv.org/abs/2506.06732v1)|**贡献点：**  <br/>1. **提出非盲音频带宽扩展新范式**：首次将非盲BWE（Non-blind BWE）与深度神经网络（DNN）结合，突破传统SBR方法依赖粗糙特征提取的局限性。  <br/>2. **优化编码流程协作机制**：通过在音频编码管道的前端和末端分别部署DNN侧信息提取与带宽扩展模块，实现更高效的信号处理协同。  <br/>3. **提升多类型音频信号处理能力**：利用DNN对低频信号进行更精确的分析，改进对各类音频信号（如复杂音色、噪声等）的高频频段重构效果。  <br/>4. **降低对原始信号先验信息的依赖**：仅基于低频信号直接估计高频内容，减少对传统SBR中高层特征的依赖，增强方法通用性。  <br/>5. **解决盲BWE的性能瓶颈**：通过非盲框架优化DNN模型，避免盲BWE因忽略原始信号信息导致的重构质量下降问题。  <br/><br/>**总结：**  <br/>本文提出一种基于DNN的非盲音频带宽扩展方法，通过优化编码流程协作机制，提升多类型音频信号的高频频段重构性能，解决传统SBR与盲BWE的局限性。|
|2506.06675v1|[Accurate analysis of the pitch pulse-based magnitude/phase structure of   natural vowels and assessment of three lightweight time/frequency voicing   restoration methods](http://arxiv.org/abs/2506.06675v1)|总结：  <br/>本文提出新型算法与三种模型实现方案，解决 whispered speech 到自然语音的转换问题，通过分析元音差异和对比听觉测试验证方法有效性。<br/><br/>贡献点：  <br/>1. **提出新型 pitch 脉冲分割算法**：用于表征自然语音中 voiced 区域的谐波相位/幅度结构，揭示持续元音与共articulated 元音的关键差异。  <br/>2. **设计三种合成语音实现方案**：包括频域重构、联合时频域重构，以及基于生理机制的单脉冲滤波方法，覆盖不同信号处理路径。  <br/>3. **系统性对比与验证**：通过客观示例和主观听觉测试（词上下文场景）对三种方法进行性能评估，为技术落地提供实证支持。|
|2506.06603v1|[CAtCh: Cognitive Assessment through Cookie Thief](http://arxiv.org/abs/2506.06603v1)|总结：  <br/>该研究首次将ADRD预测模型扩展应用于认知障碍（CI）预测，对比了多模态与单模态方法的性能差异，证实声学特征优于语言学特征，并提出了可解释声学特征在情感和语调分析中的优势，同时公开了研究代码。<br/><br/>贡献点：  <br/>1. **模型扩展应用**：首次将基于自发性语音的ADRD预测算法应用于更广泛的认知障碍（CI）识别，揭示CI作为ADRD前兆的潜在价值。  <br/>2. **多模态方法优势验证**：实验证明多模态情感分析方法在CI预测任务中优于单模态方法，为多模态研究提供实证支持。  <br/>3. **声学与语言学性能对比**：发现声学特征（如情感、语调）在预测CI中表现优于语言学特征（如BERT），探索声学模态的潜力。  <br/>4. **可解释特征的显著效果**：特定可解释声学特征（与情感和语调相关）在CI预测中显著优于BERT等语言学方法，推动特征选择研究。  <br/>5. **开放代码与可复现性**：提供完整代码库，促进研究复现和社区协作，提升科学验证的透明度。|
|2506.06566v1|[AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech   Recognition](http://arxiv.org/abs/2506.06566v1)|总结：  <br/>提出轻量化的失语症语音识别框架AS-ASR，结合混合训练与GPT-4增强方法，显著提升识别性能并保持标准语音识别能力，适用于边缘设备部署。<br/><br/>贡献点：  <br/>1. **提出AS-ASR框架**：基于Whisper-tiny构建，专为边缘设备低资源部署设计，实现轻量化与高效性。  <br/>2. **混合训练策略**：通过动态调整标准与失语症语音的训练比例，提升模型对失语症数据的泛化能力。  <br/>3. **GPT-4参考增强方法**：利用大规模语言模型优化失语症转录的监督质量，降低噪声干扰。  <br/>4. **实验验证与效果**：在多种数据混合配置和评估场景下，验证模型性能，实现失语症语音WER降低超30%。  <br/>5. **可扩展解决方案**：为实际应用提供通用、高效的失语症语音识别方案，兼顾标准与失语症语音处理。|
|2506.06537v1|[Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by   Connecting Pretrained Models](http://arxiv.org/abs/2506.06537v1)|总结：  <br/>提出零样本音频视觉分割框架，通过多模态预训练模型集成实现无需标注的精准分割，验证了跨模态融合在细粒度任务中的有效性。<br/><br/>贡献点：  <br/>1. 提出首个无需AVS专用标注的零样本框架，突破传统依赖像素级标注的局限；  <br/>2. 首创整合音频、视觉与文本多模态表示的方法，有效弥合模态间语义鸿沟；  <br/>3. 系统探索多预训练模型连接策略，为零样本AVS提供可复用的架构设计；  <br/>4. 在多数据集验证框架效果，达到当前最优的零样本分割性能。|