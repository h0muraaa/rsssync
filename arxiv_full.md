|Source|Title|Summary|
|---|---|---|
|2510.23530v1|[Learning Linearity in Audio Consistency Autoencoders via Implicit   Regularization](http://arxiv.org/abs/2510.23530v1)|总结：  <br/>提出通过数据增强诱导音频自编码器的线性潜在空间，实现编码器和解码器的线性行为，同时保持重建保真度，并成功应用于音乐创作与分离任务，为结构化音频处理提供新方法。<br/><br/>贡献点：  <br/>1. **方法创新**：设计了一种无需改变模型架构或损失函数的训练方法，通过数据增强诱导高压缩一致性自编码器（CAE）的潜在空间线性化。  <br/>2. **理论特性**：在保持重建质量的前提下，使编码器具备同构性（对数标量增益的等变性）和解码器具备可加性（保留加法操作），突破传统非线性潜在空间的限制。  <br/>3. **应用验证**：成功将线性潜在空间应用于音乐源创作和分离任务，通过简单的潜在空间算术验证了其在实际场景中的实用性与高效性。  <br/>4. **结构化空间构建**：为音频处理任务提供了一种直观、结构化的潜在空间构建方式，简化了操作复杂度并提升了处理效率。|
|2510.23403v1|[Evaluation of Spherical Wavelet Framework in Comparsion with Ambisonics](http://arxiv.org/abs/2510.23403v1)|**总结（100字以内）:**  <br/>本研究系统评估了球面小波框架（SWF）与Ambisonics的差异，通过客观指标（IACC, ITD, ILD）和听觉测试验证SWF在空间音色保真度上的优势，同时提出针对其局限性的改进方案。<br/><br/>**贡献点分点列出:**  <br/>1. **引入SWF与Ambisonics的系统比较**  <br/>   - 通过IACC、ITD、ILD估计和生态有效声源的听觉测试，量化评估两种空间音频技术的性能差异。<br/><br/>2. **多场景播放布局的适应性分析**  <br/>   - 评估正多面体、t-design、Lebedev网格等多种空间音频重建布局，结合不同Ambisonics阶数和通道数进行对比研究。<br/><br/>3. **揭示SWF的局限性及改进方向**  <br/>   - 发现SWF对球面细分的依赖性，以及无法原生表示连续方向的波的问题，提出潜在解决方案以优化其性能。<br/><br/>4. **补充感知度量研究**  <br/>   - 首次将感知指标纳入SWF评估体系，弥补了此前仅依赖特定条件实验的不足，增强结果的可信度和普适性。|
|2510.23320v1|[LibriConvo: Simulating Conversations from Read Literature for ASR and   Diarization](http://arxiv.org/abs/2510.23320v1)|总结：  <br/>本文提出LibriConvo，一个基于真实对话动态的多说话人语料库，通过语义连贯、时空合理性设计及声学真实性增强，为说话人分离和ASR研究提供高质量资源，并验证了其在模型性能上的优势。<br/><br/>贡献点：  <br/>1. 提出LibriConvo数据集，基于SASC方法构建，支持多说话人语音处理系统的训练与评估。  <br/>2. 突破传统数据集局限，通过语义连贯对话和现实时间间隔设计提升数据真实性。  <br/>3. 引入外部VAD、压缩处理及按书籍组织语句，增强对话边界可靠性与上下文一致性。  <br/>4. 开发新颖的房间脉冲响应选择方法，平衡声学现实与说话人-麦克风配置多样性。  <br/>5. 提供大规模数据（240.1小时，1496对话，830说话人）并按说话人分离方式划分，便于鲁棒性评估。  <br/>6. 验证模型效果，Sortformer在说话人分离任务中优于pyannote，而优化后的Fast Conformer-CTC XLarge模型在ASR中达到7.29% WER，超越零样本Whisper-large-v3。|
|2510.23319v1|[Arabic Little STT: Arabic Children Speech Recognition Dataset](http://arxiv.org/abs/2510.23319v1)|总结（100字以内）:  <br/>本文提出首个Levantine阿拉伯语儿童语音数据集Arabic Little STT，评估Whisper模型在儿童语音上的表现，并揭示其与成人数据的显著差异，呼吁建立儿童语音基准和伦理框架，推动阿拉伯语儿童的语音技术公平性发展。<br/><br/>贡献点:  <br/>1. **构建首个阿拉伯语儿童语音数据集**：收录288名6-13岁儿童的355句课堂录音，填补阿拉伯语低资源语言儿童语料匮乏的空白。  <br/>2. **系统评估Whisper模型性能差异**：比较不同变体在儿童与成人数据上的表现，发现最佳模型（Large_v3）在儿童语音上WER达0.66，远高于成人数据的0.20。  <br/>3. **验证儿童语音识别通用性挑战**：结果与其他英语研究一致，表明儿童语音在跨语言ASR中存在显著困难，强调需针对性改进。  <br/>4. **提出伦理与隐私保护框架**：呼吁建立严格的数据治理规范，以保障儿童敏感信息的安全，推动负责任的AI开发。  <br/>5. **推动阿拉伯语语音技术公平性**：通过公开数据集促进儿童群体的代表性，为未来更均衡的语音技术研究提供基础资源。|
|2510.23158v1|[Matching Reverberant Speech Through Learned Acoustic Embeddings and   Feedback Delay Networks](http://arxiv.org/abs/2510.23158v1)|**贡献点：**  <br/>1. 提出将盲估计人工混响参数转化为**混响信号匹配任务**，结合**学习的房间声学先验**，解决无显式测量的混响生成问题。  <br/>2. 设计了**反馈延迟网络（FDN）结构**，可同时模拟目标空间的**频率依赖衰减时间**和**直接-混响比**。  <br/>3. 超越传统自动FDN调参方法，在**估计参数准确性**和**人工混响语音的感知合理性**方面取得提升。  <br/>4. 验证了方法在**AAR应用中的效率与感知一致性**，为实时混响渲染提供了新方案。  <br/><br/>**总结（100字以内）：**  <br/>本文提出基于房间声学先验的盲估计方法与新型FDN结构，提升无测量条件下的混响参数估计与感知合理性，为AAR系统实现高效、逼真的实时混响渲染提供了创新性解决方案。|
|2510.23096v1|[TwinShift: Benchmarking Audio Deepfake Detection across Synthesizer and   Speaker Shifts](http://arxiv.org/abs/2510.23096v1)|**贡献点总结**（100字以内）:  <br/>提出TWINSHIFT基准测试，通过六种合成系统和不重叠说话人评估检测泛化能力，揭示深度伪造检测的鲁棒性缺口，为系统开发提供理论指导，并公开数据集促进社区研究。<br/><br/>**分点贡献**:<br/>1. **提出新型基准**：构建TWINSHIFT基准，专门评估音频深度伪造检测在未见过合成方法和说话人下的鲁棒性。<br/>2. **多合成系统+不重叠说话人设计**：使用六个独立合成系统，搭配互不重叠的说话人数据集，增强检测泛化性评估的严格性。<br/>3. **揭示鲁棒性缺口**：通过实验发现现有检测系统在面对生成模型与说话人身份变化时的关键局限性。<br/>4. **提供开发指导**：基于评估结果提出系统设计原则，指导未来音频深度伪造检测（ADD）技术的改进方向。<br/>5. **开源可复现**：开放数据集和代码，推动领域研究与技术验证。|
|2510.22950v1|[DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow   Matching](http://arxiv.org/abs/2510.22950v1)|**贡献点总结：**  <br/>1. **提出半自回归架构**：基于块流匹配，解决歌词与歌声对齐问题，无需外部约束，保留NAR模型的高质量与效率。  <br/>2. **设计音乐变分自编码器**：实现低帧率（5Hz）的高保真音频重建，保障长序列生成的计算可行性。  <br/>3. **创新跨对偏好优化方法**：替代传统多模型合并策略，避免RLHF中的性能下降，提升对多音乐偏好的适应性。  <br/>4. **引入随机块表示对齐损失**：增强生成歌曲的音乐性与结构一致性，优化整体音质与节奏协调性。  <br/><br/>（100字内）  <br/>本文提出DiffRhythm 2框架，通过半自回归架构解决歌词对齐问题，设计音乐VAE提升生成效率，创新跨对偏好优化避免性能损耗，并引入随机块对齐损失增强音乐性与结构一致性。|
|2510.22795v1|[SAO-Instruct: Free-form Audio Editing using Natural Language   Instructions](http://arxiv.org/abs/2510.22795v1)|**总结（100字以内）**  <br/>本研究提出了SAO-Instruct模型，支持通过任意自然语言指令编辑音频，构建了音频编辑三元组数据集，并在真实场景和新指令中展现良好泛化能力，显著优于现有方法，同时开源代码与模型权重促进后续研究。<br/><br/>**贡献点**  <br/>1. **提出SAO-Instruct模型**：基于Stable Audio Open，实现通过自由形式自然语言指令编辑音频，突破传统方法需完整描述或预定义指令的限制。  <br/>2. **构建音频编辑三元组数据集**：利用Prompt-to-Prompt、DDPM inversion及手动编辑流程生成高质量训练数据，填补自然语言音频编辑领域的数据空白。  <br/>3. **提升泛化能力**：模型虽部分依赖合成数据，但能有效适应真实场景中的未标注音频及未见过的编辑指令，增强实用性。  <br/>4. **验证优异性能**：在客观指标和主观听觉测试中均表现突出，优于现有音频编辑方法，证明其有效性。  <br/>5. **开源促进研究**：开放代码与模型权重，推动语音领域的进一步探索与技术迭代。|
|2510.22682v1|[SRP-PHAT-NET: A Reliability-Driven DNN for Reverberant Speaker   Localization](http://arxiv.org/abs/2510.22682v1)|总结（100字以内）:  <br/>提出SRP-PHAT-NET框架，结合SRP-PHAT方向图与内置可靠性评估，采用高斯加权标签训练并分析标签平滑影响，调整核宽度以适应不同应用需求，实验验证高置信预测提升定位准确性。<br/><br/>贡献点:  <br/>1. **提出SRP-PHAT-NET框架**：首次将SRP-PHAT方向图作为空间音频的深度学习特征，并集成内置可靠性评估机制，解决传统方法缺乏预测可靠性判断的问题。  <br/>2. **高斯加权标签训练**：通过中心化真实方向的高斯权重标签训练模型，增强可靠性评分的可解释性与实用性。  <br/>3. **参数调优分析**：系统研究高斯核宽度对精度和可靠性的调控作用，证明其可适应不同应用场景需求。  <br/>4. **可靠性提升定位性能**：实验表明，筛选高置信度预测可显著提高DOA估计算法在混响环境中的定位准确性。|
|2510.22588v1|[UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations   for Spoken Dialogue Models](http://arxiv.org/abs/2510.22588v1)|总结（100字以内）:  <br/>提出UltraVoice数据集，支持多维度语音风格控制，并通过微调提升模型的风格可控性与对话能力，验证其在TTS和语音交互任务中的广泛适用性。<br/><br/>贡献点:  <br/>1. **首个大规模多维语音风格数据集**：UltraVoice涵盖830小时对话，覆盖情绪、速度、音量、口音、语言和复合风格六大维度，填补细粒度风格控制的空白。  <br/>2. **提升模型风格生成能力**：通过数据集微调SLAM-Omni和VocalNet，显著增强细粒度风格控制效果（MOS提升29.12-42.33%，IFR提升14.61-40.09%），且保留对话核心能力。  <br/>3. **验证综合性能提升**：在URO-Bench基准测试中，微调模型在基础（+10.84%）和进阶（+7.87%）任务中均表现优异，证明其在理解、推理与对话能力上的改进。  <br/>4. **扩展至TTS领域**：数据集可用于训练可控TTS模型，凸显其高质量与跨任务适用性，推动表达性语音合成发展。|
|2510.22439v1|[PromptReverb: Multimodal Room Impulse Response Generation Through Latent   Rectified Flow Matching](http://arxiv.org/abs/2510.22439v1)|**贡献点总结：**  <br/>1. 提出两阶段生成框架PromptReverb，解决全频带RIR数据稀缺问题。  <br/>2. 结合VAE与条件扩散变压器模型，实现多模态输入的声学准确性生成。  <br/>3. 通过实验验证生成RIR的感知质量与声学参数真实性显著优于现有方法。  <br/>4. 支持虚拟现实、建筑声学和音频生产等实际应用场景的灵活应用。  <br/><br/>**摘要总结（100字内）：**  <br/>PromptReverb通过两阶段框架解决RIR生成中的数据局限与多模态挑战，结合VAE与扩散模型提升声学准确性，实现高质量全频带RIR合成，适用于虚拟现实和音频生产等场景。|
|2510.22263v1|[Empowering Multimodal Respiratory Sound Classification with   Counterfactual Adversarial Debiasing for Out-of-Distribution Robustness](http://arxiv.org/abs/2510.22263v1)|总结：  <br/>该研究提出了一种结合反事实与对抗学习的去偏框架，通过因果图策略、对抗训练和元数据增强，有效缓解呼吸声分类中的伪相关问题，提升模型在分布变化下的泛化能力。<br/><br/>贡献点：  <br/>1. **因果图反事实去偏策略**：抑制患者元数据中的非因果依赖，减少年龄、性别等属性对分类结果的干扰。  <br/>2. **对抗训练机制**：学习对元数据不敏感的特征表示，降低特定偏见对模型性能的影响。  <br/>3. **反事实元数据增强**：通过生成对抗样本进一步削弱伪相关，强化跨临床站点的元数据不变特征。  <br/>4. **综合效果验证**：在分布变化场景下，方法显著优于现有基线，提升分类鲁棒性并开源实现。|
|2510.22258v1|[Binaural Signal Matching with Wearable Arrays for Near-Field Sources and   Directional Focus](http://arxiv.org/abs/2510.22258v1)|总结：该研究提出NF-FoV-BSM方法，结合真实近场HRTF/ATF与头部旋转分析，优化双耳线索建模，显著提升可穿戴系统在近场场景下的音频再现质量。<br/><br/>贡献点：<br/>1. **首次提出近场BSM方法**：开发NF-BSM，将距离依赖建模引入传统远场BSM框架，填补近场声源处理空白。<br/>2. **引入FoV权重机制**：设计Field of View加权策略，强化感知关键方向的信号处理，增强复杂场景下的鲁棒性。<br/>3. **真实数据验证分析**：基于模拟的近场HRTF和ATF数据（含头部旋转因素），更贴近实际应用场景。<br/>4. **双耳线索系统评估**：全面分析ILD/ITD等关键双耳线索，量化近场声源的感知差异。<br/>5. **实验验证优越性**：通过仿真与听觉测试证实，NF-FoV-BSM在近距离和头部旋转场景下优于传统方法。<br/>6. **揭示远场模型局限**：明确传统远场BSM在近场应用中的性能缺陷，论证距离和方向加权的必要性。|
|2510.22241v1|[FOA Tokenizer: Low-bitrate Neural Codec for First Order Ambisonics with   Spatial Consistency Loss](http://arxiv.org/abs/2510.22241v1)|**贡献点：**  <br/>1. 提出首个基于神经网络的离散空间音频编解码器，专为一阶声学（FOA）信号设计。  <br/>2. 延伸WavTokenizer架构，支持四通道FOA信号的压缩与重建。  <br/>3. 引入新型空间一致性损失函数，有效保留重建信号的方向性提示。  <br/>4. 实现超高压缩率（0.9 kbps），在24 kHz下达到每秒75个离散token的压缩效果。  <br/>5. 验证编解码器在模拟混响、真实房间冲激响应及非混响场景下的鲁棒性与准确性。  <br/>6. 展示离散潜变量在声学任务（如声音事件定位与检测）中的实用价值。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出首个离散神经空间音频编解码器，支持四通道FOA信号，引入空间一致性损失提升方向性重建，实现超低比特率（0.9 kbps）压缩，验证了其在多种场景下的有效性，并证明潜变量对下游空间音频任务的潜力。|
|2510.22237v1|[Bridging the Perceptual - Statistical Gap in Dysarthria Assessment: Why   Machine Learning Still Falls Short](http://arxiv.org/abs/2510.22237v1)|**贡献点：**  <br/>1. 提出“感知-统计差距”概念，系统分析模型与人类专家在失语症检测中的性能差异。  <br/>2. 详细阐述人类专家的感知机制，对比机器学习模型的特征表示与方法局限。  <br/>3. 理论分析标签噪声和评分者间差异对模型训练的限制。  <br/>4. 提出多类实践策略：感知驱动特征设计、自监督预训练、ASR辅助目标、多模态融合、人机协作训练及可解释性方法。  <br/>5. 设计符合临床目标的实验协议与评估指标，指导未来研究开发可靠且可解释的失语症评估工具。  <br/><br/>**总结（100字以内）：**  <br/>论文提出“感知-统计差距”概念，分析模型与人类专家在失语症检测中的差异，探讨标签噪声和评分者变异的影响，并提出多维度解决方案与临床导向的评估框架，推动可靠、可解释的工具开发。|
|2510.22183v1|[A Unified Framework for Direction and Diffuseness Estimation Using   Tight-Frame Microphone Arrays](http://arxiv.org/abs/2510.22183v1)|总结：  <br/>该研究提出一种统一声场方向与扩散度估计框架，创新性地基于速度协方差无需复杂预处理，兼容多种阵列结构。通过对比A型、刚性球与紧框架阵列，证实其在保持紧凑结构的同时实现接近各向同性的方向采样，并提升扩散度表征精度，为宽带空间声场分析提供了理论与实践结合的解决方案。  <br/><br/>贡献点：  <br/>1. **统一框架设计**：首次将声场方向与扩散度估计整合到同一模型中，兼容不同空间配置的麦克风阵列。  <br/>2. **速度协方差方法**：基于协方差的扩散度估计算法，无需模式白化或球面谐波分解，简化了跨异构阵列的评估流程。  <br/>3. **新型紧框架阵列**：提出并验证紧框架结构，实现近似各向同性的方向采样，性能接近高阶球阵但物理尺寸更紧凑。  <br/>4. **DOA-强度关联分析**：在统一框架下评估基于声学强度的方向到达估计准确性，强化了理论模型与实际应用的衔接。  <br/>5. **理论-实践桥梁**：将扩散度理论分析与可实现的阵列设计结合，推动稳健、宽频带空间声场表征方法的发展。|
|2510.22172v1|[M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR](http://arxiv.org/abs/2510.22172v1)|总结：  <br/>本文提出M-CIF模型，通过多尺度对齐与字符/音素级监督提升非自回归语音识别的鲁棒性，引入新评估指标并验证在德语和法语数据集上的有效性。<br/><br/>贡献点：  <br/>1. 提出Multi-scale CIF (M-CIF)机制，通过字符和音素层级监督的逐步蒸馏增强非自回归语音识别的对齐稳定性。  <br/>2. 定义新颖的评估指标：音素混淆错误(PE)和空间相关分段错误(SE)，用于深入分析模型性能差异。  <br/>3. 实验证明M-CIF在德语和法语CommonVoice数据集上分别降低WER 4.21%和3.05%，优于Paraformer基线模型。  <br/>4. 揭示音素和字符层级对提升渐进式CIF对齐的关键作用，为跨语言语音识别提供新思路。|
|2510.22105v1|[Streaming Generation for Music Accompaniment](http://arxiv.org/abs/2510.22105v1)|总结：  <br/>本研究提出实时音频到音频伴奏生成框架，通过引入系统延迟相关设计变量分析生成质量与效率的权衡关系，并提出先进的预判训练目标以提升实时伴奏连贯性。<br/><br/>贡献点：<br/>1. **提出实时伴奏生成框架**：首次系统研究实时音频流与伴奏生成的同步问题，突破传统编辑与循环工作流限制。<br/>2. **创新延迟建模设计**：引入未来可见性（t_f）和输出块持续时间（k）作为核心设计变量，量化系统延迟对生成效果的影响。<br/>3. **揭示关键权衡关系**：通过实验验证t_f与生成连贯性、k与吞吐量之间的矛盾关系，为实际部署提供优化依据。<br/>4. **提出先进训练目标**：设计基于预判（anticipatory）和代理（agentic）的训练目标，解决传统最大似然训练在实时场景中的不足。|
|2510.21685v1|[StylePitcher: Generating Style-Following and Expressive Pitch Curves for   Versatile Singing Tasks](http://arxiv.org/abs/2510.21685v1)||
|2510.21667v1|[FlowSynth: Instrument Generation Through Distributional Flow Matching   and Test-Time Search](http://arxiv.org/abs/2510.21667v1)||
|2510.21659v1|[Smule Renaissance Small: Efficient General-Purpose Vocal Restoration](http://arxiv.org/abs/2510.21659v1)||
|2510.21581v1|[Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video](http://arxiv.org/abs/2510.21581v1)||
|2510.21317v1|[Are These Even Words? Quantifying the Gibberishness of Generative Speech   Models](http://arxiv.org/abs/2510.21317v1)||
|2510.21280v2|[WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary   Proposal Networks and Post-processing Optimisation](http://arxiv.org/abs/2510.21280v2)|总结：  <br/>本文提出边界提案网络（BPN）改进轻量级声事件检测系统，通过门控机制减少假阳性并优化超参数，显著提升检测性能，尤其在少数类识别中表现突出。<br/><br/>**贡献点：**  <br/>1. **引入边界提案网络（BPN）**：基于图像目标检测思想，扩展轻量级SED系统以减少假阳性，提升整体检测精度。  <br/>2. **门控机制设计**：利用分类模型的中间潜在表示动态调节输出，增强对事件边界的控制。  <br/>3. **双超参数优化策略**：提出正向搜索与反向搜索方法，分别优化事件级和帧级后处理参数，突破传统经验方法的局限。  <br/>4. **性能提升验证**：实验结果表明，WhaleVAD-BPN在交叉验证中实现F1-score 0.475（比基线提升9.8%），尤其在少数类检测中表现优异（d-calls +21.3%，bp-calls +9.4%）。|
|2510.21280v1|[WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary   Proposal Networks and Post-processing Optimisation](http://arxiv.org/abs/2510.21280v1)||
|2510.21257v1|[HiFi-HARP: A High-Fidelity 7th-Order Ambisonic Room Impulse Response   Dataset](http://arxiv.org/abs/2510.21257v1)||
|2510.21209v1|[SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum   Domain](http://arxiv.org/abs/2510.21209v1)||
|2510.21196v1|[PhoenixCodec: Taming Neural Speech Coding for Extreme Low-Resource   Scenarios](http://arxiv.org/abs/2510.21196v1)||
|2510.21115v1|[Robust Distortion-Free Watermark for Autoregressive Audio Generation   Models](http://arxiv.org/abs/2510.21115v1)||
|2510.21004v1|[Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](http://arxiv.org/abs/2510.21004v1)||
|2510.18724v1|[Adapting Language Balance in Code-Switching Speech](http://arxiv.org/abs/2510.18724v1)|总结：该研究提出基于嵌入语言差异的可微标签，优化代码切换识别，减少上下文偏差，提升模型鲁棒性，实验验证有效性。<br/><br/>贡献点：<br/>1. 提出代码切换识别的核心挑战在于语言嵌入的微妙变化，而非单纯数据稀缺<br/>2. 首创基于可微分代理的训练标签机制，显式引导模型学习代码切换特征<br/>3. 构建聚焦代码切换点的评估体系，强化对关键错误位置的性能分析<br/>4. 设计嵌入语言差异作为损失函数，有效缓解生成过程中的上下文偏差问题<br/>5. 通过阿拉伯语和中英混合语料实验验证，显著降低替代错误率并提升切换点预测准确性|
|2510.18723v1|[Bayesian Low-Rank Factorization for Robust Model Adaptation](http://arxiv.org/abs/2510.18723v1)|总结：提出贝叶斯因子化适配器，改进Whisper模型在多语言语言切换任务中的泛化与性能，相较LoRA减少适应损失并有效抑制灾难性遗忘。<br/><br/>贡献点：<br/>1. **提出贝叶斯因子化适配器方法**：通过引入零中心先验实现适应矩阵稀疏化，解决基础模型微调中因过度适配导致能力覆盖的问题。<br/>2. **验证多语言代码切换效果**：在Whisper模型上系统评估该方法在复杂多语言场景下的表现，证明其适应能力与泛化性能的平衡性。<br/>3. **量化性能提升**：相比LoRA方法，在新领域任务中仅损失4%精度却实现54%的后向增益，显著降低灾难性遗忘风险。<br/>4. **提供理论依据**：通过概率建模方式体现适应过程的可解释性，为语音基础模型的个性化微调提供新范式。|
|2510.18684v1|[MLMA: Towards Multilingual with Mamba Based Architectures](http://arxiv.org/abs/2510.18684v1)|总结：  <br/>提出基于Mamba架构的MLMA模型，通过隐式语言感知协同和共享表示解决多语言ASR挑战，实验验证其在标准基准上与Transformer模型竞争力相当，凸显Mamba在可扩展性、效率和准确性的潜力。<br/><br/>贡献点：  <br/>1. **创新模型架构**：首次将Mamba（高效状态空间模型）引入多语言ASR领域，替代传统Transformer架构，提升长上下文处理效率。  <br/>2. **语言感知机制**：通过隐式语言条件和跨语言共享表示，增强模型对高/低资源语言的鲁棒性与适应性。  <br/>3. **性能验证**：在标准多语言基准测试中证明MLMA的竞争力，证实Mamba在保持准确性的同时具备更强的可扩展性和计算效率。  <br/>4. **应用价值**：为多语言语音识别提供了新的方法路径，强调Mamba架构在构建高效、可扩展的跨语言系统中的潜力。|
|2510.18533v1|[Noise-Conditioned Mixture-of-Experts Framework for Robust Speaker   Verification](http://arxiv.org/abs/2510.18533v1)|**总结（100字以内）:**  <br/>该论文提出一种噪声条件混合专家框架，通过分解特征空间、专家路由机制和课程学习协议提升说话人验证在噪声环境下的鲁棒性，实验验证其显著优于基线方法。<br/><br/>**贡献点:**  <br/>1. **提出噪声条件混合专家框架**：首次将噪声信息显式引入混合专家模型，分解特征空间为针对不同噪声特征的子空间，提升模型对噪声的适应性。  <br/>2. **设计噪声感知专家路由机制**：根据输入噪声动态选择对应的专家网络，实现噪声条件下的特征差异化建模，保留说话人身份信息。  <br/>3. **引入通用模型专家专业化策略**：通过统一模型指导专家网络的训练，增强各子模型的特异性与鲁棒性的同时保持整体一致性。  <br/>4. **构建SNR衰减课程学习协议**：按信噪比（SNR）逐步训练模型，优化模型在不同噪声强度下的泛化能力。  <br/>5. **实验证明方法有效性**：在多噪声条件下验证该方法显著优于传统方法，既提升鲁棒性又不降低验证准确率。|
|2510.18530v1|[A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker   Verification](http://arxiv.org/abs/2510.18530v1)|总结：本文提出基于锚点的阶段式学习策略，有效提升说话人表示的鲁棒性与判别性，适用于多种噪声条件。<br/><br/>贡献点：<br/>1. 提出anchor-based阶段式学习框架，首次将判别性边界建立与噪声鲁棒性优化分阶段处理；<br/>2. 创新性采用固定锚点嵌入作为噪声数据微调的参考基准，实现身份保持与噪声抑制的解耦；<br/>3. 通过边界稳定与变化抑制的分离机制，显著提升模型在复杂噪声环境下的表现；<br/>4. 实验证明该策略优于传统联合优化方法，在保持说话人区分度的同时增强抗噪能力。|
|2510.18423v1|[ProLAP: Probabilistic Language-Audio Pre-Training](http://arxiv.org/abs/2510.18423v1)|总结（100字以内）:  <br/>提出ProLAP模型，通过概率分布建模语言-音频多对多关系，引入层次包含与掩码排斥损失提升语义层次理解，实现在小数据集上的高效学习，并验证其在音频遍历任务中的性能优势。<br/><br/>**贡献点分点列出**  <br/>1. **多对多关系建模**：首次将语言-音频关系建模为概率分布的扩散，克服传统方法中一对一映射的局限，更贴合真实场景。  <br/>2. **双重训练目标**：提出层次包含损失（促进语义层次理解）和掩码排斥损失（提升优化效率），增强模型对复杂结构的捕捉能力。  <br/>3. **小数据集有效性**：在无需大规模数据的前提下，成功学习音频数据的层次结构，优于依赖大规模数据的前序概率方法。  <br/>4. **新任务验证实验**：设计音频遍历任务，验证ProLAP对语义层次的建模能力，填补相关研究空白。  <br/>5. **性能优势**：在音频-文本检索任务中超越现有确定性方法，证明其优越性和实用性。  <br/><br/>（注：原始文本未明确提及“音频遍历任务”为全新任务，故该点基于上下文合理推断。）|
|2510.18391v1|[MVDR Beamforming for Cyclostationary Processes](http://arxiv.org/abs/2510.18391v1)|总结：  <br/>本文提出cMVDR波束成形器，通过利用周期性噪声的谱相关性提升降噪性能，尤其在低SNR场景下表现优异，并有效处理非谐波问题，单麦克风即可实现显著的SI-SDR增益。<br/><br/>贡献点：  <br/>1. **提出cMVDR方法**：首次将周期性噪声的统计特性（cyclostationarity）引入波束成形器，结合空间与频谱相关性提升降噪效果。  <br/>2. **基于FRESH滤波框架**：通过输入信号的频率移位版本，抑制跨频率相干噪声，增强对几乎周期性噪声源（如乐器、风扇）的处理能力。  <br/>3. **解决非谐波问题**：采用数据驱动策略，利用周期图分析估计谐振频率并计算频率移位，适应谐波分量偏离整数倍的情况。  <br/>4. **实验验证有效性**：在真实录音场景中实现5dB SI-SDR增益，且在单麦克风条件下仍保持良好性能，证明方法的实用性与鲁棒性。|
|2510.18206v1|[Adaptive Per-Channel Energy Normalization Front-end for Robust Audio   Signal Processing](http://arxiv.org/abs/2510.18206v1)|总结：  <br/>提出基于闭环神经控制器的自适应音频前端，通过动态调整Per-Channel Energy Normalization实现输入依赖的适应性，在多种音频分类任务中显著提升鲁棒性和性能。<br/><br/>贡献点：  <br/>1. **提出自适应范式**：首次将静态参数化音频前端替换为闭环神经控制器，实现参数的动态调整以增强对复杂声学环境的鲁棒性。  <br/>2. **简化架构与集成控制器**：在LEAF架构基础上简化设计，引入神经控制器以动态优化子带能量归一化，提升前端灵活性。  <br/>3. **实验验证有效性**：通过在多类音频分类任务中的实验证明，该自适应方法在清洁与复杂环境下均优于传统固定和learnable前端。|
|2510.18190v1|[Joint Estimation of Piano Dynamics and Metrical Structure with a   Multi-task Multi-Scale Network](http://arxiv.org/abs/2510.18190v1)|总结：本文提出了一种高效的多任务模型，通过多尺度网络和Bark尺度特定响度输入，实现钢琴动态、节拍、弱拍等多目标联合预测，在公开数据集上达到SOTA效果，为音乐表达分析提供资源高效的解决方案。<br/><br/>贡献点：<br/>1. 提出多任务联合预测模型，同步估计钢琴动态级别、变化点、节拍与弱拍，构建动态节拍结构的统一框架<br/>2. 采用多尺度网络架构，创新性使用Bark尺度特定响度作为输入特征，将模型参数量从14.7M压缩至0.5M<br/>3. 实现60秒超长音频输入（双倍常规节拍跟踪长度），突破序列长度限制提升处理效率<br/>4. 在MazurkaBL数据集上取得多项任务的最先进性能，建立新基准并提供紧凑型分析工具|
|2510.17788v1|[AnyRIR: Robust Non-intrusive Room Impulse Response Estimation in the   Wild](http://arxiv.org/abs/2510.17788v1)|**贡献点：**<br/>1. 提出AnyRIR方法，采用音乐作为非侵入性激励信号，无需专用测试信号。<br/>2. 将RIR估计问题转化为时频域L1范数回归模型，利用非平稳噪声的稀疏性特性。<br/>3. 结合迭代加权最小二乘（IRLS）和最小残差平方和（LSMR）算法高效求解L1回归。<br/>4. 在模拟和实际数据中验证方法有效性，显著优于L2范数和频域去卷积方法。<br/>5. 实现对真实环境噪声和编解码器不匹配的鲁棒性，适用于AR/VR等实际应用场景。<br/><br/>**总结（100字内）：**  <br/>该研究提出AnyRIR方法，利用音乐信号替代传统测试信号，通过时频域L1范数回归有效抑制非平稳噪声影响，在真实场景下实现更鲁棒的RIR估计，适用于AR/VR等应用。|
|2510.17662v1|[DELULU: Discriminative Embedding Learning Using Latent Units for   Speaker-Aware Self-Supervised Speech Foundational Model](http://arxiv.org/abs/2510.17662v1)|总结：DELULU通过整合外部监督和双目标训练，显著提升自监督语音模型的说话人辨别能力，无需任务微调即可在多项 speaker-centric 任务中取得优异性能。  <br/><br/>贡献点：  <br/>1. 提出首个结合外部监督（ReDimNet帧级嵌入）的说话人感知自监督模型DELULU，增强伪标签生成的说话人区分性；  <br/>2. 引入k-means聚类与帧级嵌入的联合优化策略，构建强说话人诱导偏置；  <br/>3. 采用掩码预测与去噪双重目标训练，提升模型鲁棒性和泛化能力；  <br/>4. 在说话人验证（EER提升62%）和零样本建模（性别、年龄、口音、说话人计数）等任务中超越现有自监督模型；  <br/>5. 验证DELULU作为通用说话人感知编码器的有效性，无需任务特定微调即可实现高性能。|
|2510.17512v1|[AWARE: Audio Watermarking with Adversarial Resistance to Edits](http://arxiv.org/abs/2510.17512v1)|**贡献点（分点）**  <br/>1. **提出AWARE方法**：无需依赖攻击模拟堆栈和手工设计的可微失真，通过对抗性优化直接增强水印的对抗鲁棒性。  <br/>2. **时频域对抗优化**：引入层级比例的感知预算，实现水印嵌入与音频质量的平衡。  <br/>3. **时序无关检测器**：结合位读取头（BRH）模块，将时间证据聚合为每个水印位的置信度评分，提升对不同步和时间切割攻击的鲁棒性。  <br/>4. **实验验证优越性**：在多种音频编辑场景下保持高音频质量（PESQ/STOI）和低误码率（BER），优于现有SOTA学习型水印系统。  <br/><br/>**总结（100字内）**  <br/>AWARE提出无需模拟攻击的对抗水印方法，通过时频域优化与时间无关检测器实现高鲁棒性，突破传统依赖人工失真设计的局限，在多种音频编辑下保持卓越的音频质量和低误码率。|
|2510.17474v1|[Not All Deepfakes Are Created Equal: Triaging Audio Forgeries for Robust   Deepfake Singer Identification](http://arxiv.org/abs/2510.17474v1)|总结：  <br/>提出双阶段系统识别高保真唱歌语音深度伪造，结合判别器过滤低质量伪造与真实录音训练模型，提升歌手身份检测效果。<br/><br/>贡献点：  <br/>1. 推出两阶段流程：通过判别器筛选低质量伪造，再利用真实录音训练模型识别高质量深度伪造及真实音频，针对性解决高仿声音检测难题。  <br/>2. 改进判别器应用：首次将判别器用于过滤未准确还原歌手声纹的低质量深度伪造，提升系统对有害内容的识别优先级。  <br/>3. 真实数据训练策略：后续模型仅基于真实录音训练，增强对高质量合成音频与真实内容的区分能力，减少误判风险。  <br/>4. 实验验证有效性：在真实和合成内容上均超越现有方法，证明该系统在识别歌手声纹和检测深度伪造的可靠性与实用性。|
|2510.17346v1|[TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart   Sound Segmentation](http://arxiv.org/abs/2510.17346v1)|总结：提出TopSeg框架，基于拓扑表示实现高效、跨数据集的心音分割，在低数据量下优于传统方法，验证了拓扑特征的数据效率优势。<br/><br/>贡献点：<br/>1. 提出TopSeg框架，首次将拓扑表示用于心音分割，通过多尺度拓扑特征捕捉PCG动态特性<br/>2. 引入轻量级时序卷积网络（TCN）解码器并设计时序约束推理机制提升系统效率<br/>3. 在PhysioNet 2016数据集上进行受试级子采样训练，验证方法的跨数据集泛化能力<br/>4. 在低数据预算下（如10%训练量）展示显著性能优势，相比谱图/包络输入提升幅度达最大<br/>5. 消融实验证实不同尺度拓扑特征具有互补性，H0和H1结合可提升S1/S2定位精度与边界稳定性<br/>6. 证明拓扑感知表示为数据高效分割提供强归纳偏置，支持有限标注数据的实用化应用|
|2510.17345v1|[DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene   Classification under Domain Shift](http://arxiv.org/abs/2510.17345v1)|总结：  <br/>提出动态双信号课程（DDSC）方法，通过在线调整训练课程解决设备域迁移问题，提升跨设备性能，且具备轻量化和通用性优势。<br/><br/>贡献点：  <br/>1. **动态课程调整**：首次引入动态课程机制，根据模型学习过程实时调整训练示例的顺序和权重，克服静态课程的局限性。  <br/>2. **双信号融合**：结合域不变性信号（减少域偏移）与学习进度信号（优化训练效率），通过时间变化的调度器实现协同优化。  <br/>3. **轻量通用性**：方法无需额外架构修改或推理开销，适用于多种ASC模型和标签预算场景。  <br/>4. **跨设备性能提升**：在DCASE 2024 Task 1官方协议下，显著改善未见过设备的数据集表现，验证其泛化能力。|
|2510.17092v1|[Event Topology-based Visual Microphone for Amplitude and Frequency   Reconstruction](http://arxiv.org/abs/2510.17092v1)|贡献点总结（100字以内）:<br/>提出基于事件拓扑的视觉麦克风，无需外部照明即可高精度重建振动振幅与频率；整合Mapper算法与分层密度聚类，首次实现多声源同步恢复，推动被动式振动传感技术发展。<br/><br/>详细贡献点:<br/>1. 首创事件拓扑驱动的振动感知框架，直接从原始事件流中提取振动信息<br/>2. 创新性融合Mapper拓扑数据分析算法与分层密度聚类技术<br/>3. 实现无外部光源条件下的高保真振动信号恢复（振幅/频率）<br/>4. 开发支持多声源同步识别的事件数据处理方法<br/>5. 通过实验验证显著提升传统事件相机方法的性能<br/>6. 推动非接触式、被动式振动传感技术的前沿进展|
|2510.16917v1|[SAKE: Towards Editing Auditory Attribute Knowledge of Large   Audio-Language Models](http://arxiv.org/abs/2510.16917v1)|**贡献点总结：**  <br/>1. **首创性**：提出SAKE，首个针对听觉属性知识编辑的基准，填补了语音领域知识编辑研究的空白。  <br/>2. **多维度评估**：系统评估七种编辑方法在可靠性、泛化性、音频/文本局部性及可移植性四个维度上的表现。  <br/>3. **挑战识别**：揭示听觉知识编辑的关键挑战，包括保留非相关属性知识、多模态推理泛化及序列更新稳定性。  <br/>4. **研究框架**：为听觉模态知识编辑提供理论框架，推动LALMs在多样化现实场景中的维护与适应研究。  <br/><br/>（总结：SAKE首次系统性地针对听觉属性知识编辑，通过多维度评估揭示关键挑战，为语音模型的适应性研究提供新方向。）|
|2510.16841v1|[SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization](http://arxiv.org/abs/2510.16841v1)|**贡献点：**  <br/>1. 提出SAC（Semantic-Acoustic Codec），通过语义-声学双流量化结构解决现有编解码器在高质量重建与语义表征之间的平衡问题。  <br/>2. 首次实现语义和声学建模的解耦，分别优化两者的专用流，提升整体性能。  <br/>3. 在多种比特率下（清洁及嘈杂环境）验证SAC的重建能力，UTMOS与WER指标显著优于其他方法。  <br/>4. 在语义表征任务中，SAC性能达到自监督学习（SSL）连续嵌入的水平，超越现有编解码器。  <br/>5. 通过理论与实验分析，揭示双流设计对可控语音应用（如语音编辑、风格迁移）的有效性与潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出SAC，一种通过语义-声学双流量化提升语音编码质量的模型，显著优化重建性能和语义表征，并验证其在可控语音应用中的潜力。|
|2510.16813v1|[Audio dequantization using instantaneous frequency](http://arxiv.org/abs/2510.16813v1)|总结（100字以内）:  <br/>提出基于相位感知正则化的音频去量化方法（PHADQ），有效保持时频表示中正弦成分的时序连续性，减少能量损失伪影，并采用SDR和PEMO-Q ODG指标评估性能。<br/><br/>贡献点：  <br/>1. **提出新型去量化方法**：设计相位感知正则化器（Phase-Aware Regularizer）应用于音频去量化，解决传统方法的局限性。  <br/>2. **保持时间连续性**：在时频表示中维护正弦成分的时序连续性，提升音频质量。  <br/>3. **避免能量损失伪影**：对比l1正则化方法，减少音频信号重建中的能量损失问题。  <br/>4. **客观评估指标**：引入SDR（Signal Distortion Ratio）和PEMO-Q ODG（Objective Quality Metric）进行性能验证。|
|2510.16756v1|[End-to-end Listen, Look, Speak and Act](http://arxiv.org/abs/2510.16756v1)|**贡献点总结（100字以内）:**<br/>ELLSA首次实现全双工端到端多模态交互，通过SA-MoE架构高效整合模态数据并减少干扰，在语音交互和机器人任务中表现优异，支持对话、行动轮替等复杂行为，推动自然交互智能发展。<br/><br/>**分点贡献：**<br/>1. **首创全双工端到端多模态处理**：提出ELLSA模型，首次同时实现视觉、文本、语音和动作的感知与生成，突破以往单向或多模态分离的限制。  <br/>2. **新型SA-MoE架构设计**：引入自注意力混合专家（SA-MoE）结构，通过路由模态到专用专家模块并统一注意力融合，提升模态整合效率并减少干扰。  <br/>3. **支持复杂交互行为**：在基准测试中表现与模态特定基线相当，同时实现对话轮替、拒绝对话、边说边做、视觉问答及动作打断等高级全双工能力。  <br/>4. **通用交互智能框架**：为构建更自然、人类化的交互系统提供通用解决方案，推动人工通用智能（AGI）的发展。  <br/>5. **开放研究资源**：论文接受后将开放数据、代码和模型检查点，促进技术复用与社区发展。|
|2510.16700v1|[Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric   Speech Recognition in Constrained Scenarios](http://arxiv.org/abs/2510.16700v1)|总结：该研究提出了一种针对文本匹配数据合成的文本覆盖策略，显著提升零/一-shot dysarthric语音识别性能，对实际康复和日常交流应用具有重要意义。<br/><br/>贡献点：<br/>1. 提出面向文本匹配的新型文本覆盖策略，解决dysarthric语音数据合成中目标领域表征不足的问题<br/>2. 实现高效的零样本/一样本数据增强方法，突破传统基于有限训练数据的合成限制<br/>3. 通过该策略显著提升DSR系统对未见过的dysarthric说话人的识别性能<br/>4. 为dysarthria康复及日常交流场景提供更可靠的语音识别解决方案|
|2510.16567v1|[Hallucination Benchmark for Speech Foundation Models](http://arxiv.org/abs/2510.16567v1)|总结:  <br/>SHALLOW提出首个系统化ASR幻觉评估框架，从词法、语音、形态和语义四维度量化分析，揭示传统WER指标的局限性，支持模型缺陷识别与优化。<br/><br/>贡献点:  <br/>1. **首次系统性分类**：建立涵盖词法、语音、形态、语义四轴的ASR幻觉现象分类体系，填补领域空白。  <br/>2. **针对性指标设计**：为各维度定义可解释的评估指标，实现对幻觉内容的精细化量化分析。  <br/>3. **与WER的关联研究**：验证SHALLOW指标在高识别质量下与WER高度相关，但降质场景下能区分更细颗粒度的错误模式。  <br/>4. **诊断与优化支持**：提供超出传统错误率的模型弱点诊断，为系统改进提供具体反馈依据。|
|2510.16489v1|[Interpreting the Dimensions of Speaker Embedding Space](http://arxiv.org/abs/2510.16489v1)|**贡献点：**  <br/>1. **揭示说话人嵌入与传统声学特征的关联**：发现9个"可解释"声学参数可预测嵌入效果，其解释力与7个主成分（解释50%以上方差）相当，证明嵌入模型与传统声学维度存在可解释性联系。  <br/>2. **发现性别隐含信息**：部分主成分在男性与女性说话人间表现差异，表明主流嵌入系统中隐含了性别识别能力，突破了传统认为嵌入为"黑箱"的假设。  <br/>3. **指出年龄信息不足**：证明当前嵌入模型对年龄的表征能力有限，为改进年龄相关的说话人建模提供了研究方向。  <br/><br/>**总结（100字内）**：  <br/>本研究揭示说话人嵌入与传统声学特征的相关性，发现性别信息隐含于嵌入模型，但年龄信息表征不足，为提升嵌入系统的可解释性和应用效果提供了新视角。|
|2510.16387v1|[Probing the Hidden Talent of ASR Foundation Models for L2 English Oral   Assessment](http://arxiv.org/abs/2510.16387v1)|**贡献点总结**（100字以内）:  <br/>本研究发掘Whisper基础模型在L2口语评估中的潜力，通过提取隐藏表征的音频与语言特征，仅用轻量分类器实现性能超越当前最优方法，并证明模型无需微调即可编码语言等级与语义信息，展示了其在SLA等任务中的广泛适用性。<br/><br/>**分点贡献**:  <br/>1. **方法创新**：首次通过分析Whisper隐藏表征的音频和语言特征，而非依赖原始转录结果，挖掘其潜在的SLA能力。  <br/>2. **高效利用**：仅需在Whisper中间层和最终输出上训练轻量分类器，显著降低计算成本并达到性能超越现有多模态基线。  <br/>3. **多模态增强**：引入图像与文本提示作为辅助线索，进一步提升模型在口语评估任务中的表现。  <br/>4. **无监督发现**：通过深入分析Whisper嵌入，揭示其无需任务微调即可内含语言等级（ordinal proficiency）与语义信息，为SLA提供无需适配的基础模型支持。|
|2510.16355v1|[Transmission of High-Amplitude Sound through Leakages of Ill-fitting   Earplugs](http://arxiv.org/abs/2510.16355v1)|总结：  <br/>本研究通过计算与实验验证，揭示了高声压环境下耳塞设计对声泄漏的影响，发现未密封硅胶耳塞在120 dB时平均TL下降18 dB，并通过数值模拟阐明声能转化为涡度的机制，强调了耳塞结构优化的重要性。<br/><br/>贡献点：  <br/>1. **方法验证**：利用已发表数据对比，验证了狭缝共振器和孔口的声学传输计算与实验方法。  <br/>2. **泄漏几何建模**：分析不同孔口直径对声学功率吸收系数和传输损失（TL）的频率依赖性影响。  <br/>3. **实验参数覆盖**：在1-5 kHz频率范围及120-150 dB SPL条件下进行实验，涵盖高声压环境。  <br/>4. **TL下降量化**：发现未密封硅胶耳塞在120 dB总入射声压级下平均TL减少约18 dB。  <br/>5. **耗散机制揭示**：通过直接数值模拟，阐明高SPL下声能转化为涡度的物理过程。  <br/>6. **设计指导意义**：首次系统关联耳塞结构设计与高SPL环境下的声泄漏控制效果。|
|2510.16156v1|[AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning](http://arxiv.org/abs/2510.16156v1)|总结：<br/>AsyncVoice Agent提出异步架构实现流式LLM与语音交互解耦，支持实时推理叙述和用户打断，显著降低交互延迟并提升任务可信度。<br/><br/>贡献点：<br/>1. 提出异步架构设计：将流式LLM后端与对话式语音前端解耦，实现推理与叙述的并行处理<br/>2. 实现实时语音交互：支持用户在模型推理过程中随时打断、查询和引导<br/>3. 突破性性能提升：通过客观基准测试验证，交互延迟降低600倍以上，同时保持高保真度和任务准确性<br/>4. 构建双向对话范式：开创人机协作新方法，使AI系统具备可解释性和可操控性，适用于高风险场景|
|2510.15865v1|[Sound Clouds: Exploring ambient intelligence in public spaces to elicit   deep human experience of awe, wonder, and beauty](http://arxiv.org/abs/2510.15865v1)|总结：  <br/>本文提出 Sound Clouds 艺术装置，通过人机互动生成实时音乐，探索情感触发的 AmI 体验，挑战传统功能限制，推动 AmI 向创造性方向发展。<br/><br/>贡献点：  <br/>1. **提出沉浸式情感体验新范式**：首次将 Ambient Intelligence (AmI) 用于公共空间艺术，设计能激发 awe、wonder 和 beauty 等深层情感的交互式体验。  <br/>2. **构建交互生成音乐的实验平台**：开发 Sound Clouds 装置，通过参与者与人形球体的互动实时生成音乐，实现物理交互与声音艺术的结合。  <br/>3. **强调 AmI 的创造性潜力**：主张将 AmI 从功能性系统转向艺术与情感维度，提出“provocation（启发）而非限制”未来技术发展的核心理念。|
|2510.15659v1|[Magnitude and Phase-based Feature Fusion Using Co-attention Mechanism   for Speaker recognition](http://arxiv.org/abs/2510.15659v1)|**贡献点总结（100字以内）**  <br/>本文提出基于co-attention机制的特征级融合框架，分别建模幅度和相位领域，动态调整两者的权重以提升说话人识别性能。在VoxCeleb数据集上，该方法优于现有系统，Top-1准确率提升0.82%，EER降低0.45%。<br/><br/>**分点贡献：**  <br/>1. **创新融合框架**  <br/>   提出结合co-attention机制的特征级融合方法，解决传统方法忽视幅度与相位领域中说话人语义差异的问题。  <br/><br/>2. **双子网络分离处理**  <br/>   设计两个独立子网络分别提取幅度和相位特征，增强对二者特征的独特建模能力。  <br/><br/>3. **动态权重调整机制**  <br/>   通过关联矩阵（correlation matrix）动态分配权重，根据发音内容自适应平衡幅度与相位的贡献。  <br/><br/>4. **实验验证与性能提升**  <br/>   在VoxCeleb数据集上验证，提出的方法在Top-1准确率（97.20%）和EER指标上均优于当前最佳系统（0.82%绝对提升）及单特征系统（0.45% EER下降）。|
|2510.15566v1|[SpikeVox: Towards Energy-Efficient Speech Therapy Framework with   Spike-driven Generative Language Models](http://arxiv.org/abs/2510.15566v1)|**贡献点总结：**<br/>1. 提出SpikeVox框架，融合脉冲驱动生成语言模型与语音识别模块，实现高效语音障碍检测与治疗建议。  <br/>2. 首次将完整反馈机制（检测、发音指导、练习生成）集成至语音治疗系统，提升解决方案的完整性。  <br/>3. 通过优化模型设计显著降低能耗，支持低功耗平台部署（如智能手机），突破资源限制。  <br/>4. 实验验证框架在语音障碍识别（88%置信度）和实际应用中的有效性，填补全球治疗资源缺口。  <br/><br/>（100字以内总结：提出SpikeVox框架，结合脉冲驱动模型与语音识别，实现高效、低能耗的语音治疗系统，提供完整反馈并显著提升识别准确率，有助于解决全球治疗资源不足问题。）|
|2510.15437v1|[MC-LExt: Multi-Channel Target Speaker Extraction with Onset-Prompted   Speaker Conditioning Mechanism](http://arxiv.org/abs/2510.15437v1)|总结：  <br/>提出MC-LExt框架，通过预置目标说话人短语音作为条件信号，无需辅助信息即可在多通道混合语音中有效提取目标说话人，解决了传统方法对方向估计和嵌入模型的依赖问题。<br/><br/>贡献点：  <br/>1. **提出新型框架**：设计MC-LExt，通过在每通道混合信号前添加目标说话人短语音作为起始提示条件信号，实现无需辅助信息的端到端MC-TSE。  <br/>2. **解决现有局限**：克服DOA方法依赖方向估计和阵列几何的缺点，以及基于嵌入的方法在噪声/混响下性能下降的问题。  <br/>3. **联合学习空间与身份信息**：使DNN能同时学习说话人身份和空间特征，提升提取鲁棒性。  <br/>4. **实验验证有效性**：在WHAMR!和MC-Libri2Mix等噪声混响基准上验证方法效果，证明其优越性。|
|2510.15432v1|[Quantization-Based Score Calibration for Few-Shot Keyword Spotting with   Dynamic Time Warping in Noisy Environments](http://arxiv.org/abs/2510.15432v1)|总结：  <br/>本文提出一种基于动态时间规整的得分校准方法，通过量化嵌入和归一化检测得分优化阈值选择，有效提升噪声及少样本场景下KWS系统的性能。<br/><br/>贡献点：  <br/>1. 提出首个将动态时间规整（DTW）与量化误差结合的阈值估计方法，适用于开放集少样本KWS系统。  <br/>2. 构建双步骤校准流程：先量化嵌入以获取误差特征，再利用误差进行得分归一化以提升阈值鲁棒性。  <br/>3. 在KWS-DailyTalk数据集上验证方法有效性，尤其在高频率噪声场景下显著优于传统阈值优化策略。  <br/>4. 解决了依赖验证集优化导致的阈值泛化能力差问题，简化了实际系统中的阈值选择过程。|
|2510.15409v1|[Towards Blind Data Cleaning: A Case Study in Music Source Separation](http://arxiv.org/abs/2510.15409v1)|总结：  <br/>提出两种噪声无关的数据清洗方法，通过无监督学习有效去除污染数据，显著提升音乐源分离性能，且具有更广泛的适用性。<br/><br/>贡献点：  <br/>1. **提出噪声无关的通用清洗方法**：设计两种无需预知污染类型的数据清洗策略，克服传统方法对特定artifact的依赖。  <br/>2. **创新性数据脱敏技术**：采用“unlearning”机制，通过识别训练样本对模型输出的贡献度筛选数据。  <br/>3. **感知相似性度量应用**：引入Fréchet Audio Distance，基于小规模干净参考集移除感知上不匹配的污染样本。  <br/>4. **实验验证有效性**：在模拟现实噪声的污染数据集上证明，清洗后模型性能超越污染基线和参考集训练模型，缩小性能差距达66.7%。  <br/>5. **提升数据集质量**：通过清洗方法生成高质量数据集，为语音/音频领域提供更可靠训练资源。|
|2510.15383v1|[DroneAudioset: An Audio Dataset for Drone-based Search and Rescue](http://arxiv.org/abs/2510.15383v1)|**贡献点：**  <br/>1. 构建了首个全面的无人机音频感知数据集 **DroneAudioset**，包含23.5小时真实环境下的标注录音。  <br/>2. 覆盖多样场景：涵盖多种无人机类型、油门设置、麦克风配置及环境噪声条件（SNR范围：-57.2 dB至-2.5 dB）。  <br/>3. 解决实际问题：提供标准化数据集，用于开发和系统性评估低信噪比环境下的噪声抑制与人类存在分类方法。  <br/>4. 促进系统设计：为无人机音频系统提供关键设计指导，如麦克风布局优化与噪声感知处理策略。  <br/>5. 开源共享：数据集在Hugging Face平台公开，并采用MIT许可证，便于学术研究和工业应用。  <br/><br/>**总结（100字以内）：**  <br/>提出首个全面且真实场景的无人机音频数据集DroneAudioset，覆盖多种噪声条件与设备配置，为低信噪比环境下的声音检测与系统设计提供基础支持。|
|2510.15364v1|[LDCodec: A high quality neural audio codec with low-complexity decoder](http://arxiv.org/abs/2510.15364v1)|总结:  <br/>提出LDCodec，实现低复杂度解码器与高保真语音重建的平衡，在6kbps下超越Opus 12kbps性能。<br/><br/>贡献点:  <br/>1. 提出LDCodec，专为流媒体客户端设计，降低解码复杂度  <br/>2. 首创残差单元与LSRVQ（长短期残差向量量化）结合的编码结构  <br/>3. 采用子带-全带频率判别器提升频域特征捕捉能力  <br/>4. 设计感知损失函数优化音频重建质量  <br/>5. 在6kbps超低比特率下实现超越Opus 12kbps的性能表现|
|2510.14934v1|[TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation](http://arxiv.org/abs/2510.14934v1)|总结：  <br/>本文提出TASLA框架，通过多层动态注意力和有限标量量化解决低帧率下声学细节丢失问题，提升韵律质量，表现优于TASTE，并解释了动态层混合与频谱波动的关联性。<br/><br/>贡献点：  <br/>1. **提出TASLA框架**：解决低帧率与文本对齐下语音标记丢失声学细节的问题。  <br/>2. **设计MLDA机制**：动态混合冻结语音编码器的浅层/深层特征，增强文本位置对声学信息的适应性。  <br/>3. **引入FSQ技术**：实现简单且平滑的逐维度离散化，优化语音标记过程。  <br/>4. **验证效果提升**：在LibriSpeech、EXPRESSO和Voxceleb数据集上实现高质量标记，速率2.62 Hz下表现优于TASTE。  <br/>5. **理论解释**：揭示动态层混合与频谱波动的关系，阐明MLDA如何在极端压缩下保留韵律信息。|
|2510.14922v1|[TRI-DEP: A Trimodal Comparative Study for Depression Detection Using   Speech, Text, and EEG](http://arxiv.org/abs/2510.14922v1)|总结（100字以内）:  <br/>该研究系统性探索了多模态抑郁检测的特征表示与模型策略，发现三模态融合优于单/双模态，预训练嵌入优于手工特征，且统一评估框架增强了结果的可复现性，为未来研究提供基准。<br/><br/>贡献点：  <br/>1. **系统性多模态研究**：首次全面比较EEG、语音和文本三类模态的特征表示与建模策略，填补了现有研究的空白。  <br/>2. **特征对比实验**：系统评估手crafted特征与预训练嵌入（如BERT等）的性能，证明预训练模型在特征提取上的优势。  <br/>3. **模态融合分析**：对比单模态、双模态和三模态配置，验证三模态联合（EEG+语音+文本）对抑郁检测的提升效果。  <br/>4. **注意力机制应用**：深入分析EEG在多模态融合中的作用，探讨注意力机制对模型性能的优化影响。  <br/>5. **统一评估框架**：采用一致的跨被试分割协议，提升实验结果的鲁棒性和可复现性，为后续研究提供标准化基准。|
|2510.14921v1|[Sound Masking Strategies for Interference with Mosquito Hearing](http://arxiv.org/abs/2510.14921v1)|**贡献点：**<br/>1. 建立了通用主动听觉系统模型与蚊虫听觉系统的对比研究框架。<br/>2. 证实聚焦单一/少数频率的声学掩蔽策略对两类系统均效果最佳。<br/>3. 提出快速频率调制掩蔽可最大化信息传递干扰并降低可懂性。<br/>4. 界定掩蔽技术在不同场景下的应用方向：生态保护（最小化干扰）与病媒防控（最大化干扰）。<br/><br/>**总结（100字内）：**  <br/>本研究通过对比通用听觉系统与蚊虫听觉系统，发现聚焦频率的掩蔽策略效果最优，并提出快速频率调制可有效干扰信息传递。该成果为生态信号设计与病媒防控提供了理论指导，分别适用于野生动物通信保护和昆虫种群控制。|
|2510.14691v1|[If You Hold Me Without Hurting Me: Pathways to Designing Game Audio for   Healthy Escapism and Player Well-being](http://arxiv.org/abs/2510.14691v1)|总结：  <br/>该论文指出音频在游戏逃避行为中的调节作用被低估，揭示其在情绪调控、过渡标记和体验闭合中的功能，同时提出方法论与可访问性研究的不足，并呼吁更系统地整合音频设计以促进健康游戏体验。<br/><br/>贡献点：<br/>1. **音频调节作用的创新视角**：首次强调音频在游戏自我调节中的核心地位，突破传统对视觉元素的注意力，提出音频可通过调节唤醒度、标记过渡、提供闭合感影响逃避行为的正负面影响。<br/>2. **福祉研究的空白填补**：系统分析音频对玩家福祉的潜在贡献，指出当前研究对其心理调节机制关注不足，为音频在心理健康领域的应用开辟新方向。<br/>3. **方法论与实践的双重批判**：明确揭示现有研究在方法论（如缺乏系统音频分析框架）和可访问性（如忽视音频对不同人群的适应性）方面的局限性，为后续研究提供改进方向。<br/>4. **跨领域整合方案设计**：提出将音频纳入游戏设计与研究的系统性建议，包括方法论优化（如动态音频分析模型）和技术应用（如个性化音频方案），推动音频与心理健康研究的交叉融合。|
|2510.14570v1|[AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation   of Text-to-Audio-Generation](http://arxiv.org/abs/2510.14570v1)|**贡献点总结（分点）：**  <br/>1. 构建首个大规模的Text-to-Audio (TTA) 评估数据集 AudioEval，包含4,200个音频样本、126,000个评分及五维感知标注。  <br/>2. 提出Qwen-DisQA多模态评分模型，联合处理文本与生成音频以预测人类质量评分，实现可靠、可扩展的评估。  <br/>3. 通过实验验证模型有效性，并公开数据集以促进TTA领域的未来研究。  <br/><br/>**总结（100字以内）：**  <br/>本文提出首个大规模TTA评估数据集AudioEval及多模态模型Qwen-DisQA，实现对音频质量的全面评估与预测，公开数据集以推动领域研究。|
|2510.14551v1|[Spatially Aware Self-Supervised Models for Multi-Channel Neural Speaker   Diarization](http://arxiv.org/abs/2510.14551v1)|**贡献点总结**  <br/>1. 提出轻量方法，通过通道通信模块使预训练WavLM具备多通道空间感知能力。  <br/>2. 融合多通道说话人嵌入时引入空间注意力权重，提升信息利用效率。  <br/>3. 方法兼容任意麦克风通道数及阵列拓扑，通用性强。  <br/>4. 在五个公开数据集上验证，性能优于单通道基线及DOVER-Lap，且计算效率更高。  <br/>5. 源代码公开，促进研究复现与应用。  <br/><br/>（注：实际字数为100字内）|
|2510.14443v1|[Big Data Approaches to Bovine Bioacoustics: A FAIR-Compliant Dataset and   Scalable ML Framework for Precision Livestock Welfare](http://arxiv.org/abs/2510.14443v1)|**贡献点分点：**  <br/>1. **构建大型多模态生物声学数据集**：提供569个真实场景下的奶牛发声样本（扩展至2900个），覆盖48类行为，符合FAIR原则，解决数据量、多样性及生态真实性挑战。  <br/>2. **提出分布式处理框架**：整合iZotope RX降噪、音频-视频同步及标准化声学特征（24个描述符），提升数据处理效率与噪声鲁棒性。  <br/>3. **揭示行为声学模式**：首次通过基准测试发现发情检测、压力分类和母性交流的显著声学差异，为动物行为分析提供依据。  <br/>4. **促进可重复研究**：发布标准化流程与详细元数据，推动大数据分析与精确畜牧业管理的结合，支持可持续农业目标。  <br/>5. **助力联合国SDG 9**：通过数据科学实现传统农业向智能、伦理导向的福利优化系统转型，满足全球粮食需求。  <br/><br/>**总结（100字内）**：  <br/>本文提出符合FAIR原则的奶牛发声数据集及分布式处理框架，揭示关键行为声学模式，推动动物中心AI发展，促进精准畜牧业与可持续农业融合，并支持联合国SDG 9目标。|
|2510.14411v1|[Revisit Modality Imbalance at the Decision Layer](http://arxiv.org/abs/2510.14411v1)|**贡献点总结（100字以内）：**  <br/>揭示了多模态学习中决策层的模态不平衡问题，分析其根源在于特征空间与决策权重分布差异，提出引入自适应决策层权重分配机制以实现模态平衡。<br/><br/>**分点贡献：**  <br/>1. **提出决策层模态不平衡现象**：首次指出模态不平衡不仅存在于表示学习，更在决策层有显著表现，挑战传统认为仅需平衡表示层的假设。  <br/>2. **实验验证系统性偏差**：通过CREMAD和Kinetic-Sounds数据集实验证明，即使经过预训练和优化，模型仍存在对主导模态（如音频）的系统性偏好。  <br/>3. **解析偏差来源**：揭示偏差源于特征空间分布及决策权重分布的固有差异，而非单纯优化过程动态。  <br/>4. **提出解决方案方向**：强调决策层需引入自适应权重分配机制，动态调整模态贡献比例，以提升弱模态的效果。  <br/>5. **为多模态系统设计提供指导**：倡导未来多模态系统应更关注决策层的平衡策略设计，推动更公平的跨模态融合。|
|2510.14391v2|[Beat Tracking as Object Detection](http://arxiv.org/abs/2510.14391v2)|总结：  <br/>本文将节拍检测任务转化为时间对象检测，结合计算机视觉的FCOS框架改进音频处理模型，提出基于WaveBeat的时序特征提取与多尺度FPN结构，在预测和后处理中采用简化的NMS策略，实现了高效且准确的音乐节拍识别。<br/><br/>贡献点：  <br/>1. **任务框架创新**：首次将节拍（beat）和弱拍（downbeat）检测重新建模为时间序列上的"对象检测"任务，突破传统帧级激活的建模方式。  <br/>2. **模型结构改进**：迁移FCOS框架至1D音频，替换原有backbone为WaveBeat的时序特征提取器，并引入Feature Pyramid Network（FPN）捕捉多尺度时间模式。  <br/>3. **后处理优化**：采用非最大值抑制（NMS）替代传统DBN方法，简化流程且减少启发式依赖，提高检测效率与鲁棒性。  <br/>4. **实验验证**：在标准音乐数据集上验证模型有效性，证明对象检测技术可直接应用于音乐节拍建模，仅需少量适配即可达到竞争性性能。|
|2510.14391v1|[Beat Detection as Object Detection](http://arxiv.org/abs/2510.14391v1)|【贡献点】<br/>1. 将beat/downbeat跟踪任务创新性重构为时序"对象检测"问题，突破传统帧级激活建模方式  <br/>2. 适配计算机视觉的FCOS检测器至1D音频领域，开发WaveBeat时间特征提取器作为新型backbone  <br/>3. 引入Feature Pyramid Network实现多尺度时序模式捕捉，提升模型对复杂节奏的表征能力  <br/>4. 设计基于非极大值抑制（NMS）的后处理策略，替代传统DBN方法，在简化流程的同时保持检测精度  <br/>5. 验证了对象检测技术在音乐节奏建模中的有效性，仅需少量调整即可在标准数据集获得竞争力结果  <br/><br/>【总结】  <br/>本文通过时序对象检测框架革新，结合WaveBeat特征提取器与FPN结构，提出简化版NMS后处理方案，在音乐节奏跟踪任务中实现高效精准建模，验证了检测技术的跨模态迁移潜力。|
|2510.14332v1|[A Robust Classification Method using Hybrid Word Embedding for Early   Diagnosis of Alzheimer's Disease](http://arxiv.org/abs/2510.14332v1)|总结（100字以内）:  <br/>提出基于混合词嵌入和优化参数的AD早期语言检测方法，结合Doc2Vec和ELMo的混淆度评分，并引入语言学特征提升模型性能，达到91%准确率和97% AUC，验证其稳定性，为AD大规模筛查提供有效工具。  <br/><br/>贡献点:  <br/>1. **混合词嵌入方法**：创新性地融合Doc2Vec与ELMo词向量，通过困惑度评分提取句子语法流畅性与语义上下文特征。  <br/>2. **语言学特征增强**：增加语法和语义分析维度，扩展词嵌入表征能力，提升对AD语言异常的识别精度。  <br/>3. **优化参数框架**：构建逻辑回归分类模型，系统调参（正则化、学习率、向量尺寸等），实现优于现有模型的SOTA性能（91%准确率, 97% AUC）。  <br/>4. **稳定性验证**：通过随机数据分割实验验证模型鲁棒性（标准差＜0.05），证明方法可靠性。  <br/>5. **实际应用价值**：提出可规模化部署的AD筛查工具，辅助医生进行早期诊断，降低医疗负担。|
|2510.14249v1|[Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?](http://arxiv.org/abs/2510.14249v1)|**贡献点：**  <br/>1. **填补研究空白**：系统评估现有联合语言-音频嵌入模型（MS-CLAP、LAION-CLAP、MuQ-MuLan）在捕捉音色感知维度（如亮度、粗糙度、温暖等）上的能力，首次关注其与人类感知的关联。  <br/>2. **多模型对比分析**：量化比较三种模型在声音与音色属性对齐中的有效性，揭示其在文本-音频语义映射中的差异。  <br/>3. **发现最优模型**：实验证明LAION-CLAP在仪器音和效果音中均能最可靠地对齐人类感知的音色语义，为模型优化和应用提供实证依据。  <br/><br/>**总结（100字以内）：**  <br/>本论文评估了三种联合语言-音频嵌入模型在捕捉音色感知维度上的表现，发现LAION-CLAP能最可靠地对齐人类感知，为音色建模和跨模态应用提供参考。|
|2510.14159v1|[Musical consonance: a review of theory and evidence on perception and   preference of auditory roughness in humans and other animals](http://arxiv.org/abs/2510.14159v1)|总结（100字以内）:  <br/>该文批判现有假说，指出粗糙度理论的定义模糊和测量依赖问题，提出未来研究应简化模型、系统评估参数、扩大刺激多样性以推动语音领域对粗糙度的理解。<br/><br/>贡献点分点列出:  <br/>1. **梳理并批判性评估现有假说**：系统总结音程起源的三种主要理论（粗糙度厌恶、和谐性偏好、文化习得），指出其证据不足和难以区分的局限性。  <br/>2. **界定粗糙度理论的关键缺陷**：提出粗糙度定义存在循环论证（tautology），且实证测量缺乏独立性，导致结果解释困难。  <br/>3. **批判模型开发的冗余问题**：指出当前模型存在重复性开发，数据质量不足及过拟合现象，强调需改进方法论。  <br/>4. **提出未来研究方向**：主张模型应简化设计，系统评估额外假设与参数，同时扩大语音刺激预测的多样性以增强理论普适性。|
|2510.13344v1|[UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity   MoE](http://arxiv.org/abs/2510.13344v1)|**贡献点总结**  <br/>1. 提出UniMoE-Audio模型，首次构建统一的语音与音乐生成框架；  <br/>2. 创新动态容量MoE架构，融合Top-P路由策略、混合专家设计（领域专用/通用/自适应跳过）；  <br/>3. 设计三阶段训练课程（独立训练→集成预热→联合训练）解决数据不平衡问题；  <br/>4. 实验证明模型在语音和音乐生成任务中达到SOTA，并显著提升跨域协同学习效果。  <br/><br/>**摘要总结**（100字以内）:  <br/>本文提出UniMoE-Audio，通过动态MoE架构与三阶段训练策略，解决语音与音乐生成的分离问题与数据不平衡，实现统一音频生成，显著提升性能与跨域协同能力。|
|2510.13308v1|[Towards Multimodal Query-Based Spatial Audio Source Extraction](http://arxiv.org/abs/2510.13308v1)|**贡献点：**  <br/>1. **多通道空间音频提取框架**：首次在第一-order ambisonics (FOA)混合音频中实现干信号恢复，充分利用多通道空间信息。  <br/>2. **双模态条件输入**：支持音频提示和文本提示作为条件输入，实现灵活的端到端源提取。  <br/>3. **三轴Transformer模型**：联合建模时域、频域及空间通道依赖，创新性地整合三维信息。  <br/>4. **多模态统一条件**：结合CLAP嵌入与FiLM技术，实现音频-文本条件的统一编码。  <br/>5. **无标注训练方案**：提出动态生成空间混合与目标信号的无标签数据流水线，降低标注成本并提升泛化能力。  <br/>6. **高保真分离范式**：通过实验验证多模态条件和三轴建模的有效性，建立沉浸式应用中高保真空间音频分离的新方法。  <br/><br/>**总结（100字内）：**  <br/>本文提出一种多通道空间音频源提取框架，支持音频和文本条件输入，结合三轴Transformer与多模态统一编码技术，通过无标注数据训练实现高保真分离，为沉浸式应用开辟新的研究范式。|
|2510.13221v1|[Acoustic Teleportation via Disentangled Neural Audio Codec   Representations](http://arxiv.org/abs/2510.13221v1)|总结：提出基于EnCodec的声学远距离传输方法，通过多任务训练有效分离内容与环境特征，实现显著质量提升并验证分离效果。<br/><br/>贡献点：<br/>1. 提出通过解耦语音内容与声学环境特征实现声学远距离传输的新方法<br/>2. 采用EnCodec架构，在非侵入式ScoreQ评分中实现3.03的显著质量提升（较传统方法提高0.59）<br/>3. 设计包含5项任务的训练策略：干净重建、混响重建、去混响及两种声学迁移变体<br/>4. 发现时间下采样会显著降低性能（2倍下采样导致质量统计显著下降）<br/>5. 通过t-SNE聚类验证声学嵌入按房间聚类、语音嵌入按说话人聚类的解耦效果|
|2510.12964v1|[VCTR: A Transformer-Based Model for Non-parallel Voice Conversion](http://arxiv.org/abs/2510.12964v1)|**贡献点总结（100字以内）**  <br/>本研究提出VCTR方法，集成Hybrid Perception Block和Dual Pruned Self-Attention模块，结合对比学习对抗框架，有效解决非并行语音转换中长程依赖缺失问题，提升转换质量与训练效率，并开源代码便于复现与验证。<br/><br/>**分点贡献：**  <br/>1. **提出VCTR架构**：首次结合Hybrid Perception Block（HPB）和Dual Pruned Self-Attention（DPSA）模块，突破传统CNN生成器对局部语义的局限，增强对全局语义和长程依赖的建模能力。  <br/>2. **改进对抗学习框架**：融合对比学习与CycleGAN的对抗机制，优化非并行语音转换的训练稳定性，缓解传统方法中生成结果不理想的问题。  <br/>3. **提升效率与效果**：通过结构设计（如DPSA的剪枝机制）降低计算复杂度，实现更高效的语音转换，同时改善语音质量与风格迁移的准确性。  <br/>4. **开源实现**：提供完整代码库（GitHub），便于学术界和工业界复现实验、验证方法，并推动非并行语音转换技术的进一步研究与应用。|
|2510.12947v1|[HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection](http://arxiv.org/abs/2510.12947v1)|总结：  <br/>该研究提出HyWA-PVAD方法，通过超网络调整标准VAD模型的部分权重实现个性化语音活动检测，无需改变架构，显著提升性能并简化部署。<br/><br/>贡献点：  <br/>1. **无需架构修改的个性化VAD**：采用超网络调整预训练VAD模型中选定层的权重，无需改变原有架构即可实现针对特定说话人的条件激活。  <br/>2. **性能提升**：在多个基线方法对比中，HyWA-PVAD持续提高PVAD的平均精度（mean average precision）。  <br/>3. **部署优势**：保留核心VAD架构，便于复用现有模型，降低部署复杂度与计算成本。|
|2510.12858v1|[A Critical Review of the Need for Knowledge-Centric Evaluation of   Quranic Recitation](http://arxiv.org/abs/2510.12858v1)|**贡献点总结：**  <br/>1. **揭示行业痛点**：批判现有自动化Tajweed评估工具依赖ASR技术，导致数据依赖性、偏见问题及反馈不足，指出其在教学有效性上的局限。  <br/>2. **提出新范式**：主张以知识为中心的计算框架，强调利用古兰经文本的固定性及Tajweed规则构建评估系统，而非单纯依赖统计模式。  <br/>3. **创新方法论**：主张基于经典规则与发音点（Makhraj）的前瞻性声学建模，提升对音韵和发音规范的精准评估能力。  <br/>4. **设计混合系统**：倡导融合深度语言知识与音频分析的混合技术方案，旨在实现更稳健、公平的自动化评估工具，推动全球学习者支持。  <br/><br/>**总结（100字以内）：**  <br/>论文批判现有Tajweed评估工具的局限性，提出基于经典规则与发音点的知识驱动框架，主张通过前瞻性声学建模与语言知识结合，开发更精准、公平的自动化评估系统，助力全球语音学习。|
|2510.12780v1|[Content Anonymization for Privacy in Long-form Audio](http://arxiv.org/abs/2510.12780v1)|总结：  <br/>提出基于ASR-TTS管道的上下文重写语音匿名化方法，有效消除说话者风格特征并保留语义，验证其在长音频场景下的隐私保护效果。<br/><br/>贡献点：  <br/>1. **指出隐私风险**：揭示现有语音匿名化技术在长语音场景中失效，因多语句可暴露说话者语言风格（词汇/语法/表达习惯）导致身份重识别风险。  <br/>2. **提出新型方法**：设计内容匿名化策略，通过ASR-TTS管道的上下文重写消除说话者特有语言风格，同时保持语音语义可用性。  <br/>3. **实验验证有效性**：在电话对话数据集上展示内容攻击对匿名语音的效果，并证明所提方法可显著降低此类攻击的成功率。  <br/>4. **实用化建议**：建议将paraphrasing（改写）作为语音隐私保护的必要步骤，确保长音频处理中的匿名性与功能性平衡。|
|2510.12720v1|[Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed   Perception](http://arxiv.org/abs/2510.12720v1)|总结：  <br/>该研究提出Omni-Detective数据生成框架，训练出高性能音频-视觉细粒度Captioning模型，并设计Omni-Cloze基准评测，推动了多模态细粒度感知技术的发展。<br/><br/>贡献点：  <br/>1. **发现核心问题**：揭示当前Omni Language Models（OLMs）中"细粒度感知与幻觉生成共成长"的内在矛盾，为后续改进提供理论依据。  <br/>2. **提出数据生成方法**：设计Omni-Detective模块化数据生成系统，集成工具调用技术，实现高细节且低幻觉的多模态数据自动生成。  <br/>3. **构建双模型体系**：基于生成数据训练Audio-Captioner（纯音频细粒度）和Omni-Captioner（音频-视觉细粒度）模型，分别取得MMAU/MMAR测试集最优性能及VDC新SOTA。  <br/>4. **创新基准评测**：开发Omni-Cloze闭合式评测体系，首次建立针对细粒度多模态Captioning的稳定、高效、可靠评估标准。  <br/>5. **验证技术有效性**：通过实验验证Omni-Detective生成高质量细粒度caption的能力，以及Omni-Cloze对模型性能评估的优越性。|
|2510.12377v1|[A Phase Synthesizer for Decorrelation to Improve Acoustic Feedback   Cancellation](http://arxiv.org/abs/2510.12377v1)|**贡献点：**  <br/>1. 提出一种统一框架（Phase Synthesizer），结合频率移位与相位调制技术，通过DFT滤波器银行实现信号去相关。  <br/>2. 扩展相位调制方法，引入可变延迟线技术（源自震颤和混响效果），增强系统灵活性。  <br/>3. 在车内语音通信场景中验证该方法的有效性，通过自适应频率域卡尔曼滤波提升系统稳定性与语音质量（PESQ指标）。  <br/><br/>**总结：**  <br/>该研究提出结合频率移位和相位调制的统一去相关框架，并扩展相位调制技术用于改善语音通信中的反馈抑制问题，实验证明其在系统稳定性和语音质量上的提升。|
|2510.12326v1|[DeePAQ: A Perceptual Audio Quality Metric Based On Foundational Models   and Weakly Supervised Learning](http://arxiv.org/abs/2510.12326v1)|总结：  <br/>本研究提出DeePAQ，首次结合度量学习与LoRA微调音乐基础模型，利用弱监督标签构建泛化音频质量评估框架，在编码伪影检测和未知失真泛化方面表现优异。<br/><br/>贡献点：  <br/>1. **首次融合度量学习与LoRA技术**：将度量学习应用于音乐基础模型MERT，通过Low-Rank Adaptation（LoRA）实现高效微调，开创性地探索了该方向。  <br/>2. **弱监督标签驱动**：基于替代标签构建嵌入空间，无需精确标注即可捕捉通用音频失真强度，降低标注负担。  <br/>3. **跨任务泛化能力**：在音频编码和源分离等不同场景中验证模型性能，表现出对未知失真的强适应性。  <br/>4. **超越现有评估指标**：在听觉测试中检测编码伪影效果优于主流方法，突出其鲁棒性和多功能性。|
|2510.12275v1|[TFGA-Net: Temporal-Frequency Graph Attention Network for   Brain-Controlled Speaker Extraction](http://arxiv.org/abs/2510.12275v1)|总结（100字以内）:  <br/>本论文提出TFGA-Net模型，结合多尺度时频特征、皮层拓扑结构、图卷积网络及自注意力机制，优化EEG驱动的说话人提取，引入MossFormer2分离器，显著提升目标说话人提取效果，并开源代码促进研究复现。<br/><br/>贡献点:  <br/>1. **提出脑控说话人提取模型**：首次设计基于EEG信号的脑机接口框架，用于目标说话人提取，突破传统语音分离方法的局限。  <br/>2. **多尺度时频特征与皮层拓扑结构**：开发多尺度时频特征提取方法，并整合任务相关的皮层拓扑结构信息，增强EEG信号表征能力。  <br/>3. **非欧几里得结构建模**：创新性采用图卷积网络（GCN）与自注意力机制，有效捕捉EEG信号的全局非线性特征。  <br/>4. **改进分离器设计**：提出结合MossFormer与RNN-Free Recurrent的MossFormer2分离器，优化融合EEG与语音特征的全局上下文建模和语音韵律提取。  <br/>5. **实验验证与开源**：在公开数据集上验证模型有效性，显著优于现有方法，并开源代码推动技术应用与研究复现。|
|2510.12210v2|[DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation](http://arxiv.org/abs/2510.12210v2)|总结（100字以内）:  <br/>DISTAR通过结合自回归语言模型与掩码扩散模型，在离散RVQ代码空间中实现高效、可控的零样本TTS，解决分布偏移问题，同时保持生成自然性和多样性。<br/><br/>贡献点:  <br/>1. **零样本TTS框架创新**：首次构建完全基于离散RVQ代码空间的文本到语音系统，无需强制对齐或持续预测器。  <br/>2. **块级并行生成机制**：通过AR语言模型生成块级RVQ标记，并利用平行掩码扩散模型填充后续块，实现长文本高效合成并缓解AR曝光偏差。  <br/>3. **推理时显式控制**：支持贪心与采样解码，通过无分类器引导实现鲁棒性与多样性的平衡，且通过RVQ层剪枝控制比特率与计算成本。  <br/>4. **性能优越性验证**：在鲁棒性、自然度、说话者/风格一致性等指标上超越现有零样本TTS系统，同时保持丰富输出多样性。|
|2510.12210v1|[DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation](http://arxiv.org/abs/2510.12210v1)|**贡献点总结（100字以内）:**  <br/>提出DISTAR框架，利用离散RVQ代码空间结合AR语言模型和diffusion模型，无需强制对齐与持续预测器，实现零样本TTS并提升鲁棒性、自然度和风格一致性，同时保持输出多样性。<br/><br/>**分点贡献:**  <br/>1. **提出零样本TTS框架DISTAR**：基于离散残差向量量化（RVQ）代码空间，无需强制对齐或持续预测器，直接解决连续表示的局限性。  <br/>2. **创新协同生成机制**：将AR语言模型与掩码扩散模型紧密耦合，通过块级RVQ草稿生成与并行填充，实现长文本合成并缓解经典AR暴露偏差。  <br/>3. **增强推理可控性**：支持贪心与采样解码，利用无分类器引导（classifier-free guidance）生成高质量音频，实现鲁棒性与多样性权衡及可变比特率控制。  <br/>4. **测试时的计算优化**：通过RVQ层剪枝实现可控计算，提升推理效率与灵活性。  <br/>5. **实验验证优越性**：在鲁棒性、自然度、说话者/风格一致性等指标上超越现有SOTA零样本TTS系统，同时保持丰富的输出多样性。|
|2510.12185v1|[Not in Sync: Unveiling Temporal Bias in Audio Chat Models](http://arxiv.org/abs/2510.12185v1)|总结（100字以内）:  <br/>本研究首次系统分析LALMs的时间偏差，提出TBI指标量化系统性错位，并揭示偏差随音频长度和事件类型的变化规律，为改进时间鲁棒性模型提供依据。<br/><br/>贡献点:  <br/>1. **首次系统性研究**：提出首个针对LALMs时间偏差的全面分析框架，揭示其在时间戳预测中的关键局限。  <br/>2. **量化方法**：引入Temporal Bias Index (TBI) 指标，系统性衡量预测时间与真实时间的偏移程度。  <br/>3. **偏差特征揭示**：发现时间偏差具有三重特性：（i）跨数据集和模型普遍存在；（ii）随音频长度增加而累积；（iii）与事件类型及位置相关。  <br/>4. **可视化工具**：开发配套的可视化框架，辅助分析和理解时间偏差的分布模式。  <br/>5. **领域引导**：指出当前LALMs的时间鲁棒性不足，并呼吁构建更强大的时间感知架构。|
|2510.12175v1|[Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning   for Controllable Foley Synthesis](http://arxiv.org/abs/2510.12175v1)|**贡献点：**<br/>1. 提出Audio Palette模型，基于DiT架构扩展Stable Audio Open，填补可控音频生成的"控制差距"。<br/>2. 引入四类时变控制信号（响度、音高、频谱中心、音色），实现对声学特征的精确可解释调节。<br/>3. 采用LoRA技术高效适配Foley合成任务，仅需训练原参数的0.85%。<br/>4. 保持高音频质量与语义对齐性，实验在FAD和LAION-CLAP等指标上与基线模型表现相当。<br/>5. 构建可扩展的模块化生成框架，支持序列条件、内存高效处理及三级无分类器引导机制。<br/><br/>**总结（100字内）：**  <br/>本文提出Audio Palette，通过四类时变控制信号实现细粒度音频生成，结合LoRA技术降低训练成本，在保持音质与语义对齐的前提下，构建了支持艺术家创作的可控音频生成框架。|
|2510.12042v2|[FakeMark: Deepfake Speech Attribution With Watermarked Artifacts](http://arxiv.org/abs/2510.12042v2)|总结：  <br/>提出新型水印框架FakeMark，通过注入与生成系统相关的artifacts而非预设bitstring，提升对domain-shifted样本和distortion的鲁棒性，同时结合水印与内在伪造特征增强溯源能力。<br/><br/>贡献点：  <br/>1. **新型水印注入策略**：引入与deepfake系统相关的artifact水印，替代传统预设bitstring消息，增强与生成模型的关联性。  <br/>2. **双线索溯源机制**：同时利用水印和内在deepfake伪造特征，即使其中一信号失效仍可检测，提升鲁棒性。  <br/>3. **跨数据集泛化能力**：显著改善分类器方法在domain-shifted样本上的泛化问题，适应不同数据分布。  <br/>4. **抗干扰检测性能**：在codec压缩、恶意移除等常见distortion下保持高检测准确率，优于传统水印方案。|
|2510.12042v1|[FakeMark: Deepfake Speech Attribution With Watermarked Artifacts](http://arxiv.org/abs/2510.12042v1)|总结：  <br/>提出FakeMark框架，通过注入与深度伪造系统相关的特征水印，解决传统分类器泛化差和水印易被破坏的问题，提升跨数据集检测能力和抗干扰性能。<br/><br/>贡献点：  <br/>1. **创新性水印设计**：首次引入与深度伪造系统相关的Artifact-Correlated水印，替代传统预设的bitstring消息，增强水印与生成特征的关联性。  <br/>2. **双线索归因机制**：结合注入水印和深度伪造的固有伪影（intrinsic artifacts），即使其中一种特征被移除或破坏，仍能保持检测有效性。  <br/>3. **跨数据集泛化能力**：在域移样本（cross-dataset samples）上显著优于分类器方法，提升模型对不同数据源的适应性。  <br/>4. **抗干扰鲁棒性**：在音视频编码压缩、恶意移除等常见扰动下，保持高检测准确率，优于传统水印方案。|
|2510.12000v1|[UALM: Unified Audio Language Model for Understanding, Generation and   Reasoning](http://arxiv.org/abs/2510.12000v1)|**贡献点：**  <br/>1. 提出Unified Audio Language Model (UALM)，首次将音频理解、文本到音频生成和多模态推理统一于单一模型。  <br/>2. 设计UALM-Gen，直接预测音频标记，性能与扩散模型相当，实现文本到音频生成的高质量输出。  <br/>3. 通过数据融合与训练策略，验证单一UALM模型在音频理解、生成和文本推理任务中达到专用模型水平。  <br/>4. 开发UALM-Reason，基于跨模态推理强化复杂生成任务，结合文本与音频信息进行中间推理步骤。  <br/>5. 首次在音频领域实现跨模态生成推理的验证，效果通过主观评估确认。  <br/><br/>**总结（100字以内）：**  <br/>本论文提出UALM模型，首次统一音频理解、文本到音频生成及多模态推理任务，展示单一模型在多个任务中的竞争力，并开发UALM-Reason实现跨模态推理，验证其在音频生成中的有效性。|
|2510.11760v1|[Audio-Guided Visual Perception for Audio-Visual Navigation](http://arxiv.org/abs/2510.11760v1)|**贡献点总结：**  <br/>1. 提出AGVP框架，解决Audio-Visual Embodied Navigation中跨音源泛化能力差的问题。  <br/>2. 引入音频自注意力提取全局听觉上下文，建立听觉与视觉区域的显式对齐机制。  <br/>3. 通过跨模态查询-特征对齐和区域重加权策略，减少对虚假声纹-场景关联的依赖。  <br/>4. 综合时间建模与策略优化，提升导航效率与跨场景鲁棒性。  <br/>5. 实验证明AGVP在未听过的音源和新环境中显著优于现有方法。  <br/><br/>**总结：**  <br/>AGVP通过跨模态对齐与区域重加权设计，提升导航效率和跨场景泛化能力，解决现有方法对特定声纹的依赖问题。|
|2510.11507v2|[Automatic Music Sample Identification with Multi-Track Contrastive   Learning](http://arxiv.org/abs/2510.11507v2)|总结：  <br/>本文提出基于自监督学习和对比学习的自动采样识别方法，显著提升识别性能，具备跨流派鲁棒性且可扩展性强，并系统分析了训练组件的重要性。  <br/><br/>贡献点：  <br/>1. **方法创新**：提出自监督学习框架，通过多轨数据集生成人工正样本对，设计对比学习目标。  <br/>2. **性能提升**：在多个数据集上超越现有SOTA基线，验证方法有效性。  <br/>3. **鲁棒性**：对不同音乐流派和噪声环境均表现稳定。  <br/>4. **可扩展性**：参考数据库中噪声歌曲数量增加时仍能保持性能。  <br/>5. **组件分析**：系统评估训练流程各组件的作用，强调高质量分离音轨的关键性。|
|2510.11507v1|[Automatic Music Sample Identification with Multi-Track Contrastive   Learning](http://arxiv.org/abs/2510.11507v1)|总结：  <br/>本文提出基于自监督学习和对比学习的自动采样识别方法，通过多轨数据集构建人工混音正样本对，实现跨音乐类型和噪声歌曲规模的鲁棒性提升，并系统分析了训练组件对性能的影响。<br/><br/>贡献点：  <br/>1. **自监督学习框架**：利用多轨数据集生成人工混音正样本对，无需标注数据即可训练模型。  <br/>2. **创新对比学习目标**：设计新型对比学习策略，显著提升采样识别性能。  <br/>3. **跨类型鲁棒性**：方法对多种音乐风格均表现稳定，且在噪声歌曲增多时仍可扩展。  <br/>4. **组件分析**：系统评估训练流程各部分贡献，强调高质量音轨分离对任务的关键性。|
|2510.11458v1|[ILD-VIT: A Unified Vision Transformer Architecture for Detection of   Interstitial Lung Disease from Respiratory Sounds](http://arxiv.org/abs/2510.11458v1)|**贡献点：**  <br/>1. 提出基于视觉Transformer（VIT）的新型ILDS检测框架（ILD-VIT），通过呼吸音记录进行疾病识别。  <br/>2. 构建三阶段处理流程：预处理、梅尔频谱图提取、基于VIT的分类，提升检测效率与准确性。  <br/>3. 在公开数据库（BRACETS和KAUH）上验证，实现84.86%准确率、82.67%灵敏度和86.91%特异性，优于现有方法。  <br/>4. 成功在Raspberry Pi 4微控制器上部署，具备实时性和便携性，可作为独立临床系统用于ILD筛查。  <br/><br/>**总结：**  <br/>本文提出基于VIT的ILD检测框架，结合三阶段处理流程与嵌入式部署，实现了高精度检测并验证了其临床实用性。|
|2510.11454v1|[Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented   Reasoning](http://arxiv.org/abs/2510.11454v1)|总结（100字以内）:  <br/>提出Audio-Maestro框架，通过集成外部工具的时间戳输出提升音频语言模型的推理能力，首次实现结构化工具信息与模型联合推理，显著提高多任务音频理解准确率。<br/><br/>贡献点：<br/>1. **框架创新**：设计首个工具增强型音频推理框架Audio-Maestro，通过融合外部工具与模型的联合推理机制，突破传统端到端模型的局限性。<br/>2. **工具调用机制**：实现音频语言模型自主调用外部工具，并有效整合工具输出的结构化时间戳信息至推理过程，提升任务解析能力。<br/>3. **性能提升验证**：实验证明框架在MMAU-Test数据集上显著提升模型表现（Gemini-2.5-flash↑4.7%、DeSTA-2.5↑4.5%、GPT-4o↑3.1%），验证其有效性。<br/>4. **方法首创性**：作为首个将结构化工具输出整合进大模型音频推理流程的框架，填补了多模态系统与专业工具协同处理音频任务的空白。|
|2510.11366v1|[Phase Aware Ear-Conditioned Learning for Multi-Channel Binaural Speaker   Separation](http://arxiv.org/abs/2510.11366v1)|总结：PEASE-8通过八麦克风配置、复数STFT与原始STFT结合、端到端SI-SDR目标训练，实现高效语音分离与混响消除，在复杂声学场景中保持优异性能。<br/><br/>贡献点：  <br/>1. **多麦克风结构**：利用八麦克风配置，增强空间信息捕捉能力，提升混响环境下的分离效果。  <br/>2. **双输入机制**：结合复数STFT与直接原始STFT输入，绕过传统编码器路径以优化重建质量。  <br/>3. **端到端训练**：基于SI-SDR目标，针对直达路径耳标进行联合分离与混响消除训练，无需排列不变训练。  <br/>4. **强鲁棒性**：在混响（T60=0.6s）、无混响及噪声条件下均保持竞争力，SI-SDR达12.37dB，STOI为0.87，PESQ为1.86。|
|2510.11098v1|[VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language   Model Conversational Agents](http://arxiv.org/abs/2510.11098v1)|**贡献点总结（100字以内）:**  <br/>提出首个基于真实中文语音的高质量评估基准VCB Bench，从指令遵循、知识理解及鲁棒性三个维度全面评测LALMs，揭示性能差距并为中文语音对话模型提供可复现、标准化的改进方向。<br/><br/>**分点贡献:**  <br/>1. **建立首个中文语音基准**：构建完全基于真实人类语音的高质量中文语音对话模型基准VCB Bench，突破现有英文中心、合成语音依赖的局限。  <br/>2. **多维评估框架**：从指令遵循（含语音级控制）、知识理解（通用知识、推理、日常对话）和鲁棒性（内容、环境、说话人属性扰动）三个互补维度系统评测模型能力。  <br/>3. **实验证明性能差距**：基于代表性LALMs的实验揭示当前模型在中文语音场景中的显著性能不足，并提出未来优化方向。  <br/>4. **标准化与实用性**：提供可复现、细粒度的评估方法，推动中文语音对话模型的标准化发展并给予实际改进参考。|
|2510.11068v1|[Efficient Edge Test-Time Adaptation via Latent Feature Coordinate   Correction](http://arxiv.org/abs/2510.11068v1)|总结：  <br/>提出了一种面向边缘设备的高效测试时适应方法TED，通过无需反向传播的正向坐标优化，显著降低计算复杂度并实现实时部署。<br/><br/>贡献点：  <br/>1. **提出TED方法**：设计首个基于单实例测试时适应的边缘设备优化方案，采用CMA-ES进行正向仅优化，无需梯度更新或批量处理。  <br/>2. **降低计算复杂度**：通过更新低维潜在表示向量，将计算复杂度降低至原有方法的1/63，实现高效资源利用。  <br/>3. **保持参数冻结**：无需反向传播，维持模型参数不变，降低内存与计算开销，防止知识遗忘。  <br/>4. **实现实用部署**：在ZYNQ-7020平台成功部署，验证方法在真实资源受限场景中的可行性与有效性。  <br/>5. **跨任务有效性**：在图像分类（ImageNet）和关键词识别（Google Speech Commands）任务中均达到SOTA性能。|
|2510.10995v1|[MSRBench: A Benchmarking Dataset for Music Source Restoration](http://arxiv.org/abs/2510.10995v1)|总结（100字以内）：  <br/>提出首个专门用于音乐源恢复的基准MSRBench，包含真实专业混音的原始-处理音轨对及12种现实降级，验证现有方法不足，证明需专用架构提升恢复质量。<br/><br/>贡献点：  <br/>1. **首次构建专用基准**：MSRBench是首个针对音乐源恢复（MSR）的评估基准，填补了现有合成数据和真实数据无法兼顾的空白。  <br/>2. **真实混合与降级数据**：包含由专业混音工程师制作的8类乐器原始-混合音轨对，混合音频覆盖真实生产效果和12种现实世界降级（如模拟设备、环境噪声、有损编码）。  <br/>3. **多维度评估指标**：通过SI-SNR和感知质量（FAD CLAP）指标，量化现有方法（U-Net、BSRNN）的性能局限（SI-SNR分别为-37.8dB和-23.4dB，FAD CLAP约0.7-0.8），强调需针对性设计恢复模型。|
|2510.10948v1|[Unify Variables in Neural Scaling Laws for General Audio Representations   via Embedding Effective Rank](http://arxiv.org/abs/2510.10948v1)|总结（100字以内）:  <br/>提出RankMe作为统一的音频表示质量度量，验证其与模型性能的幂律关系，构建理论与实证结合的框架，为音频基础模型的扩展策略提供指导。<br/><br/>贡献点：  <br/>1. **引入RankMe度量**：首次提出嵌入有效秩（RankMe）作为统一指标，整合音频长度、嵌入维度、模型结构等多因素对表示质量的影响。  <br/>2. **无标签量化方法**：基于信息论理论，开发无监督的RankMe评估框架，无需依赖人工标注即可量化音频嵌入质量。  <br/>3. **揭示幂律关系**：实证发现RankMe与表示质量之间存在一致的幂律关系，为音频模型的可扩展性提供了可靠的理论依据。  <br/>4. **指导模型设计**：建立跨模型规模、数据量、计算预算等多维度的评估体系，为音频基础模型的优化和扩展策略提供实用框架。|
|2510.10911v1|[Delayed 1T to 2H Phase Transition Upon Electrochemical Delithiation of   LiMoS2](http://arxiv.org/abs/2510.10911v1)|总结（100字以内）:  <br/>阐明MoS2在脱锂过程中的相变路径，首次发现电化学循环后其结构从亚稳态1T相恢复至稳态2H相，揭示电化学合成与调控1T-MoS2的可行性，为锂离子电池材料设计提供新见解。<br/><br/>贡献点：  <br/>1. **首次解决脱锂相变机制**：明确MoS2在电化学脱锂过程中从1T相向2H相过渡的时间依赖性，填补了锂离子嵌入/脱出相变路径研究的空白。  <br/>2. **创新实验方法**：采用单片电化学（de）锂化结合微电极阵列，实现对单层MoS2的高精度可控研究，突破传统批量实验的局限性。  <br/>3. **多技术协同分析**：融合电化学电压分析与拉曼光谱技术，同步表征电化学行为与结构演变，提供直接的实验证据支持结论。  <br/>4. **揭示亚稳态相可行性**：展示通过电化学循环可稳定合成1T-MoS2相，为调控材料相态以增强性能提供了新的实验手段和理论依据。|
|2510.10883v1|[Perceptual Compensation of Ambisonics Recordings for Reproduction in   Room](http://arxiv.org/abs/2510.10883v1)|总结：  <br/>提出基于感知驱动的Ambisonics混响补偿方法，通过频谱和空间调整提升播放环境下的语音渲染质量，表现出对头部移动的鲁棒性。<br/><br/>贡献点：  <br/>1. **混响补偿机制**：首次将感知驱动策略引入Ambisonics，通过记录并补偿播放环境中的混响声场组件，减少声学衰减对音质的影响。  <br/>2. **多维度补偿**：在球面调和域中对直接声与混响声的频谱能量、方向到达及间耳相干性（IC）进行联合优化，保留关键听觉线索。  <br/>3. **通道灵活性**：打破传统Ambisonics固定通道数的限制，支持自适应选择通道数量以实现更高效的音频渲染。  <br/>4. **感知验证**：通过对比实验验证方法在感知准确性上优于常规Ambisonics（未补偿）及理想Ambisonics（模拟消声室），提升实际应用效果。  <br/>5. **鲁棒性改进**：在主观测试中证明该方法对听者头部旋转和轻微位移具有较强鲁棒性，增强沉浸式体验稳定性。|
|2510.10785v1|[FAC-FACodec: Controllable Zero-Shot Foreign Accent Conversion with   Factorized Speech Codec](http://arxiv.org/abs/2510.10785v1)|总结：  <br/>提出可控参数的语音声调转换框架，有效平衡转换强度与说话人身份保持，性能与现有系统相当，支持更灵活的声调修改应用。<br/><br/>贡献点：  <br/>1. 引入可调节的用户控制参数，实现对声调修改程度的显式控制，解决现有方法缺乏灵活性的问题。  <br/>2. 专注于发音调整的同时，有效保持语调、音素时长等超音段特征，提升语音自然度。  <br/>3. 实验结果验证了方法在保持说话人身份方面的优越性，并与当前先进AC系统性能相当。  <br/>4. 支持可控的声调转换应用场景，拓宽了语音处理的实际应用范围。|
|2510.10774v2|[ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for   Text-to-Speech Synthesis](http://arxiv.org/abs/2510.10774v2)|总结：ParsVoice是首个大规模波斯语TTS语料库，通过自动化流程生成高质量数据，提升多说话人语音合成效果并开源共享。<br/><br/>贡献点：<br/>1. 构建全球最大规模的波斯语TTS专用语音语料库ParsVoice（3,526小时干净语音，1,804小时高质量数据）<br/>2. 开发自动化数据处理流程，包含BERT句子完成检测、音频-文本对齐优化和波斯语专属质量评估框架<br/>3. 实现多说话人数据集（>470人）的高质量语音生成，质量指标达MOS 3.6/5和SMOS 4.0/5<br/>4. 首次将XTTS模型微调至波斯语，验证数据集对多说话人语音合成的有效性<br/>5. 公开共享数据集，推动波斯语语音技术发展（HuggingFace平台开放）|
|2510.10774v1|[ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for   Text-to-Speech Synthesis](http://arxiv.org/abs/2510.10774v1)|总结：  <br/>本研究构建了最大的波斯语音语料库ParsVoice，包含1804小时高质量语音数据，用于提升TTS技术，促进低资源语言发展，并已开源。<br/><br/>贡献点：  <br/>1. **填补资源空白**：提出ParsVoice，成为首个专为TTS设计的大型波斯语高质量语音语料库，解决波斯语数据不足问题。  <br/>2. **自动化数据处理**：开发基于BERT的句子补全检测器、二分查找边界优化方法及多维质量评估框架，实现高效音频-文本对齐与清洗。  <br/>3. **规模化数据产出**：通过处理2000本有声读物，生成3526小时干净语音及1804小时高质量子集，覆盖470+说话者，提供多样化的语音样本。  <br/>4. **开源促进研究**：公开数据集以加速波斯语语音技术发展，并为其他低资源语言提供可复用的构建模板。|
|2510.10740v1|[Dual Data Scaling for Robust Two-Stage User-Defined Keyword Spotting](http://arxiv.org/abs/2510.10740v1)|**总结**：  <br/>本论文提出DS-KWS两阶段框架，结合CTC与流式音素搜索，引入双数据扩展策略，显著提升用户自定义关键词检测的鲁棒性与零样本性能。<br/><br/>**贡献点**：  <br/>1. **提出两阶段框架**：DS-KWS通过阶段一定位候选片段（CTC+流式音素搜索），阶段二进行多级验证（QbyT+音素匹配模块），提升检测准确性。  <br/>2. **整合多方法策略**：结合CTC-based方法与QbyT-based方法，分别处理音素级和语句级验证，增强模型对复杂语音场景的适应性。  <br/>3. **双数据扩展策略**：  <br/>   - 扩展ASR语料库至1,460小时，强化声学模型；  <br/>   - 利用155k锚类训练音素匹配器，显著提升易混淆词区分能力。  <br/>4. **优越的实验表现**：  <br/>   - 在LibriPhrase上实现6.13% EER和97.85% AUC（Hard子集）；  <br/>   - 在Hey-Snips实现零样本性能接近全样本模型（99.13%召回率，每小时1次误报）。|
|2510.10738v1|[Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR](http://arxiv.org/abs/2510.10738v1)|贡献点：<br/>1. 首次发现通用ASR模型在非典型说话者（如L2学习者）中存在显著性能偏差，加剧语言水平差异  <br/>2. 提出基于CEFR分级语料库的实验验证方法，揭示直接微调Whisper模型的局限性  <br/>3. 开发两个创新方案：(i) 基于语言水平的多任务学习框架 (ii) 针对性频谱增强技术  <br/>4. 建立性能提升与公平性的量化关系：WER降低29.4%、插入/删除错误降低58.6%  <br/>5. 提出解决数据不平衡问题的新思路，有效缩小真实世界中语言水平差距  <br/><br/>总结（100字内）：  <br/>该研究提出基于语言水平的多任务学习与针对性频谱增强策略，显著提升L2学习者的ASR性能并缩小语言水平差距，推动了更公平的语音识别技术发展。|
|2510.10719v1|[SS-DPPN: A self-supervised dual-path foundation model for the   generalizable cardiac audio representation](http://arxiv.org/abs/2510.10719v1)|**贡献点总结（100字以内）:**  <br/>提出SS-DPPN模型，利用自监督双路径对比学习与原型网络，实现心脏音频分类和表示，在无标签数据上达到SOTA性能，具备高数据效率与跨任务泛化能力，适用于心血管疾病诊断及生理信号分析。<br/><br/>**分点贡献:**  <br/>1. **提出自监督基础模型**：SS-DPPN是首个针对心脏音频的自监督基础模型，有效缓解专家标注数据稀缺问题。  <br/>2. **双路径对比学习架构**：联合处理1D波形与2D频谱图，采用混合损失函数提升特征学习能力。  <br/>3. **原型网络优化分类性能**：通过度量学习方法增强模型敏感度，实现可靠且校准的预测。  <br/>4. **高效数据利用**：在无监督场景下仅需1/3标注数据即可达到全监督模型性能。  <br/>5. **跨任务泛化能力**：所学表示可迁移至肺部声音分类与心率估计等其他生理信号任务。|
|2510.10676v1|[Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for   Multilingual Neural Machine Translation](http://arxiv.org/abs/2510.10676v1)|总结（100字以内）：<br/>本文提出Bhasha-Rupantarika多语言翻译系统，通过算法-硬件协同设计实现轻量化与高效性，支持低资源语言翻译。采用超低精度量化（FP4）使模型体积缩小4.1倍，推理速度提升4.2倍，FPGA部署节省1.96倍LUTs和1.65倍FFs，提供可复现的开源方案。<br/><br/>贡献点分项：<br/>1. 提出首个针对资源受限场景的轻量多语言翻译系统（Bhasha-Rupantarika），实现算法-硬件协同优化<br/>2. 首创子字节精度模型部署方法（FP8/INT8/INT4/FP4），验证FP4量化可使模型体积缩减4.1倍<br/>3. 实现推理速度4.2倍提升与吞吐量4.8倍增长，达到FPGA加速器下的实时部署性能<br/>4. 验证系统在印度语与国际语言双向翻译中的通用性，展现低资源语言处理能力<br/>5. 提供完整开源代码与可复现数据集，推动多语言AI系统在边缘设备的部署研究|
|2510.10619v1|[A Machine Learning Approach for MIDI to Guitar Tablature Conversion](http://arxiv.org/abs/2510.10619v1)|总结：  <br/>本文提出一种基于机器学习的吉他谱转录方法，通过数据增强处理非吉他音乐，提升多声部MIDI数据的转录性能，并揭示系统局限性及改进方向。<br/><br/>贡献点：  <br/>1. **首个脱离吉他表达特性的转录方法**：仅依赖MIDI音乐数据生成标准6弦吉他指法，不考虑压弦、滑音等非标准演奏技巧。  <br/>2. **手指伸展范围的建模约束**：基于现实吉他操作假设手指在品丝上的伸展限制，保证转录结果的可演奏性。  <br/>3. **跨乐器音乐转录能力**：提出数据增强策略，使系统能处理非吉他原生音乐（如交响乐部分），并通过人工数据训练提升泛化性能。  <br/>4. **实验验证与反思**：通过对比初始与增强数据的训练结果，量化数据增强对转录性能的提升效果，并分析方法的局限性。|
|2510.10509v1|[MARS-Sep: Multimodal-Aligned Reinforced Sound Separation](http://arxiv.org/abs/2510.10509v1)|总结：  <br/>本研究提出MARS-Sep框架，通过强化学习将声音分离转化为决策问题，结合多模态奖励与渐进对齐方案，显著提升分离结果的语义一致性与信号质量。<br/><br/>贡献点：  <br/>1. **提出MARS-Sep框架**：首次将声音分离任务转化为决策问题，通过强化学习优化，解决传统低层次信号度量与语义质量脱节的问题。  <br/>2. **分层Beta掩码策略**：引入可分解的Beta掩码策略，通过截断信任区域替代、熵正则化和组相对优势归一化提升学习稳定性与效率。  <br/>3. **多模态奖励机制**：利用音频-文本-视觉编码器生成跨模态奖励，直接增强分离结果与语义查询的一致性。  <br/>4. **渐进对齐方案**：设计渐进式对齐策略微调编码器，提升跨模态判别能力与奖励真实性。  <br/>5. **实验验证有效性**：在多任务基准测试中证明方法在文本、音频和图像引导分离中的显著优势，同时提升信号质量与语义表现。|
|2510.10401v1|[Knowledge-Decoupled Functionally Invariant Path with Synthetic Personal   Data for Personalized ASR](http://arxiv.org/abs/2510.10401v1)|**贡献点总结**:<br/>1. 提出知识解耦的FIP框架（KDFIP），结合门控参数隔离策略，分离通用与个性化知识学习。<br/>2. 通过分模块处理（个性化模块适配合成/真实数据，通用模块适配通用数据）减少知识遗忘问题。<br/>3. 引入动态门控机制融合模块输出，提升模型鲁棒性。<br/>4. 在目标说话者上实现29.38%的字符错误率降低，保持良好泛化能力。|
|2510.10396v3|[MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations](http://arxiv.org/abs/2510.10396v3)|总结：  <br/>MRSAudio是首个大规模多模态空间音频数据集，集成双耳/环绕音频、多视角视频、运动轨迹和细粒度标注，支持空间音频生成、理解及多种应用任务，推动沉浸式技术发展。<br/><br/>贡献点：  <br/>1. **首个系统性空间音频数据集**：填补多模态数据集中空间音频缺失的空白，涵盖听觉、视觉多模态信息，支持沉浸式技术研究。  <br/>2. **四场景覆盖**：包含MRSLife（生活场景）、MRSSpeech（语音）、MRSMusic（音乐）、MRSSing（歌唱），覆盖多样化现实环境。  <br/>3. **多模态同步数据**：集成双耳与Ambisonic音频、外向/内向视频、运动轨迹，支持360°空间感知与交互研究。  <br/>4. **细颗粒标注**：提供转录、音素边界、歌词、乐谱、提示等标注，增强数据对语音生成与分析的指导性。  <br/>5. **五大基础任务设计**：定义音频空间化、空间TTS、歌唱语音合成、音乐生成、声音事件定位等任务，推动方法论创新。  <br/>6. **实际应用验证**：实验证明数据集支持高质量空间建模，验证其在空间音频研究中的通用性与有效性。|
|2510.10396v2|[MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations](http://arxiv.org/abs/2510.10396v2)|总结：  <br/>本文提出MRSAudio，首个融合多模态空间音频数据集，涵盖双耳、Ambisonic音频及同步视频、运动轨迹等信息，支持五类关键空间音频任务，推动沉浸式技术的语音研究。<br/><br/>贡献点：  <br/>1. **首个时空感知数据集**：构建MRSAudio，首次系统性整合多模态空间音频数据，突破传统单耳音频限制，为沉浸式技术（如VR/AR）提供更全面的数据支持。  <br/>2. **多场景覆盖**：包含MRSLife、MRSSpeech、MRSMusic、MRSSing四个子集，涵盖真实生活、语音、音乐、歌唱等多样化场景，增强数据集的通用性。  <br/>3. **丰富多模态信息**：集成同步双耳与Ambisonic音频、外在与内在视频、运动轨迹，以及细粒度标注（如音素边界、歌词、乐谱等），提升任务解析能力。  <br/>4. **五类关键任务定义**：提出音频空间化、空间文本转语音、歌唱语音合成、音乐生成及声音事件定位检测五大任务，明确研究方向并验证数据集的适用性。  <br/>5. **开放共享与应用验证**：提供数据集访问链接和演示，证明其在高质量空间建模和多模态语音研究中的实用价值，推动技术落地。|
|2510.10396v1|[MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations](http://arxiv.org/abs/2510.10396v1)|总结（100字以内）:  <br/>提出MRSAudio，首个覆盖多场景的多模态空间音频数据集，包含同步双耳/声场音频、多视角视频及细粒度标注，支持空间音频生成与理解五大核心任务，推动沉浸技术研究发展。<br/><br/>贡献点:<br/>1. **首个多模态空间音频数据集**：填补现有数据集仅提供单耳音频的空白，专为提升空间音频生成与理解研究而设计。<br/>2. **多场景覆盖**：整合MRSLife（生活场景）、MRSSpeech（语音）、MRSMusic（音乐）、MRSSing（演唱）四大组件，涵盖多样真实环境。<br/>3. **多模态同步**：提供双耳音频与声场音频（ambisonic）、外中心（exocentric）与内中心（egocentric）视频、运动轨迹的同步数据。<br/>4. **细粒度标注**：包含转录文本、音素边界、歌词、乐谱、提示等多层级注释，增强数据实用性与研究深度。<br/>5. **五大任务框架**：定义音频空间化、语音-文本空间化、歌唱语音合成、音乐生成、声音事件定位等核心任务，系统性指导研究方向。|
|2510.10249v1|[ProGress: Structured Music Generation via Graph Diffusion and   Hierarchical Music Analysis](http://arxiv.org/abs/2510.10249v1)|总结：  <br/>本研究提出基于Schenkerian分析与扩散模型的可解释音乐生成框架ProGress，解决了现有模型结构不连贯和黑箱问题，通过用户可控生成过程实现高质量音乐创作。<br/><br/>贡献点：  <br/>1. **创新性模型适应**：基于DiGress模型（Vignac等，2023）开发适用于音乐生成的离散扩散框架，提升结构化生成能力。  <br/>2. **SchA启发的短语融合方法**：引入Schenkerian分析的“phrase fusion”技术，增强音乐作品的和声-旋律结构连贯性。  <br/>3. **用户可控生成系统**：设计可调节生成参数的交互框架，使用户能主动控制创作过程以生成符合音乐语义的连贯作品。|
|2510.10175v1|[Peransformer: Improving Low-informed Expressive Performance Rendering   with Score-aware Discriminator](http://arxiv.org/abs/2510.10175v1)|总结：  <br/>本研究提出Peransformer模型，结合分数感知判别器与对齐数据集提升低信息EPR性能，并开发通用EPR指标GEM以统一评估体系，推动系统间客观比较。<br/><br/>贡献点：  <br/>1. **提出Peransformer模型**：设计基于Transformer的低信息EPR系统，融合分数感知判别器（利用乐谱生成的MIDI）与分数到表演的逐音对齐数据集，弥合低/高信息EPR系统差距。  <br/>2. **开发统一评估指标GEM**：扩展现有自动评价指标，引入通用EPR指标（Generalized EPR Metrics），实现跨系统更直接、准确、可靠的客观对比。  <br/>3. **验证SOTA性能**：在低信息EPR系统中达到当前最优效果，通过主观评估证明其生成的MIDI文件在表达性上接近人类演奏。  <br/>4. **解决数据兼容性问题**：通过直接使用乐谱衍生MIDI输入，克服传统高信息系统依赖详细乐谱注释的局限，提升与数字音频工作站（DAW）的兼容性。|
|2510.10173v1|[Chord Colourizer: A Near Real-Time System for Visualizing Musical Key](http://arxiv.org/abs/2510.10173v1)|总结：  <br/>本研究提出Chord Colourizer系统，结合牛顿色彩轮与CQT技术实现音乐调性实时检测与可视化，创新性地引入物理LED展示装置，开发多模态交互界面，拓展音乐教育与艺术表演应用。  <br/><br/>贡献点：  <br/>1. **实时音乐调性检测与可视化**：设计近实时系统，检测音频信号的音乐调性并动态生成颜色编码的键盘布局。  <br/>2. **历史色彩映射**：基于牛顿原色轮为音符分配颜色，建立 Pitch 与 Hue 的历史关联性。  <br/>3. **物理交互装置**：集成 Arduino 控制的 LED 显示与 3D 打印星形扩散器，实现空间反馈的物理化可视化。  <br/>4. **高效特征提取**：采用 CQT 色度特征结合阈值过滤和音调增强技术，精准分离根音、三音及五音。  <br/>5. **置信度机制**：引入置信度评分，仅可视化高确定性的和弦以提升系统可靠性。  <br/>6. **多模态交互设计**：通过 GUI 和物理 LED 显示同步反馈，增强用户对和声内容的感知与互动体验。|
|2510.10087v1|[Matchmaker: An Open-source Library for Real-time Piano Score Following   and Systematic Evaluation](http://arxiv.org/abs/2510.10087v1)|总结：本研究提出Matchmaker开源库，构建统一的实时音乐对齐评估框架，系统比较音乐表示与对齐方法，并在多个大型数据集上进行全面验证，为研究提供基准和实用工具。<br/><br/>贡献点：<br/>1. 提出Matchmaker开源库：首个兼容现代MIR工具的实时音乐对齐框架，解决系统依赖性和兼容性问题。<br/>2. 建立双维度评估体系：系统比较音乐表示方法（如音符提取、节奏分析）与对齐算法（如动态规划、概率模型）的性能。<br/>3. 大规模数据集验证：在(n)ASAP、Batik、Vienna4x22等大型独奏钢琴数据集上进行测试，使用多指标确保评估全面性。<br/>4. 推动研究标准化：创建可复现的基准框架，促进实时音乐对齐领域模型比较的客观性和系统性。|
|2510.10003v1|[MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with   Multi-token Prediction](http://arxiv.org/abs/2510.10003v1)|总结：  <br/>提出多标记预测损失（MTP）用于语音到单元翻译，通过在中间层应用提升隐藏表示，实验验证其有效性和优越性。<br/><br/>贡献点：  <br/>1. **引入MTP损失**：首次将多标记预测损失应用于语音到单元翻译（S2UT），通过预测多个后续标记提升语义完整性。  <br/>2. **优化损失应用位置**：改进初始MTP实现，在中间层（而非最终层）应用损失，增强隐藏表示的早期信息密度。  <br/>3. **提出MTP-S2UT损失**：结合CTC损失的计算过程，设计新型损失函数（MTP-S2UT），实验证明其在翻译质量上表现最优。|
|2510.09926v1|[Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal   Applications](http://arxiv.org/abs/2510.09926v1)|**贡献点**:  <br/>1. 系统化提出CVCNNs基础理论框架，涵盖复卷积、池化层、Wirtinger微分及多复值激活函数。  <br/>2. 设计关键训练优化技术（复值批量归一化、权重初始化），提升训练稳定性。  <br/>3. 首次在图像数据集验证CVCNNs性能，证明其对复值扰动的鲁棒性及与实值网络的竞争力。  <br/>4. 探索CVCNNs在音频分类中的应用，发现保留相位信息对实值MFCCs的潜在优势及挑战。  <br/>5. 创新引入GNNs建模相位信息（边加权方法），实现音频分类性能的显著提升。  <br/>6. 强调相位信息在音频处理中的重要性，揭示复值架构的表达能力与特征利用潜力。  <br/>7. 提出未来需进一步优化相位感知设计以深化复值表示在神经网络中的应用。  <br/><br/>**总结**:  <br/>本研究系统提出CVCNNs理论与方法，验证其在图像与音频任务中的性能优势，揭示相位信息在音频处理中的关键作用，并通过GNNs创新建模相位，为复值神经网络的应用提供新方向。|
|2510.09528v1|[Accent-Invariant Automatic Speech Recognition via Saliency-Driven   Spectrogram Masking](http://arxiv.org/abs/2510.09528v1)|**贡献点总结：**  <br/>1. 提出首个整合口音与方言分类的ASR框架，提升多语言模型对口音/方言的鲁棒性。  <br/>2. 创新性地采用频谱图掩码策略，通过关键区域掩码实现数据增强与模型训练。  <br/>3. 构建首个波斯语多口音系统性基准数据集，填补低资源语言研究空白。  <br/>4. 实验验证该方法在英语和波斯语中的有效性，显著降低词错误率（WER）。  <br/>5. 开源代码与数据集，促进多语言ASR研究的可复现性与进一步发展。  <br/><br/>**总结（100字以内）：** 本文提出一个口音不变的ASR框架，通过频谱图掩码与数据增强提升模型鲁棒性，并构建首个波斯语多口音基准数据集，验证方法在英语和波斯语中的有效性，推动低资源语言研究。|
|2510.09505v1|[Spatially-Augmented Sequence-to-Sequence Neural Diarization for Meetings](http://arxiv.org/abs/2510.09505v1)|贡献点：  <br/>1. 提出结合空间信息（DOA）的SA-S2SND框架，将SRP-DNN估计的DOA特征引入S2SND模型。  <br/>2. 设计两阶段训练策略：先用单通道音频和DOA特征训练，再通过多通道输入与DOA指导优化模型。  <br/>3. 引入模拟DOA生成方法，减少对真实多通道语料的依赖，提升训练灵活性。  <br/>4. 在AliMeeting数据集验证框架有效性，展示空间线索对跨通道建模的显著提升作用。  <br/><br/>总结：  <br/>本文提出SA-S2SND框架，通过整合空间信息与两阶段训练策略提升语音识别效果，实验显示其在AliMeeting数据集上显著优于基线模型。|
|2510.09504v1|[A Study of the Removability of Speaker-Adversarial Perturbations](http://arxiv.org/abs/2510.09504v1)|**贡献点：**<br/>1. 提出针对说话人对抗扰动的**可消除性研究框架**，突破传统防御方法仅减少扰动影响的局限，探索彻底移除扰动的可能性。  <br/>2. 设计三种场景（**无知、半知情、全知情**）模拟不同扰动生成者与模型的信息交互，系统分析扰动消除的条件与效果。  <br/>3. 对比**优化方法**与**前馈扰动生成方法**的差异，揭示其在扰动消除中的表现差异（如半知情场景中前馈模型更有效）。  <br/>4. 在LibriSpeech数据集上验证实验结果，明确不同场景下扰动消除的边界（完全不可消除、部分可消除、几乎恢复原语音）。  <br/><br/>**总结（100字以内）：**  <br/>本研究探讨说话人对抗扰动的可消除性，通过不同场景与方法分析其消除效果，揭示了扰动生成者与模型信息交互对消除能力的影响，并在真实数据集上验证了理论发现。|
|2510.09344v1|[WildElder: A Chinese Elderly Speech Dataset from the Wild with   Fine-Grained Manual Annotations](http://arxiv.org/abs/2510.09344v1)|总结：  <br/>WildElder是首个基于真实场景的中文老年语音语料库，结合细粒度人工标注与专家校验，为老年语音识别与说话人建模研究提供挑战性基准，推动实际应用适配。<br/><br/>**贡献点：**  <br/>1. **首个真实场景老年语音数据集**：突破传统控制环境限制，从在线视频采集多样化的老年语音数据。  <br/>2. **多维度细粒度标注**：包含转录、年龄、性别、口音强度等人工标注信息，增强数据研究价值。  <br/>3. **结合实境数据与专家校验**：通过真实环境数据与专业标注结合，提升数据质量与适用性。  <br/>4. **展现研究潜力与挑战**：实验结果揭示老年语音识别难点，同时验证WildElder作为新基准的有效性。  <br/>5. **开源共享促进研究**：公开数据集及代码，便于学术界复现实验与进一步开发模型。|
|2510.09307v1|[Target speaker anonymization in multi-speaker recordings](http://arxiv.org/abs/2510.09307v1)|**贡献点：**  <br/>1. **提出多说话人场景下的目标匿名化方法**：针对仅需匿名化特定说话人的复杂对话任务（如客服场景），设计适用于多人语音的优化策略。  <br/>2. **改进评估体系**：开发适用于多说话人环境的新型评估框架，更精准地衡量隐私保护与语音质量的平衡。  <br/>3. **识别关键挑战**：指出传统方法在多说话人场景中的不适用性，分析其开发过程中的潜在问题，并提出针对性解决方案。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出针对多说话人对话中特定目标匿名化的有效方法，改进相关评估体系，并分析传统技术的局限性，解决客服场景下的隐私保护与语音质量平衡问题。|
|2510.09245v1|[SynthVC: Leveraging Synthetic Data for End-to-End Low Latency Streaming   Voice Conversion](http://arxiv.org/abs/2510.09245v1)|**贡献点：**<br/>1. 提出SynthVC框架，直接基于合成并行数据实现无ASR依赖的端到端实时语音转换。<br/>2. 通过预训练零样本VC模型生成数据，无需显式内容-说话人分离或识别模块。<br/>3. 融合神经音频编解码器架构，实现低延迟（77.1ms）与高输出保真度的流式推理。<br/>4. 实验验证在自然度、说话人相似度及端到端延迟表现上优于现有流式VC系统。<br/><br/>**总结：**  <br/>SynthVC通过合成数据与神经编解码器实现无ASR依赖的低延迟实时语音转换，有效解决音色泄露问题，在自然度与相似度上超越现有方法。|
|2510.09236v1|[Effects of automotive microphone frequency response characteristics and   noise conditions on speech and ASR quality -- an experimental evaluation](http://arxiv.org/abs/2510.09236v1)|**贡献点**  <br/>1. **建立实验框架**：通过真实车辆噪声信号和多种驾驶条件，系统研究麦克风特性（带宽、幅频响应）对语音通信质量和ASR性能的实际影响。  <br/>2. **量化评估指标**：采用ETSI TS 103 281标准（S-MOS、N-MOS、G-MOS）及SNR等辅助指标，全面分析不同频率响应对感知语音质量的影响。  <br/>3. **ASR性能验证**：基于Word Error Rate (WER)评估ASR引擎表现，揭示麦克风特性对语音识别准确率的关联性。  <br/>4. **关键特性识别**：明确麦克风带宽与幅频响应形状对汽车场景音频质量的核心影响，为OEMs提供明确的选型依据。  <br/>5. **填补研究空白**：针对汽车麦克风选型中缺乏共识的问题，通过实证数据提供科学依据，优化环境鲁棒性与性能的平衡。  <br/><br/>**总结**（100字以内）：研究通过真实车辆噪声数据，实验分析麦克风带宽与幅频响应对语音质量及ASR性能的影响，提出关键特性与评估指标，为汽车麦克风选型提供科学指导。|
|2510.09225v1|[Unsupervised lexicon learning from speech is limited by representations   rather than clustering](http://arxiv.org/abs/2510.09225v1)|总结:  <br/>该研究通过对比不同语音特征与聚类方法，揭示了零资源词分割中表示差异而非聚类方法是性能瓶颈，并提出多种高效系统方案。<br/><br/>贡献点:  <br/>1. 首次在零资源条件下系统分析表示方法与聚类方法的相对影响，明确性能限制因素。  <br/>2. 提出多种自监督语音特征（连续/离散，帧/词级）与聚类算法（K-means、分层、图聚类）的组合方案。  <br/>3. 设计控制实验，分离表示和聚类方法的影响，验证表示变量性是主要瓶颈。  <br/>4. 提出基于动态时间规整和余弦距离的高效系统（最佳与快速替代方案），适用于英语和中文数据。  <br/>5. 揭示跨语言词分割中的共同挑战，为后续模型优化提供理论依据。|
|2510.09161v1|[Impact of HRTF individualisation and head movements in a real/virtual   localisation task](http://arxiv.org/abs/2510.09161v1)|**贡献点总结：**  <br/>1. **评估个性化HRTF的影响**：系统分析了个性化HRTF在提升虚拟声源感知真实感与定位准确性中的作用差异。  <br/>2. **对比渲染技术**：比较了单音箱渲染与双耳渲染（含个性化/非个性化HRTF）在AAR应用中的表现。  <br/>3. **引入头动变量**：研究头动对定位任务的影响，发现其对定位性能的显著作用。  <br/>4. **静态与动态场景差异**：揭示个性化HRTF在静态场景提升真实感，却在动态场景中反而影响定位的反直觉结果。  <br/>5. **实验设计创新**：采用真实视觉对象与半消音环境，增强沉浸感和评估的现实有效性。  <br/><br/>**100字内摘要：**  <br/>本研究评估个性化HRTF对虚拟声源定位与真实感的影响，比较单音箱与双耳渲染技术，并引入头动变量，发现个性化HRTF在动态场景中削弱定位性能，而静态场景下提升真实感，为AAR系统优化提供关键启示。|
|2510.09085v1|[FLToP CTC: Frame-Level Token Pruning via Relative Threshold for   Efficient and Memory-Saving Decoding on Diverse Platforms](http://arxiv.org/abs/2510.09085v1)|**贡献点**  <br/>1. 提出FLToP CTC算法，通过帧级标记剪枝（Frame Level Token Pruning）减少计算与内存消耗。  <br/>2. 引入相对阈值概率机制，动态过滤低概率标记，显著降低资源需求。  <br/>3. 实验验证在LibriSpeech数据集上实现10.5倍运行时加速和2.78倍内存节省，WER影响可忽略。  <br/>4. 简单易用，兼容CPU/GPU等多平台，便于集成到现有CTC解码系统。  <br/>5. 为资源受限环境和实时语音识别提供可扩展解决方案，提升系统效率与可访问性。  <br/><br/>**总结**  <br/>本研究提出FLToP CTC算法，通过帧级标记剪枝显著降低CTC解码的计算和内存负担，实现实时场景下的高效语音识别，兼顾性能与兼容性，推动资源有限环境下的应用落地。|
|2510.09065v1|[MMAudioSep: Taming Video-to-Audio Generative Model Towards   Video/Text-Queried Sound Separation](http://arxiv.org/abs/2510.09065v1)|总结：提出MMAudioSep，基于预训练视频-音频模型实现高效音频分离，优于现有方法且保持原生生成能力，验证了预训练模型在声学任务中的泛化潜力。<br/><br/>贡献点：<br/>1. 提出首个视频/文本引导的音频分离生成模型MMAudioSep，整合预训练视频-音频生成模型与声学分离机制<br/>2. 通过迁移预训练音频生成模型对视频-音频关联性的理解，实现无需从头训练的高效微调<br/>3. 在对比实验中超越现有确定性分离模型和生成模型基准，验证方法有效性<br/>4. 发现微调后的模型仍能保持原始视频-音频生成能力，证明预训练模型的多任务适应性<br/>5. 为声学生成模型在下游任务中的应用提供新的范式，推动跨模态音频处理发展|
|2510.09061v1|[O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion](http://arxiv.org/abs/2510.09061v1)|贡献点总结（100字以内）：  <br/>提出基于合成语音数据的VC方法，通过相同语言不同说话人的配对直接学习语音映射；引入适用于任意说话人-语言转换的灵活策略，提升零样本泛化能力；实验验证在WER和说话人相似度上显著优于现有方法。<br/><br/>分点贡献：  <br/>1. **合成数据驱动的VC框架**：采用高质预训练多说话人TTS模型生成合成语音对（相同语言内容、不同说话人），构建直接的源-目标语音映射，有效保留语言特征的同时捕捉说话人身份特性。  <br/>2. **通用性训练策略**：设计适用于任意对说话人/语言转换的训练机制，显著提升模型对未见过说话人和新语言的零样本适应能力。  <br/>3. **性能提升与对比验证**：在实验中实现16.35%的相对词错误率（WER）降低和5.91%的说话人余弦相似度提升，超越多项SOTA方法，验证了方法的有效性。|
|2510.09025v1|[Déréverbération non-supervisée de la parole par modèle hybride](http://arxiv.org/abs/2510.09025v1)|总结（100字以内）:  <br/>本文提出一种无需配对数据的无监督语音去混响训练策略，利用混响时间（RT60）等有限声学信息提升系统性能，实验表明该方法在多种客观指标上优于现有先进方法，具有更高的性能一致性。<br/><br/>贡献点分点列表:<br/>1. **无监督训练策略**：首次提出基于仅混响语音数据的训练方法，突破传统依赖干燥/混响配对数据的限制。<br/>2. **声学信息利用创新**：引入混响时间（RT60）作为关键特征，通过有限的声学参数优化模型训练。<br/>3. **性能提升**：在多种客观评价指标上验证，方法表现优于当前主流的语音去混响技术，且性能更稳定一致。|
|2510.09016v1|[DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer   and Implicit Alignment](http://arxiv.org/abs/2510.09016v1)|贡献点：<br/>1. **数据构建创新**：提出两阶段方法，通过固定旋律与多样化LLM生成歌词配对构建紧凑种子数据集，并以此训练旋律特异性模型处理大规模高质量中文 singing 数据。<br/>2. **模型架构优化**：设计DiTSinger模型，结合RoPE（旋转位置编码）与qk-norm（查询-键归一化），在深度、宽度和分辨率上系统扩展以提升合成保真度。<br/>3. **对齐机制改进**：引入隐式对齐机制，无需音素级时长标签，通过字符级约束音素-声学注意力，增强模型在噪声或不确定对齐场景中的鲁棒性。<br/>4. **实验验证效果**：通过大量实验验证方法在可扩展性、无对齐依赖性和高保真度方面的有效性，为语音合成领域提供新解决方案。<br/><br/>总结：  <br/>本文提出两阶段数据构建与DiTSinger模型，通过隐式对齐机制解决数据稀缺和模型扩展性问题，实验验证其在中文歌唱语音合成中的高效性与高质量表现。|
|2510.08878v1|[ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible   Audio Generation via Progressive Diffusion Modeling](http://arxiv.org/abs/2510.08878v1)|**贡献点总结**（100字以内）：  <br/>提出ControlAudio框架，将可控TTA生成转化为多任务学习问题，结合渐进式扩散模型与细粒度控制信号；创新数据构建方法增强文本、时间与音素条件信息；设计推理阶段的渐进式引导生成策略，显著提升时序准确性和语音清晰度。<br/><br/>**分点贡献**：  <br/>1. **方法框架创新**：首次将可控TTA生成建模为多任务学习问题，提出渐进式扩散模型（ControlAudio），通过分步策略实现对文本、时间、音素等细粒度条件的精准建模。  <br/>2. **数据增强策略**：开发兼容注释与模拟的跨模态数据构建方法，系统扩充文本、时间序列及音素特征的条件信息，缓解数据稀缺问题。  <br/>3. **推理流程优化**：引入渐进式引导生成机制，按文本→时间→音素顺序逐步强化控制信号，与扩散模型的粗粒度到细粒度采样特性高度契合。  <br/>4. **性能突破**：在大规模数据上实现可扩展的TTA生成，并通过客观指标（如时序准确率）和主观评估显著超越现有方法，验证了方案的有效性。|
|2510.08816v1|[Audible Networks: Deconstructing and Manipulating Sounds with Deep   Non-Negative Autoencoders](http://arxiv.org/abs/2510.08816v1)|总结：本文提出基于非负自编码器（NAEs）的音频分解框架，实现可解释、分层且可控的声音编辑，引入新操作如跨层合成与随机化策略，拓展音乐创作和声音处理的应用可能性。<br/><br/>贡献点：<br/>1. **提出NAEs新应用**：将非负自编码器引入声音分解领域，用于可解释的音频解构与创造性编辑。<br/>2. **方法改进**：通过投影梯度下降实现非负性约束，使分解结果的权重与激活可直接对应频谱形状和时间包络。<br/>3. **分层架构创新**：多层Deep NAE支持层次化表示，实现从宏观音符包络到微观频谱细节的多粒度分解。<br/>4. **新型编辑操作**：引入跨组件合成、跨层合成及多随机化策略，实现音色和事件密度的可控转换。<br/>5. **可视化验证**：通过实际例子的可视化与重合成，证明框架在对象化声音编辑中的灵活性与有效性。|
|2510.08392v1|[MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean   Flows](http://arxiv.org/abs/2510.08392v1)|**总结**：本文提出MeanVC，一种轻量流式零样本语音转换方法，结合AR与NAR框架优势，通过引入mean flows和扩散对抗后训练，显著提升转换质量与效率，参数更少且效果优于现有方案。<br/><br/>**贡献点**：  <br/>1. **提出MeanVC框架**：首次结合自回归（AR）与非自回归（NAR）机制，实现轻量、高效的流式零样本语音转换。  <br/>2. **设计mean flows策略**：在训练中回归平均速度场，通过单步采样直接映射语音流起点到终点，提升转换质量与泛化能力。  <br/>3. **引入扩散对抗后训练**：解决过平滑问题，进一步优化语音质量与真实感。  <br/>4. **实验证明优越性**：在参数量、推理速度和转换效果上超越现有流式零样本VC系统，验证了方法的有效性与实用性。|
|2510.08176v1|[Leveraging Whisper Embeddings for Audio-based Lyrics Matching](http://arxiv.org/abs/2510.08176v1)|**贡献点：**<br/>1. 提出WEALY，首个完全可复现的音频歌词匹配框架，解决现有方法复现性差的问题。  <br/>2. 建立稳健且透明的基准，提升领域研究的可比性与可信度。  <br/>3. 探索多模态扩展，融合文本与声学特征增强模型性能。  <br/>4. 通过标准数据集验证，证明其性能与不可复现的SOTA方法相当。  <br/>5. 提供语言鲁棒性分析、损失函数及嵌入策略的消融研究，深入探讨模型关键组件。  <br/>6. 构建可靠基准，推动音乐信息检索领域语音技术的应用与发展。  <br/><br/>**总结（100字以内）：**  <br/>本文提出可复现的WEALY框架，通过多模态特征融合和系统消融分析，为音频歌词匹配任务建立可靠基准，验证语音技术在音乐信息检索中的潜力。|
|2510.08078v2|[Detecting and Mitigating Insertion Hallucination in Video-to-Audio   Generation](http://arxiv.org/abs/2510.08078v2)|总结：  <br/>提出插入幻觉（Insertion Hallucination）问题，设计系统评估框架与新指标IH@vid/IH@dur，并提出无需训练的PFC方法有效缓解该问题，推动更可靠视频到音频生成模型发展。<br/><br/>贡献点：  <br/>1. **首次定义新问题**：提出“插入幻觉”现象，揭示现有模型生成无视觉对应声源的音频事件（如语音、音乐）的系统性风险。  <br/>2. **系统评估框架**：构建基于多音频事件检测器多数投票的评估体系，增强对插入幻觉的检测能力。  <br/>3. **创新量化指标**：提出IH@vid（视频幻觉比例）和IH@dur（幻觉时长占比）两个新型指标，全面量化问题的严重性。  <br/>4. **无训练缓解方法**：设计Posterior Feature Correction（PFC）推理时方法，通过两阶段生成流程降低幻觉，无需额外训练。  <br/>5. **实验验证有效性**：在主流V2A基准上验证PFC方法，显著减少幻觉（平均超50%），且不损害传统音频质量与同步性指标。|
|2510.08078v1|[Detecting and Mitigating Insertion Hallucination in Video-to-Audio   Generation](http://arxiv.org/abs/2510.08078v1)|总结：  <br/>该研究首次系统性提出并解决视频到音频生成中的插入幻觉问题，通过新指标和训练无关的后处理方法显著降低幻觉发生率，提升生成模型的可靠性。<br/><br/>贡献点：  <br/>1. **定义新问题**：首次提出"插入幻觉（Insertion Hallucination）"概念，揭示视频音频生成中模型常生成无视觉对应源的音频事件（如语音/音乐）的现象。  <br/>2. **构建评估体系**：开发基于多音频检测器集成的系统性评估框架，通过多数投票机制全面检测幻觉问题。  <br/>3. **提出量化指标**：创新性设计两个度量指标：IH@vid（视频幻觉发生率）和IH@dur（幻觉时长占比），实现对幻觉问题的量化分析。  <br/>4. **开发解决方案**：提出训练无关的后处理方法Posterior Feature Correction（PFC），通过双通道音频生成流程（先检测后掩码）有效抑制幻觉。  <br/>5. **验证有效性**：实验证明PFC方法在主流基准测试中将幻觉发生率和时长降低超50%，且不损害传统音频质量评估指标。|
|2510.08062v1|[Attribution-by-design: Ensuring Inference-Time Provenance in Generative   Music Systems](http://arxiv.org/abs/2510.08062v1)|总结：  <br/>本论文提出直接归因与透明版税分配框架，区分训练集与推理集，支持推理时归因以实现可验证补偿，保障艺术家权益与用户使用透明性，为AI音乐生成提供伦理且可扩展的补偿解决方案。<br/><br/>贡献点：  <br/>1. **问题识别**：指出AI生成音乐对现有版税体系的冲击及现行补偿机制的结构性缺陷（可扩展性差、技术不严谨、归因不确定性）。  <br/>2. **框架创新**：设计以直接归因、透明分配为核心的新音乐生成基础设施，明确区分训练数据与生成内容的来源。  <br/>3. **归因机制**：提出两种互补归因方式（训练时归因与推理时归因），并论证推理时归因更符合直接补偿需求，提升可验证性。  <br/>4. **用户赋能**：赋予用户以特定歌曲为条件生成内容的权限，并提供明确的使用许可与归因信息透明度。  <br/>5. **伦理实践结合**：强调技术方案的伦理价值，确保生成系统的公平性与溯源性，满足AI音乐时代的补偿刚性需求。|
|2510.08047v1|[Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic   Speech Recognition](http://arxiv.org/abs/2510.08047v1)|**贡献点：**  <br/>1. 提出一种无需目标真实数据的参数空间修正方法，通过对比源域真实与伪标签数据训练的模型权重差异，直接捕捉伪标签偏差。  <br/>2. 解决传统伪标签方法中系统性、口音特异性错误的问题，提升ASR在域偏移场景下的鲁棒性。  <br/>3. 实验证明该方法在低资源非洲口音数据集（AfriSpeech-200）中对Whisper tiny模型有效，实现高达35%的相对WER下降。  <br/>4. 强调方法的简洁性和实用性，仅需源域数据即可实现对伪标签模型的优化，无需额外标注或复杂流程。  <br/><br/>**总结（100字以内）：**  <br/>提出一种基于源域真实与伪标签数据的参数修正方法，无需目标标签即可消除伪标签偏差，显著提升低资源场景下ASR的鲁棒性，实验证明在十种非洲口音中WER降低35%。|
|2510.08004v1|[Personality-Enhanced Multimodal Depression Detection in the Elderly](http://arxiv.org/abs/2510.08004v1)|**贡献总结（100字以内）：**  <br/>提出面向老年人的多模态抑郁检测模型，融合语音与视觉特征，创新性设计共注意机制和交互模块，有效结合个性特征，显著提升检测性能，为老年抑郁研究提供新思路。<br/><br/>**分点贡献：**  <br/>1. **多模态融合框架**：首次针对老年人群体构建结合语音和视频的抑郁检测模型，综合多模态数据提升诊断准确性。  <br/>2. **音频特征融合**：提出基于共注意机制的多特征融合方法，整合LLDs、MFCCs和Wav2Vec2.0语音特征。  <br/>3. **视觉特征整合**：融合OpenFace、ResNet与DenseNet提取的视频特征，构建全面的视觉表征。  <br/>4. **个性-多模态交互模块**：设计专门交互模块，建模个性特征与多模态数据的协同关系，提升抑郁检测的个性化能力。  <br/>5. **实验验证与性能提升**：在MPDD 2025老年抑郁检测赛道中验证方法有效性，显著优于现有技术，为后续研究提供基准。|
|2510.07979v1|[IntMeanFlow: Few-step Speech Generation with Integral Velocity   Distillation](http://arxiv.org/abs/2510.07979v1)|1. 提出IntMeanFlow框架，通过积分速度蒸馏解决MeanFlow在TTS中因Jacobian-Vector Products（JVP）导致的GPU内存开销和自引导训练不稳定性问题。  <br/>2. 开发Optimal Step Sampling Search（O3S）算法，优化模型特定的采样步数以提升合成质量，同时避免增加推理计算开销。  <br/>3. 实现低NFE（1-NFE或3-NFE）的高效推理，保持高质量文本到语音合成效果。  <br/>4. 提供实际演示样本，验证方法在token-to-spectrogram和text-to-spectrogram任务中的有效性。|
|2510.07909v1|[Bloodroot: When Watermarking Turns Poisonous For Stealthy Backdoor](http://arxiv.org/abs/2510.07909v1)|贡献点总结（100字以内）:  <br/>本文提出Watermark-as-Trigger新概念，结合Bloodroot框架与对抗性LoRA微调，显著提升音频后门攻击的隐蔽性与效果，在语音识别和说话人识别任务中实现高触发成功率和干净样本准确率，并验证其在声学过滤、模型剪枝等场景下的鲁棒性，同时揭示数据所有权保护与对抗性误用风险。<br/><br/>分点贡献:  <br/>1. **提出新型Watermark-as-Trigger机制**：首次将音频水印与后门触发器结合，实现隐蔽性与攻击效果的平衡。  <br/>2. **改进Bloodroot框架**：通过对抗性LoRA微调技术，提升感知质量与触发成功率，同时保持干净样本准确率。  <br/>3. **验证鲁棒性与风险**：实验表明水印投毒在语音识别（SR）和说话人识别（SID）任务中对声学过滤、模型剪枝等攻击具有抗性，并凸显数据所有权保护与潜在对抗性误用风险。|
|2510.07908v2|[Guitar Tone Morphing by Diffusion-based Model](http://arxiv.org/abs/2510.07908v2)|**贡献点总结**（100字以内）:  <br/>提出基于学习的吉他音色变换方法，结合LoRA微调与Music2Latent球面插值，实验证明生成更平滑自然的过渡效果，提升音乐制作效率。  <br/><br/>**分点贡献**  <br/>1. **引入LoRA微调框架**：针对有限数据场景，优化模型性能，降低训练成本。  <br/>2. **设计Music2Latent球面插值方法**：提出简化技术路径，显著优于传统复杂微调方式。  <br/>3. **验证实际应用价值**：实验表明该架构在音乐制作和实时音频处理中表现更自然、高效。|
|2510.07908v1|[Guitar Tone Morphing by Diffusion-based Model](http://arxiv.org/abs/2510.07908v1)|总结：  <br/>本研究提出基于学习的吉他音色变形方法，优化LoRA微调并引入更简单的球形插值技术，显著提升音色过渡的平滑性与自然度，为音乐制作和实时音频处理提供高效实用工具。<br/><br/>贡献点：  <br/>1. 提出适用于吉他音色建模的全新学习框架，解决数据有限场景下的模型性能提升问题；  <br/>2. 首次将LoRA微调技术应用于吉他音色变形，实现高效参数优化；  <br/>3. 引入"Music2Latent球形插值"方法，通过简化流程显著优于传统复杂微调方案；  <br/>4. 验证方法在音乐制作场景的实用性，实现高质量实时音色过渡效果；  <br/>5. 为MIR领域提供新的音色转换范式，推动个性化音乐创作技术发展。|
|2510.07840v1|[ACMID: Automatic Curation of Musical Instrument Dataset for 7-Stem Music   Source Separation](http://arxiv.org/abs/2510.07840v1)|总结：提出ACMID数据集，通过网络爬虫和自动清洗技术获取高质量音乐源分离数据，扩展乐器分类至7类，并验证数据清洗对模型性能的显著提升，开源代码促进研究复现。<br/><br/>贡献点：  <br/>1. **构建高质量音乐源分离数据集ACMID**：通过大规模网络爬虫收集原始数据，并利用预训练音频编码器构建的自动清洗模型，过滤并聚合干净目标乐器段，生成ACMID-Cleaned数据集。  <br/>2. **扩展乐器分类粒度**：将传统4类（Vocal/Bass/Drums/Others）扩展至7类（Piano/Drums/Bass/Acoustic Guitar/Electric Guitar/Strings/Wind-Brass），支持更高精度的音乐源分离任务。  <br/>3. **验证数据清洗有效性**：实验证明，基于ACMID-Cleaned数据集训练的模型在SDR指标上比未清洗数据提升2.39dB，且用于训练可使模型平均性能提升1.16dB。  <br/>4. **开源实现与可复现性**：公开数据爬取代码、清洗模型代码及预训练权重，方便研究者复现和扩展该数据集及相关方法。|
|2510.07838v1|[Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex   Dialogue Systems with an Automated Examiner](http://arxiv.org/abs/2510.07838v1)|**总结（100字以内）:**  <br/>该论文提出FDB-v2框架，系统评估多轮全双工语音系统的性能问题，支持多种任务类型，通过标准化协议和可扩展性设计促进社区对全双工对话技术的研究与验证。<br/><br/>**贡献点:**  <br/>1. **填补研究空白**：首次系统性评估全双工语音系统在多轮交互中的连贯性与任务执行能力。  <br/>2. **双节奏目标设计**：引入“快速”与“慢速”两种节奏模式，模拟不同对话场景下的控制策略。  <br/>3. **多任务覆盖**：构建包含日常对话、更正、实体跟踪和安全四类任务的完整测试框架。  <br/>4. **可扩展性支持**：兼容商业API与开源模型，提升框架的通用性与灵活适应性。  <br/>5. **标准化协议**：提供开源、标准化的流式交互协议，简化新任务家族的集成与评估流程。|
|2510.07837v1|[IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text   Intermediaries](http://arxiv.org/abs/2510.07837v1)|总结：  <br/>提出端到端手语视频到语音翻译框架IsoSignVid2Aud，通过整合I3D特征提取、专用变换网络和音频生成模块，结合改进的NMS算法，在无需文本中间步骤的情况下实现高效准确的翻译。<br/><br/>贡献点：  <br/>1. **孤立手势处理**：专注于非连贯语法化手语序列的视频翻译，适用于教育和交互场景。  <br/>2. **端到端框架设计**：直接将手语视频转化为语音，避免传统多阶段翻译的延迟和误差传递。  <br/>3. **多模态结构创新**：融合I3D特征提取、专用特征变换网络与音频生成模块，提升整体性能。  <br/>4. **新型NMS算法**：提出针对非连贯手势序列的时序检测方法，优化关键帧识别准确率。  <br/>5. **实验验证性能**：在ASL-Citizen-1500和WLASL-100数据集上达到72.01%和78.67%的Top-1准确率，语音质量指标（PESQ: 2.67, STOI: 0.73）表现优异。  <br/>6. **代码开源**：提供可复现的代码，促进研究社区的技术共享与进一步开发。|
|2510.07592v1|[SALAD-VAE: Semantic Audio Compression with Language-Audio Distillation](http://arxiv.org/abs/2510.07592v1)|总结（100字以内）:  <br/>提出SALAD-VAE模型，实现高效音频压缩与高质量重建，在低潜在帧率下保持语义结构，改进语义损失函数提升跨域泛化能力，并支持零样本音频字幕生成和分类任务。<br/><br/>贡献点:  <br/>1. **高效语义音频压缩**：在频域操作，以极低潜在帧率（7.8Hz）实现SOTA压缩性能，同时保留音频语义结构和高质量重建。  <br/>2. **增强语义学习机制**：改进标准VAE的语义损失与增强策略，引入对比学习和CLAP嵌入蒸馏，提升跨音频领域的泛化能力。  <br/>3. **轻量高效架构**：相比现有SOTA VAE模型，SALAD-VAE具有显著更低的计算复杂度，重建质量相当且分类性能更优。  <br/>4. **零样本任务支持**：通过额外损失函数训练CLAP投影层，可直接用于零样本音频字幕生成及分类任务，匹配预训练CLAP嵌入效果。|
|2510.07442v1|[INFER : Learning Implicit Neural Frequency Response Fields for Confined   Car Cabin](http://arxiv.org/abs/2510.07442v1)||
|2510.07437v1|[LASER: An LLM-based ASR Scoring and Evaluation Rubric](http://arxiv.org/abs/2510.07437v1)|总结：  <br/>提出基于LLM的新型语音评估框架LASER，通过上下文学习和多语言示例提升评估精度，实现Hindi高达94%的相关性，并验证其跨语言适应性和对小型模型的可行性。<br/><br/>贡献点：  <br/>1. **提出LASER评估框架**：利用LLM的上下文学习能力，通过详细提示示例改进传统ASR指标（如WER）对形态/语法差异的不合理惩罚。  <br/>2. **验证跨语言有效性**：Hindi LASER示例在分析Marathi、Kannada等其他印度语言错误时表现优异，证明其泛化能力。  <br/>3. **小模型可行性**：展示Llama 3等小型LLM通过词对训练样本可达到89%的惩罚预测准确率，降低对大模型的依赖。  <br/>4. **高相关性结果**：Gemini 2.5 Pro在Hindi数据上实现与人类标注94%的强相关性，证明方法的可靠性与优越性。|
|2510.06961v2|[Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual   and Long-Form Speech Recognition Evaluation](http://arxiv.org/abs/2510.06961v2)|**贡献点总结**  <br/>1. 提出Open ASR Leaderboard，首次综合评估多语言及长文本场景下的ASR系统，涵盖60+模型与11个数据集；  <br/>2. 标准化文本规范化流程，同时报告WER与RTFx指标，实现准确率与效率的公平比较；  <br/>3. 分析Conformer+LLM（高准确率但效率低）与CTC/TDT（高效率适合长/离线场景）解码器的性能差异；  <br/>4. 开源所有代码与数据集加载器，确保评估过程透明且可扩展。  <br/><br/>（99字）|
|2510.06544v2|[Benchmarking Fake Voice Detection in the Fake Voice Generation Arms Race](http://arxiv.org/abs/2510.06544v2)|**贡献点总结（分点）**  <br/>1. **提出生态系统级基准**：首个系统性评估17种生成器与8种检测器互作用的基准，通过一对一协议揭示方法间差异。  <br/>2. **解决传统评估局限**：突破现有聚合数据集的弊端，避免检测器性能被掩盖，暴露生成器与检测器间的隐藏漏洞。  <br/>3. **统一量化评分体系**：设计统一指标量化生成器的欺骗性与检测器的鲁棒性，实现公平且直接的对比分析。  <br/>4. **发现架构驱动的有效性差异**：实验证明神经音频编解码器和流匹配生成器对顶级检测器具有显著逃避能力，检测器性能依赖生成器架构。  <br/>5. **揭示防御泛化不足**：强调当前检测系统的泛化能力缺陷，为构建更有效的下一代检测系统提供针对性指导。  <br/><br/>**总结（100字内）**  <br/>本研究提出首个生态系统级语音生成-检测互评基准，通过一对一协议与统一评分体系，揭示生成器与检测器间的隐藏漏洞及架构依赖关系，为提升检测系统的泛化能力提供关键洞见。|
|2510.06204v1|[Modulation Discovery with Differentiable Digital Signal Processing](http://arxiv.org/abs/2510.06204v1)|总结：  <br/>本文提出一种基于神经网络的声匹配方法，结合modulation提取、约束参数化和可微DSP技术，有效解析复杂音频中的调制信号，并探讨了可解释性与匹配精度的平衡，提供了开源工具和VST插件。<br/><br/>贡献点：  <br/>1. **提出神经声匹配框架**：首次将modulation提取、受限控制信号参数化与DDSP结合，系统性解析音频中的调制信号。  <br/>2. **考虑调制曲线结构**：在参数估计中引入形状、结构及路由分析，突破传统高维帧级预测的局限。  <br/>3. **验证方法通用性**：在多种DDSP合成器架构上测试，证明其适用于不同合成技术。  <br/>4. **平衡可解释性与精度**：量化分析方法在模型透明度和声匹配效果之间的权衡关系。  <br/>5. **开源实现与工具支持**：发布代码、音频样本及VST插件，便于实际应用与研究复现。|
|2510.06201v1|[TokenChain: A Discrete Speech Chain via Semantic Token Modeling](http://arxiv.org/abs/2510.06201v1)|总结：  <br/>TokenChain通过全离散语义标记的ASR与两阶段TTS联合训练，结合直通反馈与动态加权，提升了语音处理效率，实验证明其在多个数据集上显著优于基线模型。<br/><br/>贡献点：  <br/>1. **提出TokenChain框架**：构建全离散语音链，集成语义标记ASR与两阶段TTS（文本到语义+语义到声学生成）。  <br/>2. **创新训练策略**：采用自回归文本到语义模型与ASR联合训练，引入掩码生成语义到声学模型，通过直通argmax/Gumbel-Softmax实现端到端反馈。  <br/>3. **动态平衡机制**：利用动态权重平均平衡监督ASR与生成目标，优化模型收敛稳定性。  <br/>4. **温度调度优化**：通过消融实验确定域内/跨域迁移的最佳温度策略，提升泛化能力。  <br/>5. **高效性能验证**：在LibriSpeech和TED-LIUM数据集上，实现比基线更早收敛、更低错误率（ASR WER降低56%，T2S WER降低31%），且遗忘率低。|
|2510.06195v1|[Latent Speech-Text Transformer](http://arxiv.org/abs/2510.06195v1)|总结（100字以内）:  <br/>本研究提出Latent Speech-Text Transformer（LST），通过动态聚合语音标记为高层单元，解决语音-文本模型预训练中的计算不平衡问题，在多个基准测试中实现更优代表对齐和更快扩展律，同时提升语音和文本任务性能并开源相关资源。<br/><br/>贡献点:  <br/>1. **提出LST模型**：设计一种新型自回归语音-文本模型，通过动态、低成本聚合语音标记为潜在语音块（latent speech patches），提升数据效率。  <br/>2. **解决计算不平衡问题**：通过压缩语音标记序列长度，缓解预训练和推理阶段语音与文本模态间的计算资源分配不均。  <br/>3. **高层语义对齐机制**：潜在语音块可与文本单元对齐，促进跨模态能力迁移，同时封装常见语音模式（如静默）以优化计算效率。  <br/>4. **实验验证效能**：在控制数据和计算的实验中，LST在语音-语音、文本-文本基准测试中均优于传统方法，且在HellaSwag任务中提升语音准确率6.5%（计算控制）和5.3%（数据控制）。  <br/>5. **开源促进研究**：公开模型、代码及评估数据，推动语音-文本跨模态研究的进一步发展。|
|2510.05984v1|[ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency   Tuning](http://arxiv.org/abs/2510.05984v1)|总结：该论文提出ECTSpeech框架，首次将ECT策略应用于语音合成，通过多尺度门控模块提升去噪效果，在单步生成中实现与SOTA相当的音质，同时有效降低训练成本与复杂度。<br/><br/>贡献点：<br/>1. 提出ECTSpeech框架，实现首个基于Easy Consistency Tuning (ECT)策略的一步语音合成方法<br/>2. 通过逐步强化一致性约束机制，在保持高质量生成的同时显著降低训练复杂度<br/>3. 设计多尺度门控模块(MSGate)，增强去噪器在不同特征尺度上的融合能力<br/>4. 在LJSpeech数据集上验证，在单步采样条件下达到SOTA水平的音质表现，且训练效率大幅提升|
|2510.05934v1|[Revisiting Modeling and Evaluation Approaches in Speech Emotion   Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions](http://arxiv.org/abs/2510.05934v1)|总结：该论文提出多角度改进语音情感识别方法，通过保留个体标注差异、引入软标签分布、允许多情感预测，构建更符合人类情感主观性与多样性的SER系统。<br/><br/>贡献点：<br/>1. **多标签建模**  <br/>   - 首次将多标注者差异视为有效信息，而非噪声  <br/>   - 采用软标签分布表示情感，整合个体标注数据提升共识标注测试性能<br/><br/>2. **全包含评估框架**  <br/>   - 突破传统单标签评估范式，支持多情感共现  <br/>   - 提出"all-inclusive rule"，通过最大化标签多样性增强模型泛化能力<br/><br/>3. **情感冲突约束机制**  <br/>   - 构建惩罚矩阵抑制不合理情感组合  <br/>   - 将约束项融入损失函数，实现更精准的情感预测<br/><br/>（99字）|
|2510.05922v1|[Revisiting MFCCs: Evidence for Spectral-Prosodic Coupling](http://arxiv.org/abs/2510.05922v1)|贡献点：<br/>1. 挑战传统假设：首次系统论证MFCCs并非缺乏时序信息，推翻长期存在的语音处理领域认知。<br/>2. 建立评估框架：采用null hypothesis significance testing方法量化分析MFCCs与韵律特征的统计关联性。<br/>3. 韵律特征验证：明确MFCCs与能量、F0、发声三个核心韵律特征存在显著统计依赖关系。<br/>4. 信息隐含揭示：证明MFCCs蕴含可提取的语音韵律信息，为特征提取方法改进提供理论依据。<br/>5. 模型设计启示：为语音分析与识别模型的特征工程和架构优化提供新的研究方向。<br/><br/>总结：该研究通过统计检验揭示MFCCs与关键语音韵律特征存在显著关联，推翻其缺乏时序信息的传统认知，证实其蕴含重要韵律信息，为语音模型设计提供新思路。|
|2510.05881v1|[Segment-Factorized Full-Song Generation on Symbolic Piano Music](http://arxiv.org/abs/2510.05881v1)|总结：本文提出SFS模型，通过分段与注意力机制提升音乐生成质量与效率，并构建交互式网页应用实现用户协作创作。<br/><br/>贡献点：  <br/>1. 提出Segmented Full-Song Model（SFS）模型，支持用户定义歌曲结构和种子段进行全曲生成。  <br/>2. 引入分段生成机制，通过选择性注意力关联相关段落，实现更高质量与更高效音乐生成。  <br/>3. 开发基于钢琴滚卷的交互式网页应用，允许用户迭代式协作创作音乐，具备结构可定制和顺序灵活的特性。|
|2510.05829v1|[FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal   Encoders](http://arxiv.org/abs/2510.05829v1)|贡献点：  <br/>1. 提出 FoleyGRAM，通过跨模态对齐（GRAM）实现视频到音频生成的语义控制  <br/>2. 构建基于扩散模型的音频合成框架，结合语义对齐嵌入与波形包络实现时序同步  <br/>3. 首次将 Gramian 表示对齐度量应用于多模态编码器对齐，提升音频生成与视频内容的语义一致性  <br/>4. 在 Greatest Hits 数据集上验证方法有效性，推动视频到音频合成技术的发展  <br/><br/>总结：  <br/>本研究提出 FoleyGRAM，通过GRAM对齐多模态嵌入和扩散模型，实现视频到音频生成的语义控制，提升了合成质量。|
|2510.05828v1|[StereoSync: Spatially-Aware Stereo Audio Generation from Video](http://arxiv.org/abs/2510.05828v1)|总结：  <br/>StereoSync提出了一种新颖的视频对齐音频生成模型，通过空间感知与预训练模型结合，在保持高效的同时实现时空同步，显著提升音频沉浸感和真实性。<br/><br/>贡献点：  <br/>1. **填补研究空白**：首次系统性研究视频对齐音频生成，解决传统方法仅关注时间同步的不足。  <br/>2. **时空同步创新**：引入空间意识，使音频生成与视频场景的视觉结构和动态变化同步。  <br/>3. **高效模型设计**：利用预训练基础模型减少训练需求，同时保持高质量音频合成。  <br/>4. **空间线索提取**：通过深度图和边界框提取空间特征，作为扩散模型的交叉注意力条件。  <br/>5. **动态音频适应**：生成的立体声音频可实时响应视频场景的空间布局和运动变化。  <br/>6. **实验证明效果**：在Walking The Maps数据集上验证了模型的时空对齐能力及增强的沉浸式体验。|
|2510.05799v1|[Data-efficient Targeted Token-level Preference Optimization for   LLM-based Text-to-Speech](http://arxiv.org/abs/2510.05799v1)|**贡献点：**<br/>1. 提出无需配对数据的TKTO方法，突破现有TTS偏好优化对配对样本的依赖，提升数据效率。<br/>2. 直接优化token级单位，自动提供细粒度的对齐信号，无需人工标注token级反馈。<br/>3. 在日语TTS任务中实现显著性能提升：准确率提高39%，CER降低54%，且对目标token的奖励强度增强12.8倍。<br/><br/>**总结（100字以内）：**  <br/>本文提出TKTO方法，通过消除配对数据需求与直接token级优化，提升日语TTS的准确性和自然度，显著降低字符错误率（CER），并增强对关键token的奖励机制。|
|2510.05757v1|[Neural Forward Filtering for Speaker-Image Separation](http://arxiv.org/abs/2510.05757v1)|总结：  <br/>该研究提出CxNet方法，通过结合物理模型与双DNN结构，在混响环境下有效分离多说话人语音，保留混响特性。<br/><br/>贡献点：  <br/>1. 提出CxNet，采用双DNN系统架构，引入神经前向滤波模块，显式建模混响语音与直接路径信号间的物理约束。  <br/>2. 设计端到端训练策略，使第一DNN同时预测直接路径信号与混响语音，第二DNN利用滤波后的混响估计作为判别特征提升性能。  <br/>3. 通过直接路径估计反推线性滤波器，并结合卷积操作捕捉混响尾部信息，增强分离鲁棒性。  <br/>4. 在SMS-WSJ数据集上验证算法有效性，表明该方法在混响条件下的多说话人语音分离表现优于传统端到端方法。|
|2510.05756v1|[Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music](http://arxiv.org/abs/2510.05756v1)|总结：  <br/>提出三步骤框架实现高精度吉他节奏模式转录，构建专家标注数据集，引入可读性评估指标并自动检测节拍与拍号，推动音乐节奏分析研究。<br/><br/>贡献点：  <br/>1. 构建首个基于专家转录的流行歌曲节奏模式数据集（410首）。  <br/>2. 提出三阶段框架：音轨分离→扫弦检测（MERT模型）→模式解码（专家词汇库）。  <br/>3. 实现高准确率的自动化节奏模式表示，含节拍线与拍号标记。  <br/>4. 设计评估指标体系，量化节奏序列的准确性与可读性。  <br/>5. 首次结合人类专家知识与自动检测，提升音乐节奏编码的实用性。|
|2510.05718v1|[Investigation of perception inconsistency in speaker embedding for   asynchronous voice anonymization](http://arxiv.org/abs/2510.05718v1)|总结：  <br/>本研究通过分析说话人嵌入的子空间特性，提出了一种异步语音匿名化方法，在保持人类感知的同时完全消除机器识别能力。<br/><br/>贡献点：  <br/>1. **首次揭示感知差异**：发现机器与人类对说话人属性的感知在嵌入空间中存在本质差异，突破传统语音生成框架的局限。  <br/>2. **提出子空间理论**：通过实验确定了一类可移除的子空间，其去除会导致机器感知失效但人类感知保持不变。  <br/>3. **开发匿名化方法**：基于子空间理论构建异步语音匿名化系统，实现100%人类感知保留率，有效隐藏语音特征。  <br/>4. **提供公开资源**：开放音频样本数据集，便于验证方法效果及后续研究。|
|2510.05696v1|[Sparse deepfake detection promotes better disentanglement](http://arxiv.org/abs/2510.05696v1)|总结：  <br/>本研究提出基于稀疏表示的深度伪造语音检测方法，通过TopK激活机制提升检测性能，并揭示潜在空间中攻击模式的编码特性。<br/><br/>贡献点：  <br/>1. **提出稀疏表示方法**：基于AASIST架构最后一层嵌入，引入TopK激活机制（受SAEs启发），生成高度稀疏的特征表示（95%稀疏度）。  <br/>2. **提升检测性能**：实验表明该方法在ASVSpoof5测试集上实现23.36%的EER，显著优于传统方法。  <br/>3. **解耦效果验证**：通过互信息的完成度和模块度指标，证明稀疏表示具有更优的潜在特征解耦能力。  <br/>4. **揭示潜在空间特性**：发现部分攻击在潜在空间中被直接编码，为深度伪造检测提供新的理论视角。|
|2510.05619v1|[Teaching Machines to Speak Using Articulatory Control](http://arxiv.org/abs/2510.05619v1)|**总结（100字以内）:**  <br/>提出显式发音控制框架，将语音生成视为动作控制任务，结合强化学习与声学反馈训练发音策略，通过预训练解码器转换发音轨迹为语音，在目标音节上实现高相似度与可理解性生成。<br/><br/>**贡献点：**  <br/>1. **框架创新**：首次将语音生成建模为发音器官（如舌、唇、颌）的显式运动控制任务，突破传统黑箱模型的局限。  <br/>2. **强化学习应用**：采用PPO算法结合自研声学感知器Sylber，通过反馈训练发音策略，实现音节级语音生成。  <br/>3. **端到端系统**：集成Sylber（声学反馈模块）与SPARC（预训练发音-语音解码器），构建从运动指令到语音的完整生成链。  <br/>4. **实验验证**：在6个目标音节上验证框架有效性，生成音频与目标相似度超0.85，且人类转录准确证明语音可理解性。  <br/>5. **可解释性提升**：通过显式控制发音器官运动，增强系统对人类语音物理机制的建模与解释能力。|
|2510.05478v1|[AQA-TTRL: Self-Adaptation in Audio Question Answering with Test-Time   Reinforcement Learning](http://arxiv.org/abs/2510.05478v1)|总结：本文提出AQA-TTRL框架，通过测试时自适应提升LALMs性能，采用伪标签生成、强化学习优化及置信度加权方法，在多基准测试中实现显著性能提升。<br/><br/>贡献点：<br/>1. 提出AQA-TTRL框架，首次实现LALM在测试时的动态自适应优化，无需额外标注数据<br/>2. 创新性结合多数投票伪标签生成与强化学习，构建端到端的自监督优化流程<br/>3. 设计置信度加权机制，有效降低自生成标签的噪声影响<br/>4. 引入多尝试抽样策略，解决优势崩溃问题并提升训练稳定性<br/>5. 在MMAU/MMAR/MMSU基准上验证方法有效性，实现3B模型超越7B模型性能|
|2510.05305v1|[WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech   Deepfake Detection](http://arxiv.org/abs/2510.05305v1)|总结（100字以内）:  <br/>本研究提出参数高效的语音深度伪造检测前端方法，将提示微调与信号处理变换结合，设计FourierPT-XLSR及两种小波变体，并开发WaveSP-Net架构。通过多分辨率特征注入提升伪影定位能力，在新基准上取得显著性能优势，且参数量低，代码开源。<br/><br/>贡献点分点：  <br/>1. **参数高效方法**：提出无需全微调大模型（如XLSR）的参数高效前端框架，解决传统方法的泛化不足问题。  <br/>2. **信号处理融合**：创造性结合傅里叶变换与小波变换（WSPT-XLSR、Partial-WSPT-XLSR），增强对语音特征的建模能力。  <br/>3. **新型架构WaveSP-Net**：设计融合Partial-WSPT-XLSR前端与双向Mamba后端的模型，提升检测性能。  <br/>4. **多分辨率特征注入**：通过将多尺度信号特征嵌入提示嵌入，无需修改冻结参数即可增强合成伪影的定位精度。  <br/>5. **基准测试验证**：在Deepfake-Eval-2024和SpoofCeleb等新挑战性数据集上验证有效性，展示低参数量下的性能优势。  <br/>6. **开源代码与模型**：提供完整实现资源，促进技术复现与应用。|
|2510.04956v2|[MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive   Hierarchical Neural Modeling](http://arxiv.org/abs/2510.04956v2)|总结：本文提出MuFFIN模型，联合处理发音错误检测与诊断（MDD）及发音能力评估（APA），引入音素对比有序正则化机制和针对性训练目标，有效解决数据不平衡问题，在基准数据集上取得SOTA性能。<br/><br/>贡献点：<br/>1. 提出MuFFIN模型，首创将MDD与APA联合建模的交互式层次神经架构，突破传统独立任务处理范式<br/>2. 创新性设计phoneme-contrastive ordinal regularization机制，通过捕获音素特征空间细微差异并考虑评分顺序性，提升特征判别能力<br/>3. 针对MDD的数据不平衡问题，开发phoneme-specific扰动训练目标，优化音素分类器输出分布并保留发音错误特性<br/>4. 在Speechocean762数据集上验证方法有效性，实现APA和MDD任务的SOTA性能，展示跨任务建模优势|
|2510.04956v1|[MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive   Hierarchical Neural Modeling](http://arxiv.org/abs/2510.04956v1)|**贡献点（分点）**  <br/>1. **提出联合模型**：设计MuFFIN，通过交互式分层神经架构联合解决MDD（误发音检测与诊断）和APA（自动发音评估）任务，而非分开建模。  <br/>2. **引入对比正则化机制**：提出音素对比有序正则化方法，优化特征生成，增强音素区分性并考虑评估维度的有序性。  <br/>3. **解决数据不平衡问题**：设计针对MDD的训练目标，通过音素特定的变化调整分类器输出，改善预测分布与误发音特征建模。  <br/>4. **实验验证有效性**：在Speechocean762数据集上展示方法优于现有基准，实现MDD和APA任务的SOTA性能。  <br/><br/>**总结（100字以内）**  <br/>本文提出MuFFIN模型，联合处理发音检测与评估任务，引入对比正则化机制优化特征生成，并设计数据平衡训练目标，实验验证其在发音评估与诊断上的优越性能。|
|2510.04937v1|[Perceptual Evaluation of Extrapolated Spatial Room Impulse Responses   From a Mono Source](http://arxiv.org/abs/2510.04937v1)|总结：  <br/>该研究提出通过单声道测量外推空间RIR的新方法，并利用3AFC测试验证其可信度，证明无需专业设备即可生成高质量空间音频，显著降低测量成本和时间。<br/><br/>贡献点：  <br/>1. 引入基于单声道测量的RIR外推方法，替代传统需多声源-麦克风对和专业空间麦克风的测量流程。  <br/>2. 设计3AFC听觉测试框架，系统评估外推RIR在语音、交响乐和乐器音乐场景下的听觉可信度。  <br/>3. 实验证明外推RIR在20名参与者测试中准确率达38%，显著高于随机猜测水平，验证了方法有效性。  <br/>4. 展示该方法可在减少测量时间和设备成本的同时，保持空间音频的沉浸感和真实性。|
|2510.04934v1|[AURA Score: A Metric For Holistic Audio Question Answering Evaluation](http://arxiv.org/abs/2510.04934v1)|贡献点：<br/>1. 提出首个系统性AQA评估基准AQEval（含10,000个多标注模型响应）<br/>2. 揭示现有指标（BLEU/METEOR/BERTScore）与人类评判的弱相关性（尤其长答案）<br/>3. 设计新型AURA评分，实现SOTA相关性表现（显著优于所有基线）<br/><br/>总结：本研究构建首个AQA基准AQEval，剖析现有评估指标缺陷，提出AURA新指标实现更精准的开放问答评估。|
|2510.04738v1|[Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with   Cross-Attentive Mamba](http://arxiv.org/abs/2510.04738v1)|**贡献点：**<br/>1. 提出MAVE架构，结合Mamba的结构化状态空间建模与交叉注意力机制，实现文本条件下的语音编辑与高保真TTS合成。  <br/>2. 在语音编辑任务中达到SOTA性能，零样本TTS任务中表现优异（无需额外训练）。  <br/>3. 通过双盲测试验证其自然性和说话人一致性：57.2%的听众认为编辑后语音与原声无差异，仅18%偏好原声。  <br/>4. 内存消耗显著降低（~6倍于VoiceCraft），延迟相近，提升实际应用效率。  <br/>5. 首次系统性整合结构化状态空间建模与跨模态注意力，推动语音编辑与合成技术发展。  <br/><br/>**总结（100字以内）：**  <br/>MAVE通过融合Mamba与交叉注意力机制，实现高效、高质量的语音编辑与零样本TTS合成，显著降低内存成本并在实测中超越现有模型，为语音生成领域树立新标准。|
|2510.04577v1|[Language Model Based Text-to-Audio Generation: Anti-Causally Aligned   Collaborative Residual Transformers](http://arxiv.org/abs/2510.04577v1)|**贡献点总结：**  <br/>1. **揭示关键矛盾**：发现RVQ层数增加与传统LM生成能力不足的冲突，限制了T2A性能提升。  <br/>2. **提出理论分析**：识别RVQ动态中两个核心问题：跨层特征正交性阻碍LM训练，深层token语义贫乏导致暴露偏差。  <br/>3. **设计Siren框架**：创新性结合多隔离Transformer、因果条件与反因果对齐的强化学习策略。  <br/>4. **实验验证优势**：Siren在LM和扩散模型基准上均取得SOTA性能，证明其有效性。  <br/>5. **推动多模态统一**：通过语义对齐，为融合文本与音频生成的多模态框架提供新路径。  <br/><br/>**总结（100字内）：**  <br/>本论文提出Siren框架，解决RVQ与LM结合的局限性，实验证明其在T2A任务中超越现有方法，为统一多模态生成提供新思路。|
|2510.04459v1|[Differentiable physics for sound field reconstruction](http://arxiv.org/abs/2510.04459v1)|**贡献点：**<br/>1. 提出了一种基于可微物理的声场重构方法，首次将物理方程的初始条件通过神经网络建模，并结合可微数值求解器实现微分运算。  <br/>2. 通过直接嵌入物理约束至数值求解过程，替代传统方法将物理约束仅作为损失函数项，提升模型稳定性与准确性。  <br/>3. 引入稀疏性促进约束，显著增强模型在严重欠采样下的鲁棒性，实现可行的声场重构。  <br/>4. 实验验证方法在极端数据稀缺场景下性能优于传统物理信息神经网络，展现更优的重构精度与收敛特性。  <br/><br/>**总结（100字以内）：**  <br/>本文提出可微物理声场重构框架，结合神经网络建模初始条件与可微数值求解器，强化物理约束并引入稀疏性约束，实验表明该方法在极端数据下实现更高精度与更好收敛。|
|2510.04339v1|[Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre   Latent Space](http://arxiv.org/abs/2510.04339v1)|**贡献点：**  <br/>1. 提出双阶段半监督学习框架，解决传统高维潜在空间难以操控的问题。  <br/>2. 构建音高-音色分离的2D潜在空间，通过变分自编码器实现音色与音高的解耦表示。  <br/>3. 将2D潜空间作为条件输入接入Transformer生成模型，提升音频生成的可控性与质量。  <br/>4. 实验验证模型在音色细微变化捕捉与音高准确性上的优势，证明其生成效果的可靠性。  <br/>5. 开发交互式Web应用，展示方法的实用性，推动音乐生产环境向直观、创作友好方向发展。  <br/><br/>**总结（100字内）：**  <br/>本文提出双阶段半监督框架，结合VAE与Transformer模型，实现音高-音色分离的2D潜在空间，为音乐生成提供直观可控的界面。实验验证了其音色和音高精度，并通过Web应用证明实用性，推动了未来音乐创作环境的发展。|
|2510.04219v1|[Probing Whisper for Dysarthric Speech in Detection and Assessment](http://arxiv.org/abs/2510.04219v1)|总结：  <br/>本研究通过探针分析揭示了Whisper模型在病理语音中的嵌入表示特性，发现中层编码器最有效，微调影响有限，为构建可解释的临床评估工具提供理论支持。<br/><br/>贡献点：  <br/>1. **首次系统分析病理语音的模型表示**：针对Whisper-Medium模型，首次探讨其在dysarthric speech（构音障碍语音）的内部表示特征，揭示不同层对病理语音的敏感性。  <br/>2. **多角度评估嵌入信息量**：结合线性分类器（单/多任务设定）、Silhouette评分、互信息指标，量化分析各层嵌入的表征能力与区分度。  <br/>3. **验证模型可微调性与稳定性**：通过微调实验检验模型对病理语音任务的适应性，发现微调仅带来小幅改进，证明预训练模型在病理语音中的潜在泛化能力。  <br/>4. **推动临床应用解释性**：明确中层编码器（13-15层）的病理语音表征优势，为后续基于预训练模型的临床评估工具开发提供理论依据和优化方向。|
|2510.04213v2|[Enhancing Speaker Verification with w2v-BERT 2.0 and Knowledge   Distillation guided Structured Pruning](http://arxiv.org/abs/2510.04213v2)|总结：  <br/>本文提出基于w2v-BERT 2.0的说话人验证方法，结合MFA+Layer Adapter和LoRA技术，实现高效特征提取与模型压缩，在Vox1测试集上取得SOTA性能，同时开源代码与模型。<br/><br/>贡献点：  <br/>1. **大规模预训练基础**：使用6亿参数、跨143种语言训练450万小时无标签数据的w2v-BERT 2.0作为模型基础。  <br/>2. **多层特征处理架构**：提出MFA结构结合Layer Adapter，优化多层特征输出以提取更鲁棒的说话人嵌入。  <br/>3. **高效微调机制**：引入LoRA技术，在保持模型性能的同时显著降低微调计算成本。  <br/>4. **轻量化压缩方案**：通过知识蒸馏引导的结构化剪枝，将模型体积缩小80%仅导致0.04% EER性能损失。  <br/>5. **开源实现**：发布源码和模型，推动语音验证领域的技术复用与研究进展。|
|2510.04213v1|[Enhancing Speaker Verification with w2v-BERT 2.0 and Knowledge   Distillation guided Structured Pruning](http://arxiv.org/abs/2510.04213v1)|**贡献点总结：**  <br/>1. 提出基于w2v-BERT 2.0的跨语言大规模预训练模型，参数量达6亿，训练语料覆盖143种语言。  <br/>2. 设计MFA结构结合Layer Adapter，有效提取多层特征的说话人嵌入。  <br/>3. 引入LoRA实现高效微调，降低计算成本。  <br/>4. 结合知识蒸馏与结构化剪枝，模型体积缩减80%仅导致0.04% EER下降。  <br/>5. 在Vox1-O和Vox1-H测试集上实现SOTA的0.12%和0.55% EER，开源代码和模型。  <br/><br/>**摘要（100字以内）：**  <br/>本文提出基于w2v-BERT 2.0的跨语言预训练说话人验证模型，结合MFA与Layer Adapter提取特征，采用LoRA高效微调，并通过知识蒸馏剪枝优化模型体积，实现SOTA性能与高效部署。|
|2510.03836v1|[From Qubits to Rhythm: Exploring Quantum Random Walks in Rhythmspaces](http://arxiv.org/abs/2510.03836v1)|总结：  <br/>提出基于量子计算的节奏生成算法，通过二维量子随机游走与MIDI音序映射，拓展了量子计算在音乐创作中的应用，验证了其在多维声学空间中的可扩展性。<br/><br/>贡献点：  <br/>1. **创新算法框架**：首次将量子随机游走应用于节奏生成，构建二维"节奏空间"用于插值节奏模式。  <br/>2. **优化电路实现**：通过分解二维量子游走为两维一维游走，降低电路深度以提升计算效率。  <br/>3. **势场动态控制**：引入四种经典势场（含惯性动态）调节量子态概率分布，实现节奏模式的方向性偏置。  <br/>4. **音乐应用验证**：完整实现从量子路径到MIDI鼓音序的转换，通过DAW生成可听音乐，证明量子计算在音频生成中的可行性。  <br/>5. **通用性扩展**：算法不限于节奏生成，可适配多维声学空间，为量子计算在音乐与音频领域的应用提供通用范式。|
|2510.03825v1|[A MATLAB toolbox for Computation of Speech Transmission Index (STI)](http://arxiv.org/abs/2510.03825v1)|总结（100字以内）:  <br/>本文提供符合IEC标准的开源Matlab STI实现，涵盖直接/间接方法及缩短STIPA协议，并通过测试与商业设备验证其准确性，推动了STI在语音可懂度评估中的应用。<br/><br/>贡献点:  <br/>1. **开源实现**：首次公开符合IEC 60268-16:2020标准的Matlab STI代码，支持直接和间接计算方法及缩短STIPA协议。  <br/>2. **标准合规性**：通过参考信号测试验证实现满足标准要求，确保计算结果的准确性与可靠性。  <br/>3. **硬件独立性**：无需专有设备即可完成验证测量，提高STI评估的可操作性和普及性。  <br/>4. **对比验证**：与商用测量设备对比，证明实现的性能与行业标准一致，增强可信度。|
|2510.03758v1|[Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's   Disease Diagnosis from Speech](http://arxiv.org/abs/2510.03758v1)|总结（100字以内）:  <br/>本研究提出一种多语言帕金森病语音检测的粒度感知方法，通过自动化时间对齐提取音素/音节/单词，结合双向LSTM与多头注意力机制，实现93.78% AUROC的优异性能，并验证语音特征与临床标准的一致性，代码公开供复现。<br/><br/>贡献点:  <br/>1. **提出粒度感知方法**：开发了多语言PD检测模型，通过分析不同语音粒度（音素、音节、单词）提升诊断精度。  <br/>2. **自动化时间对齐提取**：构建了可提取时间对齐音素、音节和单词的自动化处理流程，解决传统方法对局部特征忽略的问题。  <br/>3. **跨语言验证有效性**：在意大利语、西班牙语和英语数据集上测试模型，证明其跨语言适用性及性能优势（AUROC 93.78%）。  <br/>4. **临床特征一致性验证**：通过注意力机制分析发现，关键语音特征（如元音、连读音节）与临床协议一致，增强方法可信度。  <br/>5. **开源代码支持**：提供公开代码库（https://github.com/jetliqs/clearpd），促进研究复现和进一步应用。|
|2510.03750v1|[Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with   Musically Informed Metrics](http://arxiv.org/abs/2510.03750v1)|总结：  <br/>提出融合动作与手势级评估的框架，揭示MIDI信息模型在音乐相关指标上的显著优势，推动钢琴踏板深度估计评估更贴近音乐表现需求。<br/><br/>贡献点：  <br/>1. **提出音乐感知导向的评估框架**：引入动作级（方向/时间）与手势级（轮廓相似性）分析，弥补传统帧级指标对音乐关键特征的忽视。  <br/>2. **多模型对比验证**：在统一架构下对比音频基线模型与MIDI辅助模型、二进制训练模型，明确音乐相关信息对性能提升的作用。  <br/>3. **发现MIDI模型优势**：实验证明MIDI信息模型在动作和手势级表现显著优于其他模型，即使帧级指标提升有限。  <br/>4. **强调评估实用性**：框架能捕捉传统方法忽略的音乐相关改进，为踏板深度估计任务提供更可靠的模型评估依据。|
|2510.03741v1|[Désentrelacement Fréquentiel Doux pour les Codecs Audio Neuronaux](http://arxiv.org/abs/2510.03741v1)|贡献点：  <br/>1. 提出基于频谱分解的离散神经音频编解码器，通过时域信号分解提升表示可解释性；  <br/>2. 突破传统方法对特定数据集或任务的强依赖，增强模型的通用性；  <br/>3. 实验证明在重建保真度和感知质量上优于现有最先进基线。  <br/><br/>总结：  <br/>本研究提出一种通过频谱分解提升可解释性的离散神经音频编解码器，有效减少对特定数据的依赖，并在重建质量和感知表现上超越现有方法。|
|2510.03735v1|[Soft Disentanglement in Frequency Bands for Neural Audio Codecs](http://arxiv.org/abs/2510.03735v1)|总结（100字以内）:  <br/>提出通用语音解纠缠框架，结合频谱分解与多分支编解码结构，在重建和感知性能上超越现有基线，并展现对语音修复任务的潜在优势。<br/><br/>贡献点:  <br/>1. **通用性方法创新**：开发无需依赖特定任务或数据特性的解纠缠框架，突破现有方法的高度任务依赖性限制。  <br/>2. **频谱分解与多分支架构**：首次将频谱分解应用于时域信号，通过多分支编解码器分离并处理不同频谱成分，提升特征可解释性。  <br/>3. **性能与应用优势**：在重建质量与感知效果上超越SOTA基线，且为语音修复等任务提供新的潜在应用价值。|
|2510.03728v1|[Lightweight and Generalizable Acoustic Scene Representations via   Contrastive Fine-Tuning and Distillation](http://arxiv.org/abs/2510.03728v1)|**贡献点总结：**  <br/>1. 提出ContrastASC方法，通过结构化嵌入空间保留声学场景语义关系，实现无再训练的跨类别适应。  <br/>2. 结合监督对比微调与对比表示蒸馏技术，将结构化知识迁移至紧凑的学生模型。  <br/>3. 支持少样本（few-shot）适应能力，提升模型在新类别上的泛化效果。  <br/>4. 在保持闭集性能的同时，显著优化开放集场景下的迁移效率。  <br/>5. 实验验证了方法在实际应用中的有效性，尤其针对边缘设备的部署需求。  <br/><br/>**总结（100字以内）：**  <br/>ContrastASC通过结构化嵌入空间与对比学习技术，实现声学场景分类模型的无再训练跨类别适应，兼顾少样本性能与闭集表现，适用于边缘设备的动态场景分类任务。|
|2510.03723v1|[Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker   Speech Recognition](http://arxiv.org/abs/2510.03723v1)|**贡献点：**<br/>1. **提出融合目标说话人建模与串行输出训练（SOT）的框架**：首次结合SA（Speaker-attributed）与SOT策略，通过Diarization-Conditioned Whisper编码器提取目标说话人嵌入，并整合为单一表示进行联合解码。<br/>2. **设计共享解码器结构**：替代传统逐个解码每名说话人的方式，采用共享解码器同时建模所有说话人上下文，实现跨说话人信息的深度融合。<br/>3. **实现带时序信息的串行输出**：在解码过程中输出包含说话人标签和时间戳的串行语音转录流，解决多说话人重叠语音的时序对齐问题。<br/>4. **验证模型有效性**：在多说话人混合数据集（如LibriMix）上，实验结果表明该模型优于现有SOT方法，并超越了DiCoW模型的性能。|
|2510.03630v1|[Scaling Multi-Talker ASR with Speaker-Agnostic Activity Streams](http://arxiv.org/abs/2510.03630v1)|**贡献点**  <br/>1. **提出活动流解耦方法**：将说话人特定的活动信号转化为两个与说话人无关的流，使推理成本与说话人数量无关，提升系统可扩展性。  <br/>2. **设计对话连贯性保持启发式**：通过改进活动信号合并策略，解决预训练模型对单说话人连续输入的依赖问题，避免识别性能下降。  <br/>3. **验证兼容性与效率提升**：在Diarization-Conditioned Whisper (DiCoW)框架下实现，显著减少AMI和ICSI数据集的运行时间，同时保持性能竞争力。  <br/><br/>**总结**  <br/>本文提出一种解耦说话人活动信号的方法，降低多说话人ASR的推理成本，设计对话连续性保持策略并验证其在DiCoW中的兼容性与效率提升。|
|2510.03387v2|[Synthetic Audio Forensics Evaluation (SAFE) Challenge](http://arxiv.org/abs/2510.03387v2)|总结：  <br/>提出SAFE挑战，构建全盲评估框架，设计多阶段测试场景，创建大规模合成音频数据集，分析现有方法优劣，推动合成语音检测研究。<br/><br/>贡献点：  <br/>1. 提出首个全盲的合成语音取证检测挑战（SAFE），系统性评估检测模型在不同场景下的性能。  <br/>2. 设计包含原始合成语音、处理音频（如压缩、重采样）和清洗音频（试图规避分析）的多级测试场景。  <br/>3. 构建大规模数据集（90小时音频，21,000个样本），涵盖21个真实声源、17个TTS模型和3个任务。  <br/>4. 提供当前检测方法的初步分析与局限性的见解，为后续研究提供基准和方向。|
|2510.03387v1|[Audio Forensics Evaluation (SAFE) Challenge](http://arxiv.org/abs/2510.03387v1)|**贡献点**  <br/>1. **提出SAFE挑战**：构建首个针对合成音频取证的全盲评估框架，系统化测试检测模型在不同对抗场景下的性能。  <br/>2. **设计渐进式评估场景**：涵盖原始合成语音、处理音频（如压缩、重采样）和清洗音频（意在逃避分析），模拟真实复杂环境。  <br/>3. **建立大规模多维度数据集**：包含90小时音频、21,000个样本，覆盖21个真实声源与17种TTS模型，提供丰富基准数据。  <br/>4. **明确三项检测任务**：覆盖合成音频识别、处理痕迹分析和抗攻击能力评估，规范研究方向。  <br/>5. **分析当前方法局限性**：通过实验证明现有技术的优劣，为后续研究提供方向性参考。  <br/>6. **开源共享资源**：提供公开数据集与评估平台，推动领域研究协作与技术进步。  <br/><br/>**总结**：本文提出了首个全盲合成音频取证评估框架SAFE，通过多场景数据集和任务设计推动检测方法研究。|
|2510.03117v1|[Taming Text-to-Sounding Video Generation via Advanced Modality Condition   and Interaction](http://arxiv.org/abs/2510.03117v1)|**总结**（100字以内）：  <br/>该研究提出HVGC框架和BridgeDiT模型，通过分离视频与音频描述生成与双向特征交互，解决T2SV任务中模态干扰与同步机制问题，取得SOTA结果并开源代码。<br/><br/>**贡献点**：  <br/>1. **Hierarchical Visual-Grounded Captioning (HVGC)**：首次生成分离的视频与音频描述（video caption & audio caption），消除单个文本描述对跨模态生成的干扰。  <br/>2. **BridgeDiT模型**：设计双塔扩散变换器，引入Dual CrossAttention（DCA）机制，在语义与时间上实现双向信息协同，提升同步性能。  <br/>3. **全面验证与开源**：基于三个基准数据集的实验与人类评估证明方法有效性，消融研究提供关键洞察，并开源代码与模型权重。|
|2510.03115v1|[Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought   Speech-to-Text Translation](http://arxiv.org/abs/2510.03115v1)|总结：论文指出S2TT系统存在错误传播与声学信息利用不足问题，发现CoT方法实质依赖转录文本而非语音特征，并提出通过直接S2TT数据和噪声转录注入等训练干预提升鲁棒性，强调需设计显式整合声学信息的翻译架构。<br/><br/>贡献点：<br/>1. 识别S2TT系统核心缺陷：揭示ASR-T2TT级联结构在错误传播和声学特征利用上的关键限制<br/>2. 揭示CoT方法局限性：通过归因分析与抗干扰测试发现CoT仍主要依赖转录文本而非语音输入<br/>3. 提出有效训练干预：验证添加直接S2TT数据和噪声转录注入可增强系统鲁棒性并提升语音特征利用<br/>4. 指明架构改进方向：挑战CoT的假设优势，呼吁开发显式整合声学信息的翻译模型架构|
|2510.03111v1|[Evaluation of preprocessing pipelines in the creation of in-the-wild TTS   datasets](http://arxiv.org/abs/2510.03111v1)|总结：  <br/>提出一种可重复的指标驱动评估方法，系统比较24种预处理配置，并发现降噪与语音保留的最优平衡方案，显著提升低资源场景下预处理效率。<br/><br/>贡献点：  <br/>1. **提出可重复性与指标驱动的评估方法**：为野外TTS语料库预处理管道提供标准化、可复现的评估框架。  <br/>2. **首次应用低成本管道处理阿根廷西班牙语数据集**：构建首个野外采集的阿根廷西班牙语语料库并验证预处理效果。  <br/>3. **系统比较24种管道配置**：结合降噪与质量过滤的多种变体，分析不同策略对数据集规模、信号质量及语音保留的影响。  <br/>4. **多维度客观评估体系**：综合使用PESQ、SI-SDR、SNR、T30、C50、F0-STD、MCD等指标量化性能。  <br/>5. **揭示关键权衡关系并推荐最优方案**：发现宽松过滤的降噪变体在数据集规模与语音保留间取得最佳平衡。  <br/>6. **无需训练TTS模型即可评估管道效果**：降低预处理开发的实验成本和时间消耗，适用于资源有限场景。|
|2510.03025v1|[CVSM: Contrastive Vocal Similarity Modeling](http://arxiv.org/abs/2510.03025v1)|总结：  <br/>本文提出CVSM，一种对比自监督学习框架，通过有标签和无标签协议有效学习音乐信号表示，显著提升声乐和音乐相似性建模性能，并验证其在真实和人工混合数据中的鲁棒性。<br/><br/>贡献点：  <br/>1. **提出CVSM框架**：首次将对比学习应用于音乐信号表示学习，通过最大化声乐与含相同声乐的音乐混合物的相似性，提升音乐和声乐相似性建模效果。  <br/>2. **双协议设计**：  <br/>   - 标签驱动：利用艺术家身份信息生成对比对，增强模型对目标任务的针对性。  <br/>   - 标签无关：通过随机采样声乐与伴奏片段人工合成混合数据，实现无监督对比学习。  <br/>3. **多维度评估**：  <br/>   - 客观评估：通过线性探测测试在多个下游任务中的表示能力。  <br/>   - 主观验证：在推荐查询设置下开展用户研究，验证模型在实际场景中的效果。  <br/>4. **性能优势**：CVSM在孤立声乐和完整音乐混合物上均超越多数基线，且标签无关方案通过混合预训练达到与标签驱动方案相当的艺术家识别与感知相似性效果。|
|2510.02916v1|[SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos](http://arxiv.org/abs/2510.02916v1)|总结：  <br/>SALSA-V提出了一种视频到音频生成模型，通过masked diffusion目标和shortcut loss实现高效高质量音频合成，并在对齐与同步性上超越现有方法，适用于专业音频合成任务。<br/><br/>贡献点：  <br/>1. **提出SALSA-V模型**：首个能从静音视频生成高度同步、高保真长音频的多模态模型。  <br/>2. **引入masked diffusion目标**：支持音频条件生成，实现无约束长度音频序列的无缝合成。  <br/>3. **集成shortcut loss**：仅需8步采样即可生成高质量音频，支持近实时应用无需额外微调。  <br/>4. **超越SOTA性能**：在定量评估和人类听觉测试中，显著优于现有方法的音频-视觉对齐与同步能力。  <br/>5. **随机掩码训练策略**：匹配参考音频频谱特征，拓展模型在 Foley 生成与专业音频设计中的适用性。|
|2510.02915v1|[WavInWav: Time-domain Speech Hiding via Invertible Neural Network](http://arxiv.org/abs/2510.02915v1)|总结（100字以内）：  <br/>提出基于可逆流网络的新框架，引入时间频率损失和加密技术，显著提升音频信息隐藏的可逆性与鲁棒性，实验验证优于现有方法。<br/><br/>贡献点分点列出：  <br/>1. **提出可逆流网络框架**：构建直接关联隐写音频、载体音频与秘密音频的模型，增强嵌入和提取的可逆性。  <br/>2. **设计时间频率损失机制**：通过时域信号处理解决传统方法中时间-频率变换导致的隐写质量下降问题。  <br/>3. **集成加密保护**：增加加密技术以防止隐藏数据被非法访问，提升安全性。  <br/>4. **验证效果优势**：在VCTK和LibriSpeech数据集上证明方法在主观/客观指标和抗噪能力上优于现有方案。|
|2510.02864v1|[Forensic Similarity for Speech Deepfakes](http://arxiv.org/abs/2510.02864v1)|总结：  <br/>提出基于深度学习的音频取证方法，用于检测语音深度伪造中的相似痕迹，展示其泛化能力和多任务适用性，提升数字音频取证的灵活性和实用性。<br/><br/>贡献点：  <br/>1. **首个音频领域取证相似度框架**：首次将图像领域的"forensic similarity"概念迁移至音频，提出针对语音深度伪造的取证方法。  <br/>2. **无需先验知识的泛化能力**：系统无需训练时知晓具体取证痕迹，可泛化应对未知模式，突破传统方法的局限。  <br/>3. **双阶段深度学习结构**：设计特征提取器（基于深度伪造检测模型）与浅层相似度网络的组合架构，实现高效特征映射与相似度计算。  <br/>4. **源验证任务有效性**：验证系统能准确识别音频样本是否源自同一生成模型，推动语音伪造溯源技术发展。  <br/>5. **扩展至分割检测**：方法可作为补充分割检测工具，提升音频篡改分析的多场景适应性。  <br/>6. **鲁棒性验证**：通过实验证明模型对未见过的取证痕迹具有强泛化能力，体现其实际应用价值。|
|2510.02848v1|[Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating   and Dynamic Pacing Zero-shot Text-to-Speech](http://arxiv.org/abs/2510.02848v1)|**贡献点：**  <br/>1. 提出Flamed-TTS框架，首次在低计算成本、低延迟、高语音保真度与丰富时间多样性之间实现平衡，解决传统zero-shot TTS的合成可靠性与效率问题。  <br/>2. 通过重新设计流匹配训练范式，并融合离散与连续语音属性表示，增强模型对语音特征的控制与学习能力。  <br/>3. 实验表明Flamed-TTS在语音自然度、识别率（WER 4%）、说话人相似性及动态语速等关键指标上超越现有SOTA模型，同时保持高效推理性能。  <br/><br/>**总结：**  <br/>提出Flamed-TTS框架，融合离散-连续表示与优化训练范式，实现低资源、高保真度与时间多样性的平衡，显著提升zero-shot TTS性能并开源验证。|
|2510.02813v1|[Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph   Neural Network](http://arxiv.org/abs/2510.02813v1)|总结：  <br/>该研究提出利用图神经网络和神经细分技术提升低分辨率Photogrammetry数据的3D网格精度，解决传统HRTF获取方法的可访问性难题，并通过多维度验证其在HRTF合成中的有效性。<br/><br/>贡献点：  <br/>1. **提出新型HRTF合成方法**：首次结合图神经网络（GNN）与神经细分技术，将低分辨率Photogrammetry重建的头网格升级为高分辨率，突破传统设备依赖。  <br/>2. **优化数据处理流程**：采用SONICOM数据集与Apple Photogrammetry API生成低分辨率网格，降低高质量3D扫描硬件门槛。  <br/>3. **创新损失函数设计**：引入基于Hausdorff距离的损失函数，提升网格上采样的几何精度与保真度。  <br/>4. **多维度性能验证**：通过几何分析、HRTF合成对比及行为实验（如定位与SRM任务）全面评估方法的声学适用性与感知效果。|
|2510.02797v2|[SongFormer: Scaling Music Structure Analysis with Heterogeneous   Supervision](http://arxiv.org/abs/2510.02797v2)|**贡献点**  <br/>1. **提出SongFormer框架**：通过融合短/长窗口自监督音频表示和学习源嵌入，有效处理部分、噪声及结构不匹配的标签，提升对音乐结构的细粒度与长程依赖建模能力。  <br/>2. **构建大规模语料库**：发布SongFormDB（超10,000首跨语言/音乐风格的多模态数据）及SongFormBench（300首专家标注基准），推动音乐结构分析的可扩展性与公平评估。  <br/>3. **突破性能指标**：在SongFormBench上实现严格边界检测（HR.5F）和功能性标签准确性的SOTA，同时在宽松容忍度下（HR3F）保持竞争力，超越Gemini 2.5 Pro等强基线。  <br/>4. **开源促进研究**：提供代码、数据集和模型的公开访问，助力社区验证与扩展研究。  <br/><br/>**总结（100字以内）**：  <br/>SongFormer通过多尺度自监督表示和学习源嵌入解决音乐结构分析挑战，构建全球最大语料库及专家验证基准，取得严格与宽松指标下的SOTA性能，并开源推动领域发展。|
|2510.02797v1|[SongFormer: Scaling Music Structure Analysis with Heterogeneous   Supervision](http://arxiv.org/abs/2510.02797v1)|**总结**：  <br/>本文提出SongFormer框架，通过融合多尺度自监督表示和学习源嵌入提升音乐结构分析效果，并发布最大规模的SongFormDB数据集及SongFormBench基准，在严格和宽松标注下均取得SOTA性能，代码与数据公开。<br/><br/>**贡献点**：  <br/>1. **框架创新**：SongFormer融合短/长窗口自监督音频表示，捕捉细粒度与长程依赖；引入学习源嵌入，提升对不完整、噪声、格式不匹配标注的鲁棒性。  <br/>2. **大规模数据集**：发布SongFormDB（超10k曲目，多语言/流派），构建SongFormBench（300首专家验证）作为首个权威基准，推动领域发展。  <br/>3. **性能突破**：在SongFormBench上实现严格边界检测（HR.5F）和功能标签准确率新SOTA，超越主流基线与Gemini 2.5 Pro，在宽松容忍度（HR3F）下仍具竞争力。  <br/>4. **开源资源**：提供代码、数据集与模型，促进研究复现与应用。|
|2510.02672v1|[STSM-FiLM: A FiLM-Conditioned Neural Architecture for Time-Scale   Modification of Speech](http://arxiv.org/abs/2510.02672v1)|**贡献点总结（100字以内）:**  <br/>提出STSM-FILM神经网络框架，融合FiLM实现连续速度因子的条件化，结合WSOLA监督方法提升时间缩放质量，并验证其在多种编码器和极端拉伸场景下的感知一致性与泛化能力。  <br/><br/>**分点贡献:**  <br/>1. **提出STSM-FILM模型**：构建全神经网络架构，通过FiLM条件化机制直接控制时间缩放速度因子。  <br/>2. **FiLM条件化创新**：首次将特征级线性调制（FiLM）应用于时间缩放，提升模型对连续速度变化的适应性。  <br/>3. **WSOLA监督策略**：利用传统WSOLA生成的输出监督神经网络，平衡对齐与合成行为，减少伪影。  <br/>4. **多编码器验证**：评估STFT-HiFiGAN、WavLM-HiFiGAN、Whisper-HiFiGAN和EnCodec四种变体，证明模型在广泛时间缩放下的鲁棒性。  <br/>5. **泛化能力提升**：通过实验验证FiLM条件化显著增强神经TSM模型的灵活性与感知一致性。|
|2510.02597v1|[TART: A Comprehensive Tool for Technique-Aware Audio-to-Tab Guitar   Transcription](http://arxiv.org/abs/2510.02597v1)|总结：该研究提出首个端到端吉他乐谱生成框架，通过音频到MIDI转换、表达技巧分类、弦品分配和tablature生成四阶段流程，解决现有系统对吉他演奏技巧识别和泛化能力不足的问题。<br/><br/>贡献点：<br/>1. 提出首个将音频直接转化为详细吉他tablature的端到端框架，包含准确指法和表达标签<br/>2. 设计四阶段处理流程：音频→MIDI→表达技巧分类→弦品分配→tablature生成<br/>3. 自适应应用钢琴转录模型进行音频到MIDI的音高转换<br/>4. 引入MLP处理表达技巧分类任务（如滑音、弯音等）<br/>5. 采用Transformer模型实现弦和品的准确映射<br/>6. 使用LSTM进行tablature序列生成，提升结构化输出质量|
|2510.02556v1|[Multi-Source Position and Direction-of-Arrival Estimation Based on   Euclidean Distance Matrices](http://arxiv.org/abs/2510.02556v1)|总结：  <br/>本研究提出基于欧氏距离矩阵和Gram矩阵的多源定位与DOA估计新方法，通过减少优化变量数量和引入相对坐标系，显著降低计算复杂度，同时在准确率上超越传统SRP方法。<br/><br/>贡献点：  <br/>1. 提出基于EDM和Gram矩阵的多源定位与DOA估计框架，替代传统SRP方法；  <br/>2. 多源定位仅需优化单一连续变量（源与参考麦克风距离）；  <br/>3. DOA估计通过相对坐标系设计完全避免连续变量优化；  <br/>4. 利用秩减少Gram矩阵特征值定义成本函数，提升计算效率；  <br/>5. 实验证明方法在位置与DOA估计准确率上优于SRP方法，且计算成本更低。|
|2510.02500v2|[Latent Multi-view Learning for Robust Environmental Sound   Representations](http://arxiv.org/abs/2510.02500v2)|总结：  <br/>提出结合对比与生成方法的多视角统一框架，通过双自监督目标提升环境声音表征，验证在传感器网络数据集上的分类效果，并探索模型对环境声音属性的解耦能力。<br/><br/>贡献点：  <br/>1. 构建首个融合对比学习与生成模型的多视图框架，实现声源与设备信息的联合建模。  <br/>2. 引入双自监督目标：对比学习引导子空间间的信息流动，重建目标确保整体信息保留。  <br/>3. 在城市声景传感器网络数据集上验证方法有效性，显著优于传统SSL技术的下游性能。  <br/>4. 分析模型在多样化训练配置下对环境声音属性的解耦潜力，揭示其结构化潜空间的可解释性。|
|2510.02500v1|[Latent Multi-view Learning for Robust Environmental Sound   Representations](http://arxiv.org/abs/2510.02500v1)|总结：  <br/>本文提出一种融合对比学习与生成方法的多视图框架，有效捕捉环境声音的声源和设备信息，通过结构化潜空间提升分类性能，并探索其在解耦声音属性方面的潜力。<br/><br/>贡献点：  <br/>1. **提出多视图联合框架**：将对比学习原理嵌入生成式流程，首次系统性整合两种SSL方法用于环境声音表示学习。  <br/>2. **双目标引导编码机制**：通过对比学习实现跨子空间信息流动，利用重建目标保障整体信息保留，形成协同优化策略。  <br/>3. **任务性能提升**：在城市声音传感器网络数据集上验证方法有效性，相比传统SSL技术显著提高下游任务的分类准确率。  <br/>4. **属性解耦分析**：研究模型在不同训练配置下的潜空间结构，揭示其潜在的环境声音属性解耦能力。|
|2510.02181v1|[EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative   Captioning](http://arxiv.org/abs/2510.02181v1)|总结：  <br/>EvolveCaptions通过实时协作和现场微调，显著降低聋人语音识别词错误率，减少用户录音负担，提升沟通公平性。<br/><br/>贡献点：  <br/>1. 提出实时协作式的ASR自适应系统EvolveCaptions，通过听力参与者动态纠正错误实现个性化。  <br/>2. 设计轻量级方案：仅需5分钟录音时间生成短提示，降低DHH用户参与成本。  <br/>3. 验证系统有效性：在1小时内使12名DHH用户的WER显著下降，且用户评价系统易用性高。  <br/>4. 推动更公平的沟通：将模型适应负担转移至听力辅助者，支持DHH在实时对话中的无障碍交流。|
|2510.02158v1|[Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial   Attacks on Polyphonic Sound Event Detection Systems](http://arxiv.org/abs/2510.02158v1)|总结：提出M2A框架，通过Preservation Loss约束非目标区域保持原输出，结合新型EP指标，在提升对抗攻击有效性的同时显著增强精度。<br/><br/>贡献点：<br/>1. 首次针对多音轨SED系统设计专门的对抗攻击框架（M2A），解决现有攻击方法在效果与精度间的平衡问题<br/>2. 提出Preservation Loss约束机制，确保攻击仅影响目标区域而不扰动非目标区域输出<br/>3. 开发新型评估指标Editing Precision (EP)，量化攻击对目标事件的修改精度与有效性<br/>4. 在两个SOTA SED模型上验证成果，实现94.56%和99.11%的EP指标，证明框架的有效性与精准性|
|2510.02110v1|[SoundReactor: Frame-level Online Video-to-Audio Generation](http://arxiv.org/abs/2510.02110v1)|总结：  <br/>提出帧级在线视频到音频生成任务，设计SoundReactor框架实现端到端因果性与低延迟同步，结合扩散预训练与一致性微调，有效生成高质量立体音频，在游戏视频基准测试中验证其性能。<br/><br/>贡献点：  <br/>1. **首次提出帧级在线V2A生成任务**：突破传统离线模型限制，支持实时交互场景。  <br/>2. **SoundReactor框架创新**：首个专为在线生成设计的简单高效框架。  <br/>3. **端到端因果结构设计**：确保音频生成仅依赖当前及历史视频帧，实现同步与低延迟。  <br/>4. **连续音频潜变量Transformer**：采用解码器-only架构优化生成效率与质量。  <br/>5. **高效视觉条件编码**：利用DINOv2最小变体提取并聚合帧级网格特征，平衡效率与性能。  <br/>6. **扩散预训练+一致性微调**：结合两种训练方式加速扩散头解码过程。  <br/>7. **实测延迟与音质表现**：在30FPS视频上实现26.3ms（NFE=1）的波形级低延迟，生成高质量全频立体声。  <br/>8. **实际应用验证**：提供公开演示样本，验证框架在游戏视频生成中的实用性。|
|2510.02066v1|[Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken   Dialogue Systems](http://arxiv.org/abs/2510.02066v1)|总结：  <br/>提出SCoT框架，通过流式块状处理解决Duplex SDS中VAD的局限性，实现更连贯的响应和低延迟交互。<br/><br/>贡献点：  <br/>1. **解决VAD局限性**：无需显式VAD，通过预测静音标记区分停顿与轮次结束。  <br/>2. **简化架构**：采用流式链式推理（CoT）替代复杂双通道结构，提升效率。  <br/>3. **帧级对齐技术**：生成对齐的中间用户转录与系统响应，增强语义连贯性。  <br/>4. **低延迟与重叠交互**：支持块状生成与流式处理，实现更自然的对话响应延迟优化。  <br/>5. **实验验证优势**：在响应连贯性、可解释性和交互效率上超越现有Duplex方法。|
|2510.01968v1|[Multi-bit Audio Watermarking](http://arxiv.org/abs/2510.01968v1)|总结:  <br/>提出了一种无需训练嵌入-检测器模型的音频水印方案Timbru，通过预训练VAE潜在空间梯度优化实现高鲁棒性与隐蔽性平衡，在多种攻击下表现最优且保持音频质量。<br/><br/>贡献点:  <br/>1. **无需训练嵌入-检测器模型**：首次实现仅依靠后处理方式添加水印，避免了传统嵌入器-检测器联合训练的复杂性。  <br/>2. **潜在空间渐变优化策略**：利用预训练音频VAE的潜在空间进行梯度优化，通过结合消息损失和感知损失生成不可感知扰动。  <br/>3. **预训练模型提取水印**：采用预训练CLAP模型实现水印提取，无需额外训练，简化了系统流程。  <br/>4. **多攻击场景下的性能验证**：在MUSDB18-HQ数据集上对比现有方法，证明其在滤波、噪声、压缩等常见攻击下具备最佳平均比特错误率。  <br/>5. **数据集无关性**：提供了一种无需依赖特定训练数据的通用水印解决方案，提升方法的普适性。|
|2510.01963v1|[Bias beyond Borders: Global Inequalities in AI-Generated Music](http://arxiv.org/abs/2510.01963v1)|总结：  <br/>提出GlobalDISCO数据集，覆盖全球音乐风格与语言，揭示音乐生成模型在跨地区和跨文化表现上的显著偏差。<br/><br/>贡献点：  <br/>1. 构建首个全球多样性音乐生成数据集GlobalDISCO（73k生成曲目+93k参考曲目），涵盖147种语言和79个国家。  <br/>2. 引入音乐风格提示词（从MusicBrainz和Wikipedia提取），增强数据集的语义多样性。  <br/>3. 首次发现高资源与低资源地区在音乐生成质量及与参考曲目对齐度上的显著差异。  <br/>4. 揭示主流音乐风格与地域性音乐风格在模型性能上的明显差异，部分模型对地域风格的生成更偏向主流分布。  <br/>5. 提供跨文化、跨语言音乐生成偏见的研究基准，推动公平性和多样性研究。|
|2510.01940v1|[Clustering of Acoustic Environments with Variational Autoencoders for   Hearing Devices](http://arxiv.org/abs/2510.01940v1)|**总结**（100字以内）：  <br/>提出基于VAE的无监督音频环境聚类方法，结合Gumbel-Softmax和时间窗口方案，构建结构化潜在空间，有效解决高维音频数据分类难题，在复杂城市声景中实现优于传统变分方法的聚类性能。<br/><br/>**贡献点**：  <br/>1. **方法创新**：首次将变分自编码器（VAE）应用于无监督音频环境聚类，突破传统监督学习对标签的依赖。  <br/>2. **结构化潜在空间**：设计适用于音频处理的结构化潜在空间，提升高维数据表征能力。  <br/>3. **关键技术融合**：引入Gumbel-Softmax重参数化与时间上下文窗口机制，增强模型对时序和频域特征的捕捉。  <br/>4. **实际场景适配**：针对听力设备等复杂场景（如城市声景）优化模型架构，解决时间-频率重叠数据的聚类难题。  <br/>5. **实验验证**：在spoken digits（简单任务）和urban soundscapes（复杂任务）中验证方法有效性，证明其在挑战性场景中的优势。|
|2510.01903v2|[MelCap: A Unified Single-Codebook Neural Codec for High-Fidelity Audio   Compression](http://arxiv.org/abs/2510.01903v2)|总结（100字以内）:  <br/>MelCap提出统一单代码本神经音频编解码器，通过两阶段结构和感知损失实现语音、音乐、一般声音的高质量压缩，兼顾计算效率与下游任务适应性。<br/><br/>贡献点:  <br/>1. **统一单代码本设计**：首次提出适用于多类型音频（语音、音乐、一般声）的单代码本神经音频编解码器，突破传统单/多代码本方案的局限。  <br/>2. **双阶段重构框架**：将音频重建分为mel频谱图压缩和波形生成两阶段，保留更多声学细节并提升重建质量。  <br/>3. **感知损失与实时解码**：通过感知损失减少频谱过平滑伪影，并结合单次前向传播的Vocoder实现高效实时解码，平衡性能与计算复杂度。|
|2510.01903v1|[MelCap: A Unified Single-Codebook Neural Codec for High-Fidelity Audio   Compression](http://arxiv.org/abs/2510.01903v1)|总结：  <br/>MelCap提出一种统一的神经音频编码器，通过分阶段处理和优化设计，在保持计算效率的同时实现高质量多模态音频压缩，适用于语音、音乐与一般声音，且性能优于传统单/多码本方法。<br/><br/>贡献点：  <br/>1. **统一编码框架**：提出"one-codebook-for-all"的神经音频编码器，突破单码本仅处理语音或多码本复杂性的局限，适用于语音、音乐和通用声音。  <br/>2. **分阶段重建策略**：将音频重建分为mel谱压缩与波形恢复两阶段，相比单码本方法保留更多声学细节，同时达到多码本水平的性能。  <br/>3. **2D Tokenizer设计**：采用高效的2D编码器对mel谱进行压缩与量化，生成紧凑的单个token表示，提升压缩效率与语义表达。  <br/>4. **感知损失优化**：引入感知损失函数，有效缓解传统谱重建中的过平滑失真问题，提升音频质量。  <br/>5. **实时解码能力**：通过单次前向传递的Vocoder实现波形恢复，支持实时解码需求。  <br/>6. **平衡效率与性能**：在保留单码本计算简洁性的同时，达到与主流多码本方法相当的压缩质量，为下游任务提供高效表征。|
|2510.01891v1|[HRTFformer: A Spatially-Aware Transformer for Personalized HRTF   Upsampling in Immersive Audio Rendering](http://arxiv.org/abs/2510.01891v1)|总结：  <br/>提出基于Transformer的HRTF上采样方法，结合球面调和域和邻居相似性损失提升精度与空间一致性，实验验证在高保真HRTF生成上优于现有技术。<br/><br/>贡献点：  <br/>1. **提出Transformer架构**：首次将Transformer模型应用于HRTF空间上采样，利用自注意力机制捕捉球面空间的长距离相关性。  <br/>2. **球面调和域重建**：通过在球面调和（SH）域中处理数据，实现从稀疏测量点高效生成高分辨率HRTF。  <br/>3. **邻居相似性损失**：设计新型损失函数，提升幅度平滑性和空间一致性，增强音频渲染的现实感。  <br/>4. **多维评估验证**：结合感知定位模型与客观频谱失真度量，全面验证方法在真实感和保真度上的优势。|
|2510.01860v1|[SLAP: Learning Speaker and Health-Related Representations from Natural   Language Supervision](http://arxiv.org/abs/2510.01860v1)|**贡献点总结（100字以内）**  <br/>提出SLAP，首个通过对比学习对齐语音与说话人/健康元数据的模型；融合ViT音频编码器与文本编码器；基于跨9数据集的3400小时多语言数据；在38个二分类任务中实现62.9%零样本F1，较CLAP提升48%；展示强大分布外泛化能力；微调后健康任务达57.9%，超越大模型。  <br/><br/>**分点贡献：**  <br/>1. **首个跨模态对齐模型**：SLAP是首个通过对比学习将语音与自然语言描述的说话人属性及健康元数据对齐的音频基础模型。  <br/>2. **创新模型架构**：结合Vision Transformer（ViT）音频编码器与文本编码器，提升跨模态特征融合能力。  <br/>3. **大规模数据训练**：基于超过3400小时、涵盖9个数据集且注释多样的语音数据，增强泛化性。  <br/>4. **多语言广泛评估**：在14个数据集（7种语言）的38个二分类任务中验证性能，覆盖人口统计、语音特征及临床评估。  <br/>5. **显著零样本表现**：零样本F1平均达62.9%，较CLAP提升48%，且具备强分布外（OOD）泛化能力。  <br/>6. **健康任务领先**：微调后健康相关任务F1达57.9%，超越现有更大基础模型，在健康领域表现最佳。|
|2510.01818v1|[Joint Optimization of Speaker and Spoof Detectors for Spoofing-Robust   Automatic Speaker Verification](http://arxiv.org/abs/2510.01818v1)|**贡献点：**  <br/>1. **模块化设计**：保持说话人认证和欺骗检测的子系统独立性，通过可训练的后端分类器整合输出，提升系统可解释性。  <br/>2. **任务对齐优化**：直接针对SASV新性能指标a-DCF优化后端分类器，探索更高效的融合策略（如非线性融合）。  <br/>3. **SOTA性能**：提出结合加权余弦得分（speaker detection）与SSL-AASIST（spoof detection）的混合方法，实验验证将min a-DCF降至0.196，SPF-EER达7.6%。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出模块化SASV框架，通过可训练后端分类器整合子系统，并直接优化a-DCF指标，实现非线性得分融合与任务对齐。实验表明，混合方法显著提升性能，达到SOTA水平。|
|2510.01812v2|[SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment](http://arxiv.org/abs/2510.01812v2)|总结：  <br/>本研究提出SingMOS-Pro数据集，扩展了歌唱质量评估的注释维度，提供大规模多样化样本与专业标注，为未来研究建立基准参考。<br/><br/>贡献点：  <br/>1. **扩展注释维度**：在原有SingMOS整体评分基础上，新增歌词、旋律和整体质量三类细粒度标注，提升评估覆盖范围。  <br/>2. **构建大规模数据集**：包含41个模型生成的7,981个歌唱片段，涵盖12个数据集，覆盖从早期到最新系统的广泛研究范围。  <br/>3. **专业标注保障**：每个片段由至少5名专业标注者评分，确保数据可靠性与一致性。  <br/>4. **方法基准与适配性研究**：探索多标准MOS数据的融合利用，基准测试相关任务的常用评估方法，为后续研究提供实践参考。|
|2510.01812v1|[SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment](http://arxiv.org/abs/2510.01812v1)|总结：本文提出SingMOS-Pro数据集，扩展了MOS注释维度并覆盖多模型，为歌唱质量评估提供可靠基准。<br/><br/>贡献点：  <br/>1. **提出SingMOS-Pro数据集**：在原有SingMOS基础上，新增歌词、旋律和整体质量的注释维度，实现更全面的感知评估。  <br/>2. **构建大规模多模型数据集**：收录41个模型在12个数据集生成的7981个歌唱片段，涵盖早期至最新系统的广泛对比样本。  <br/>3. **建立基准评估框架**：探索跨标准MOS数据的利用方法，对多类相关任务的评估方法进行基准测试，提供强基线与研究参考。|
|2510.01462v1|[RealClass: A Framework for Classroom Speech Simulation with Public   Datasets and Game Engines](http://arxiv.org/abs/2510.01462v1)|总结：  <br/>提出基于游戏引擎的教室噪声与RIR合成方法，构建首个融合合成与真实数据的RealClass教育语音数据集，验证其在模拟真实课堂场景中的有效性，为教育AI研究提供关键资源。<br/><br/>贡献点：  <br/>1. **提出合成方法**：首次利用游戏引擎构建可扩展的教室噪声及Room Impulse Response（RIR）合成框架，适配多领域应用。  <br/>2. **构建RealClass数据集**：整合合成噪声数据与公开语音数据，形成首个模拟真实课堂环境的教育语音数据集。  <br/>3. **场景化数据配对**：将儿童语音与教学语音结合，构建接近真实课堂互动的干净语音数据，填补教育场景数据空白。  <br/>4. **实验验证有效性**：通过对比干净与嘈杂语音的实验，证明RealClass在真实场景建模中的准确性与实用性。  <br/>5. **解决数据稀缺问题**：为缺乏大规模真实课堂数据的研究提供可替代的高质量数据资源。|
|2510.01109v1|[NLDSI-BWE: Non Linear Dynamical Systems-Inspired Multi Resolution   Discriminators for Speech Bandwidth Extension](http://arxiv.org/abs/2510.01109v1)|总结（100字以内）:  <br/>本文提出两种基于非线性动力系统的判别器MSRD与MRLD，结合递归表示和李雅普诺夫指数，通过深度可分离卷积优化，将BWE模型参数量压缩至原1/44，首次实现由语音生成的非线性混沌物理监督。<br/><br/>贡献点:  <br/>1. **提出MSRD与MRLD**：首次设计基于非线性动力系统的判别器，分别使用递归表示（MSRD）和李雅普诺夫指数（MRLD）建模语音中的确定性混沌特性。  <br/>2. **参数高效优化**：通过深度可分离卷积和设计优化，显著降低判别器参数量（22M → 0.48M），实现44倍压缩。  <br/>3. **物理监督机制**：首次将语音生成中的非线性混沌物理（如波动和初始条件敏感性）作为监督信号，提升模型效率与表征能力。|
|2510.01082v1|[HVAC-EAR: Eavesdropping Human Speech Using HVAC Systems](http://arxiv.org/abs/2510.01082v1)|贡献点：<br/>1. 提出HVAC-EAR系统，首次实现从0.5kHz低采样率压力数据中重建可理解语音，突破现有仅限于热词检测的局限性<br/>2. 开发复值Conformer模型与Complex Unified Attention Block，有效捕捉语音音素依赖关系<br/>3. 创新性地同时重建缺失频率的幅度与相位，抑制瞬态HVAC噪声干扰<br/>4. 通过真实场景部署验证，证明系统具备显著语音可懂度，引发新型隐私安全问题<br/><br/>总结：本研究提出HVAC-EAR系统，突破低采样率限制实现可理解语音重建，创新性地结合复值模型与相位恢复技术，首次在真实HVAC环境中验证有效性，揭示传感器窃听新风险。|
|2510.00982v1|[Spiralformer: Low Latency Encoder for Streaming Speech Recognition with   Circular Layer Skipping and Early Exiting](http://arxiv.org/abs/2510.00982v1)|总结（100字以内）:  <br/>提出Spiralformer编码器，结合层降与提前退出技术，通过螺旋式计算层分配优化块处理中的编码延迟，在保持计算成本和词错误率相近的前提下，显著降低Librispeech和CSJ数据集的平均token延迟。<br/><br/>贡献点：<br/>1. **提出Spiralformer编码器**：设计新型Transformer结构，结合层降（layer dropping）与提前退出（early exiting）技术，专为降低块处理编码延迟优化。<br/>2. **螺旋式计算层分配策略**：在块处理中循环跳过部分层计算，并通过螺旋式移动计算位置，实现所有层的渐进计算，提升效率。<br/>3. **实验验证有效性**：在Librispeech（21.6%）和CSJ（7.0%）数据集上验证，显著减少平均token发射延迟，且与基线保持相似的计算成本和词错误率。|
|2510.00981v2|[FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates](http://arxiv.org/abs/2510.00981v2)|总结（100字以内）:  <br/>FlexiCodec提出动态帧率音频编解码方法，通过自适应合并语义相似帧和ASR辅助的双流编码架构，有效解决低帧率导致的语义信息丢失问题，在TTS等任务中实现高质量音频重建与语义保留。<br/><br/>贡献点:  <br/>1. **动态帧率控制机制**：首次实现推理时可控制帧率范围（3Hz-12.5Hz），通过自适应合并语义相似帧减少冗余信息，降低计算成本。  <br/>2. **语义增强架构设计**：引入ASR特征辅助的双流编码结构与Transformer瓶颈网络，显著提升语义信息的保留能力。  <br/>3. **低帧率音频重建性能**：在6.25Hz、8.3Hz、12.5Hz等低帧率下验证了音频重建质量的优越性，超越传统基线系统。  <br/>4. **语言模型驱动TTS应用**：证明FlexiCodec在基于语言模型的语音合成任务中的有效性，拓展低帧率编码器的实际应用场景。|
|2510.00952v2|[CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation](http://arxiv.org/abs/2510.00952v2)|总结：  <br/>提出针对SRE24挑战的多套系统，结合X-vector与预训练模型，利用不同数据集实现音频-only及音频-视觉的语音识别性能评估。<br/><br/>贡献点：  <br/>1. **多系统提交**：为NIST SRE 2024挑战的闭集和开放条件分别提交了针对性系统，覆盖音频-only和音频-视觉任务。  <br/>2. **X-vector方法优化**：在闭集条件中，使用Kaldi框架开发的X-vector系统实现音频-only试评。  <br/>3. **纯视觉模型应用**：音频-视觉结果仅基于视觉模态模型，探索跨模态信息融合的可行性。  <br/>4. **预训练模型策略**：采用预训练模型（基于VoxBlink2和VoxCeleb2数据集）提升开放集条件下的性能。  <br/>5. **数据集创新**：针对闭集条件，从头训练X-vector模型时使用CTs超集数据集，增强泛化能力。  <br/>6. **绩效分析与报告**：在竞赛中提交结果的同时，系统性分析并报告所提系统的SRE24评估性能。|
|2510.00952v1|[CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation](http://arxiv.org/abs/2510.00952v1)|【贡献点总结】<br/>1. 参与NIST SRE2024挑战，提交固定条件和开放条件系统方案  <br/>2. 封闭集采用X-vector系统（基于Kaldi框架），开放集结合预训练模型与多数据集（VoxBlink2/VoxCeleb2）  <br/>3. 音频-视觉任务仅使用视觉模型，探索多模态数据融合有效性  <br/>4. 使用CTS超集数据集从头训练X-vector模型，提升封闭集性能  <br/>5. 提交SRE24评估结果并分析系统表现，推动语音识别技术研究  <br/><br/>【100字内总结】  <br/>本研究针对NIST SRE2024挑战，提出基于X-vector和视觉模型的多条件语音识别方案，结合预训练模型与CTS/Celeb数据集，优化封闭集与开放集性能，并通过竞赛成果验证方法有效性，为语音识别技术发展提供新思路。|
|2510.00934v1|[A Robust Proactive Communication Strategy for Distributed Active Noise   Control Systems](http://arxiv.org/abs/2510.00934v1)|总结：  <br/>提出稳健通信框架，结合自适应-固定滤波切换和混合梯度组合策略，有效解决通信延迟对分布式主动降噪系统的稳定性影响，实现与集中式算法相当的降噪性能。<br/><br/>贡献点：  <br/>1. **提出混合通信机制**：通过自适应-固定滤波切换与混合梯度组合策略，减少通信开销对算法稳定性的影响。  <br/>2. **动态性能监控与切换**：实时检测降噪性能衰减，触发开关机制，切换至固定滤波并请求通信，避免不稳定风险。  <br/>3. **优化信息传输效率**：仅传递控制滤波器的梯度差值，降低通信负载，同时保留关键性能信息。  <br/>4. **实验验证实用性**：通过仿真证明该方法在通信受限条件下仍能保持稳定性，降噪效果接近集中式系统，适合实际部署。|
|2510.00914v1|[Reconstruction of the Complete Vocal Tract Contour Through Acoustic to   Articulatory Inversion Using Real-Time MRI Data](http://arxiv.org/abs/2510.00914v1)|总结：  <br/>提出首个完整声学-发音学反演模型，利用实时动态MRI数据及双向LSTM技术，实现从声带到嘴唇的全口腔结构重建，精度接近像素级。<br/><br/>贡献点：  <br/>1. **首次全面反演**：突破传统EMA数据限制，实现从声带至嘴唇的整个口腔结构的声学-发音学反演。  <br/>2. **创新数据集**：采用超过3小时的实时动态MRI数据库，包含降噪语音信号和自动分割的发音器官轮廓。  <br/>3. **双向LSTM模型**：提出基于双向LSTM的多模态反演方法，支持单个发音器官独立反演与全发音器官联合反演。  <br/>4. **性能突破**：平均RMSE精度达1.65 mm，接近MRI像素分辨率（1.62 mm），验证模型的高精度有效性。|
|2510.00771v1|[UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free   Flow Matching](http://arxiv.org/abs/2510.00771v1)|**贡献点**  <br/>1. **提出vocoder-free框架**：首次将flow matching生成模型应用于音频超分辨率，直接通过逆短时傅里叶变换（iSTFT）生成波形，无需依赖外部vocoder。  <br/>2. **解决两阶段方法瓶颈**：突破传统“先生成mel-spectrogram再合成波形”的流程限制，避免因vocoder性能不足导致的音频质量下降。  <br/>3. **端到端优化简化**：通过统一的生成模型实现端到端训练，减少模块间的复杂依赖，提升训练效率与模型稳定性。  <br/>4. **高保真音频生成**：在多倍上采样场景下（如×4），生成高质量48kHz音频，优于现有方法在语音与通用音频数据集上的性能。  <br/>5. **条件分布建模创新**：利用flow matching模型有效捕捉复数谱系数的条件分布，增强生成音频的频谱重构能力。  <br/><br/>**总结**（100字以内）：  <br/>本文提出vocoder-free的音频超分辨率框架，通过flow matching模型直接生成高质量音频，克服传统两阶段方法的瓶颈，实现端到端优化并达到SOTA性能。|
|2510.00743v1|[From Scores to Preferences: Redefining MOS Benchmarking for Speech   Quality Reward Modeling](http://arxiv.org/abs/2510.00743v1)|**总结（100字以内）**  <br/>该论文提出MOS-RMBench基准，构建三种奖励模型，并设计MOS-aware GRM，显著提升合成语音的细粒度质量评估，推动更严谨和可扩展的语音质量研究。<br/><br/>**贡献点**  <br/>1. **提出MOS-RMBench**：首次将多 MOS 数据集统一为偏好比较任务，解决传统主观评分依赖人工标注、标准不一致和可重复性差的问题。  <br/>2. **系统构建三种奖励模型**：设计并评估标量、半标量和生成奖励模型（GRMs），为语音生成的奖励建模提供方法论框架。  <br/>3. **改进MOS-aware GRM**：引入 MOS 差异感知的奖励函数，使模型能动态适应不同难度样本对，显著提升细粒度质量区分能力。  <br/>4. **揭示关键实验发现**：明确标量模型优势、合成语音评估的模型缺口及小 MOS 差异对的挑战，为后续研究提供方向。  <br/>5. **推动研究范式**：建立统一基准与方法框架，促进语音质量评估领域更规范、高效的研究进程。|
|2510.00657v2|[XPPG-PCA: Reference-free automatic speech severity evaluation with   principal components](http://arxiv.org/abs/2510.00657v2)|**贡献点**：<br/>1. 提出XPPG-PCA方法：一种无监督、参考自由的语音病理严重程度评估新框架。  <br/>2. 验证方法有效性：在三个荷兰口腔癌数据集上表现优于或等同于传统参考方法。  <br/>3. 强化鲁棒性：显著抵抗数据捷径（spurious shortcuts）和噪声干扰。  <br/>4. 实现通用性：适用于多种语音障碍的客观评估，突破特定任务限制。  <br/>5. 开源实现：提供可复现实验与临床应用的开源工具。  <br/><br/>**总结**：  <br/>本研究提出XPPG-PCA，一种无监督参考自由的语音病理评估方法，在多个数据集上验证其有效性与鲁棒性，为临床评估提供通用、高效的解决方案，并开源代码促进应用。|
|2510.00639v1|[Reference-free automatic speech severity evaluation using acoustic unit   language modelling](http://arxiv.org/abs/2510.00639v1)|总结：  <br/>本研究提出参考无关的SpeechLMScore方法，构建改进数据集NKI-SpeechRT，验证其在噪音鲁棒性和语音严重程度评估中的优越性，突破传统模型的局限性。<br/><br/>贡献点：  <br/>1. **提出参考无关的语音严重程度评估方法 SpeechLMScore**  <br/>   - 基于语言模型预测语音自然度，无需病理语音数据或转录文本，解决传统方法对参考材料的依赖问题。  <br/><br/>2. **构建更全面的基准数据集 NKI-SpeechRT**  <br/>   - 在现有 NKI-CCRT 数据集基础上扩展，提升语音严重程度评估的数据基础与适用性。  <br/><br/>3. **对比研究参考无关与参考相关模型的性能差异**  <br/>   - 量化评估 SpeechLMScore 与传统声学特征方法的效果差距，验证其泛化能力。  <br/><br/>4. **分析噪声对模型的影响**  <br/>   - 引入主观噪声评分数据，证明 SpeechLMScore 在嘈杂环境下的鲁棒性优于传统方法。|
|2510.00628v1|[Hearing the Order: Investigating Selection Bias in Large Audio-Language   Models](http://arxiv.org/abs/2510.00628v1)|总结：本文首次系统性研究LALM顺序偏差问题，发现模型性能受答案选项顺序显著影响，提出排列策略缓解偏差，警示现有评估方法的可靠性问题。<br/><br/>贡献点：<br/>1. 首次提出并系统分析LALMs中的顺序偏差问题（selection bias），发现其预测易受答案选项排列顺序影响。<br/>2. 通过六种LALMs在三个主流基准及对应语音数据集上的大规模实验，实证证明所有模型均存在该偏差。<br/>3. 量化展示打乱选项顺序可导致性能波动最高达24%，甚至改变模型排名，揭示当前评估体系的脆弱性。<br/>4. 提出基于排列的策略有效性研究，证明其在多数情况下可显著缓解顺序偏差问题。<br/>5. 引发对语音模型评测方法可靠性的反思，推动该领域更严谨的偏差分析与改进方向研究。|
|2510.00626v1|[When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning   in Large Audio-Language Models](http://arxiv.org/abs/2510.00626v1)|**贡献点总结：**  <br/>1. 系统性分析无关音频（静音、合成噪声、环境声）对文本推理任务的负面影响。  <br/>2. 首次揭示音频时长、振幅及解码温度与干扰严重性相关性。  <br/>3. 证实静音对模型输出的破坏性与合成噪声相当，挑战其“中性”假设。  <br/>4. 评估缓解策略，指出自一致性方法可提升稳定性但增加计算成本。  <br/>5. 提出跨模态干扰是LALMs鲁棒性核心挑战，强调高效融合策略的必要性。  <br/><br/>**摘要总结（100字内）：**  <br/>研究发现无关音频显著降低文本推理准确性，干扰程度与音频长度、振幅及温度相关。静音干扰效果与合成噪声相当，自一致性策略虽提升稳定性但成本较高，凸显需高效融合策略应对跨模态干扰的迫切需求。|
|2510.00582v1|[SAGE-LD: Towards Scalable and Generalizable End-to-End Language   Diarization via Simulated Data Augmentation](http://arxiv.org/abs/2510.00582v1)|总结：  <br/>本文提出一种支持多语言的神经语音语言分割模型，在统一框架中实现跨语言泛化，通过可学习的query-based架构和大规模模拟代码切换预训练，显著提升性能并推动代码切换语音技术发展。<br/><br/>贡献点：  <br/>1. **多语言统一框架**：构建首个支持广泛语言集的神经语言识别模型，兼容无约束语言跨度。  <br/>2. **query-based架构**：引入可学习查询机制，结合多语言感知能力提升语言分割效率。  <br/>3. **模拟代码切换预训练**：基于大规模模拟数据进行预训练，增强模型对真实多语言场景的适应性。  <br/>4. **性能突破**：在多个基准测试中达到SOTA，相较传统方法提升23%-52%。  <br/>5. **应用拓展**：为代码切换语音技术提供基础框架，推动跨语言语音处理研究。|
|2510.00522v3|[ARIONet: An Advanced Self-supervised Contrastive Representation Network   for Birdsong Classification and Future Frame Prediction](http://arxiv.org/abs/2510.00522v3)|总结：提出ARIONet自监督对比网络，通过增强音频表示与Transformer结构提升鸟类声音分类准确性与时序建模能力，在多个数据集上验证其高性能及生态应用潜力。<br/><br/>贡献点：<br/>1. **提出自监督对比学习框架**：首次将对比分类与未来帧预测任务联合优化，无需依赖大规模标注数据。<br/>2. **改进音频特征表示**：利用增强音频表示结合Transformer编码器，整合多类互补音频特征，提升表征能力。<br/>3. **建模时间动态特性**：通过预测未来音频帧，有效捕捉鸟类声音的时序依赖性，解决传统方法忽略时间信息的问题。<br/>4. **实验验证优越性能**：在4个公开鸟类声音数据集上实现98.41%-93.07%的分类准确率和97.84%-94.10%的F1分数，显著优于现有方法。<br/>5. **量化时间建模效果**：在未来帧预测任务中达到95%的余弦相似度和低均方误差，证明时序建模能力。<br/>6. **强调实际应用价值**：验证模型在生态监测和生物多样性研究中的实用性，为真实场景部署提供支持。|
|2510.00522v2|[ARIONet: An Advanced Self-supervised Contrastive Representation Network   for Birdsong Classification and Future Frame Prediction](http://arxiv.org/abs/2510.00522v2)|**贡献点总结：**<br/>1. 提出自监督对比网络ARIONet，联合优化鸟鸣分类与未来帧预测。<br/>2. 构建基于Transformer的多特征整合模型，提升音频表示能力。<br/>3. 无需大规模标注数据，实现物种特异性表征学习与时间动态建模。<br/>4. 在4个多样化鸟类数据集上验证，分类准确率达98.41%-93.07%，F1分数达97.84%-90.94%。<br/>5. 未来帧预测任务中表现优异，误差低至均方误差，余弦相似度达95%。<br/>6. 突出自监督策略对复杂声学模式和时序依赖的建模能力，具备生态监测的实际应用潜力。|
|2510.00395v2|[SAGE-Music: Low-Latency Symbolic Music Generation via   Attribute-Specialized Key-Value Head Sharing](http://arxiv.org/abs/2510.00395v2)|总结：  <br/>本文提出AS-KVHS方法，解决音乐生成中推理速度与质量的权衡问题，实现30%推理加速和0.4%质量损失，并发布SAGE-Music开源基准，推动音乐生成研究。<br/><br/>贡献点：  <br/>1. **首次系统研究BPE在多轨符号音乐中的泛化性**：分析BPE方法在单轨与多轨数据间的性能差异，揭示其局限性。  <br/>2. **提出AS-KVHS方法**：通过专门化键值头共享机制，优化音乐生成的低延迟特性，兼顾生成速度与质量。  <br/>3. **构建SAGE-Music开源基准**：提供高质量的评估工具，匹配或超越当前最优模型的生成效果。|
|2510.00264v2|[Low Resource Audio Codec Challenge Baseline Systems](http://arxiv.org/abs/2510.00264v2)|总结：  <br/>本论文提出2025 LRAC挑战的官方基线系统，覆盖透明度与增强两类音频编解码任务，采用残差向量量化与端到端对抗-重建联合训练框架，并系统性优化了数据处理与模型评估方案。<br/><br/>贡献点：  <br/>1. **构建LRAC挑战基准体系**：首次明确划分透明度编解码（Track1）与增强编解码（Track2）任务目标，为低资源音频编码研究建立标准化测试框架。  <br/>2. **设计端到端训练框架**：提出结合对抗目标与重建目标的卷积神经编解码器模型，采用残差向量量化（RVQ）实现高效压缩与质量保持的平衡。  <br/>3. **创新数据处理策略**：开发针对性的数据过滤与增强方法，提升模型在噪声、混响等复杂场景下的鲁棒性与泛化能力。  <br/>4. **提出优化与评估方案**：系统性制定模型架构选择、优化策略及检查点筛选准则，为低资源音频编码研究提供可复现的技术路线。|
|2509.26634v1|[Scaling Spoken Language Models with Syllabic Speech Tokenization](http://arxiv.org/abs/2509.26634v1)|总结（100字以内）：  <br/>该论文首次系统研究音节级token化在语音语言建模中的效果，验证其在保持性能的同时显著降低计算成本，为构建高效长上下文语音模型提供新方向。<br/><br/>贡献点：  <br/>1. **首次提出系统性研究**：系统评估音节级token化对语音语言建模（SLM）的影响，填补该领域研究空白。  <br/>2. **多基准验证有效性**：在多个SLU基准任务中测试模型，证明音节token的广泛适用性。  <br/>3. **性能与效率平衡**：音节token在效果上匹配或超越传统高帧率token，同时降低训练时间（>2倍）和FLOPs（5倍）。  <br/>4. **高效长上下文路径**：揭示音节级语言建模的潜力，为开发低成本、长上下文语音模型提供理论支持。|
|2509.26580v1|[Source Separation for A Cappella Music](http://arxiv.org/abs/2509.26580v1)|总结：  <br/>提出一种基于功率集的数据增强策略和改进的SepReformer模型架构，实现对无伴奏音乐中动态人数多歌手分离的高效处理，显著提升分离性能并具备良好泛化能力。  <br/><br/>贡献点：  <br/>1. **动态人数处理**：针对无伴奏音乐中活跃歌手数量变化的混合物，提出功率集数据增强策略，扩展有限数据集为指数级训练样本。  <br/>2. **模型改进**：改进SepReformer架构，引入周期性激活机制，增强模型对静音状态的鲁棒性。  <br/>3. **复合损失函数**：设计组合损失函数，在歌手静音时仍维持有效，提升分离精度。  <br/>4. **实验验证**：在JaCappella数据集上验证方法，达到全编队和子集分离的SOTA性能，优于传统频谱图基线并具备现实场景泛化能力。|
|2509.26542v1|[Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap](http://arxiv.org/abs/2509.26542v1)|总结（100字以内）: <br/>VERA构建首个语音推理基准，揭示语音与文本在推理能力上的显著差距，并分析实时交互对语音系统性能的限制，为改进语音助手的推理与流畅性提供关键评估工具。<br/><br/>贡献点: <br/>1. 提出首个针对实时语音交互系统推理能力的基准VERA，包含5个tracks（Math, Web, Science, Long-Context, Factual）及2931个语音化对话片段。  <br/>2. 量化语音与文本模型的模态性能差距（如数学任务中文本模型74.8% vs 语音6.1%），揭示语音系统可靠性瓶颈。  <br/>3. 发现低延迟语音系统存在性能 plateau（<10%准确率），接近文本性能需牺牲实时性，提出延迟-准确率矛盾的实证证据。  <br/>4. 评估多种缓解方案有效性，证明增加"思考时间"无明显效果，而解耦推理-叙述结构虽提升准确率但仍有不足。  <br/>5. 提供规范化的测试框架与诊断工具，支持针对语音助手架构的对比分析，推动实时推理系统研究。|
|2509.26521v1|[MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph   Classification Models](http://arxiv.org/abs/2509.26521v1)|**贡献点总结：**  <br/>1. 提出MUSE-Explainer方法，专门用于音乐领域图神经网络的可解释性分析。  <br/>2. 引入反事实解释生成机制，通过修改音乐乐谱图结构实现预测结果的可解释性。  <br/>3. 针对音乐数据特性设计解释策略，避免不现实或混淆的输出。  <br/>4. 验证方法在音乐分析任务中的有效性，支持通过标准工具（如Verovio）可视化解释结果。  <br/><br/>**100字内总结：**  <br/>本文提出MUSE-Explainer，通过修改音乐图结构生成反事实解释，增强模型决策透明度。该方法适配音乐数据特性，避免不现实输出，并能在标准工具中直观展示分析结果。|
|2509.26471v1|[On Deepfake Voice Detection -- It's All in the Presentation](http://arxiv.org/abs/2509.26471v1)|总结：该论文提出新型数据框架与研究方法，显著提升深度伪造音频检测准确率，并强调数据集优化比模型规模更重要的研究价值。<br/><br/>贡献点：<br/>1. 指出现有深度伪造数据集与真实传输场景的差异导致检测系统泛化能力不足<br/>2. 提出新的数据创建框架和研究方法，提升反欺骗措施的现实适用性<br/>3. 在实验室环境中实现39%的检测准确率提升，在真实场景基准上实现57%的提升<br/>4. 首次论证数据集质量对检测性能的影响大于模型参数量，呼吁加强数据收集投入|
|2509.26409v1|[IR-UWB Radar-Based Contactless Silent Speech Recognition with   Attention-Enhanced Temporal Convolutional Networks](http://arxiv.org/abs/2509.26409v1)|总结：  <br/>本研究提出一种注意力增强的时序卷积网络架构，利用IR-UWB雷达信号实现静默语音识别，通过端到端学习显著提升识别准确率，验证了深度学习在生物信号处理中的有效性。<br/><br/>贡献点：  <br/>1. **创新架构设计**：结合自注意力机制与压缩激励模块，构建了适用于IR-UWB雷达信号的时序卷积网络，有效捕捉发音时序特征。  <br/>2. **端到端信号处理**：直接从少量预处理的雷达原始信号中学习判别表示，无需依赖传统手工特征工程。  <br/>3. **性能提升验证**：在50词识别任务中，采用留一场景法交叉验证，实现91.1%的测试准确率，较传统方法提升17.1个百分点。  <br/>4. **生物信号应用拓展**：针对非接触式雷达传感场景，探索了生物信号在静默语音识别中的可行性与优势。|
|2509.26388v1|[Game-Time: Evaluating Temporal Dynamics in Spoken Language Models](http://arxiv.org/abs/2509.26388v1)|总结：  <br/>本论文提出Game-Time Benchmark框架，系统评估对话式语音模型的时序能力，揭示其在同步响应和全双工交互上的不足，并提供数据集和demo支持后续研究。<br/><br/>贡献点：  <br/>1. 提出Game-Time基准，首次系统评估对话SLMs的时序动态能力；  <br/>2. 发现当前模型在基础指令任务的性能差异及在时序约束下的显著退化；  <br/>3. 提供开放数据集与demo，推动时序感知对话AI的研究发展。|
|2509.26291v1|[Representation-Based Data Quality Audits for Audio](http://arxiv.org/abs/2509.26291v1)|总结：  <br/>本文提出将图像领域SelfClean框架迁移至音频领域，通过自监督表示实现数据质量问题识别与统一排名处理，验证了其在多个数据集上的有效性，显著提升审核效率并减少人工标注成本。<br/><br/>贡献点：  <br/>1. **跨领域迁移**：首次将图像数据审计框架SelfClean适配至音频领域，解决音频数据质量评估难题。  <br/>2. **自监督表示应用**：利用自监督音频特征识别离题样本、近似重复和标签错误等常见问题。  <br/>3. **统一处理流程**：通过单一流程同时识别多种数据质量问题，生成按优先级排序的审查列表。  <br/>4. **全面验证**：在ESC-50、GTZAN和工业数据集上测试，涵盖合成与现实场景中的数据干扰。  <br/>5. **效率提升**：方法优于针对单一问题的基线，显著降低人工标注需求，提升审核效率。|
|2509.26276v1|[Optimizing Speech Language Models for Acoustic Consistency](http://arxiv.org/abs/2509.26276v1)|总结：  <br/>该研究提出语义初始化与规划损失方法，通过不同规模模型实验验证语音生成的稳健性和一致性，并分析交织技术对语言模型与语音对齐的双重影响。<br/><br/>贡献点：  <br/>1. 提出结合语义初始化和规划损失的语音语言模型框架，提升生成的稳健性和一致性。  <br/>2. 引入自监督特征初始化、轻量对齐损失及薄化和辅助目标，增强模型鲁棒性与内容规划能力。  <br/>3. 通过对比实验验证语音-only模型在跨说话人、性别、情感等多因素下表现优于更大系统。  <br/>4. 分析交织文本-语音模型对词汇语法能力及语义-语音对齐的提升作用，同时揭示其对一致性的负面影响。  <br/>5. 通过线性探测研究发现初始化偏向内容结构而牺牲韵律细节，揭示模型设计中的关键权衡。  <br/>6. 证明LM侧设计与训练策略可调控声学稳定性与语义关联的平衡，无需修改分词器或运行时架构。  <br/>7. 提供开源模型及权重，便于进一步研究与实验验证。|
|2509.26207v1|[The silence of the weights: an investigation of structural pruning   strategies for attention-based audio signal architectures](http://arxiv.org/abs/2509.26207v1)|总结（100字以内）:  <br/>该论文提出针对Transformer注意力机制的分层剪枝技术，通过分离查询、键、值和输出投影矩阵的剪枝，结合头与通道维度策略，显著降低参数量和硬件需求，且在AST模型上实现50%剪枝时性能损失<1%。<br/><br/>贡献点:  <br/>1. **提出面向注意力机制的专用剪枝技术**：首次针对Transformer的注意力层设计专门的剪枝方法，而非通用模型剪枝。  <br/>2. **分层剪枝策略**：将注意力块的四个关键投影矩阵（查询、键、值、输出）独立剪枝，突破传统统一剪枝的限制。  <br/>3. **多维度剪枝探索**：系统研究并对比沿**头维度**和**通道维度**的剪枝方案，分析其对模型性能的影响差异。  <br/>4. **实证效果验证**：在音频领域主流模型AST上验证剪枝有效性，证明50%参数剪枝仅导致<1%的性能下降，显著提升效率。|
|2509.26177v1|[Benchmarking Diarization Models](http://arxiv.org/abs/2509.26177v1)|贡献点总结（100字以内）:  <br/>本研究系统评估了五种先进说话人日志模型在多语言、多场景数据集上的性能，发现PyannoteAI表现最优，DiariZen为开源优质方案，并揭示了高说话人数量下错误主要由语音段遗漏和混淆引发。<br/><br/>详细贡献点：  <br/>1. **多语言数据集评估**：采用涵盖英语、中文、德语、日语和西班牙的4个数据集（共196.6小时），全面验证模型在不同语言和声学条件下的鲁棒性。  <br/>2. **性能对比分析**：对比五种前沿模型，明确PyannoteAI（DER=11.2%）与DiariZen（DER=13.3%）在关键指标（DER）上的表现差异。  <br/>3. **失败模式归因**：系统分析发现，日志错误主要源于语音段遗漏与说话人混淆，尤其在高人数场景中问题显著。  <br/>4. **开源方案验证**：为DiariZen提供实证支持，证明其作为开源替代方案的有效性，推动技术可及性。  <br/>5. **关键问题聚焦**：强调说话人日志对下游系统的重大影响，为改进相关应用提供理论依据。|
|2509.26133v1|[Zimtohrli: An Efficient Psychoacoustic Audio Similarity Metric](http://arxiv.org/abs/2509.26133v1)|**贡献点总结（100字以内）：**  <br/>提出Zimtohrli，结合心理声学模型与非线性共振器，优化DTW和NSIM算法，实现高效且感知准确的音频相似性评估，性能超越开源ViSQOL并接近商业POLQA，适用于编码器开发与生成音频系统评估。<br/><br/>**分点贡献：**  <br/>1. **创新心理声学模型**：融合128-bin gammatone滤波器组（模拟耳蜗频率分辨率）与非线性共振器（模仿鼓膜响应），构建兼具生理机制与感知一致性的音频特征提取前端。  <br/>2. **改进相似度算法**：引入非线性修正的DTW和NSIM算法，通过增强非线性特性提升与人类听觉判断的匹配度。  <br/>3. **性能优势**：在开源ViSQOL基础上显著提升，同时缩小与商业POLQA的性能差距，验证其有效性与竞争力。  <br/>4. **高效实用性**：在保持高感知相关性的同时，降低计算复杂度，为现代音频工程提供兼顾性能与效率的可解释评估工具。|
|2509.26007v1|[MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms](http://arxiv.org/abs/2509.26007v1)|总结：  <br/>MARS提出多尺度自回归方法，将频谱图视为多通道图像并引入CMX技术，实现高效高保真音频生成，超越现有基准模型。<br/><br/>贡献点：  <br/>1. **多尺度自回归框架**：首次将多尺度自回归思想应用于频谱图生成，替代传统基于token的自回归，提升音频生成的连贯性与细节。  <br/>2. **通道复用技术（CMX）**：通过重塑频谱图尺寸压缩信息，保留关键结构，降低计算成本。  <br/>3. **统一Tokenizer设计**：构建跨尺度的共享离散表示，使Transformer模型能高效从粗到细优化频谱图，推动高保真音频生成的可扩展性。|
|2509.25670v1|[LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with   Cross-Lingual Transfer Learning](http://arxiv.org/abs/2509.25670v1)|总结：提出LTA-L2S方法，通过跨语言迁移学习和流匹配技术提升普通话唇语合成的声调准确性和可懂度。<br/><br/>贡献点：<br/>1. 提出跨语言迁移策略：利用英语预训练SSL模型，解决普通话L2S合成中viseme-to-phoneme映射的复杂性及训练成本问题。<br/>2. 融合声调信息建模：通过ASR微调的SSL语音单元引导流匹配模型生成F0轮廓，增强声调对语音可懂度的关键作用。<br/>3. 两阶段训练提升质量：以流匹配postnet精修粗谱图，显著改善整体语音质量与声调准确性，实验验证优于现有方法。|
|2509.25652v1|[Iterative Residual Cross-Attention Mechanism: An Integrated Approach for   Audio-Visual Navigation Tasks](http://arxiv.org/abs/2509.25652v1)|**贡献点总结**：<br/>1. 提出端到端框架IRCAM-AVN，整合多模态信息融合与序列建模，替代传统分阶段模块设计；<br/>2. 引入迭代残差跨注意力机制，实现多级残差设计以优化特征提取；<br/>3. 通过跨模态信息交互减少冗余与传输不一致性，提升模型稳定性与泛化能力；<br/>4. 验证所提方法在语音导航任务中的有效性，优于现有方法。<br/><br/>**100字内摘要**：<br/>本研究提出IRCAM-AVN框架，通过迭代残差跨注意力机制整合多模态信息融合与序列建模，替代传统分阶段模块设计，有效解决了冗余处理与信息传输不一致问题，提升语音导航的模型稳定性与泛化能力，实验验证其优越性能。|
|2509.25601v1|[Echoes of Humanity: Exploring the Perceived Humanness of AI Music](http://arxiv.org/abs/2509.25601v1)|**贡献点：**  <br/>1. 设计盲测实验，通过类图灵测试方法评估听众区分AI音乐（AIM）与人类创作音乐的能力。  <br/>2. 引入随机化控制交叉试验，控制配对相似性并支持因果推断分析。  <br/>3. 首次使用真实场景中商业模型（如Suno）的未受控制数据集，提升研究现实性。  <br/>4. 通过混合方法内容分析揭示听众在判断AIM时关注的语音特征与技术线索。  <br/><br/>**总结（100字以内）：**  <br/>本研究首次基于真实数据集，通过盲测与随机化交叉实验评估人类对AI音乐的感知能力，揭示听众关注的语音和技术特征，并建立听众区分能力与配对相似性的因果关系，为模型优化与用户教育提供依据。|
|2509.25496v1|[Ethics Statements in AI Music Papers: The Effective and the Ineffective](http://arxiv.org/abs/2509.25496v1)|总结（100字以内）:  <br/>该文揭示AI音乐研究伦理声明的现状问题，提出需加强伦理反思而非形式化合规，并为会议与研究者提供针对性建议，推动AI音乐领域的负责任研究实践。<br/><br/>贡献点:  <br/>1. **系统分析伦理声明现状**：首次系统梳理ISMIR、NIME等会议及近五年AI音乐文献中伦理声明的使用情况，指出其不足（如流于形式）。  <br/>2. **批判性视角审视实践**：揭示当前伦理声明在音乐AI领域未能有效引导对技术伦理的深入讨论，强调其局限性。  <br/>3. **提出改进框架与建议**：为音频会议和研究者提供具体策略，以提升伦理声明的反思价值，促进更具社会责任感的研究实践。|
|2509.25296v1|[Learning Relationships Between Separate Audio Tracks for Creative   Applications](http://arxiv.org/abs/2509.25296v1)|总结：  <br/>提出基于分离轨道数据库的音乐生成框架，整合符号决策模块与感知模块，实现实时音乐输出与输入关系的训练与预测，并通过定量评估验证模型有效性。<br/><br/>贡献点：  <br/>1. **首创音乐代理框架**：提出首个通过训练建立实时音乐输入与输出间关系的框架，基于分离轨道数据库的构建。  <br/>2. **多模块架构设计**：集成符号决策模块（学习音乐关系）与感知模块（基于Wav2Vec 2.0），结合拼接合成作为音频渲染。  <br/>3. **模型实现方法**：采用Transformer作为决策模块的离线实现，实现音乐关系的提取与复现。  <br/>4. **定量评估体系**：建立决策模块在复现训练中提取的音乐关系方面的量化评估方法。  <br/>5. **实际效果验证**：证明决策模块可根据引导轨道A预测出连贯的轨道B，展示其在配对轨道生成中的有效性。|
|2509.25028v1|[The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI   Music Tools](http://arxiv.org/abs/2509.25028v1)|**贡献点：**  <br/>1. 提出“结构化不确定性”作为分析AI音乐系统中随机性设计的理论框架。  <br/>2. 系统比较六种AI音乐系统（Musika、MIDI-DDSP、Melody RNN等），归纳共性设计模式。  <br/>3. 阐明随机性如何兼顾音乐连贯性、用户控制与协作创作的平衡。  <br/>4. 首次以“主题性回顾”形式探讨AI音乐中的随机性与不确定性，为设计者提供实践指导。  <br/><br/>**总结（100字内）：**  <br/>本研究首次系统分析AI音乐系统中随机性设计，提出结构化不确定性框架，归纳六类系统的设计模式，揭示如何平衡创意与连贯性，为音乐交互设计提供理论与实践参考。|
|2509.24924v1|[SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution](http://arxiv.org/abs/2509.24924v1)|**贡献点总结（100字以内）：**  <br/>本文提出SAGA-SR模型，结合语义和声学引导，通过DiT主干与流匹配训练，在4kHz-32kHz任意输入采样率下实现44.1kHz高分辨率音频重建，达到SOTA性能，并开源代码和示例。<br/><br/>**分点贡献：**  <br/>1. **跨域高分辨率音频重建**：首次在语音、音乐、音效等多领域中实现通用音频超分辨率，统一处理不同场景的低频输入信号。  <br/>2. **语义-声学双指导机制**：通过文本描述和频谱滚降嵌入的多模态条件化，增强音频高频率成分的语义一致性与重建质量。  <br/>3. **广范围采样率适配**：支持从4 kHz到32 kHz任意输入采样率直接升至44.1 kHz，突破传统方法对输入频率的限制。  <br/>4. **SOTA性能与开源实现**：在客观和主观评估中均验证了模型的先进性，并提供可复现的代码与音质示例。|
|2509.24901v2|[Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio   Classification](http://arxiv.org/abs/2509.24901v2)|总结（100字以内）:<br/>本研究揭示了音频自监督学习中全局池化导致的信息瓶颈问题，提出二值化原型探针方法，通过逐类信息聚合有效解决cls-token信息丢失问题。该轻量方法在13个数据集和6种编码器上的测试中显著优于传统线性及注意力探针，证明了探查作为替代微调的高效评估范式可行性。<br/><br/>贡献点：<br/>1. 首次系统分析音频SSL中全局池化的信息瓶颈问题，揭示cls-token在处理多标签音频时对分散事件信息的丢失<br/>2. 提出二值化prototypical probes方法，创新性地采用原型学习进行类级信息聚合<br/>3. 在13个音频数据集与6种光谱图编码器的广泛基准测试中，验证该方法性能超越传统线性及注意力探针<br/>4. 建立探查作为替代微调的评估范式，挑战了现有依赖耗时微调的实践标准<br/>5. 构建轻量级、简单的评估框架，为音频SSL模型的高效验证提供新途径|
|2509.24901v1|[Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio   Classification](http://arxiv.org/abs/2509.24901v1)|总结：  <br/>提出二进制原型探针方法，解决音频自监督学习中全局池化导致的信息瓶颈问题，显著提升模型评估效率与效果。<br/><br/>贡献点：  <br/>1. **揭示全局池化瓶颈**：首次系统分析音频自监督学习（SSL）中全局池化对嵌入质量的影响，指出cls-token忽略多标签音频的局部事件信息。  <br/>2. **提出轻量探针方法**：设计二进制原型探针，通过类内信息聚合替代复杂池化操作，简化评估流程。  <br/>3. **验证方法有效性**：在13个数据集和6种频谱图编码器上验证，方法显著优于线性和注意力探针。  <br/>4. **推动评估范式变革**：证明探针可作为高效、低成本的替代方案，挑战传统依赖微调的评估依赖。|
|2509.24853v1|[Enhanced Automatic Drum Transcription via Drum Stem Source Separation](http://arxiv.org/abs/2509.24853v1)|总结（100字以内）:  <br/>本文提出通过结合ADTOF包与开源stem分离模型，实现从5类到7类鼓转录的扩展，并估算MIDI速度值，显著提升鼓转录的现实感和适用性，为MIR和音乐制作提供更精确的转录结果。<br/><br/>贡献点:  <br/>1. **扩展鼓转录类别**：通过后处理步骤将ADT输出从5类（kick/snare/hi-hats/toms/cymbals）扩展至7类，增加对特定镲片（crash/ride）的识别。  <br/>2. **MIDI速度值估计**：基于分离的鼓组stem，提出方法估算MIDI力度值，增强转录的动态表达力。  <br/>3. **工具集成创新**：结合现有ADTOF包与开源stem分离模型，探索协同作用以提升转录结果的现实感和准确性。  <br/>4. **性能验证**：在8类鼓转录基线评估中表现优异，生成适合MIR及音乐生产任务的高质量MIDI转录。|
|2509.24834v1|[Room Impulse Response Prediction with Neural Networks: From Energy Decay   Curves to Perceptual Validation](http://arxiv.org/abs/2509.24834v1)|总结：  <br/>提出基于神经网络的RIR预测框架，结合几何参数与吸声特性，通过大规模仿真数据训练并实现高精度、感知可靠的RIR重建，为音频建模与渲染提供可扩展解决方案。<br/><br/>贡献点：  <br/>1. **方法创新**：设计首个神经网络框架，直接从房间几何参数、材料吸声系数和声源-接收位置预测能量衰减曲线（EDCs），再通过反向微分重建RIRs。  <br/>2. **数据集构建**：生成包含真实几何结构、频率相关吸声特性和多样声源-接收配置的高质量训练数据，提升模型泛化能力。  <br/>3. **多维度评估**：提出结合客观指标（RMSE、MSE、频谱相似性）和主观测试（MUSHRA）的综合验证体系，确保预测结果的准确性与感知可靠性。  <br/>4. **实用价值**：验证模型在实际应用中的可行性，为语音处理中的沉浸式音频和环境建模提供高效、可扩展的解决方案。|
|2509.24793v1|[Sparse Autoencoders Make Audio Foundation Models more Explainable](http://arxiv.org/abs/2509.24793v1)|总结：本文提出利用稀疏自编码器分析语音预训练模型的隐藏表示，通过唱歌技巧分类案例验证其保留原始信息与标签的能力，并展示其在解耦语音属性方面的有效性，为理解自监督学习系统提供新方法。<br/><br/>贡献点：<br/>1. 提出将稀疏自编码器（SAEs）应用于预训练语音模型的隐藏表示分析，突破传统线性探针的局限性<br/>2. 在唱歌技巧分类任务中验证SAEs的表征能力，证明其能保留原始输入信息与类别标签的关联<br/>3. 揭示SAEs在语音属性解耦方面的优势，为探究自监督学习模型的潜在特征提供有效工具|
|2509.24773v2|[VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning](http://arxiv.org/abs/2509.24773v2)|总结：  <br/>本文提出VSSFlow，通过统一框架整合视频到声音和视觉文本到语音生成任务，引入条件聚合机制与注意力层的归纳偏差差异，实现端到端高效联合学习，并在多基准测试中超越现有方法。<br/><br/>贡献点：  <br/>1. 提出VSSFlow统一框架，首次将V2S与VisualTTS整合为单一生成模型  <br/>2. 设计新型条件聚合机制，有效处理异构视频与文本输入信号  <br/>3. 发现交叉注意力与自注意力层具不同归纳偏差，针对性处理视频条件与语音转录本  <br/>4. 验证端到端联合训练无需复杂策略，通过共享通用音频先验提升生成效果与稳定性|
|2509.24773v1|[VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning](http://arxiv.org/abs/2509.24773v1)|总结：  <br/>提出VSSFlow统一视频到声音与视觉文本到语音生成，通过条件聚合和注意力机制处理异构输入，结合共享音频先验实现端到端联合学习，实验验证其在两项任务上的优越性。<br/><br/>贡献点：  <br/>1. **统一框架**：首次将视频到声音（V2S）与视觉文本到语音（VisualTTS）任务整合至单一流匹配框架中，解决传统分离处理的局限性。  <br/>2. **条件聚合机制**：设计新型条件聚合方式，有效处理视频和文本等异构输入条件的差异性。  <br/>3. **注意力机制区分任务**：利用交叉注意力（处理模糊视频条件）与自注意力（处理确定性文本）的归纳偏差，分别优化两类条件的生成效果。  <br/>4. **端到端联合学习**：无需复杂训练策略，通过端到端联合学习实现稳定生成，验证了联合训练对性能的提升作用。  <br/>5. **共享音频先验**：揭示任务间共享的通用音频先验可加速收敛、增强条件生成并稳定引导过程，为后续研究提供理论依据。|
|2509.24769v1|[Deep Learning-Based Prediction of Energy Decay Curves from Room Geometry   and Material Properties](http://arxiv.org/abs/2509.24769v1)|总结：  <br/>该研究提出基于深度学习的EDC预测框架，利用合成数据集和LSTM网络实现高效建模，显著提升声学参数估计精度，适用于多场景的早期设计与实时应用。<br/><br/>贡献点：  <br/>1. **构建大规模合成数据集**：生成包含真实尺寸、声源接收位置及频率依赖吸音特性的6000个鞋盒房间数据，提升模型泛化能力。  <br/>2. **提出直接预测EDC的深度学习框架**：首次通过神经网络直接从房间几何参数和吸音信息预测能量衰减曲线，无需传统物理仿真。  <br/>3. **引入LSTM时序建模**：利用长短期记忆网络捕捉EDC的时间依赖特性，优化配置到EDC的映射过程。  <br/>4. **高精度参数估计**：通过预测EDC计算EDT、T20、C50等关键指标，误差控制在毫秒级（如EDT MAE 0.017 s）。  <br/>5. **跨场景应用支持**：模型可适应多样化的房间环境，为声学设计与实时应用提供高效解决方案。|
|2509.24674v1|[Advancing Zero-Shot Open-Set Speech Deepfake Source Tracing](http://arxiv.org/abs/2509.24674v1)|摘要贡献点总结（100字内）：  <br/>提出基于说话人验证的新零样本源追踪框架，调整SSL-AASIST系统用于攻击分类并确保训练与指纹试样攻击不重叠，对比分析零样本与少量样本方法在封闭/开放集场景下的性能差异，实验表明少量样本方法在封闭集更优，零样本在开放集表现更好。<br/><br/>分点贡献：  <br/>1. **提出新框架**：基于说话人验证技术的零样本源追踪方法，创新性地将SSL-AASIST系统应用于攻击分类任务。  <br/>2. **训练攻击分离**：确保训练数据中的攻击类型与指纹-试样对的攻击类型互斥，提升模型泛化能力。  <br/>3. **多方法对比研究**：系统评估零样本（余弦相似度、Siamese）与少量样本（MLP、Siamese）方法在后端评分中的效果差异。  <br/>4. **场景适应性分析**：揭示不同学习范式在封闭集和开放集场景下的性能表现，少量样本方法在封闭集优于零样本，反之亦然。  <br/>5. **实验验证**：基于自建STOPA数据集的对比实验，量化不同方法的性能差异（EER指标），为实际应用提供参考依据。|
|2509.24650v1|[VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and   True-to-Life Voice Cloning](http://arxiv.org/abs/2509.24650v1)|贡献点总结（100字以内）：  <br/>提出无需预训练分词器的VoxCPM-TTS框架，通过分层语义-声学建模与半离散残差表示，实现端到端训练和高效语音生成，达到SOTA零样本性能，并具备上下文感知表达，开源促进研究。<br/><br/>详细贡献点分列：  <br/>1. **提出tokenizer-free TTS框架**：通过半离散残差表示和分层建模，消除对外部语音分词器的依赖，解决语义-声学割裂问题。  <br/>2. **端到端扩散目标训练**：整个架构以简单扩散目标进行端到端训练，提升生成一致性和稳定性，避免多阶段流水线的误差累积。  <br/>3. **可微量化瓶颈设计**：引入可微分的量化机制，实现语义-韵律生成与声学细节恢复的自然分工，增强模型专精性。  <br/>4. **大规模双语语料训练**：基于1.8百万小时双语数据，VoxCPM-0.5B模型在零样本任务中取得SOTA性能，验证方法有效性。  <br/>5. **上下文感知表达能力**：模型能通过理解文本动态生成合适的韵律与风格，实现自然语音流和语义驱动的表达性。  <br/>6. **开源促进研究**：模型作为开源项目（Apache 2.0授权）提供，推动社区协作与技术普及。|
|2509.24635v1|[When Audio Generators Become Good Listeners: Generative Features for   Understanding Tasks](http://arxiv.org/abs/2509.24635v1)|总结：  <br/>该论文首次提出利用生成式特征提升音频理解，通过融合生成模型的时空感知与判别模型的语义抽象，解决了传统方法丢失细节的问题，在多个任务中验证了其有效性，并为音频表示学习开辟了新视角。<br/><br/>贡献点：  <br/>1. **首次引入生成式特征**：开创性地将生成模型应用于音频理解任务，突破传统判别式特征的局限。  <br/>2. **揭示特征差异与互补性**：系统分析生成式特征（时空感知+语义先验）与判别式特征（语义抽象但丢失细节）的差异及协同潜力。  <br/>3. **提出融合策略**：设计有效的生成-判别特征融合方法，兼顾细粒度感知与高层语义理解。  <br/>4. **多任务验证有效性**：在声景分类、标签生成及细粒度音频字幕等任务中实现性能提升，证明方法普适性。  <br/>5. **理论视角创新**：强调生成-判别互补性对音频表示学习的指导意义，推动领域方法论革新。|
|2509.24613v2|[HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition](http://arxiv.org/abs/2509.24613v2)|总结：  <br/>本文提出HiKE，首个全球可访问的韩英混合语言语音识别基准框架，通过高质量自然数据与层次化标注方案，系统评估多语言ASR模型的代码切换能力，并验证合成数据微调能有效提升其表现。<br/><br/>贡献点：  <br/>1. **首个韩英代码切换基准框架**：HiKE是首个公开的、全球可访问的韩英混合语言语音识别评估基准，填补了该领域的空白。  <br/>2. **多主题自然数据集**：包含高质量、多样化的韩英混合语料，覆盖日常交流场景，增强模型泛化能力。  <br/>3. **层次化标注方案**：引入词、短语、句子三级代码切换标注体系，并提供细致的外来词标签，支持多粒度性能评估。  <br/>4. **微调提升性能的实证**：通过实验表明，多语言ASR模型在代码切换任务上需依赖合成数据微调才能显著提升表现，推动未来研究方向。|
|2509.24613v1|[HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition](http://arxiv.org/abs/2509.24613v1)|总结（100字以内）:  <br/>HiKE是首个全球开放的韩英混合语言语音识别基准，提供高质量数据和分层标注方案，验证了通过代码切换数据微调可提升多语言ASR性能，推动相关研究发展。<br/><br/>贡献点：  <br/>1. **首个韩英混合语言基准**：构建HiKE框架，填补韩英代码切换语音识别领域缺乏统一评测标准的空白。  <br/>2. **高质量自然数据集**：涵盖多主题的双语混杂真实语料，支持多样化的模型评估场景。  <br/>3. **精细化标注体系**：提供词汇、短语、句子三级代码切换标签及借词标注，实现对不同切换层级的系统性分析。  <br/>4. **验证模型改进潜力**：通过多模型实验表明，代码切换数据微调可显著提升多语言ASR对CS的处理能力。  <br/>5. **开源促进研究**：公开数据与标注，为学术界和工业界提供可复现、可扩展的代码切换研究资源。|
|2509.24603v1|[Discovering "Words" in Music: Unsupervised Learning of Compositional   Sparse Code for Symbolic Music](http://arxiv.org/abs/2509.24603v1)|总结：  <br/>提出无监督算法识别音乐数据中的"music-words"模式，通过两阶段EM框架实现字典构建与数据重建，验证其在音乐结构分析中的有效性，为AI音乐和音乐学提供新工具。<br/><br/>贡献点：  <br/>1. **创新方法**：首次将音乐-词语发现建模为统计优化问题，提出基于EM算法的两阶段框架（字典构建+数据重建）。  <br/>2. **效果验证**：在人类标注数据上取得0.61 IoU得分，证明算法对音乐结构的捕捉能力。  <br/>3. **理论突破**：揭示通过最小化编码长度可缓解音乐语义模糊，提出人类编码优化塑造音乐语义的假设。  <br/>4. **应用拓展**：为AI音乐生成、分类、风格迁移等任务提供基础，同时推动音乐学对创作模式的分析研究。|
|2509.24570v1|[ISSE: An Instruction-Guided Speech Style Editing Dataset And Benchmark](http://arxiv.org/abs/2509.24570v1)|总结：  <br/>本文提出ISSE数据集，通过自然语言指令实现灵活的语音风格编辑，结合大语言模型生成高质量数据，训练指导模型，验证其在准确性和泛化能力上的优势。<br/><br/>贡献点：  <br/>1. **构建首个指令引导语音风格编辑数据集**：包含400小时语音与10万+高质量源-目标配对样本，每对均对应详细文本指令，覆盖多样的风格属性。  <br/>2. **提出系统化生成框架**：利用大语言模型、文本到语音与语音转换技术，实现无需参考音频的高质量数据生成，提升数据多样性与实用性。  <br/>3. **开发指令引导自回归模型**：在ISSE数据集上训练，有效实现指令遵循、音色保持与内容一致性，验证数据集在风格编辑任务中的优越性。|
|2509.24550v1|[Training-Free Multimodal Guidance for Video to Audio Generation](http://arxiv.org/abs/2509.24550v1)|**贡献点总结：**  <br/>1. 提出训练无关的多模态引导机制（MDG），无需联合训练即可实现视频与音频的语义对齐。  <br/>2. 利用模态嵌入的体积空间约束，增强视频、音频、文本的全局多模态一致性。  <br/>3. 实现轻量级、即插即用的控制信号，兼容任意预训练音频扩散模型。  <br/>4. 在VGGSound与AudioCaps数据集上验证有效性，显著提升感知质量和跨模态对齐性能。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出一种无需训练的多模态引导机制，通过模态嵌入体积约束实现视频-音频生成的统一对齐，有效提升感知质量与跨模态一致性，且兼容任意预训练音频模型，适用于视频编辑与辅助多媒体场景。|
|2509.24482v1|[Beyond Genre: Diagnosing Bias in Music Embeddings Using Concept   Activation Vectors](http://arxiv.org/abs/2509.24482v1)|总结：该研究首次揭示音乐模型中非音乐属性的潜在偏见，提出基于CAVs的去偏方法，强调偏见意识设计的重要性。<br/><br/>贡献点：<br/>1. 首次系统分析音乐表示模型中的文化偏见，关注非音乐属性（如性别、语言）对流派编码的潜在影响；<br/>2. 采用STraDa数据集并控制训练集平衡，确保实验设计严谨性；<br/>3. 发现模型间显著的偏见差异，揭示MIR领域与音乐社会学的现存不平等现象；<br/>4. 提出概念向量操控的后处理去偏策略，验证其在缓解表示偏见中的有效性；<br/>5. 证明概念化可解释性方法可作为音乐信息检索中诊断和修正偏见的实用工具。|
|2509.24463v2|[An Agent-Based Framework for Automated Higher-Voice Harmony Generation](http://arxiv.org/abs/2509.24463v2)|总结：  <br/>提出一种基于多智能体协作的AI和声生成系统，融合Transformer、GPT、RNN和GAN技术，实现高保真音乐创作与音频合成。<br/><br/>贡献点：  <br/>1. **多智能体协作架构**：设计包含音乐解析、和弦理解、和声生成、音频合成四大模块的系统，模拟人类协作流程。  <br/>2. **专用Agent分工**：Music-Ingestion Agent标准化输入，Chord-Knowledge Agent解析复杂和弦符号，Harmony-Generation Agent结合旋律与节奏生成互补和声。  <br/>3. **模型创新融合**：引入Chord-Former（Transformer）处理和弦，Harmony-GPT与Rhythm-Net（RNN）协同生成，GAN实现高保真音频合成。  <br/>4. **模块化处理能力**：支持高效数据处理、理论理解、创造性作曲及真实音频生成，实现上下文相关的高声部和声生成。|
|2509.24463v1|[An Agent-Based Framework for Automated Higher-Voice Harmony Generation](http://arxiv.org/abs/2509.24463v1)|总结：  <br/>本文提出了一种多代理AI系统，通过模块化协作生成高质量音乐和声，结合Transformer、RNN和GAN等技术，模拟人类创作过程，实现复杂旋律的语境适配和高保真音频合成。<br/><br/>贡献点：  <br/>1. **创新性框架**：首次提出Agentic AI驱动的Higher Harmony Music Generator，通过多代理协作模拟人类音乐创作流程。  <br/>2. **模块化设计**：构建四代理系统（音乐摄入、和弦知识、和声生成、音频生产），实现任务分工与高效协同。  <br/>3. **复杂和弦解析**：引入Chord-Former（Transformer模型）精准解析和弦符号并生成构成音，提升理论深度与准确性。  <br/>4. **多模态技术融合**：结合Harmony-GPT（语言模型）与Rhythm-Net（RNN）生成旋律与节奏互补的和声线，再通过GAN实现符号到音频的高保真合成。  <br/>5. **上下文适配**：系统可生成语境恰当的高声和声，满足音乐创作中复杂声部配合的需求。|
|2509.24457v1|[Assessing speech quality metrics for evaluation of neural audio codecs   under clean speech conditions](http://arxiv.org/abs/2509.24457v1)|**贡献点：**  <br/>1. **系统性评估**：全面比较了45种客观语音质量评估指标在17种神经编解码器条件下的表现，填补了该领域的研究空白。  <br/>2. **指标有效性验证**：发现基于神经网络的指标（如scoreq和utmos）在主观评分相关性上显著优于传统方法。  <br/>3. **局限性揭示**：指出非侵入性指标在高主观质量水平下存在饱和现象，为实际应用提供关键参考。  <br/><br/>**总结：**  <br/>研究系统评估了45种语音质量指标，发现神经网络基指标表现最佳，并揭示了非侵入性指标在高评分时的局限性。|
|2509.24404v1|[From Sound to Setting: AI-Based Equalizer Parameter Prediction for Piano   Tone Replication](http://arxiv.org/abs/2509.24404v1)|总结：本文提出AI系统直接预测音频EQ参数，实现可解释的音调匹配，通过系统化钢琴数据集验证模型性能，并为复杂音频效果扩展奠定基础。<br/><br/>贡献点：<br/>1. 提出首个基于AI的音频特征直接预测EQ参数的系统，突破传统音频-音频转换模式。<br/>2. 输出可解释的参数值（如EQ频段增益），支持音乐人后续调整优化。<br/>3. 构建包含系统化EQ变化的钢琴录音数据集，为研究提供高质量基准。<br/>4. 神经网络模型在多频段任务中达到0.0216的均方误差，展现优异预测精度。<br/>5. 实现音乐制作中的自动化音调匹配，提升效率并拓展至更复杂音频效果应用。|
|2509.24391v1|[UniFlow-Audio: Unified Flow Matching for Audio Generation from   Omni-Modalities](http://arxiv.org/abs/2509.24391v1)|**总结（100字以内）**:  <br/>提出UniFlow-Audio统一框架，结合流匹配与双融合机制，支持多模态生成，在7类音频任务中以少参数实现高性能，为音频生成提供非自回归基础模型范式。  <br/><br/>**贡献点分点列出**:<br/>1. **统一框架设计**：首次构建兼顾时间对齐（TA）与非时间对齐（NTA）任务的通用音频生成框架，突破传统任务分离范式。  <br/>2. **双融合机制**：创新性地将时间对齐特征通过时序对齐融合至音频潜变量，非时间对齐特征则通过交叉注意机制整合。  <br/>3. **任务平衡采样**：提出数据采样策略以均衡TA与NTA任务的训练效果，提升模型跨任务泛化性。  <br/>4. **高参数效率**：在少于8K小时训练数据和1B参数下实现多任务性能，小版本（~200M参数）仍具竞争力。  <br/>5. **多模态支持**：框架兼容文本、音频、视频等多模态输入，扩展了音频生成的适用场景。  <br/>6. **非自回归范式**：基于流匹配的非自回归方法，避免传统自回归模型的序列生成限制。|
|2509.24286v1|[SynthCloner: Synthesizer Preset Conversion via Factorized Codec with   ADSR Envelope Control](http://arxiv.org/abs/2509.24286v1)|总结：提出SynthCloner模型与SynthCAT数据集，实现电子合成器音色属性分解与独立控制，提升预设转换效果与数据多样性。<br/><br/>贡献点：<br/>1. 提出SynthCloner模型：通过因子化编码实现音频信号解耦为ADSR包络、音色和内容三个属性，支持独立控制的预设转换。<br/>2. 构建SynthCAT数据集：包含250种音色、120种ADSR包络及100条MIDI序列，采用任务特定渲染流程提升数据多样性与实用性。<br/>3. 验证模型有效性：在客观指标（如MSE）和主观评估中均优于现有方法，证明属性解耦对合成器预设转换的增强效果。<br/>4. 开源实现：提供代码、模型权重和音频示例，推动语音合成与音色转换领域的研究与应用。|
|2509.24172v1|[Predictability and Statistical Memory in Classical Sonatas and Quartets](http://arxiv.org/abs/2509.24172v1)|**贡献点（分点列出）：**  <br/>1. **方法创新**：提出并应用了高阶马尔可夫链模型结合时间延迟互信息和混合转移分布分析，系统性研究音乐中的统计依赖关系。  <br/>2. **数据集构建**：建立包含605个MIDI文件的跨作曲家、跨作品形式（钢琴奏鸣曲与弦乐四重奏）数据集，涵盖莫扎特、海顿、贝多芬、舒伯特等经典作曲家。  <br/>3. **风格差异发现**：揭示莫扎特音乐的统计依赖显著不同于其他三位作曲家，高阶模型对贝多芬、海顿、舒伯特更优，而莫扎特即可用低阶模型。  <br/>4. **形式对比分析**：发现不同音乐形式（如弦乐四重奏）中存在作曲家间的细微差异，例如莫扎特与贝多芬在部分指标上的结果相似。  <br/>5. **理论延伸**：拓展了音乐统计依赖研究的框架，强调古典作曲家在音乐可预测性上的系统性差异，为跨风格、跨时代音乐分析提供方法论基础。  <br/><br/>**总结（100字以内）：**  <br/>通过高阶马尔可夫模型等方法，系统分析古典音乐的统计依赖差异，揭示莫扎特风格的独特性，并发现形式间及作曲家间的隐藏关联，为音乐结构研究提供新视角。|
|2509.24039v1|[End-to-end Topographic Auditory Models Replicate Signatures of Human   Auditory Cortex](http://arxiv.org/abs/2509.24039v1)|总结：  <br/>本文提出TopoAudio模型，通过引入布线约束损失，首次实现端到端生物基础的听觉模型，使其具备与人类听觉皮层一致的拓扑结构并保持高任务准确性。<br/><br/>贡献点：  <br/>1. 指出现有听觉感知模型未评估听觉皮层的拓扑结构，提出亟需该方向的研究。  <br/>2. 首次发现前最佳模型在预测人类听觉fMRI响应时缺乏拓扑组织特性。  <br/>3. 提出TopoAudio模型，将视觉领域布线约束损失迁移至听觉感知，强制模型生成拓扑结构。  <br/>4. 确保TopoAudio在保持拓扑特性的同时，其基准任务准确率与非拓扑模型相当。  <br/>5. 显示TopoAudio能预测fMRI响应，同时形成平滑的频率拓扑图和模块化的语音/音乐响应区。  <br/>6. 首次构建端到端的生物基础听觉模型，验证布线长度约束作为通用正则化工具的潜力。|
|2509.23878v1|[Disentangling Score Content and Performance Style for Joint Piano   Rendering and Transcription](http://arxiv.org/abs/2509.23878v1)|总结：  <br/>本文提出一种联合EPR与APT的统一框架，通过分离音符内容与表现风格，利用Transformer序列到序列架构及扩散模型实现风格迁移与灵活渲染，实验验证其在两项任务中的竞争力。<br/><br/>贡献点：  <br/>1. 首次构建统一框架，同时建模EPR和APT，解决传统独立处理的局限性；  <br/>2. 提出无需细粒度音符对齐的训练方法，仅依赖序列对齐数据；  <br/>3. 引入扩散模型生成风格嵌入，实现从得分内容到风格的直接转化；  <br/>4. 支持多种表达风格的灵活渲染与风格迁移，提升生成性能的多样性；  <br/>5. 实验证明框架在内容-风格分离、风格迁移可靠性及渲染适配性方面表现优异。|
|2509.23833v1|[AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset   with Speech Recognition Baselines](http://arxiv.org/abs/2509.23833v1)|总结：  <br/>本文提出首个大规模中文普通话语音识别数据集AISHELL6-Whisper，结合音频与同步视频数据，并构建基于Whisper-Flamingo框架的AVSR基线模型，实现了在静默场景下的语音识别性能提升。<br/><br/>贡献点：  <br/>1. **首个大规模中文普通话语音数据集**：提供30小时同步音频与正面面部视频数据，填补了领域空白。  <br/>2. **跨模态对齐技术**：设计并行训练策略，实现语音类型（静默与正常）的嵌入对齐。  <br/>3. **频谱适应性改进**：引入投影层优化模型对静默语音的频谱特性适应能力。  <br/>4. **SOTA性能验证**：在wTIMIT基准测试中取得新最优结果（4.13% CER for whisper speech）。  <br/>5. **开源共享**：公开数据集和模型代码，促进研究复现与应用。|
|2509.23727v1|[AudioMoG: Guiding Audio Generation with Mixture-of-Guidance](http://arxiv.org/abs/2509.23727v1)|总结（100字以内）:  <br/>提出混合指导框架AudioMoG，结合CFG与AG优势，提升跨模态音频生成质量与效率，验证在T2A、V2A等任务中的有效性。<br/><br/>贡献点:<br/>1. **提出混合指导框架AudioMoG**：首次将多种指导原则（如CFG的条件对齐与AG的得分准确）结合，解决单一指导原则在保真度与多样性间的权衡问题。<br/>2. **灵活设计**：支持按需组合不同指导策略，可平行增强或恢复单一原则，保持方法通用性与可扩展性。<br/>3. **跨任务验证**：在文本到音频、视频到音频、文本到音乐、图像生成等多场景实验中证明效果提升，展示框架泛化能力。<br/>4. **效率与质量兼得**：在保持推理速度的前提下，通过混合指导实现更高质量的生成，揭示跨模态音频系统存在"免费午餐"现象。<br/>5. **开源示范**：提供公开demo资源（https://audio-mog.github.io），便于研究者复现与对比实验。|
|2509.23238v2|[WavJEPA: Semantic learning unlocks robust audio foundation models for   raw waveforms](http://arxiv.org/abs/2509.23238v2)|**贡献点分点总结：**  <br/>1. 提出WavJEPA，作为基于波形的Joint-Embedding Predictive Architecture，通过高阶语义表示解决语音单元/令牌级别的局限性。  <br/>2. WavJEPA在多种下游任务中显著优于现有时域音频基础模型，且计算资源需求更低。  <br/>3. 设计WavJEPA-Nat，通过多通道结构和自然场景模拟训练，提升模型对噪声与混响的鲁棒性。  <br/>4. 验证原始波形音频表示学习在通用场景中的可行性与计算效率，推动低延迟、鲁棒的时域模型在实际应用中的潜力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出WavJEPA及WavJEPA-Nat，通过高阶语义学习和自然场景训练，实现高效、低延迟的通用音频表示，显著提升噪声与混响环境下的鲁棒性，为实际应用提供新的解决方案。|
|2509.22728v2|[Prompt-aware classifier free guidance for diffusion models](http://arxiv.org/abs/2509.22728v2)|总结：  <br/>本研究提出一种prompt-aware的动态指导尺度选择框架，通过合成数据集和轻量级预测器优化语音生成质量，实现无训练的预训练扩散模型性能提升。<br/><br/>贡献点：  <br/>1. **提出prompt-aware框架**：首次将提示内容与指导尺度选择关联，通过动态预测不同尺度下的生成质量来优化音频生成效果。  <br/>2. **构建大规模合成数据集**：生成多尺度样本并结合可靠评估指标进行评分，为尺度选择提供数据基础。  <br/>3. **设计轻量级预测器**：基于语义嵌入和语言复杂度，估计多维度质量曲线，并通过效用函数和正则化机制确定最佳尺度。  <br/>4. **验证有效性**：在MSCOCO 2014和AudioCaps数据集上实验证明，相较于传统CFG方法，显著提升生成保真度、对齐度及感知偏好。  <br/>5. **提供训练无关增强方案**：无需额外训练即可直接提升预训练扩散模型的性能，具有广泛的适用性。|
|2509.22461v1|[MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark](http://arxiv.org/abs/2509.22461v1)|**贡献点**：  <br/>1. 提出MDAR基准，填补现有音频任务中多场景、动态交互和多源音频的评估空白。  <br/>2. 构建包含3,000个问答对的数据集，覆盖五类复杂推理任务和三种问题类型（单选、多选、开放）。  <br/>3. 系统评估26个音频语言模型，揭示其在复杂推理任务中的整体性能局限性。  <br/>4. 提供具体模型对比结果，显示GPT-4o Audio在多选和开放任务上显著优于Qwen2.5-Omni。  <br/>5. 开源数据与代码，推动音频推理研究的可复现性和进一步发展。  <br/><br/>**总结**：本文提出MDAR基准，涵盖复杂多场景音频推理任务，评估26个模型并揭示其性能局限，为推动语音AI研究提供重要参考。（99字）|
|2509.22317v1|[Cross-Dialect Bird Species Recognition with Dialect-Calibrated   Augmentation](http://arxiv.org/abs/2509.22317v1)|总结：  <br/>本文提出基于TDNN的方言鲁棒性鸟类识别框架，结合频率归一化与对抗训练，通过多级数据增强和DCA技术显著提升跨方言准确率，同时保持区域性能，并实现可解释性分析。<br/><br/>贡献点：  <br/>1. **提出方言鲁棒性框架**：基于TDNN构建可部署的模型，解决方言变化对鸟类鸣叫识别的干扰问题。  <br/>2. **创新归一化与对抗学习方法**：结合频率敏感归一化（Instance Frequency Normalisation和gated Relaxed-IFN）与梯度反转对抗训练，学习区域不变嵌入。  <br/>3. **多级数据增强策略**：融合波形扰动、Mixup增强稀有类、CycleGAN跨区域音频合成，并通过DCA技术软性降权合成样本以减少伪影。  <br/>4. **验证跨方言性能提升**：系统在跨方言识别准确率提升至基线TDNN模型的20个百分点，同时保持区域内识别性能。  <br/>5. **可解释性分析**：利用Grad-CAM和LIME验证模型聚焦于稳定谐波频段，为生态学研究提供有意义解释。  <br/>6. **轻量透明模型实现**：证明低资源、高可解释性且方言鲁棒的鸟类声识别系统具有可行性。|
|2509.22167v1|[Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech   Synthesis](http://arxiv.org/abs/2509.22167v1)|总结：本文提出Semantic-VAE框架，通过语义对齐正则化解决语音生成中重建与生成的权衡问题，显著提升合成质量与训练效率，并开源代码促进研究。<br/><br/>贡献点：  <br/>1. 提出Semantic-VAE方法，基于语义对齐正则化优化潜在空间，解决高/低维latent表示的重建-生成矛盾。  <br/>2. 通过在潜在空间中捕捉语义结构，缓解传统VAE在重建质量和可懂度之间的权衡问题。  <br/>3. 在LibriSpeech-PC数据集上验证效果，集成F5-TTS后实现更低WER（2.10%）和更高说话人相似度（0.64）。  <br/>4. 对比实验表明优于mel-spectrogram基线（WER 2.23%，相似度0.60）和纯声学VAE基线（WER 2.65%，相似度0.59）。  <br/>5. 开源代码与模型，推动语音合成领域进一步研究。|
|2509.22153v1|[Towards Cross-Task Suicide Risk Detection via Speech LLM](http://arxiv.org/abs/2509.22153v1)|**贡献点：**<br/>1. **提出跨任务整合框架**：首次将多种青少年自杀风险评估任务统一于单一模型，突破传统单任务方法的局限性。  <br/>2. **创新混合专家机制**：结合语音大语言模型与混合DoRA专家（MoDE），动态捕捉多任务间的互补线索。  <br/>3. **大规模实验验证**：基于1,223名参与者、10项自发语音任务，验证模型的泛化能力和实际效果。  <br/>4. **性能优势**：在检测准确率和置信度校准上显著优于单任务模型及传统联合调优方法，适用于医疗场景。  <br/><br/>**总结（100字以内）**：  <br/>本研究首次提出跨任务整合框架，结合语音大语言模型与Mix-DoRA专家方法，通过大规模实验验证，显著提升青少年自杀风险检测的准确率与置信度，为医疗应用提供更可靠的解决方案。|
|2509.22148v1|[Speaker Anonymisation for Speech-based Suicide Risk Detection](http://arxiv.org/abs/2509.22148v1)|总结：  <br/>本研究首次系统分析语音匿名化在青少年自杀风险检测中的应用，提出兼顾隐私保护与检测性能的综合方法，构建了评估框架并验证了多技术融合的有效性。<br/><br/>贡献点：  <br/>1. **首次系统研究**：提出首个针对语音数据自杀风险检测的系统性匿名化研究框架。  <br/>2. **多方法对比**：综合评估传统信号处理、神经语音转换与语音合成等匿名化技术。  <br/>3. **评估框架创新**：设计全面评估体系，量化隐私保护与风险检测信息的权衡。  <br/>4. **性能与隐私平衡**：通过互补方法组合实现检测性能与身份保护的兼顾，适用于脆弱人群。|
|2509.22063v1|[High-Quality Sound Separation Across Diverse Categories via   Visually-Guided Generative Modeling](http://arxiv.org/abs/2509.22063v1)|总结： <br/>DAVIS提出基于扩散模型的生成框架，结合U-Net架构和Flow Matching等技术，实现高质量音频-视觉声源分离，在标准数据集上超越现有方法。<br/><br/>贡献点： <br/>1. 提出首个将扩散模型应用于音频-视觉声源分离的生成框架DAVIS，突破传统mask回归方法的局限。  <br/>2. 设计专用的Separation U-Net结构，融合Denoising Diffusion Probabilistic Models（DDPM）和Flow Matching（FM）技术。  <br/>3. 通过条件生成策略，同步利用混合音频与视觉信息合成声谱，实现对多类别声音的高质量分离，并在AVE和MUSIC数据集上验证效果。|
|2509.22061v1|[Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based   Model Bias](http://arxiv.org/abs/2509.22061v1)|总结（100字以内）:  <br/>该研究首次系统评估语音延续任务中的偏见，揭示性别和发声类型对生成结果的影响，并发现模型与性别在文本指标上的显著交互作用，强调SC作为探测社会偏见的受控工具的潜力。<br/><br/>贡献点:  <br/>1. **首次系统性研究**：提出首个针对语音延续（SC）任务中偏见的系统性评估框架，填补了语音基础模型偏见研究的空白。  <br/>2. **多维度指标分析**：综合考察说话者相似度、声音质量保持（模态发声倾向）与文本偏见（如语句极性），提供全面评估视角。  <br/>3. **性别与模型交互影响**：发现模型性能与性别在文本指标上存在显著交互作用，尤其当连贯性足够高时，性别差异在文本生成中更明显。  <br/>4. **揭示语音质量偏见**：证明女性语音提示的延续行为更偏向模态发声（气声、喉声等），暴露语音模型中的系统性声音质量偏见。  <br/>5. **方法论与应用价值**：证明SC任务可作为研究社会相关表征偏见的有效受控实验环境，为模型改进提供诊断依据。|
|2509.22060v1|[Decoding Deception: Understanding Automatic Speech Recognition   Vulnerabilities in Evasion and Poisoning Attacks](http://arxiv.org/abs/2509.22060v1)|**贡献点分点总结：**  <br/>1. 提出低成本高效的白盒攻击方法，优化攻击效率与实用性。  <br/>2. 设计非迁移性黑盒对抗攻击策略，突破传统迁移性攻击的局限。  <br/>3. 首次揭示数据污染攻击对状态--of-the-art ASR 模型的破坏性影响。  <br/>4. 实验验证混合模型可生成微小扰动（SNR 35dB）且高影响力的对抗样本，仅需1分钟。  <br/>5. 强调开源 ASR 模型的安全漏洞，呼吁加强对抗安全防护研究。  <br/><br/>**摘要总结（100字以内）：**  <br/>论文提出高效白盒攻击与非迁移黑盒对抗攻击方法，揭示数据污染对 ASR 模型的威胁，展示低扰动对抗样本生成效果，强调开源模型的安全风险，推动对抗安全技术发展。|
|2509.21968v1|[AUV: Teaching Audio Universal Vector Quantization with Single Nested   Codebook](http://arxiv.org/abs/2509.21968v1)|总结：  <br/>提出AUV，一种基于单码本的统一神经音频编解码器，实现语音与通用音频的高质量重建，支持16kHz混合域音频在700bps码率下的高效编码。<br/><br/>贡献点：  <br/>1. **统一编解码框架**：设计AUV，首次将语音与通用音频（如人声、音乐、环境音）统一处理，通过单个码本降低复杂度。  <br/>2. **嵌套码本结构**：采用分层嵌套的领域特定子码本（matryoshka codebook），结合教师模型知识蒸馏，实现单阶段训练。  <br/>3. **高效的量化能力**：在16kHz混合域音频上，以约700bps的低码率实现与领域专用单层量化器相当的重建质量。  <br/>4. **改进的编码器-解码器**：引入Conformer架构与STFT特征作为音频表示，显著提升音质表现。  <br/>5. **开源与可复现性**：提供预训练模型和演示样例，方便研究和应用。|
|2509.21964v1|[A Parallel Ultra-Low Power Silent Speech Interface based on a Wearable,   Fully-dry EMG Neckband](http://arxiv.org/abs/2509.21964v1)|总结：  <br/>提出了一种可穿戴、超低功耗且适合日常使用的EMG系统，实现在无声和有声条件下的高效语音识别，验证了其鲁棒性与节能潜力。<br/><br/>贡献点：  <br/>1. **创新设计**：开发了可穿戴、全干（无需湿润电极）的EMG系统，整合于纺织品颈带，实现舒适、非侵入式使用。  <br/>2. **低功耗技术**：基于BioGAP-Ultra平台，系统功耗仅为22mW，具备高效的无线传输能力。  <br/>3. **多通道采集**：配置14个全差分EMG通道，提升信号质量与多通道处理能力。  <br/>4. **实际场景验证**：通过会话间位置变化模拟真实使用条件，评估系统在不同位置下的鲁棒性。  <br/>5. **性能表现**：在语音命令识别任务中，分别达到87±3%（有声）与68±3%（无声）的分类准确率，证明了无声语音解码的可行性。|
|2509.21919v1|[Text2Move: Text-to-moving sound generation via trajectory prediction and   temporal alignment](http://arxiv.org/abs/2509.21919v1)|总结：  <br/>提出可控文本生成移动空间音频框架，构建双耳格式合成数据集，开发文本-轨迹预测模型，结合预训练音频生成模型实现空间化，并验证方法的可扩展性与实用性。<br/><br/>贡献点：  <br/>1. 构建首个包含移动声源三维轨迹、双耳空间音频及文本描述的合成数据集，填补生成模型在动态空间音频领域的数据空白。  <br/>2. 提出文本到三维轨迹预测模型，实现从自然语言描述自动生成移动声源的空间运动路径。  <br/>3. 整合预训练文本-音频生成模型，通过轨迹对齐生成单声道音频并模拟空间音频，形成端到端生成流程。  <br/>4. 验证方法在空间音频理解上的有效性，并证明其可扩展性，支持接入其他空间音频格式及现有生成工作流。|
|2509.21900v1|[IPDnet2: an efficient and improved inter-channel phase difference   estimation network for sound source localization](http://arxiv.org/abs/2509.21900v1)|贡献点总结（100字以内）:  <br/>提出IPDnet2架构，结合oSpatialNet提升空间特征提取和可扩展性，引入频率-时间池化机制降低计算成本，实验验证其在保持性能的同时显著优于IPDnet，达到SOTA水准。  <br/><br/>分点贡献：  <br/>1. **改进模型架构**：采用oSpatialNet作为骨干网络，增强空间线索提取能力并提升模型可扩展性。  <br/>2. **优化频率-时间处理**：提出简单有效的频率-时间池化机制，压缩频域与时域分辨率以降低计算成本，同时维持定位性能。  <br/>3. **高效与性能平衡**：在扩大模型规模的同时保持较低复杂度，实现与IPDnet相当的定位精度，但计算成本仅为IPDnet的2%以下。  <br/>4. **突破性SSL性能**：通过架构优化，IPDnet2达到语音领域声源定位（SSL）的当前最优水平（SOTA）。|
|2509.21749v1|[Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning   in Large Audio-Language Models](http://arxiv.org/abs/2509.21749v1)|总结：  <br/>本文提出TwS框架，通过Audio CoT结合语言推理与实时音频分析，显著提升LALMs在复杂声学场景下的鲁棒性，并构建MELD-Hard1k基准验证效果。<br/><br/>贡献点：  <br/>1. 提出**Thinking-with-Sound (TwS)** 框架，首次将语言推理与音频时域分析结合，实现无需重新训练的音频推理鲁棒性提升。  <br/>2. 引入**Audio CoT**（音频链式推理）机制，支持模型对音频信号进行动态数值分析和数字操作。  <br/>3. 构建**MELD-Hard1k** 新基准，系统评估模型在噪声、混响等复杂声学条件下的表现。  <br/>4. 验证TwS在不同规模模型上的有效性，小模型提升24.73%，大模型提升36.61%。  <br/>5. 展示音频增强工具（如噪声抑制、源分离）与语言模型结合的潜力，为鲁棒音频理解系统提供新方向。|
|2509.21739v1|[Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic   Drum Transcription](http://arxiv.org/abs/2509.21739v1)|**Contributions:**  <br/>1. **Reformulate ADT as a conditional generative task** and introduce the Noise-to-Notes (N2N) framework, which uses diffusion modeling to generate drum events (with velocities) from audio-conditioned Gaussian noise.  <br/>2. **Propose the Annealed Pseudo-Huber loss** to address challenges in jointly optimizing binary onset and continuous velocity generation.  <br/>3. **Integrate features from music foundation models (MFMs)** to augment low-level spectrogram features, leveraging high-level semantic information for better robustness.  <br/>4. **Demonstrate state-of-the-art performance** on multiple ADT benchmarks through experiments, highlighting the effectiveness of the N2N framework and MFM integration.  <br/><br/>**Summary (Chinese, within 100 characters):**  <br/>该工作重新定义鼓转录为生成任务，提出N2N框架结合扩散模型和MFMs特征，设计新颖损失函数提升鲁棒性，实验验证在多个基准上达到SOTA。|
|2509.21728v1|[Frustratingly Easy Zero-Day Audio DeepFake Detection via Retrieval   Augmentation and Profile Matching](http://arxiv.org/abs/2509.21728v1)|总结：  <br/>提出无需训练的零日语音深度伪造检测框架，通过知识检索与集成方法实现与微调模型相当的检测性能，并验证关键参数对系统效果的影响。<br/><br/>贡献点：  <br/>1. **训练框架创新**：首次提出基于知识表示、检索增强和语音特征匹配的零日攻击检测框架，无需额外模型训练。  <br/>2. **高效检测方法**：设计简单有效的知识检索与集成策略，在DeepFake-Eval-2024数据集上达到微调模型性能水平。  <br/>3. **参数优化验证**：通过消融实验分析检索池规模和语音特征属性对系统有效性的影响，为模型优化提供实证依据。  <br/>4. **应对快速攻击场景**：解决传统微调方法在实时响应场景下的局限性，适用于需灵活应对新攻击方式的部署需求。|
|2509.21718v1|[Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided   Online Preference Optimization](http://arxiv.org/abs/2509.21718v1)|**贡献点：**  <br/>1. **提出基于GRPO的TTS框架**：首次将Group Relative Policy Optimization应用于低资源语言的TTS系统，通过预训练模型和未对齐数据实现跨语言适应。  <br/>2. **三阶段优化策略**：  <br/>   - 用IPA符号构建语言无关的多语言基线；  <br/>   - 有限对齐数据微调以捕捉目标语言韵律特征；  <br/>   - 仅依赖未配对文本和说话人提示，结合多目标奖励机制优化。  <br/>3. **多目标奖励机制**：利用预训练ASR、说话人验证和音频质量评估模型，实现无监督或弱监督的TTS优化，降低对数据配对的依赖。  <br/>4. **跨语言有效性**：在低资源语言上显著提升语音可懂度和说话人一致性，优于传统微调方法。  <br/>5. **通用性扩展**：方法同样适用于高资源语言，超越DPO等离线对齐技术，在质量与一致性上表现更优。  <br/><br/>**总结（100字以内）**：  <br/>本研究提出基于GRPO的TTS框架，通过IPA基线、有限微调和多目标奖励机制，有效解决低资源语言语音合成难题，并提升高资源语言性能，超越传统方法。|
|2509.21714v1|[MusicWeaver: Coherent Long-Range and Editable Music Generation from a   Beat-Aligned Structural Plan](http://arxiv.org/abs/2509.21714v1)|总结（100字以内）：<br/>提出MusicWeaver模型，通过节拍对齐的结构计划实现长程结构建模与编辑，引入SCS和EFS指标，实验验证其优越性。<br/><br/>贡献点：<br/>1. **结构化生成框架**：首次将节拍对齐的结构计划作为可编辑的中间表示，解耦音乐生成的全局结构与局部纹理建模。<br/>2. **可编辑音乐生成**：设计规划器与扩散生成器协同架构，支持专业用户对音乐结构进行精确局部修改。<br/>3. **专用评估指标**：提出结构连贯性得分（SCS）与编辑保真度得分（EFS），分别量化长程结构一致性和编辑实现精度。<br/>4. **SOTA性能验证**：通过实验证明模型在音乐生成保真度与可控性上达到当前最优水平，生成音乐更贴近人类创作。|
|2509.21625v1|[Guiding Audio Editing with Audio Language Model](http://arxiv.org/abs/2509.21625v1)|**贡献点总结**:<br/>1. 提出SmartDJ框架，首次结合音频语言模型与潜在扩散模型实现多通道音频编辑  <br/>2. 引入声明式编辑范式，通过高阶指令自动生成原子编辑操作（如增删事件、空间定位）  <br/>3. 设计专用数据合成流水线，生成指令-操作-音频三元组用于训练和验证  <br/>4. 实现立体音频编辑的突破，显著提升感知质量、空间真实感和语义对齐效果  <br/>5. 开源演示平台（https://zitonglan.github.io/project/smartdj/smartdj.html）验证方法有效性|
|2509.21597v1|[AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit](http://arxiv.org/abs/2509.21597v1)|总结（100字以内）：  <br/>本文系统分析28个音频deepfake数据集，构建开源工具包AUDDT，自动化评估预训练检测器的泛化能力，揭示不同场景与篡改类型下的性能差异，并指出现有数据集在实际应用中的局限性。<br/><br/>贡献点：  <br/>1. **系统性数据集分析**：全面梳理28个现有音频deepfake数据集的组成与特性，为研究提供基础参考。  <br/>2. **开源评估工具开发**：提出AUDDT工具包，支持跨数据集自动化检测器性能评估，提升研究效率。  <br/>3. **泛化能力验证**：通过预训练检测器的in-domain与out-domain测试，量化其在多样场景下的表现差异。  <br/>4. **子群体划分研究**：明确音频篡改类型和场景的细分，为针对性检测方法改进提供方向。  <br/>5. **数据集局限性剖析**：分析现有数据集与实际部署需求的差距，推动更贴近现实场景的数据构建。|
|2509.21560v1|[Preserving Russek's "Summermood" Using Reality Check and a DeltaLab DL-4   Approximation](http://arxiv.org/abs/2509.21560v1)|总结：创建Pure Data补丁库，实现DL-4延迟单元功能，用于表演Antonio Russek的“Summermood”，通过对比乐谱与录音优化模拟效果，并确保跨平台兼容性与纯数据更新适应性。<br/><br/>贡献点：<br/>1. **DL-4延迟单元仿真**：开发Pure Data补丁准确还原DeltaLab DL-4的音频处理功能与音色特性。<br/>2. **乐谱参数优化**：通过比对原始乐谱与两段官方录音，调整补丁参数以更精准匹配现场表演效果。<br/>3. **实时表演系统集成**：将仿真模块嵌入基于Null Piece的实时性能框架，实现完整的音乐作品表演环境。<br/>4. **跨平台兼容性测试**：采用Reality Check框架进行回归测试，确保作品可在不同计算机环境下稳定运行。<br/>5. **长期维护机制**：建立持续更新的补丁库体系，适配Pure Data语言版本迭代与硬件环境变化。|
|2509.21544v1|[Real-time implementation of vibrato transfer as an audio effect](http://arxiv.org/abs/2509.21544v1)|总结：  <br/>该论文提出一种基于实时实例的颤音转移算法，结合高效基频估计和时域多相IIR滤波器模拟分析信号，并扩展振幅调制转移功能，为声音设计和实时合成提供新工具。<br/><br/>贡献点：  <br/>1. **实时颤音转移算法**：基于真实颤音示例，提出可通过延迟线实现的颤音模式迁移方法。  <br/>2. **高效基频估计**：集成计算高效的基频检测算法，优化实时处理性能。  <br/>3. **时域多相IIR滤波器**：使用时间域多相IIR滤波器近似分析信号，提升算法效率。  <br/>4. **振幅调制迁移**：首次将振幅调制迁移纳入颤音传输，突破传统延迟式效果的限制。  <br/>5. **开源实现支持**：详细描述算法的实时修改方案，并提供VST插件源码用于实际应用。  <br/>6. **多领域应用拓展**：为声音设计、声形融合及合成声音的实时颤音控制提供技术方案。|
|2509.21463v1|[Enhanced Generative Machine Listener](http://arxiv.org/abs/2509.21463v1)|总结（100字以内）:  <br/>GMLv2通过贝塔分布损失函数和扩展NAC数据集，显著提升主观音频质量预测性能，超越PEAQ/ViSQOL等传统指标，在多样化内容与编解码配置中实现可靠评估，为音频编码技术提供自动化的评估框架。<br/><br/>贡献点:  <br/>1. **创新损失函数**：首次引入基于贝塔分布的损失函数，更精准建模听众评分分布，提升预测一致性。  <br/>2. **数据扩展**：整合更多神经音频编码（NAC）主观数据集，增强模型泛化能力与应用广度。  <br/>3. **性能突破**：在多类测试集上验证，GMLv2在相关性与预测可靠性上均优于PEAQ、ViSQOL等主流指标。  <br/>4. **框架通用性**：构建可扩展、自动化的感知音频质量评估系统，加速现代音频编码技术的研发进程。|
|2509.21060v2|[Measuring Audio's Impact on Correctness: Audio-Contribution-Aware   Post-Training of Large Audio Language Models](http://arxiv.org/abs/2509.21060v2)|总结（100字以内）:<br/>该研究提出AudioMCQ数据集及音频贡献过滤方法，设计Weak-to-Strong和Mixed-to-Strong双阶段训练框架，通过数据分配优化显著提升LALM性能，在DCASE 2025及多个基准测试中取得SOTA结果。<br/><br/>贡献点分点:<br/>1. 构建AudioMCQ数据集：首个包含571k样本、双类型思维链注释的音频多选题数据集<br/>2. 揭示零音频贡献问题：发现LALMs存在仅依赖文本信息完成音频任务的现象<br/>3. 提出音频贡献过滤机制：将数据划分为弱/强音频贡献子集以优化训练策略<br/>4. 开发双阶段训练范式：创新设计SFT→RL的多阶段架构（Weak-to-Strong/Mixed-to-Strong）<br/>5. 实现SOTA性能表现：在DCASE 2025竞赛中获第一名，并在MMAU-test-mini等基准测试中刷新记录|
|2509.21060v1|[Measuring Audio's Impact on Correctness: Audio-Contribution-Aware   Post-Training of Large Audio Language Models](http://arxiv.org/abs/2509.21060v1)|总结（100字以内）:  <br/>该论文提出AudioMCQ数据集及Audio-Contribution Filtering方法，设计Weak-to-Strong和Mixed-to-Strong多阶段微调范式，在DCASE 2025挑战赛中取得最优成绩，并在多个基准测试中刷新SOTA表现。<br/><br/>贡献点:  <br/>1. **提出AudioMCQ数据集**：构建包含571k样本的音频多选题数据集，配备两种链式思维注释（Chain-of-Thought），为研究提供大规模高质量资源。  <br/>2. **解决零音频贡献问题**：首次识别并分析LALMs中"仅依赖文本信息"的现象，提出Audio-Contribution Filtering方法以区分音频贡献强弱的训练数据。  <br/>3. **设计多阶段训练范式**：提出Weak-to-Strong（SFT弱音频数据+RL强音频数据）和Mixed-to-Strong（SFT混合音频数据+RL强音频数据）两种分阶段微调策略，提升模型音频处理能力。  <br/>4. **实验验证有效性**：在DCASE 2025音频问答挑战赛中取得第一名，并在MMAU-test-mini、MMAU、MMAR、MMSU等基准测试中刷新性能记录（78.2%-75.6%）。|
|2509.21003v2|[TF-Restormer: Complex Spectral Prediction for Speech Restoration](http://arxiv.org/abs/2509.21003v2)|总结：  <br/>TF-Restormer提出了一种高效跨采样率语音恢复框架，融合时间频率双路径编码与轻量解码器，通过采样频率无关判别器和因果时间模块提升流式处理与鲁棒性，并引入缩放对数频谱损失优化模型性能，实现信号保真度与感知质量的平衡。  <br/><br/>贡献点：  <br/>1. **创新架构**：提出时间-频率双路径编码器与轻量解码器的编码器-解码器结构，针对性分析输入带宽并重建高频成分。  <br/>2. **无需重采样**：支持任意输入-输出采样率转换，避免冗余计算，实现高效处理。  <br/>3. **采样频率无关判别器**：设计共享的SFI STFT判别器，适应多样化采样率的对抗训练。  <br/>4. **流式处理能力**：引入因果时间模块，保障实时应用中的处理连续性。  <br/>5. **频谱归纳偏置**：通过注入频谱偏置提升模型在极端降质情况下的鲁棒性。  <br/>6. **优化损失函数**：提出缩放对数频谱损失，增强优化稳定性并突出频谱细节。  <br/>7. **统一模型表现**：作为跨采样率的单一模型，平衡信号保真度与感知质量，性能优于现有方法。|
|2509.21003v1|[TF-Restormer: Complex Spectral Prediction for Speech Restoration](http://arxiv.org/abs/2509.21003v1)|**贡献点总结**  <br/>TF-Restormer创新性地提出时频双路径编码器与轻量解码器结构，支持跨采样率语音恢复，无需冗余重采样。引入SFI STFT判别器实现对抗训练，结合因果时间模块与频谱归纳偏差提升实时性和鲁棒性，最后提出缩放对数谱损失优化严重失真场景下的训练稳定性。|
|2509.20969v1|[SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement](http://arxiv.org/abs/2509.20969v1)|总结：  <br/>本研究提出SingVERSE，首个真实场景歌唱语音增强基准数据集，揭示模型性能权衡，并验证领域内训练提升效果，为该领域提供关键资源与方向指引。<br/><br/>贡献点：  <br/>1. **构建首个真实场景基准数据集**：提出SingVERSE，覆盖多样声学环境，提供高质量配对清洁参考数据，解决该领域缺乏真实评估数据的瓶颈。  <br/>2. **揭示模型性能的关键权衡**：通过全面评估先进模型，发现感知质量与可懂度之间的系统性矛盾，为算法优化提供理论依据。  <br/>3. **验证领域内训练的有效性**：证明基于真实歌唱数据训练可显著提升增强效果，同时不影响语音能力，为实际应用提供可行路径。  <br/>4. **推动领域发展**：提供社区可用的基础基准与关键洞察，促进未来歌唱语音增强技术的探索与改进。|
|2509.20891v1|[AIBA: Attention-based Instrument Band Alignment for Text-to-Audio   Diffusion](http://arxiv.org/abs/2509.20891v1)|贡献点：  <br/>1. **方法创新**：提出AIBA，一种无需训练且轻量级的管道，用于量化文本到音频扩散模型在时间-频率（T-F）平面的注意力分布。  <br/>2. **无修改推理**：在推理阶段直接钩取交叉注意力，记录注意力权重，避免对模型参数进行任何修改。  <br/>3. **统一评估框架**：引入可解释的评估指标（T-F IoU/AP、频率轮廓相关性、指向游戏），实现注意力分布与真实音频能量的直接对比。  <br/>4. **实验验证**：在Slakh2100数据集上验证方法有效性，揭示乐器依赖的注意力趋势（如贝斯偏好低频带）并达到高精度与中等召回率的平衡。  <br/><br/>总结：  <br/>AIBA通过无训练、无修改的注意力提取方法，结合统一评估框架，揭示文本到音频模型的T-F注意力模式，为模型可解释性提供新工具。|
|2509.20799v1|[AuthGlass: Enhancing Voice Authentication on Smart Glasses via Air-Bone   Acoustic Features](http://arxiv.org/abs/2509.20799v1)|**贡献点总结：**  <br/>1. 提出AuthGlass方法，结合空气传导与骨传导语音特征提升认证准确性与活体检测能力；  <br/>2. 构建搭载14个空气传导麦克风和2个骨传导单元的智能眼镜原型，实现多模态特征采集；  <br/>3. 通过42名参与者实验验证，多模态特征融合显著增强系统对伪造攻击和环境噪声的抵抗力；  <br/>4. 演示AuthGlass在复杂场景下的实用性，证明其高准确率与可扩展性。  <br/><br/>**摘要总结（100字以内）：**  <br/>本文提出AuthGlass，通过融合空气与骨传导语音特征提升智能眼镜语音认证的鲁棒性和活体检测能力，构建多麦克风原型并验证其在抗攻击和环境噪声下的有效性，证明方法适用于实际场景部署。|
|2509.20682v1|[Addressing Gradient Misalignment in Data-Augmented Training for Robust   Speech Deepfake Detection](http://arxiv.org/abs/2509.20682v1)|**贡献点：**  <br/>1. 提出双路径数据增强（DPDA）训练框架，通过对比原始与增强输入的梯度方向，解决深度伪造检测（SDD）中因数据增强导致的梯度不一致问题。  <br/>2. 分析RawBoost数据增强方法中大约25%的训练迭代存在梯度冲突，验证了该问题的普遍性与严重性。  <br/>3. 引入梯度对齐机制，有效减少优化冲突，加速模型收敛并降低训练周期需求。  <br/>4. 在In-the-Wild数据集上实验证明，相比基线方法，方法使Equal Error Rate（EER）相对降低18.69%，显著提升检测性能。  <br/><br/>**总结（100字以内）：**  <br/>本文设计双路径数据增强训练框架，通过梯度对齐解决SDD中的优化冲突问题，验证了RawBoost增强的梯度冲突现象，并在真实数据集上实现18.69%的EER降低，显著提升了模型泛化能力与检测效果。|
|2509.20679v1|[QAMO: Quality-aware Multi-centroid One-class Learning For Speech   Deepfake Detection](http://arxiv.org/abs/2509.20679v1)|**贡献点：**<br/>1. **提出QAMO模型**：首次将语音质量评估融入单类学习框架，构建多中心点（Multi-Centroid）质量感知模型，突破传统单中心点的局限。<br/>2. **增强真实语音建模**：通过多质量子空间建模，更好捕捉真实语音的内在变化（如语音质量差异），提升对未见过攻击的检测能力。<br/>3. **无需质量标签的集成策略**：引入多中心点联合评分机制，优化决策阈值，降低推理阶段对质量标注的依赖。<br/>4. **性能验证**：在In-the-Wild真实场景数据集上实现5.09%的等错误率，超越现有单类和质量感知检测方法。<br/><br/>**总结（100字以内）**：  <br/>提出QAMO模型，结合多中心点与语音质量感知，有效建模真实语音的多样性，通过集成评分策略降低对质量标注的依赖，在真实数据集上实现更优检测性能。|
|2509.20655v1|[Building Tailored Speech Recognizers for Japanese Speaking Assessment](http://arxiv.org/abs/2509.20655v1)|**贡献点：**  <br/>1. **提出针对日语评估任务的定制化语音识别器**：输出包含音素标签和重音标记的识别结果，适配日本语专项评估需求。  <br/>2. **解决标注数据稀疏问题**：引入多任务训练框架，通过辅助损失函数联合学习拼写文本和音高模式，利用仅有拼写注释的数据训练模型。  <br/>3. **设计融合算法**：结合音素字符串和文本标记序列的两个估计器，基于有限状态转录框架实现更精准的预测。  <br/>4. **验证方法有效性**：实验表明，该方法在CSJ核心语料库中将音素错误率从12.3%降至7.1%，优于通用多语言模型。  <br/><br/>**总结（100字以内）**：  <br/>本文提出两种方法缓解日语音素标注数据稀疏问题，结合多任务学习与融合估计器，开发基于有限状态转录框架的算法，显著降低错误率，优于通用多语言模型。|
|2509.20485v1|[Objective Evaluation of Prosody and Intelligibility in Speech Synthesis   via Conditional Prediction of Discrete Tokens](http://arxiv.org/abs/2509.20485v1)|**贡献点总结：**  <br/>1. 提出TTScore框架，解决现有语音合成评估中参考依赖和维度单一的问题。  <br/>2. 分为TTScore-int（可懂度）和TTScore-pro（韵律）两个子模型，分别通过内容词和韵律词进行条件预测。  <br/>3. 采用参考无关的序列到序列模型，生成可解释的评分以评估与目标语言内容及韵律的对齐程度。  <br/>4. 在SOMOS、VoiceMOS和TTSArena等基准上验证，显著提升与人类质量判断的相关性。  <br/><br/>**总结（100字以内）：**  <br/>提出参考无关的TTScore框架，分可懂度与韵律两方面评估语音合成质量，通过条件预测离散语音token实现更精准、与人类感知强关联的评价。|
|2509.20103v1|[Enabling Multi-Species Bird Classification on Low-Power Bioacoustic   Loggers](http://arxiv.org/abs/2509.20103v1)|贡献点：<br/>1. 提出WrenNet模型，专为低功耗微控制器设计，支持实时多物种鸟类音频分类<br/>2. 开发半可学习频谱特征提取器，适应鸟类鸣叫特性，性能优于传统mel-scale和全可学习方法<br/>3. 在70物种专家数据集上实现90.8%（可区分物种）和70.1%（全任务）分类准确率<br/>4. 部署在AudioMoth设备（≤1MB RAM）时，单次推理能耗仅77mJ<br/>5. 相比Birdnet在Raspberry Pi 3B+的能效提升达16倍<br/>6. 首创连续多物种声学监测框架，实现低功耗边缘设备部署<br/><br/>总结：本文提出WrenNet，通过半可学习特征提取器实现高效鸟类音频分类，在低功耗设备上达到70.1%准确率，单次推理能耗77mJ，能效较Birdnet提升16倍，构建首个实用的连续生物多样性监测框架。|
|2509.20060v1|[Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens](http://arxiv.org/abs/2509.20060v1)|**贡献点：**  <br/>1. 提出基于离散扩散模型（DDM）的文本对齐语音分词与重建框架，替代传统自回归解码器。  <br/>2. 实现更优的语音重建质量、更强的ASR性能（WER降低）及更快的推理速度。  <br/>3. 系统分析DDM在语音重建中的应用，研究采样方法、推理步数及长度估计误差鲁棒性。  <br/>4. 优化原TASTE模型，通过对比向量量化模块（FSQ vs. RVQ），验证FSQ在AR模型中有效提升性能（+0.14 UT-MOS）。  <br/>5. 支持高效的单步语音生成，仅需10步去噪，且质量损失可忽略。  <br/><br/>**总结：**  <br/>本文提出离散扩散模型框架，显著提升语音重建与ASR性能，优化向量量化策略，并实现高效单步生成。|
|2509.19999v1|[MultiSoundGen: Video-to-Audio Generation for Multi-Event Scenarios via   SlowFast Contrastive Audio-Visual Pretraining and Direct Preference   Optimization](http://arxiv.org/abs/2509.19999v1)|**总结**：  <br/>提出MultiSoundGen框架，通过SF-CAVP模型和AVP-RPO方法解决V2A多事件场景下的语义对齐与音频质量问题，实现SOTA性能，提升分布匹配、语义-时序对齐及音频生成效果。<br/><br/>**贡献点**：  <br/>1. **提出SF-CAVP模型**：首个统一双流架构的音频-视觉预训练模型，通过显式对齐语义表示和动态特征处理多事件复杂性。  <br/>2. **设计AVP-RPO方法**：将DPO引入V2A任务，基于SF-CAVP量化语义-时序匹配并优化音频质量，提升生成效果。  <br/>3. **实现SOTA性能**：在多事件场景下，显著改善分布匹配、音频质量、语义对齐和时序同步。  <br/>4. **开源支持**：计划发布完整代码与数据集，促进领域研究与应用。|
|2509.19974v1|[On the Invariance of Cross-Correlation Peak Positions Under Monotonic   Signal Transformations, with Application to Fast Time Difference Estimation](http://arxiv.org/abs/2509.19974v1)|总结：  <br/>该论文提出了一种基于互相关峰位置不变性的新时间差估计方法，利用低比特整数量化信号并采用整数运算，显著提升计算效率，并通过实验验证其优于传统FFT方法。<br/><br/>贡献点：  <br/>1. **理论创新**：证明了互相关峰位置在任意单调变换下保持不变，为时间差估计提供了新理论依据。  <br/>2. **方法提出**：设计了一种基于低比特整数量化信号的高效时间差估计算法，无需传统FFT的复数运算。  <br/>3. **计算优化**：通过整数运算和数论算法，降低计算复杂度，提高算法效率。  <br/>4. **实验验证**：实验证明新方法在处理时间上优于传统FFT方法，具有实际应用价值。|
|2509.19946v1|[Evaluating pretrained speech embedding systems for dysarthria detection   across heterogenous datasets](http://arxiv.org/abs/2509.19946v1)|总结：  <br/>该研究系统评估了17种预训练语音嵌入系统在构音障碍检测中的表现，通过多数据集交叉验证和零假设对比验证有效性，揭示了数据集差异对模型性能的影响，并指出跨数据集泛化能力不足的挑战。<br/><br/>贡献点：  <br/>1. **全面评估现有语音嵌入系统**：系统性地测试了17种公开预训练模型在构音障碍检测任务中的性能。  <br/>2. **解决数据集局限性**：选取覆盖多种相关条件的数据集，并采用多交叉验证运行减少数据偏倚和不平衡影响。  <br/>3. **引入零假设对比方法**：通过比较得分分布与精心设计的零假设，确保评估结果超出随机水平。  <br/>4. **跨数据集泛化分析**：报告训练-测试数据集不一致时的性能，揭示系统泛化能力的不足。  <br/>5. **强调数据集选择的重要性**：发现不同数据集内部结果差异显著，质疑当前基准数据集的适配性。|
|2509.19928v2|[Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark,   and Exploration](http://arxiv.org/abs/2509.19928v2)|总结：  <br/>该论文提出ProsodyEval数据集及DS-WED新指标，解决了零样本TTS中韵律多样性评估难题，揭示影响因素并验证了现有大语言模型在韵律表达上的局限性。<br/><br/>贡献点：<br/>1. **构建多维度评估体系**：提出ProsodyEval数据集，结合传统声学指标与PMOS（人类评分），系统性评估韵律多样性。<br/>2. **创新客观评价指标**：设计DS-WED（离散化语音加权编辑距离），基于语义标记的加权编辑距离量化韵律变化，提升与人类感知的相关性。<br/>3. **验证指标有效性**：实验表明DS-WED在HuBERT/WavLM语音标记下具有高鲁棒性，与人类评分相关性显著优于现有声学指标。<br/>4. **分析影响因素**：通过DS-WED基准测试发现生成模型范式、时长控制、强化学习等对韵律多样性有显著影响。<br/>5. **揭示模型局限性**：指出当前大语言音频模型（LALMs）在捕捉韵律变化方面仍存在不足，推动模型改进方向研究。|
|2509.19928v1|[Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark,   and Exploration](http://arxiv.org/abs/2509.19928v1)|总结：  <br/>本文提出ProsodyEval数据集及DS-WED新指标，评估TTS系统并揭示影响韵律多样性的关键因素，指出当前大规模语音语言模型的局限性，推动韵律多样性量化研究。<br/><br/>贡献点：  <br/>1. **构建首个专业韵律多样性评估数据集（ProsodyEval）**：包含7种主流TTS系统的1000个语音样本与2000人次人类评分，引入PMOS评分以更贴近主观感知。  <br/>2. **提出DS-WED新指标**：基于语义标记的加权编辑距离，有效量化韵律变化，相较于传统声学度量具有更高与人类评分的关联性。  <br/>3. **验证DS-WED的鲁棒性**：在HuBERT和WavLM等语音标记模型上表现稳定，适用于实际场景。  <br/>4. **系统性评估开源TTS性能**：通过DS-WED在LibriSpeech与Seed-TTS测试集上对比主流模型，发现生成范式、时长控制、强化学习等要素对韵律多样性的影响。  <br/>5. **揭示大规模语音模型的局限性**：指出当前LALMs在捕捉复杂韵律变化方面仍存在不足，为未来研究提供方向。|
|2509.19906v1|[Voice Privacy Preservation with Multiple Random Orthogonal Secret Keys:   Attack Resistance Analysis](http://arxiv.org/abs/2509.19906v1)|**贡献点总结（100字以内）**：  <br/>提出多密钥随机正交矩阵加密方法，增强语音隐私保护的抗攻击能力；放松模型约束以兼容更广泛深度学习架构；通过扩展攻击场景验证方法有效性，证明其在强攻击下仍能保持隐私保护性能。<br/><br/>**分点贡献**：  <br/>1. **加密算法改进**：采用多个随机正交矩阵作为密钥，显著提升传统语音隐私保护方法的抗攻击能力。  <br/>2. **模型兼容性扩展**：设计灵活机制，放宽对深度学习模型的约束，提高方法的适用性。  <br/>3. **鲁棒性验证**：基于Voice Privacy Challenge场景进行扩展攻击实验，证实方法在更复杂攻击下的有效性。|
|2509.19883v1|[CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With   Structured Melody Control and Guidance](http://arxiv.org/abs/2509.19883v1)|总结：提出CoMelSinger框架，实现结构化与解耦的旋律控制，结合MaskGCT架构、对比学习策略及轻量级SVT模块，显著提升零样本Singing Voice Synthesis的音高准确性和音色一致性。<br/><br/>贡献点：  <br/>1. **提出CoMelSinger框架**：首个基于离散编解码器的零样本SVS方法，实现结构化且解耦的旋律控制，解决传统提示生成中的音高信息混杂问题。  <br/>2. **替换输入模式**：将文本输入转换为歌词与音高标记，保留上下文泛化能力的同时增强旋律条件建模。  <br/>3. **粗到细对比学习策略**：通过显式调控音频提示与旋律输入之间的音高冗余，有效抑制prosody leakage。  <br/>4. **轻量级SVT模块**：引入编码器-only的Singing Voice Transcription模块，实现音频标记与音高、时长的帧级对齐监督。  <br/>5. **性能提升**：在音高准确性、音色一致性和零样本迁移能力上超越现有基线模型。|
|2509.19881v2|[MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model](http://arxiv.org/abs/2509.19881v2)|**贡献点：**  <br/>1. 提出**稀缺感知的粗到细掩码策略**，通过优先处理高频token提升效率，后期聚焦低频token以增强泛化能力。  <br/>2. 设计**轻量级校正模块**，检测低置信度预测并动态重新掩码，提高推理稳定性与质量。  <br/>3. 采用**参数精简技术**（基于BigCodec和Qwen2.5-0.5B微调），将模型压缩至200M参数，显著降低计算开销。  <br/>4. 验证**语音增强与识别任务的协同优化**，在DNS Challenge和noisy LibriSpeech上实现领先的感知质量与更低的词错误率（WER）。  <br/><br/>**总结（100字内）：**  <br/>MAGE通过稀缺感知掩码策略和轻量校正模块，实现高效高质量语音增强，在DNS Challenge和noisy LibriSpeech上超越大模型，参数量仅200M，显著降低WER并提升感知质量。|
|2509.19879v1|[Weakly Supervised Phonological Features for Pathological Speech Analysis](http://arxiv.org/abs/2509.19879v1)|总结：本研究提出文本无关且可解释的帧级音韵特征瓶颈层，通过弱监督方法提升语音病理分析效果，达到与主流声学特征相当的性能，为语音治疗提供新视角。<br/><br/>贡献点：<br/>1. 提出弱监督训练框架，利用已知音素声学属性构建可解释的帧级音韵特征瓶颈层<br/>2. 首次将文本无关的音韵特征应用于语音病理分析任务，突破传统特征依赖文本标注的局限<br/>3. 实验验证该特征在语音可理解性预测（8.43 RMSE）和病理分类（75%准确率）中的有效性，展现与主流声学特征相当的性能|
|2509.19865v2|[SEA-Spoof: Bridging The Gap in Multilingual Audio Deepfake Detection for   South-East Asian](http://arxiv.org/abs/2509.19865v2)|**总结（100字以内）**：  <br/>提出首个针对东南亚语言的音频深度伪造检测数据集SEA-Spoof，涵盖6种语言及300+小时真实/伪造语音，揭示跨语言检测性能下降问题，证明数据集微调可显著提升多语言检测效果，推动区域安全技术研究。<br/><br/>**贡献点**：  <br/>1. **填补空白**：首个大规模针对东南亚（SEA）语言的音频深度伪造检测（ADD）数据集，解决现有数据集对SEA语言覆盖不足的问题。  <br/>2. **多语言支持**：包含泰米尔语、印地语、泰语、印尼语、马来语、越南语等6种SEA主要语言，覆盖广泛区域及语言特性。  <br/>3. **多样化伪造样本**：伪造语音由多种先进开源与商业系统生成，体现风格和保真度的广泛差异，增强数据集的挑战性。  <br/>4. **性能验证**：通过基准测试验证跨语言检测的性能退化问题，并证明基于SEA-Spoof的微调可有效恢复多语言检测能力。  <br/>5. **研究基础**：为开发鲁棒、跨语言、抗欺诈的音频伪造检测系统提供关键数据支持，强调SEA专项研究的紧迫性。|
|2509.19865v1|[SEA-Spoof: Bridging The Gap in Multilingual Audio Deepfake Detection for   South-East Asian](http://arxiv.org/abs/2509.19865v1)|总结（100字以内）：  <br/>提出首个面向东南亚语言的音频深度伪造检测数据集SEA-Spoof，覆盖六种语言及多源生成技术，验证其对提升跨语言检测性能的作用，强调该地区研究的重要性。<br/><br/>贡献点：  <br/>1. **首个东南亚语言专项数据集**：构建首个大规模、专注于东南亚语言（泰米尔、印地、泰语、印尼语、马来语、越南语）的音频深度伪造检测数据集（SEA-Spoof）。  <br/>2. **多语言与多源伪造样本**：覆盖300+小时真实与伪造语音对，仿真多种前沿开源及商业系统的音频伪造技术，体现语言特异性与生成质量差异。  <br/>3. **跨语言性能提升**：通过数据集微调，显著缓解现有检测模型在东南亚语言上的跨语言性能退化问题，验证其有效性。  <br/>4. **研究必要性强调**：揭示东南亚音频伪造检测的紧迫性，为开发鲁棒、跨语言抗欺骗系统提供基准资源。|
|2509.19831v1|[SCORE: Scaling audio generation using Standardized COmposite REwards](http://arxiv.org/abs/2509.19831v1)|总结：  <br/>本研究提出训练无关的Inference-Time Scaling方法与多奖励引导机制，结合新音频-文本对齐度量，在推理阶段显著提升音频生成的感知质量与语义对齐。<br/><br/>贡献点：  <br/>1. **提出Inference-Time Scaling方法**：首次将训练无关的推理时扩展技术应用于音频生成，通过增加计算资源提升生成质量。  <br/>2. **设计多奖励引导机制**：构建了多奖励体系，对感知关键组件进行归一化加权求和，实现稳定引导与显式控制。  <br/>3. **引入新型音频-文本对齐度量**：基于音频语言模型设计更鲁棒的对齐评估指标，提升模型评测的准确性。  <br/>4. **实验证明有效性**：在语义对齐与感知质量双重指标上，方法均优于传统生成和现有奖励引导技术。|
|2509.19812v1|[Efficient Speech Watermarking for Speech Synthesis via Progressive   Knowledge Distillation](http://arxiv.org/abs/2509.19812v1)|总结（100字以内）:  <br/>提出PKDMark方法，通过渐进式知识蒸馏构建轻量级语音水印模型，显著降低计算成本并在强攻击下保持高鲁棒性与隐蔽性，适用于实时语音合成场景。<br/><br/>贡献点:  <br/>1. **提出轻量级深度学习水印框架**：设计PKDMark模型，结合渐进式知识蒸馏（PKD）技术，在保持高鲁棒性的同时降低计算成本达93.6%。  <br/>2. **创新教师-学生训练机制**：采用可逆神经网络架构训练高性能教师模型，并通过分阶段蒸馏将知识迁移至紧凑的学生模型，优化模型效率与稳定性。  <br/>3. **实验验证强抗攻击能力**：在高级别失真条件下，模型实现99.6%的检测F1分数和4.30的PESQ值，证明其在真实场景下的有效性和实用性。  <br/>4. **平衡效率与性能**：解决了传统深度学习水印计算成本高与DSP方法抗攻击性差的矛盾，推动语音水印技术向实时应用落地。|
|2509.19721v1|[Short-Segment Speaker Verification with Pre-trained Models and   Multi-Resolution Encoder](http://arxiv.org/abs/2509.19721v1)|**贡献点总结：**  <br/>1. **多分辨率时域编码器设计**：提出使用25、50、100、200样本步长的多分辨率时间域编码器，显著提升短时段语音的时域特征分辨率。  <br/>2. **混合特征融合**：结合自监督预训练模型（PTM）特征、传统滤波器组特征及多分辨率时域编码器特征，增强信息提取能力。  <br/>3. **短时段SV优化**：通过调整窗口步长和特征融合策略，在输入片段短于2秒的场景下实现更优的说话人验证性能，实验验证效果显著提升。|
|2509.19686v1|[Non-locally averaged pruned reassigned spectrograms: a tool for glottal   pulse visualization and analysis](http://arxiv.org/abs/2509.19686v1)|**贡献点总结：**  <br/>1. 提出非局部平均修剪重分配频谱图（NAPReS），改进重分配频谱图的数据可视化能力，实现对声门脉冲模式的简化展示。  <br/>2. 支持多种共振峰拟合方法，如高斯混合模型（GMM），增强分析灵活性。  <br/>3. 在高噪声场景下，NAPReS比传统LPC拟合更可重复，提升语音分析的鲁棒性。  <br/>4. 验证NAPReS对低振幅周期结构的可视化效果，便于捕捉语音细节特征。  <br/><br/>**一句话总结：**  <br/>该研究提出NAPReS方法，优化重分配频谱图的数据呈现与分析，在高噪声环境下提升共振峰拟合的可重复性，并增强对语音周期性结构的可视化能力。|
|2509.19668v1|[Selective Classifier-free Guidance for Zero-shot Text-to-speech](http://arxiv.org/abs/2509.19668v1)|总结（100字以内）:  <br/>本研究首次将图像生成中的CFG策略迁移至语音合成，提出分离条件CFG方法，发现时间步切换策略可提升说话者相似性并保持文本遵循，揭示文本表示对选择性CFG效果的关键影响。<br/><br/>贡献点：<br/>1. **迁移性验证**：系统评估了图像生成领域中分类器自由引导（CFG）策略在零样本文本到语音（TTS）中的适用性，证明其直接迁移效果有限。<br/>2. **分离条件策略**：提出通过分离CFG条件实现语音合成中说话者保真度与文本内容遵循的权衡，为多目标优化提供新思路。<br/>3. **时间步切换机制**：设计标准CFG与选择性CFG的分阶段应用策略，早期时间步提升说话者相似性，后期时间步维持文本忠实度。<br/>4. **文本表示依赖性**：揭示选择性CFG效果高度依赖文本表示，不同语言（如英语与中文）的差异可能导致相同模型产生不同合成结果。<br/>5. **跨语言实验证明**：通过对比英语和中文的实验结果，验证了文本表示对CFG策略有效性的影响，为多语言语音合成提供启示。|
|2509.19295v1|[Audio-Based Pedestrian Detection in the Presence of Vehicular Noise](http://arxiv.org/abs/2509.19295v1)|总结：  <br/>本研究构建了首个全面的1321小时路边音频数据集，系统评估了音频步态检测在车辆噪声环境中的跨数据集性能、噪声数据影响及模型鲁棒性，推动了该领域的研究进展。<br/><br/>贡献点：  <br/>1. **构建首个综合性数据集**：提出包含1321小时真实交通声景的路边音频数据集，提供16kHz同步音频、帧级步态标注及1fps视频快照。  <br/>2. **跨环境性能评估**：比较噪音环境与噪声受限场景下的检测效果，揭示环境差异对模型表现的影响。  <br/>3. **噪声数据影响分析**：量化噪声数据对模型性能的干扰，强调声学上下文在检测中的关键作用。  <br/>4. **领域外鲁棒性验证**：评估模型在未见过的交通声音场景中的泛化能力，为实际应用提供可靠性依据。|
|2509.19270v1|[SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](http://arxiv.org/abs/2509.19270v1)|总结：  <br/>本研究提出SloPalSpeech，首个大型斯洛伐克议会语音数据集，通过优化处理流程和模型微调显著提升低资源语言ASR性能，并公开所有数据与模型促进研究。<br/><br/>贡献点：  <br/>1. 构建首个大规模斯洛伐克ASR数据集SloPalSpeech，包含2,806小时议会辩论语音及6000万词对齐转录本。  <br/>2. 开发稳健的处理流水线，实现长音频的精准分割与对齐，生成标准化的30秒音频-转录对。  <br/>3. 首次在斯洛伐克基准测试（Common Voice、FLEURS）中评估OpenAI Whisper模型微调效果，显著降低Word Error Rate（如Whisper-small降幅达70%）。  <br/>4. 公开完整数据集、分割转录本及所有微调模型，为低资源语言语音研究提供重要资源。|
|2509.19231v1|[Finding My Voice: Generative Reconstruction of Disordered Speech for   Automated Clinical Evaluation](http://arxiv.org/abs/2509.19231v1)|**贡献点总结：**<br/>1. 提出ChiReSSD框架，专为儿童语音障碍（SSD）设计，在保持说话者身份的同时抑制错误发音，重点关注音高与语调调整。<br/>2. 在STAR数据集上验证，显著提升词汇准确率和身份保持效果。<br/>3. 引入PCC指标自动预测语音内容，辅音修正率与临床评估指标相当。<br/>4. 自动标注与专家标注的皮尔逊相关系数达0.63，减轻人工转录负担。<br/>5. 在TORGO数据集展现跨人群泛化能力，有效重建成人失语症语音。<br/>6. 展示分离风格的TTS重建方法在多元临床群体中的身份保持潜力。<br/><br/>**100字内总结：**  <br/>提出ChiReSSD框架，专为儿童语音障碍设计，实现身份保持与发音修正，显著提升词汇准确率和身份一致性，结合PCC指标验证自动预测效果，并成功泛化至成人失语症语音，为临床语音处理提供高效解决方案。|
|2509.19219v1|[MUSHRA-1S: A scalable and sensitive test approach for evaluating   top-tier speech processing systems](http://arxiv.org/abs/2509.19219v1)|总结（100字以内）: <br/>MUSHRA 1S通过单刺激评估方法，结合MUSHRA的敏感性与ACR的可扩展性，在保持高保真度的同时有效识别系统偏差，为语音系统基准测试提供更稳健的解决方案。<br/><br/>贡献点：<br/>1. **提出混合方法MUSHRA 1S**：结合MUSHRA的敏感性（能检测细微质量问题）与ACR的可扩展性（高效评估多个系统），弥补两者单独使用时的不足。<br/>2. **解决高保真度饱和问题**：在高质量语音系统评估中保持敏感性，避免ACR因质量过高导致评分饱和。<br/>3. **精准偏差识别能力**：通过固定参考和锚点，有效定位特定系统偏差，提升评估的针对性。<br/>4. **减少范围均衡偏差**：通过统一上下文设置，降低不同系统间因评分尺度差异导致的偏差。|
|2509.19186v1|[Improving Test-Time Performance of RVQ-based Neural Codecs](http://arxiv.org/abs/2509.19186v1)|总结：  <br/>本论文提出了一种改进RVQ编码的算法，通过优化码本选择降低量化误差，显著提升神经音频编解码器的合成质量，并验证了其有效性。<br/><br/>贡献点：  <br/>1. 指出传统RVQ方法生成的量化向量存在次优性，量化误差可通过码本选择改进。  <br/>2. 提出新型编码算法，用于识别实现更低量化误差的离散码本集合。  <br/>3. 将方法应用于预训练模型，通过多维度评估验证其提升合成质量的效果。|
|2509.19097v2|[On-device Internet of Sounds Sonification with Wavetable Synthesis   Techniques for Soil Moisture Monitoring in Water Scarcity Contexts](http://arxiv.org/abs/2509.19097v2)|**贡献点**  <br/>1. **提出设备级Sonification方法**：首次在物联网声音（IoS）网络中，将sonification技术应用于传感器数据的本地处理，突破传统应用和服务层级的局限。  <br/>2. **创新性使用wavetable合成技术**：通过映射传感器数据到声学参数，构建了基于wavetable的声学数据表示方案，增强信息传达的直观性与实时性。  <br/>3. **系统化原型设计与验证**：开发了可运行的IoS土壤湿度监控原型系统，并通过实验验证其在复杂数据流中的可行性。  <br/>4. **场景化应用探索**：将sonification方法与土壤湿度监测任务紧密结合，为应对全球水短缺问题提供了一种新颖的监测解决方案。  <br/><br/>**总结**  <br/>本文提出设备级IoS sonification方法，利用wavetable技术实现传感器数据的声学映射，开发原型系统并应用于土壤湿度监测，为水资源管理提供创新工具。|
|2509.19097v1|[On-device Internet of Sounds Sonification with Wavetable Synthesis   Techniques for Soil Moisture Monitoring in Water Scarcity Contexts](http://arxiv.org/abs/2509.19097v1)|**贡献点总结（100字以内）：**  <br/>提出设备级声学化方法，利用wavetable合成技术将传感器数据映射到声学参数，构建土壤湿度监测原型系统，解决IoS网络中水资源短缺的实时信息传达问题。<br/><br/>**分点贡献：**  <br/>1. **方法创新**：首次在IoS网络设备层实现声学化（Sonification），突破以往应用层/服务层研究的局限。  <br/>2. **技术实现**：提出基于wavetable合成的传感器数据到声学参数的映射框架，构建可部署的硬件原型。  <br/>3. **场景应用**：将声学化技术具体应用于土壤湿度监测，契合全球水资源短缺的现实需求。  <br/>4. **理论拓展**：系统化定义设备端声学化流程，增强IoS网络中非视觉信息传递的理论基础。|
|2509.19091v1|[Training Flow Matching Models with Reliable Labels via Self-Purification](http://arxiv.org/abs/2509.19091v1)|总结：  <br/>本研究提出SPFM方法，通过模型自身过滤噪声数据提升语音生成质量，在真实场景数据集上实现优于现有基准的性能。<br/><br/>贡献点：  <br/>1. **提出SPFM方法**：首次在流匹配框架中引入自净化机制，利用模型自身识别不可靠数据，无需预训练模型或额外模块。  <br/>2. **解决噪声标签问题**：证明在含误标签数据的训练下，SPFM仍能生成符合指定条件的高质量样本。  <br/>3. **验证鲁棒性**：在真实场景（TITW）数据集上测试SPFM，展示其对未标注噪声数据的强抗干扰能力及性能优势。|
|2509.19025v2|[Enhancing Noise Robustness for Neural Speech Codecs through   Resource-Efficient Progressive Quantization Perturbation Simulation](http://arxiv.org/abs/2509.19025v2)|**贡献点：**  <br/>1. **提出噪声鲁棒性增强的训练策略**：通过在量化级别直接模拟噪声扰动，无需依赖噪声-清洁配对数据，仅基于清洁语音进行训练。  <br/>2. **改进残差向量量化（RVQ）机制**：引入**距离加权概率Top-K采样策略**，替代传统确定性最近邻选择，降低码字偏移对语音质量的影响。  <br/>3. **设计渐进式训练方案**：从最后一个量化器到第一个量化器逐步引入扰动，实现对噪声的可控适应。  <br/>4. **实验证明有效性**：在Encodec和WavTokenizer上验证，显著提升低信噪比（15 dB）下的语音质量（UTMOS从3.475提升至3.586），同时优化清洁语音的编码质量。  <br/><br/>**总结（100字以内）**：  <br/>提出量化级噪声模拟训练策略，改进RVQ与渐进式训练方法，无需噪声数据，显著提升语音编解码器在噪声环境中的鲁棒性与清洁语音编码质量。|
|2509.19025v1|[Enhancing Noise Robustness for Neural Speech Codecs through   Resource-Efficient Progressive Quantization Perturbation Simulation](http://arxiv.org/abs/2509.19025v1)|**贡献点总结：**  <br/>1. 提出资源高效的噪声鲁棒性训练策略，通过量化级扰动模拟提升模型性能。  <br/>2. 引入距离加权概率Top-K采样机制，替代传统确定性最近邻选择。  <br/>3. 设计渐进式训练方案，按量化器顺序可控引入扰动。  <br/>4. 仅需干净语音数据训练，无需噪声-干净配对数据。  <br/>5. 实验证明显著提升Encodec和WavTokenizer在噪声环境下的UTMOS评分及编码质量。  <br/><br/>**摘要总结（100字内）：**  <br/>该研究提出量化级扰动模拟训练策略，通过距离加权Top-K采样和渐进式扰动引入，增强神经语音编码器的噪声鲁棒性。仅需干净数据训练，实验验证在15dB SNR下UTMOS提升，并提高编码质量。|
|2509.18928v1|[Direct Preference Optimization for Speech Autoregressive Diffusion   Models](http://arxiv.org/abs/2509.18928v1)|总结:  <br/>本文提出新型语音生成方法ARDM-DPO，结合扩散模型与直接偏好优化技术，提升了零样本文本到语音的语音表现力和长文本生成的鲁棒性。<br/><br/>贡献点:  <br/>1. 提出ARDM-DPO框架：首次将直接偏好优化（DPO）应用于语音生成领域，改进基于扩散模型的文本到语音生成方法。  <br/>2. 增强语音表现力：通过方法创新，显著提升生成语音的自然度与表达能力，优于传统next-token预测范式。  <br/>3. 改进长文本鲁棒性：针对长文本生成的稳定性问题，通过模型微调优化语音生成的持续性和连贯性。  <br/>4. 基于DiTAR模型验证：在最新零样本语音生成模型DiTAR上实验证明方法有效性，推动RL技术在语音领域的应用。|
|2509.18823v1|[Towards Evaluating Generative Audio: Insights from Neural Audio Codec   Embedding Distances](http://arxiv.org/abs/2509.18823v1)|总结：  <br/>提出DACe提升音频编码保真度，验证FAD优于MMD，展示NACs在压缩与感知评估的双重价值。<br/><br/>贡献点：  <br/>1. **提出DACe模型**：开发更高保真度的NACs版本，基于多样化的合成与真实音调数据训练，提升音频压缩质量。  <br/>2. **对比评估指标**：系统比较FAD和MMD在MUSHRA测试中的表现，证实FAD在感知质量评估中更优。  <br/>3. **验证人类相关性**：证明高保真NACs（如DACe）的嵌入向量与人类主观判断相关性更强，提升评估有效性。  <br/>4. **零样本评估方法**：提出NACs嵌入作为零样本音频质量评估手段，仅需未编码音频即可训练，具有实用优势。  <br/>5. **双重应用价值**：揭示NACs在音频压缩与感知驱动评估中的协同作用，拓展其应用范围。|
|2509.18816v1|[Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal   Attention in Large Audio Language Models](http://arxiv.org/abs/2509.18816v1)|贡献点总结（100字以内）:  <br/>提出训练无关的MATA方法，动态调整自注意力机制以增强音频标记关注度，无需额外参数或计算成本，有效解决多模态模型音频-文本注意力不平衡问题，在MMAR基准上实现开源模型首次超越Gemini 2.0 Flash，推动音频处理能力研究。<br/><br/>分点贡献：  <br/>1. **揭示关键问题**：指出大型音频-语言模型在多模态融合层存在音频-文本注意力不平衡问题，导致音频推理任务性能下降。  <br/>2. **提出MATA方法**：设计训练无关的动态调整机制，通过自注意力中提升音频token的权重，优化模型对音频信息的利用。  <br/>3. **高效干预策略**：仅对中间层最后一个token进行干预，避免引入额外参数和计算开销，提升方法可行性。  <br/>4. **实验证明有效性**：在MMAU和MMAR基准测试中验证MATA效果，尤其在MMAR中实现开源模型超越专有模型Gemini 2.0 Flash。  <br/>5. **开创研究方向**：为缓解多模态模型注意力偏差提供高效解决方案，并推动音频处理能力的进一步研究。|
|2509.18806v1|[Rethinking the joint estimation of magnitude and phase for   time-frequency domain neural vocoders](http://arxiv.org/abs/2509.18806v1)|总结：  <br/>本文提出三种策略优化APNet2声码器的时频域联合预测，有效缓解性能崩溃问题，桥接单双流模型的性能差距。<br/><br/>贡献点：  <br/>1. **揭示性能问题**：发现APNet2在大规模数据集上存在严重性能崩溃现象，指出当前时频域声码器联合预测机制的不足。  <br/>2. **三重空间优化策略**：提出针对拓扑空间（改进网络结构）、源空间（引入先验知识）、输出空间（优化反向传播）的三项创新性解决方案。  <br/>3. **性能提升与对比**：显著改善APNet2的幅度与相位联合估计效果，缩小其与单流模型（如Vocos）之间的性能差异，推动时频域声码器发展。|
|2509.18722v1|[LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](http://arxiv.org/abs/2509.18722v1)|**总结（100字以内）:**  <br/>提出了LOTUSDIS泰语会议语料库，涵盖多距离和麦克风类型数据，展示了微调提升语音识别鲁棒性的效果，并开源促进研究。<br/><br/>**分点贡献:**  <br/>1. **首个泰语远场会议语料库**：构建LOTUSDIS，包含114小时自发对话，模拟真实场景中的重叠语音、混响和噪音。  <br/>2. **多设备与距离多样性**：使用不同类型的六种麦克风，覆盖0.12-10米距离，保留设备特性与环境干扰，无需麦克风阵列。  <br/>3. **标准数据划分与基准系统**：释放训练/验证/测试集划分及可复现的基准系统，促进研究可复现性。  <br/>4. **模型性能评估**：对比零样本与微调条件下的Whisper变种，验证预训练数据与泰语远场语音的不匹配问题。  <br/>5. **显著性能提升**：微调后WER降低（总体从64.3→38.3，远场从81.6→49.5），尤其改善远距离麦克风识别效果。  <br/>6. **开源促进研究**：语料库及训练评估脚本以CC-BY-SA 4.0开放，支持学术社区共享与开发。|
|2509.18691v1|[An overview of neural architectures for self-supervised audio   representation learning from masked spectrograms](http://arxiv.org/abs/2509.18691v1)|总结：  <br/>本文系统综述了自监督音频表示学习中的掩码频谱建模与循环序列建模方法，对比Transformer、Mamba和xLSTM在十个音频分类任务中的性能，为实际应用提供模型选择依据。<br/><br/>贡献点：  <br/>1. **首次综合比较**：系统梳理并对比了掩码频谱建模（MM）与循环序列建模（如Mamba、xLSTM）在语音领域的研究进展与技术特点。  <br/>2. **统一框架实验**：基于统一可复现框架，在十个多样化音频分类任务中对比Transformer、Mamba和xLSTM的MM模型效果。  <br/>3. **填补研究空白**：针对当前缺乏跨方法（MM与循环模型）综合分析的现状，提供了跨领域的全面性能评估与趋势总结。  <br/>4. **实际应用指导**：通过实验结果为研究者提供模型选型建议，助其根据具体应用场景选择合适方法。|
|2509.18620v1|[Scalable Evaluation for Audio Identification via Synthetic Latent   Fingerprint Generation](http://arxiv.org/abs/2509.18620v1)|1. 提出音-free合成指纹方法，利用预训练神经音频指纹系统生成近似真实指纹分布的合成数据。  <br/>2. 合成指纹可作为真实干扰项，实现大规模检索性能模拟而无需额外音频数据。  <br/>3. 通过合成干扰项增强真实数据库，验证其在多个音频指纹框架上的扩展趋势与真实数据一致。  <br/>4. 构建大规模合成干扰数据库，提供一种不依赖音频语料的系统扩展性评估标准。|
|2509.18603v1|[SynSonic: Augmenting Sound Event Detection through Text-to-Audio   Diffusion ControlNet and Effective Sample Filtering](http://arxiv.org/abs/2509.18603v1)|总结（100字以内）:  <br/>SynSonic通过文本到音频扩散模型结合能量包络ControlNet，解决SED任务中数据稀缺和时序标注不足的问题，引入双分类器联合过滤策略提升样本质量，实验验证其在时序定位与分类性能上的显著提升。<br/><br/>贡献点:  <br/>1. 首次将生成式模型应用于SED任务，突破传统数据增强方法的样本多样性限制。  <br/>2. 提出基于能量包络的ControlNet引导框架，实现声事件的高质量时序生成。  <br/>3. 设计双分类器联合得分过滤策略，有效控制噪声干扰并保障增强数据质量。  <br/>4. 验证SynSonic在提高多音源检测指标（PSDS1/2）及分类准确率上的实际效果。|
|2509.18561v1|[SoundCompass: Navigating Target Sound Extraction With Effective   Directional Clue Integration In Complex Acoustic Scenes](http://arxiv.org/abs/2509.18561v1)|总结：  <br/>本文提出SoundCompass框架，通过SPIN模块和链式推断策略，有效整合方向线索与空间信息，提升目标声源提取的鲁棒性和适应性。<br/><br/>贡献点：  <br/>1. 提出SPIN模块：通过复频谱图域建模跨通道空间相关性，保留完整多通道信号的空间信息。  <br/>2. 结合SH编码：将球面调和编码作为方向线索，与空间相关性特征融合，提升方向判别能力。  <br/>3. 引入重叠子带融合：在频段分割架构基础上采用重叠频率子带融合，增强频率分辨率和信号适应性。  <br/>4. 链式推断策略（CoI）：通过迭代递归融合方向线索与声事件激活估计，优化目标提取精度。  <br/>5. 实验验证有效性：在多样化信号类别和空间配置下，展示方法的鲁棒性与泛化能力。|
|2509.18531v1|[No Verifiable Reward for Prosody: Toward Preference-Guided Prosody   Learning in TTS](http://arxiv.org/abs/2509.18531v1)|总结（100字以内）:  <br/>本文提出迭代DPO方案，通过少量人类偏好数据优化TTS的prosody自然性，同时保持语音识别准确率，在KoCC-TTS数据集上优于GRPO和商业基线，验证了人类偏好优化在缺乏自动奖励场景下的有效性。<br/><br/>**贡献点分点列出**:  <br/>1. **提出新的优化框架**：设计迭代式直接偏好优化（DPO）方案，通过少量人类标注的偏好对（~数百对/轮）直接优化prosody自然性，避免传统GRPO因依赖转录信号导致的prosody崩溃问题。  <br/>2. **引入任务导向数据集**：构建KoCC-TTS数据集，包含真实韩语客服对话，用于验证方法在任务相关场景下的表现。  <br/>3. **平衡自然性与准确性**：在KoCC-TTS上取得最高人类偏好（ELO）与竞争力CER（字符错误率），优于传统GRPO和主流商业TTS系统。  <br/>4. **强调数据效率**：证明在缺乏自动prosody奖励的情况下，人类偏好优化可有效提升TTS质量，同时减少对大规模标注数据的依赖。|
|2509.18470v2|[Discrete-Time Diffusion-Like Models for Speech Synthesis](http://arxiv.org/abs/2509.18470v2)|总结：  <br/>本论文提出新型离散时间扩散模型，解决连续训练与离散推理的不匹配问题，实现更高效且质量相当的语音生成。<br/><br/>贡献点：  <br/>1. 探索并设计多种离散时间扩散过程（加性/乘性高斯噪声、模糊噪声及混合噪声）。  <br/>2. 突破传统连续时间扩散模型的训练限制，提升推理效率与训练一致性。  <br/>3. 通过实验验证离散时间模型在语音质量上与连续模型相当，且具备更优的训练/推理效率。|
|2509.18470v1|[Discrete-time diffusion-like models for speech synthesis](http://arxiv.org/abs/2509.18470v1)|**贡献点总结：**  <br/>1. 提出离散时间扩散模型变体，解决连续训练与离散推理的不匹配问题；  <br/>2. 引入多种噪声类型（加性高斯、乘性高斯、模糊噪声及混合噪声）的离散过程；  <br/>3. 证明离散模型在语音质量与连续模型相当的同时，提升训练和推理效率及一致性。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出离散时间扩散模型新变体，采用多种噪声机制，解决了连续训练与离散推理的不匹配问题，在保持语音质量的同时显著提升效率与一致性。|
|2509.18424v2|[Scattering Transformer: A Training-Free Transformer Architecture for   Heart Murmur Detection](http://arxiv.org/abs/2509.18424v2)|**贡献点：**<br/>1. 提出Scattering Transformer，一种无需训练的轻量级Transformer架构，无需反向传播即可引入上下文依赖，解决资源受限场景下的心音分析问题。  <br/>2. 将标准波浪散射网络与Transformer结构结合，创新性融合时频分析与自注意力机制，提升心音检测效率。  <br/>3. 在公开CirCor DigiScope数据集上验证性能，WAR达0.786、UAR达0.697，与主流基础模型表现相当，证明其有效性。  <br/>4. 为医疗领域提供无需大量标注数据的自动化心音分析方案，降低临床依赖与计算成本。  <br/><br/>**总结：**  <br/>论文提出一种无需训练的轻量级Scattering Transformer模型，在心音检测任务中实现高效性能，为资源受限场景提供可行的自动化医疗解决方案。|
|2509.18424v1|[Scattering Transformer: A Training-Free Transformer Architecture for   Heart Murmur Detection](http://arxiv.org/abs/2509.18424v1)|**贡献点：**  <br/>1. 提出轻量级Scattering Transformer架构，解决传统自监督音频基础模型计算量大问题；  <br/>2. 首次结合小波散射网络与Transformer结构，无需反向传播实现上下文依赖建模；  <br/>3. 在CirCor DigiScope数据集上验证性能，达到WAR 0.786和UAR 0.697，与当前SOTA方法相当；  <br/>4. 为资源受限场景下的心音分析提供无需训练的高效解决方案。  <br/><br/>**总结：**  <br/>本文提出无需训练的Scattering Transformer模型，通过融合小波散射网络与Transformer结构，在心音检测中实现轻量化与高性能，为临床资源不足场景提供有效替代方案。|
|2509.18412v1|[Identifying birdsong syllables without labelled data](http://arxiv.org/abs/2509.18412v1)|**贡献点总结：**  <br/>1. 提出首个完全无监督的鸟类鸣叫音节分解算法，无需人工标注数据。  <br/>2. 构建检测-聚类-匹配追求的三阶段方法流程，实现音节序列提取。  <br/>3. 在Bengalese finch数据集上验证自动注释的高精度，接近人工标注水平。  <br/>4. 展示方法可通过独特发声特征实现同一物种个体识别（适用于Bengalese finch和great tit）。|
|2509.18375v1|[A Dimensional Approach to Canine Bark Analysis for Assistance Dog   Seizure Signaling](http://arxiv.org/abs/2509.18375v1)|总结：该研究提出将犬吠分类转化为连续回归任务，利用调整后的孪生网络处理稀疏数据，显著提升效价维度性能，并通过真实数据验证模型的有效性。  <br/><br/>贡献点：  <br/>1. **框架创新**：将犬类发声分类问题重构为二维情感空间中的连续回归任务，突破传统分类方法的局限。  <br/>2. **模型改进**：设计调整后的孪生网络，基于样本对的序数与数值距离进行训练，而非二元相似性。  <br/>3. **性能提升**：在公开数据集上实现效价维度性能提升（Turn-around Percentage减少50%），验证模型有效性。  <br/>4. **真实场景验证**：通过真实数据集的定性分析，证明所学情感空间具备语义意义，提供在严重数据限制下的可行性方案。|
|2509.18340v1|[Qubit Instrumentation of Entanglement](http://arxiv.org/abs/2509.18340v1)|总结: 本研究通过量化音调关系实现音乐家间量子纠缠模拟，创新性地将MIDI数据与量子态结合，探索新型量子音乐表达方式，并提供实验验证与开源代码。<br/><br/>贡献点：<br/>1. 提出"tonal centrality"概念，量化音乐家间音调关系的相似性/差异性<br/>2. 开发基于MIDI的量子纠缠参数映射系统，实现音乐交互的量子化表达<br/>3. 在嵌入式设备(Raspberry Pi Pico)上实现实时量子模拟实验<br/>4. 首次将量子态|Φ⁺⟩与|Ψ⁺⟩应用于音乐演奏的协同控制<br/>5. 验证量子相关性和反相关性在音乐表演中的可实现性<br/>6. 为"entangled ensembles"（纠缠合奏）提供新的技术实现路径和实验范式|
|2509.18272v3|[StereoFoley: Object-Aware Stereo Audio Generation from Video](http://arxiv.org/abs/2509.18272v3)|总结：本文提出首个端到端的立体声物体感知视频到音频生成框架StereoFoley，通过合成数据解决专业数据稀缺问题，实现高语义准确性与空间同步的音频生成。<br/><br/>贡献点：<br/>1. 提出首个端到端的立体声物体感知视频-音频生成框架StereoFoley，实现语义对齐、时间同步与空间准确的高保真立体声生成<br/>2. 开发合成数据生成系统，整合视频分析、物体跟踪与动态音频合成技术，突破专业空间音频数据不足的限制<br/>3. 引入立体声物体感知度量体系，并通过人工听觉实验验证其与感知的强相关性<br/>4. 在音频生成质量（语义准确性、时间同步性）和空间音频特性（对象定位、距离感知）双重维度实现SOTA表现|
|2509.18272v2|[StereoFoley: Object-Aware Stereo Audio Generation from Video](http://arxiv.org/abs/2509.18272v2)|**贡献点：**  <br/>1. **提出StereoFoley框架**：首次实现端到端视频到立体声音频生成，具备语义对齐、时间同步和空间准确性（48kHz采样率）。  <br/>2. **开发基线模型**：生成立体音频的模型在语义准确性和同步性上达到当前最优水平。  <br/>3. **构建合成数据生成Pipeline**：集成视频分析、物体跟踪与动态音频合成（含距离感知响度控制），突破专业混合数据集不足的限制。  <br/>4. **提出立体声物体感知度量**：设计新指标并通过人类听觉实验验证，与感知结果高度相关。  <br/>5. **填补领域空白**：解决现有模型缺乏物体感知立体声生成能力的问题，设立新的研究基准。  <br/><br/>**总结（100字以内）**：  <br/>提出首个端到端视频到立体声音频生成框架StereoFoley，通过合成数据与模型优化实现空间准确的物体感知音效，填补专业数据集与物体感知生成的空白。|
|2509.18272v1|[StereoFoley: Object-Aware Stereo Audio Generation from Video](http://arxiv.org/abs/2509.18272v1)|总结：  <br/>提出StereoFoley框架，实现对象感知的高质量立体声视频到音频生成，并引入新评估指标验证其有效性，填补领域空白。<br/><br/>贡献点：  <br/>1. **首个端到端框架**：构建了首个支持语义对齐、时间同步及空间准确的立体声视频到音频生成框架（48kHz）。  <br/>2. **基础模型优化**：开发并训练达到语义准确性和同步性的最先进基础模型，突破现有模型的单声道限制。  <br/>3. **合成数据生成**：设计集成视频分析、对象跟踪与动态立体声合成（动态摆动和距离控制）的合成数据流水线，解决专业数据集缺失问题。  <br/>4. **对象-音频对齐**：通过合成数据微调模型，实现清晰的物体与音频的对应关系，增强空间感知能力。  <br/>5. **新评估指标**：提出立体声对象感知度量化标准，结合人类听觉测试验证其与感知的强相关性。|
|2509.18235v1|[Automated Analysis of Naturalistic Recordings in Early Childhood:   Applications, Challenges, and Opportunities](http://arxiv.org/abs/2509.18235v1)|总结：  <br/>该论文探讨了自然录音在儿童早期发展研究中的应用，指出现有语音技术对儿童的适配不足，并呼吁跨学科合作推动相关技术发展。<br/><br/>贡献点：  <br/>1. **强调自然录音在儿童研究中的独特价值**：提出自然长期录音能更真实地反映儿童自发行为，突破传统实验环境限制。  <br/>2. **识别儿童语音技术研究的空白**：指出现有关键技术（如说话人区分、语音分类等）主要针对成人，儿童领域的应用仍待探索。  <br/>3. **系统分析技术挑战与机遇**：综述了当前技术在儿童自然录音中的进展，并明确其在认知与社会发展中面临的特殊问题。  <br/>4. **倡导跨学科合作**：呼吁信号处理领域与心理学、教育学等学科协作，推动儿童语音分析技术的创新与应用。|
|2509.18010v1|[Cross-Attention is Half Explanation in Speech-to-Text Models](http://arxiv.org/abs/2509.18010v1)|**分点贡献：**  <br/>1. 首次系统评估跨注意力在语音领域（S2T）的解释力，填补了其在语音处理中作为依赖性分析工具的理论空白。  <br/>2. 通过对比注意力分数与特征归因生成的输入显著性图，揭示跨注意力与输入相关性的关联程度（50%-75%）。  <br/>3. 发现跨注意力在多尺度、多任务模型中表现差异，且需跨头/层聚合才能更充分对齐显著性解释。  <br/>4. 强调跨注意力作为解释性代理的局限性，指出其仅部分反映模型决策因素，为语音模型可解释性研究提供新视角。  <br/><br/>**总结（100字以内）：**  <br/>该研究揭示跨注意力在语音模型中仅部分反映输入相关性，表明其作为解释工具的不完整性，推动了语音领域模型可解释性的理论发展。|
|2509.18004v1|[WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation   for Dialectal Speech Processing](http://arxiv.org/abs/2509.18004v1)|总结：本研究构建了首个10,000小时高质量川渝方言语音语料库，提出完整数据处理框架，发布权威ASR/TTS基准，验证模型性能达开源最优，推动方言语音技术公平与减小偏见。<br/><br/>贡献点：<br/>1. 构建全球最大开放源代码的川渝方言语料库（10,000小时），填补方言语音数据稀缺的空白；<br/>2. 提出全流程方言语音数据处理框架Chuan-Pipeline，实现高质量数据构建；<br/>3. 发布包含人工校验转录文本的ASR/TTS基准测试集WenetSpeech-Chuan-Eval；<br/>4. 通过实验验证，所建模型在开放源码系统中达到SOTA水平，性能接近商业服务；<br/>5. 降低方言语音研究门槛，促进AI技术普惠，推动减少语音技术中的方言偏见。|
|2509.17988v1|[Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech](http://arxiv.org/abs/2509.17988v1)|总结：  <br/>提出Nord-Parl-TTS，为芬兰语和瑞典语提供大规模真实语音数据，填补资源缺口，推动TTS研究发展。<br/><br/>贡献点：  <br/>1. **构建首个开放TTS数据集**：针对芬兰语和瑞典语，提供基于真实语料的高质量语音数据，解决低资源语言的TTS数据不足问题。  <br/>2. **大规模语音数据覆盖**：从北欧议会记录中提取900小时芬兰语和5090小时瑞典语语音，显著提升数据量和多样性。  <br/>3. **改进数据处理流程**：采用优化的Emilia数据处理框架，实现高效、标准化的语音数据构建与校准。  <br/>4. **统一评估集设计**：提供标准化的测试集，便于模型训练、评估及跨语言性能对比研究。|
|2509.17883v1|[Brainprint-Modulated Target Speaker Extraction](http://arxiv.org/abs/2509.17883v1)|总结：  <br/>提出BM-TSE框架，通过脑图调制机制实现个性化和高保真目标说话人提取，解决EEG非稳态与个体差异问题，取得SOTA效果，并公开代码。<br/><br/>贡献点：  <br/>1. 提出Brainprint-Modulated Target Speaker Extraction (BM-TSE)框架，首次融合脑图调制机制实现个性化与高保真TSE。  <br/>2. 设计Spatio-Temporal EEG编码器与Adaptive Spectral Gain (ASG)模块，提取抗非稳态的稳定特征。  <br/>3. 引入联合监督的个性化调制机制，通过Subject Identification (SID)与Auditory Attention Decoding (AAD)任务学习统一的脑图嵌入。  <br/>4. 实验证明BM-TSE在KUL和Cocktail Party数据集上显著优于现有方法，达到SOTA性能。  <br/>5. 公开实现代码，推动领域研究与应用。|
|2509.17800v1|[Convolutional Neural Network Optimization for Beehive Classification   Using Bioacoustic Signals](http://arxiv.org/abs/2509.17800v1)|总结：提出使用Cochleagram时频表示提升蜂群状态分类准确率，结合剪枝、量化与知识蒸馏优化模型，显著压缩体积并加速推理，推动实时蜂群监测技术发展。<br/><br/>贡献点：<br/>1. 提出Cochleagram作为时频图像表示方法，实现98.31%的无监督数据分类准确率，优于传统Spectrogram等方法；<br/>2. 首次系统性整合剪枝、量化与知识蒸馏等优化策略，将模型体积减少91.8%并提升推理速度66%；<br/>3. 验证了优化模型对实时蜂群监测应用的可行性，强调了模型轻量化与高效推理对实际部署的重要性。|
|2509.17765v1|[Qwen3-Omni Technical Report](http://arxiv.org/abs/2509.17765v1)|**总结**：  <br/>Qwen3-Omni是首个在多模态任务中保持SOTA性能的模型，尤其在音频任务中表现优异，支持多语言处理，并通过创新架构和延迟优化提升流式合成效率，推出专用音频字幕模型。  <br/><br/>**贡献点**：  <br/>1. **多模态性能保持**：首次实现文本、图像、音频、视频四模态的SOTA性能，未因多模态融合而退化。  <br/>2. **音频任务卓越表现**：在36个音频/视听基准中，开源SOTA占比32%，整体SOTA达22%，超越Gemini-2.5-Pro等闭源模型。  <br/>3. **创新架构设计**：采用Thinker-Talker MoE架构，统一感知与生成，实现流畅文本和自然实时语音输出。  <br/>4. **多语言支持**：支持119种语言文本交互、19种语言语音理解、10种语言语音生成。  <br/>5. **流式合成延迟优化**：通过多码本方案和轻量级cnn网络替代扩散模型，实现首包延迟234ms。  <br/>6. **多模态推理增强**：引入Thinking模型，提升跨模态输入的推理能力。  <br/>7. **专用音频字幕模型**：针对音频字幕任务微调模型（Qwen3-Omni-30B-A3B-Captioner），减少幻觉并生成详细描述。|
|2509.17760v1|[Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term   Research](http://arxiv.org/abs/2509.17760v1)|总结：  <br/>提出增强型NAO机器人，通过硬件升级与系统优化提升对话质量，同时构建通用框架延长传统机器人寿命。<br/><br/>贡献点：  <br/>1. **硬件升级**：集成高精度麦克风、RGB-D/热成像摄像头及增强计算资源，提升感知与交互能力。  <br/>2. **系统架构创新**：融合云端与本地模型，实现高效的感知与对话处理，保留NAO原有身体表达能力。  <br/>3. **性能验证**：实验表明增强型NAO在对话质量与用户偏好上优于NAO AI Edition，且保持低延迟。  <br/>4. **音频优化**：采用波束成形与低延迟音频处理技术，显著减少自听干扰并提升多用户语音分离能力。  <br/>5. **视觉与热感扩展**：增强视觉和热成像能力为未来交互功能奠定基础。  <br/>6. **通用性框架**：提供平台无关的解决方案，使传统机器人适应现代技术需求，延长研究与应用价值。|
|2509.17741v1|[GAN-Based Multi-Microphone Spatial Target Speaker Extraction](http://arxiv.org/abs/2509.17741v1)|**贡献点：**  <br/>1. **首次探索GAN在空间目标说话人提取中的应用**：提出利用生成对抗网络（GAN）结合噪声混合和空间信息（如DoA）进行语音提取，突破传统判别方法的局限性。  <br/>2. **引入判别模型中间特征与DoA的联合条件化策略**：通过融合判别空间滤波模型的中间特征与DoA信息，实现高空间分辨率（5度）的可调控目标语音提取。  <br/>3. **感知质量超越现有判别方法**：在客观指标（基于感知质量）上，所提方法性能优于当前最先进的判别模型方法。  <br/><br/>**总结（100字以内）：**  <br/>论文提出基于GAN的空间目标说话人提取方法，结合判别模型中间特征与DoA条件化，实现5度高分辨率可调控提取，显著提升感知质量，突破传统判别方法性能瓶颈。|
|2509.17661v1|[Comparator Loss: An Ordinal Contrastive Loss to Derive a Severity Score   for Speech-based Health Monitoring](http://arxiv.org/abs/2509.17661v1)|**贡献点：**  <br/>1. **提出新的疾病进展度量**：首次引入“严重程度评分”（severity score）作为监测神经退行性疾病（如MND）进展的量化指标，超越传统分类任务。  <br/>2. **设计比较损失函数**：开发“比较损失”（comparator loss），通过强制评分满足特定顺序关系（如诊断等级或时间顺序），提升模型对疾病阶段的敏感性。  <br/>3. **多源健康指标融合**：使系统能够整合不同健康指标信息，即使在小数据集下也能有效利用多模态数据，增强疾病监测的全面性。  <br/>4. **临床验证有效性**：在MND患者数据上验证了模型的实用性，证明评分与临床指标（如ALSFRS-R）高度相关，并能有效区分患者与健康对照。  <br/><br/>**总结（100字以内）：**  <br/>提出基于比较损失的严重程度评分，用于量化神经退行性疾病进展，实现多源健康指标整合并验证在MND患者中的有效性。|
|2509.17641v1|[AuditoryBench++: Can Language Models Understand Auditory Knowledge   without Hearing?](http://arxiv.org/abs/2509.17641v1)|总结（100字以内）:  <br/>本研究提出AuditoryBench++基准和AIR-CoT方法，提升语言模型在文本环境下的听觉推理能力，通过实验验证其有效性。<br/><br/>贡献点:<br/>1. **提出AuditoryBench++**：构建首个全面评估文本环境下听觉知识与推理能力的基准，覆盖基础比较至情境化推理任务，支持细粒度分析。  <br/>2. **开发AIR-CoT方法**：创新性引入特殊标记与跨度检测技术，结合知识注入实现听觉信息生成与整合，解决语言模型听觉推理能力不足问题。  <br/>3. **验证方法有效性**：通过广泛实验对比，证明AIR-CoT在通用LLMs和多模态LLMs中均显著优于基准模型及知识增强模型。|
|2509.17609v2|[Audio Super-Resolution with Latent Bridge Models](http://arxiv.org/abs/2509.17609v2)|总结：  <br/>提出潜在桥梁模型（LBMs）及改进方法，突破48kHz以上音频超分辨率限制，实现无缝级联处理，达到语音、音频和音乐信号的SOTA质量。<br/><br/>贡献点：  <br/>1. **提出潜在桥梁模型（LBMs）**：通过压缩音频波形至连续潜在空间，设计潜到潜的生成过程，自然匹配LR到HR上采样，提升先验信息利用效率。  <br/>2. **引入频率感知LBMs**：将频率先验与目标频率作为输入，训练任意到任意的上采样能力，增强模型泛化性。  <br/>3. **级联结构与先验增强策略**：首次实现超越48kHz的音频上采样，设计级联SR流程并提出两种先验增强方法，提升灵活性与处理效率。  <br/>4. **实验验证SOTA性能**：在VCTK、ESC-50、Song-Describer等数据集上，取得任意到48kHz的最优客观与感知质量，以及任意到192kHz的首次记录。|
|2509.17609v1|[Audio Super-Resolution with Latent Bridge Models](http://arxiv.org/abs/2509.17609v1)|**分点贡献：**  <br/>1. **提出潜在桥模型（LBMs）**：将音频波形压缩至连续潜在空间，设计潜在到潜在的生成过程，自然匹配LR到HR的upsampling，有效利用LR先验信息提升生成质量。  <br/>2. **引入频率感知LBMs**：通过将频率信息作为模型输入，实现任意输入-输出的upsampling学习，在HR样本稀缺时增强训练效果。  <br/>3. **级联LBMs与先验增强策略**：首次突破48kHz上限，提出级联SR框架，增强音频后生产的灵活性，实现无缝的多阶段超分辨率。  <br/>4. **实验验证与性能突破**：在VCTK、ESC-50、Song-Describer等数据集及自建测试集上，达到SOTA的客观与主观质量，在any-to-192kHz音频SR中创纪录。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出频率感知和级联潜在桥模型，解决传统方法先验信息不足问题，突破48kHz音频超分辨率限制，在多任务场景下实现SOTA性能。|
|2509.17585v1|[Attention-based Mixture of Experts for Robust Speech Deepfake Detection](http://arxiv.org/abs/2509.17585v1)|总结：  <br/>提出基于Mixture of Experts架构的语音深度伪造检测系统，在多个数据集和 SAFE 挑战中表现最优，通过注意力门控网络动态融合多专家模型，实现对语音伪造的高效识别。<br/><br/>贡献点：  <br/>1. **提出新型检测架构**：设计基于Mixture of Experts（MoE）的检测框架，动态融合多个专家模型以提升检测性能。  <br/>2. **引入注意力门控机制**：通过注意力网络动态调整各专家模型的权重，增强对输入语音信号的针对性分析。  <br/>3. **多模型协同学习**：各专家模型利用归纳偏置学习共享数据的互补特征，形成专业化且互补的检测能力。  <br/>4. **验证泛化性能**：在多数据集和 SAFE 挑战中均验证了方法的优越性，系统综合排名领先。|
|2509.17523v1|[Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models](http://arxiv.org/abs/2509.17523v1)|总结：  <br/>提出引入视觉 grounding 以提升双语语音SSL模型性能，显著缩小零样本音素识别的性能差距。<br/><br/>贡献点：  <br/>1. 首次探讨将视觉信息融入双语语音SSL模型以弥补多语言性能劣势；  <br/>2. 设计有限视觉grounding机制，平衡多语言任务与单语性能；  <br/>3. 验证视觉grounding对双语模型的显著提升效果（零样本音素区分准确率提升23.46%）；  <br/>4. 通过对比实验揭示视觉信息对多语言场景的特殊增益作用；  <br/>5. 为多语言语音表示学习提供新的可解释性增强策略。|
|2509.17516v1|[Audiobook-CC: Controllable Long-context Speech Generation for Multicast   Audiobook](http://arxiv.org/abs/2509.17516v1)|总结：  <br/>提出面向多播有声书的上下文感知与情感可控TTS框架，通过三种创新技术提升生成连贯性与情感表达，并验证了方法有效性。<br/><br/>贡献点：  <br/>1. **多播有声书专用框架**：针对现有系统无法处理长文本连贯性问题，设计专门用于多播有声书的语音合成框架。  <br/>2. **三重创新技术**：  <br/>   - 上下文机制：确保生成内容的上下文一致性；  <br/>   - 解耦范式：将风格控制与语音提示分离，提升语义一致性；  <br/>   - 自蒸馏方法：增强情感表达与指令可控性。  <br/>3. **实验验证**：通过消融研究证明方法有效性，并在多场景（叙述、对话、整章）中超越现有基线。  <br/>4. **应用展示**：提供demo样例（链接）供实际效果参考。|
|2509.17490v2|[FUN-SSL: Full-band Layer Followed by U-Net with Narrow-band Layers for   Multiple Moving Sound Source Localization](http://arxiv.org/abs/2509.17490v2)|总结（100字以内）:  <br/>本文提出FUN-SSL架构，通过U-Net多尺度窄带处理和跨块跳连优化，显著降低SSL模型的计算复杂度，同时保持或提升性能，优于传统IPDnet等方法。<br/><br/>贡献点分点：  <br/>1. **双路径处理优化**：引入U-Net结构实现多分辨率窄带信号处理，替代传统全窄网络模块，降低计算复杂度。  <br/>2. **FUN块设计**：提出包含全带层与多尺度窄带层的FUN块，通过分层结构平衡频谱与时序信息提取效率。  <br/>3. **跨模块跳连机制**：在FUN块间引入跳连，增强信息传递与特征融合，提升模型性能。  <br/>4. **实验验证**：在保持高定位精度的同时，证明FUN-SSL的计算效率显著优于IPDnet等现有方法。|
|2509.17490v1|[FUN-SSL: Full-band Layer Followed by U-Net with Narrow-band Layers for   Multiple Moving Sound Source Localization](http://arxiv.org/abs/2509.17490v1)|**贡献点：**  <br/>1. **双路径优化架构**：提出FUN-SSL架构，通过U-Net实现多分辨率窄带处理，替代原有IPDnet的全窄网络块，显著降低计算复杂度。  <br/>2. **结构创新**：将全带层与多尺度U-Net窄带层结合，形成FUN块，同时引入跨FUN块跳接连接以加强信息传递。  <br/>3. **性能提升**：实验验证FUN-SSL在定位多移动声源任务中优于现有方法，且在计算效率上大幅优于IPDnet。  <br/><br/>**总结（100字以内）：**  <br/>本文提出FUN-SSL架构，通过U-Net实现多尺度窄带处理并优化双路径结构，引入跨块跳接连接，有效降低计算复杂度，同时提升多声源定位性能。|
|2509.17410v1|[Neural acoustic multipole splatting for room impulse response synthesis](http://arxiv.org/abs/2509.17410v1)|总结：  <br/>提出NAMS方法，通过神经声学多极子与剪枝策略高效预测任意接收位置RIR，显著优于单极子模型且保持快速推理。<br/><br/>贡献点：  <br/>1. 提出Neural Acoustic Multipole Splatting（NAMS）方法，利用神经网络学习多极子位置及信号特性，实现任意接收位置RIR合成。  <br/>2. 通过多极子表示声场，在满足物理约束（如亥姆霍兹方程）的同时，灵活表达复杂声学场景。  <br/>3. 引入动态剪枝策略，从密集多极子分布出发逐步删除冗余极子，优化模型复杂度。  <br/>4. 在真实与合成数据集上验证，NAMS在多数指标上优于现有方法，且推理速度高效；消融实验显示剪枝后模型仅需20%极子即超越单极子模型。|
|2509.17404v1|[SongPrep: A Preprocessing Framework and End-to-end Model for Full-song   Structure Parsing and Lyrics Transcription](http://arxiv.org/abs/2509.17404v1)|**贡献点总结：**  <br/>1. 提出SongPrep框架，实现歌曲数据自动化预处理（源分离、结构分析、歌词识别）。  <br/>2. 开发SongPrepE2E端到端模型，无需额外源分离即可完成结构识别和歌词时间戳标注。  <br/>3. 结合预训练语言模型与全局上下文，提升歌词识别的准确率（低DER和WER）。  <br/>4. 验证方法在SSLD-200数据集上的有效性，显著改善下游歌曲生成模型的输出质量。  <br/><br/>**总结（100字以内）：**  <br/>本文提出SongPrep自动化预处理框架及SongPrepE2E端到端模型，解决歌曲数据标注效率低的问题，结合预训练语言模型提升歌词识别精度，在SSLD-200数据集上验证了其在歌曲生成任务中的有效性，生成歌曲更接近人类作品。|
|2509.17375v1|[Improving Active Learning for Melody Estimation by Disentangling   Uncertainties](http://arxiv.org/abs/2509.17375v1)|**贡献点：**<br/>1. 提出区分**aleatoric**（数据不确定性）与**epistemic**（模型不确定性）的框架，用于指导主动学习中的旋律估计任务。  <br/>2. 验证**epistemic不确定性**在跨领域适应中比aleatoric不确定性更可靠，且能减少标注需求。  <br/>3. 展示模型仅需少量标记样本即可实现高效领域适应，提升资源有限场景下的性能。  <br/>4. 为语音领域主动学习研究提供新的理论视角和实验依据。  <br/><br/>**总结（100字内）：**  <br/>本研究提出区分aleatoric与epistemic不确定性的框架，通过实验验证epistemic不确定性在领域适应中的有效性，显著降低标注成本，推动语音领域主动学习方法的发展。|
|2509.17286v1|[RADE for Land Mobile Radio: A Neural Codec for Transmission of Speech   over Baseband FM Radio Channels](http://arxiv.org/abs/2509.17286v1)|**贡献点：**  <br/>1. 提出基于自编码器的现代机器学习方法，优化基带FM（BBFM）通道的语音传输质量。  <br/>2. 实现8 kHz带宽下高质量语音在BBFM信道中的传输，性能优于传统模拟FM系统。  <br/>3. 验证该方法在模拟LMR衰落信道中的鲁棒性，提升抗干扰能力。  <br/>4. 展示系统在商用UHF无线电上的实际运行演示，证明可行性。  <br/><br/>**总结（100字以内）：**  <br/>论文提出基于自编码器的BBFM语音传输方案，利用数字脉冲驱动传统模拟架构，在8 kHz带宽下实现优于模拟FM的高质量语音通信，并通过实验验证其在衰落信道下的性能，成功应用于商用UHF无线电。|
|2509.17277v1|[BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and   Psychoacoustics Research](http://arxiv.org/abs/2509.17277v1)|总结（100字以内）:  <br/>本文提出BeepBank-500，一个紧凑的全合成耳语/警报数据集，支持快速实验，包含300-500个样本，覆盖多种参数化音频特性，并提供开源授权与可复现基线模型。<br/><br/>贡献点分点列出:  <br/>1. **数据集构建**：创建首个紧凑（300-500个片段）的全合成耳语/警报数据集，专为HCI与音频机器学习设计。  <br/>2. **参数化生成**：通过参数控制波形类型（正弦、方波、三角波、FM）、基频、持续时间、振幅包络、振幅调制及轻量级Schroeder回声。  <br/>3. **环境配置**：提供三种回声场景（dry、rir small、rir medium）以模拟不同声学环境。  <br/>4. **资源配套**：开放单声道48kHz WAV音频、信号/频谱特征元数据表及小型可复现基线模型（波形分类与f0回归）。  <br/>5. **应用导向**：针对耳语分类、音色分析、起始检测等任务，并明确标注许可证（CC0-1.0）与使用限制。  <br/>6. **开放获取**：数据与代码分别采用CC0-1.0和MIT许可证开源，提供DOI与GitHub链接便于访问与复现。|
|2509.17270v1|[Reference-aware SFM layers for intrusive intelligibility prediction](http://arxiv.org/abs/2509.17270v1)|总结:  <br/>该论文提出结合参考信号与多层语音基础模型（SFMs）的侵入式语音可懂度预测方法，在CPC3数据集上取得最优结果，为构建SFM-based系统提供了实用指导。<br/><br/>贡献点:  <br/>1. 指出侵入式系统受限于SFMs的有限利用，提出参考信号与多层SFMs结合的新方法  <br/>2. 首次将多层SFMs表示引入侵入式预测框架，提升模型对语音特征的捕捉能力  <br/>3. 实验结果在CPC3数据集上达到RMSE 22.36（开发集）和24.98（评估集），性能排名第一  <br/>4. 为后续基于SFMs的侵入式语音可懂度预测研究提供可复现的实践指导框架|
|2509.17247v2|[DeepASA: An Object-Oriented One-for-All Network for Auditory Scene   Analysis](http://arxiv.org/abs/2509.17247v2)|**贡献点总结：**  <br/>1. 提出统一框架DeepASA，实现多任务（源分离、去混响、声事件检测、分类、到达方向估计）联合处理。  <br/>2. 引入对象导向处理（OOP）策略，通过对象中心表示消除传统track-wise处理的参数关联歧义。  <br/>3. 设计链式推理（CoI）机制，结合时序一致性匹配（TCM）实现多任务融合与特征迭代优化。  <br/>4. 构建动态时序核、Transformer聚合器和对象分离器的多模块处理流水线。  <br/>5. 在ASA2、MC-FUSS、STARSS23等基准数据集上验证模型性能，达到所有任务的SOTA水平。  <br/><br/>**摘要总结（100字以内）：**  <br/>DeepASA提出统一框架整合多任务声学场景分析，通过OOP与CoI机制优化对象特征，解决复杂场景下参数关联问题，实现高效多任务融合与迭代提升，实验验证其在多个基准数据集上的领先性能。|
|2509.17247v1|[DeepASA: An Object-Oriented One-for-All Network for Auditory Scene   Analysis](http://arxiv.org/abs/2509.17247v1)|总结：  <br/>提出DeepASA统一框架，整合多任务处理，通过对象导向策略解决参数关联问题，引入时间一致性匹配机制实现多任务协同优化，达到多任务处理的最先进性能。<br/><br/>贡献点：  <br/>1. **统一多任务框架**：首次构建适用于复杂听觉场景的多输入多输出（MIMO）模型，整合声音分离、消混响、声事件检测、音频分类和方向估计等任务。  <br/>2. **对象导向处理（OOP）**：采用对象中心表示，将多样的听觉特征封装为对象相关特征，并通过链式推理机制（CoI）进行动态优化，解决传统track-wise处理的参数关联模糊性。  <br/>3. **模块化结构设计**：包含动态时间核特征提取器、Transformer聚合器和对象分离模块，通过级联处理生成精准的多对象特征。  <br/>4. **时间一致性匹配（TCM）**：在链式推理中引入TCM机制，实现多任务特征融合与迭代优化，增强下游任务的鲁棒性。  <br/>5. **实验验证性能**：在ASA2、MC-FUSS、STARSS23等基准数据集上验证模型，展现多任务处理的优越性及对复杂空间场景的适应性。|
|2509.17219v1|[Virtual Consistency for Audio Editing](http://arxiv.org/abs/2509.17219v1)|**贡献点分点总结：**  <br/>1. 提出基于虚拟一致性的音频编辑框架，无需逆过程即可高效生成编辑结果。  <br/>2. 构建模型无关系统，避免微调和架构修改，显著提升编辑速度。  <br/>3. 通过定量基准和16人用户研究验证，在保持编辑质量的同时实现效率突破。  <br/><br/>**总结：**  <br/>提出模型无关的虚拟一致性音频编辑系统，无需逆过程即实现高效高质量编辑，显著提升速度并验证效果。|
|2509.17164v1|[STAR: Speech-to-Audio Generation via Representation Learning](http://arxiv.org/abs/2509.17164v1)|总结：  <br/>本文提出STAR框架，首次实现端到端语音到音频生成，通过语音表征学习和两阶段训练策略解决级联系统的效率与误差问题，显著提升生成性能。<br/><br/>贡献点：  <br/>1. **首創端到端框架**：提出STAR，首次直接利用语音作为输入生成音频，避免级联系统中的误差传播与效率问题。  <br/>2. **语音交互模態優化**：摒棄依賴文本或視覺模態，采用自然的语音交互方式，提升語义捕捉的相關性。  <br/>3. **表征學習提取語義**：通過實驗驗證從原始语音中可有效提取聲音事件語义，同時捕捉聽覺事件與場景資訊。  <br/>4. **橋接網絡與訓練策略**：引入橋接網絡實現表征映射，配合兩階段訓練策略，優化端到端合成流程。  <br/>5. **性能提升與應用價值**：實現76.9%的語音處理延遲降低，生成效果超越傳統級聯系統，并提供生成樣本供驗證。|
|2509.17162v2|[FakeSound2: A Benchmark for Explainable and Generalizable Deepfake Sound   Detection](http://arxiv.org/abs/2509.17162v2)|贡献点总结：  <br/>1. **突破传统二分类局限**：FakeSound2作为首个多维基准，推动检测任务从单纯分类转向更全面的评估。  <br/>2. **三维度评估框架**：涵盖**定位**（伪造位置识别）、**可追溯性**（来源追踪）和**泛化能力**（适应新来源）。  <br/>3. **多样化数据覆盖**：包含**6种伪造类型**和**12种不同来源**，增强基准的挑战性和适用性。  <br/>4. **揭示技术瓶颈**：实验表明当前系统在伪造模式识别和解释可靠性方面存在显著不足。  <br/>5. **促进可信认证研究**：通过暴露检测漏洞，引导开发更安全、可解释的音频认证技术。  <br/><br/>总结（100字内）：  <br/>提出FakeSound2基准，突破传统二分类，以定位、可追溯性、泛化能力三维度评估深度伪造声音检测，覆盖6种类型和12种来源，揭示现有系统弱点，推动可信音频认证方法发展。|
|2509.17162v1|[FakeSound2: A Benchmark for Explainable and Generalizable Deepfake Sound   Detection](http://arxiv.org/abs/2509.17162v1)|**贡献点：**  <br/>1. 构建FakeSound2基准，突破传统二分类准确率的局限，推动深度伪造音频检测向更全面的方向发展。  <br/>2. 引入三大评估维度（定位、可追溯性、泛化能力），系统化衡量检测模型的多维度性能。  <br/>3. 覆盖6种篡改类型及12种多样化数据源，提升基准的代表性和挑战性。  <br/>4. 通过实验揭示当前系统在伪造模式识别和可解释性上的不足，明确技术瓶颈。  <br/>5. 为可信音频认证提供研究框架，促进鲁棒、可解释且泛化的检测方法研究。  <br/><br/>**总结（100字内）：**  <br/>FakeSound2提出多维度评估基准，突破二分类局限，覆盖6种篡改类型和12种来源，揭示现有检测系统的不足，推动更可靠的音频认证技术发展。|
|2509.17143v1|[MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion   With Increased Controllability via Multiple Guidances](http://arxiv.org/abs/2509.17143v1)|总结：  <br/>MaskVCT提出了一种零样本语音转换框架，通过多因子可控性实现更灵活的语音风格迁移，实验验证其在说话人相似度和口音匹配方面表现最优，同时保持语音可懂度和韵律控制能力。<br/><br/>贡献点：  <br/>1. **零样本语音转换**：无需目标说话人数据，突破传统VC依赖目标说话人条件的限制。  <br/>2. **多因子可控性**：通过多分类器自由引导（CFGs）实现说话人身份、语言内容和韵律因素的协同控制。  <br/>3. **多条件整合**：在同一模型中支持连续/量化语言特征及可选音高轮廓，增强语音保真度和可调控性。  <br/>4. **性能优势**：在目标说话人相似度和口音匹配上取得最佳结果，同时保持与基线相当的语音识别准确率。  <br/>5. **开放资源**：提供音频样本，便于复现与评估模型效果。|
|2509.17112v1|[RISE: Adaptive music playback for Realtime Intensity Synchronization   with Exercise](http://arxiv.org/abs/2509.17112v1)|总结：  <br/>本文提出RISE系统，通过动态调整音乐段与运动强度匹配，提升锻炼体验与效率，实验验证其在保持强度估计准确性方面优于传统非自适应方法。<br/><br/>贡献点：  <br/>1. **提出RISE系统**：首款基于运动强度动态调整音乐的系统，实现音乐高能量段与锻炼高强度阶段的精准对齐。  <br/>2. **动态音乐重组技术**：利用组件化方法动态延长或缩短音乐段，使音乐节奏与用户实时运动状态同步。  <br/>3. **用户输入导向**：通过用户设定的休息与运动时长指导音乐调整，优化传统预定义计划或手动输入的局限。  <br/>4. **实证效果验证**：在实验室环境下对12名用户进行测试，证明系统能有效维持强度感知准确性并增强锻炼持续性。|
|2509.17091v2|[SVeritas: Benchmark for Robust Speaker Verification under Diverse   Conditions](http://arxiv.org/abs/2509.17091v2)|**贡献点总结（100字以内）**:  <br/>提出SVeritas，首个全面覆盖语音识别中多种现实与合成挑战（如语言、年龄、编解码、噪声等）的基准测试套件；揭示现有模型在跨语言、年龄差异及编解码压缩等场景下的薄弱点；发现模型鲁棒性在人口学群体间的差异，推动更公平可靠的说话人验证系统发展。<br/><br/>---<br/><br/>**分点贡献**:<br/>1. **提出全面基准测试**：构建SVeritas，首次系统性评估SV系统在录音时长、自发性、内容、噪声、麦克风距离、混响、通道不匹配等多维度应力条件下的表现。<br/>2. **引入新应力条件**：新增未被研究的现实挑战（如编解码器压缩、跨语言试听），覆盖更广泛的场景。<br/>3. **模型性能分析**：通过SVeritas评估先进SV模型，发现其在跨语言、年龄不匹配及编解码压缩等场景下显著性能下降。<br/>4. **群体差异研究**：分析不同年龄、性别、语言背景的用户群体，揭示模型鲁棒性存在系统性差异。<br/>5. **标准化评估框架**：为SV系统提供真实且合成的统一测试环境，支持模型弱点精准诊断与公平性改进。|
|2509.17091v1|[SVeritas: Benchmark for Robust Speaker Verification under Diverse   Conditions](http://arxiv.org/abs/2509.17091v1)|**总结（100字以内）:**  <br/>提出SVeritas基准套件，全面评估语音识别系统在复杂现实场景下的鲁棒性，涵盖跨语言、年龄、编解码器等新挑战，揭示模型性能偏差及人群差异，推动更可靠、公平的声纹识别技术发展。<br/><br/>**贡献点:**  <br/>1. **提出SVeritas基准套件**：首次构建覆盖多种现实挑战（如噪声、混响、信道差异、压缩等）和新兴场景（如跨语言、年龄、编解码器）的全面语音识别基准，填补现有研究空白。  <br/>2. **引入新评估维度**：包含之前未被基准化的重要条件（如年龄、跨语言、对抗攻击），扩展了传统语音识别测试的范围。  <br/>3. **揭示模型局限性**：通过SVeritas评估现有先进模型，发现其在特定场景（如跨语言、年龄不匹配、编解码器压缩）下性能显著下降。  <br/>4. **分析人群公平性**：首次量化不同年龄、性别、语言背景下的模型鲁棒性差异，推动对算法公平性的研究。  <br/>5. **标准化评估流程**：通过合成与真实场景结合的标准化测试，赋能模型缺陷诊断，并为构建可靠、公平的语音识别系统提供基础。|
|2509.17052v2|[Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for   Large-scale Dataset Cleansing](http://arxiv.org/abs/2509.17052v2)|**贡献点分点：**  <br/>1. **提出Sidon模型**：首个面向多语言场景的快速、开源语音修复系统，解决野外噪声语音到专业级语音的转换问题。  <br/>2. **双模型架构**：结合w2v-BERT 2.0微调的特征预测器（用于清洗噪声）与声码器（用于合成），提升修复效果。  <br/>3. **性能与效率**：修复质量接近Google内部模型Miipher，单GPU运行速度达实时的500倍，显著优化计算效率。  <br/>4. **零样本应用**：证明使用Sidon清洗的ASR语料训练TTS系统可提升合成质量，拓展跨语言语音合成的泛化能力。  <br/>5. **开源促进研究**：提供代码和模型，推动语音修复与合成领域的可复现性研究。  <br/><br/>**总结（100字内）：**  <br/>提出Sidon，一种高效、开源的多语言语音修复模型，通过特征预测与声码器协同工作，性能媲美Google内部系统，提升TTS质量，并开源资源助力研究社区。|
|2509.17052v1|[Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for   Large-scale Dataset Cleansing](http://arxiv.org/abs/2509.17052v1)|**贡献点**：  <br/>1. **提出Sidon模型**：首个开源、支持多语言的语音修复模型，可将野外噪声语音转化为高质量语音，扩展性达数十种语言。  <br/>2. **创新模型架构**：结合w2v-BERT 2.0微调特征预测器（清洗噪声语音特征）与高效声码器（合成修复语音），实现端到端修复流程。  <br/>3. **性能与效率优势**：修复效果媲美Google内部的Miipher模型，单GPU运行速度达实时的3,390倍，显著提升计算效率。  <br/>4. **TTS应用优化**：验证Sidon清洗后的ASR语料可提升TTS模型合成质量，尤其在零样本设置下表现优异。  <br/>5. **开源促进研究**：开放代码和模型，推动语音合成领域的可重复实验与数据清洗方法研究。  <br/><br/>**总结**：  <br/>提出Sidon，一种高效、多语言的语音修复模型，可将野外噪音语音提升至专业质量，开源促进研究。|
|2509.17021v1|[Bridging the gap between training and inference in LM-based TTS models](http://arxiv.org/abs/2509.17021v1)|**贡献点：**  <br/>1. 提出提示引导的混合训练方案，结合教师强制与自由运行，缓解语言模型基TTS系统中的暴露偏差问题。  <br/>2. 引入自生成token到训练流程，提升训练与推理的一致性，缩小训练-推理差距。  <br/>3. 设计EOS（End-of-Sentence）预测机制，动态检测错误序列终止并控制自由运行过程。  <br/>4. 通过实验验证暴露偏差对长文本语音合成的影响，证明方法有效提升合成质量。  <br/><br/>**总结：**  <br/>本研究提出混合训练方案和EOS预测机制，缓解语言模型TTS的暴露偏差问题，提升了长文本语音合成质量。|
|2509.17006v1|[MBCodec:Thorough disentangle for high-fidelity audio compression](http://arxiv.org/abs/2509.17006v1)|总结：  <br/>本研究提出MBCodec，通过分层结构和自监督技术实现音频语义与声学特征的解耦，显著提升TTS语音压缩质量与效率，达到170倍压缩和2.2kbps低比特率，实验表现优于现有基线。<br/><br/>贡献点：  <br/>1. **提出MBCodec框架**：基于Residual Vector Quantization（RVQ）设计多码本结构，实现分层语义-声学解耦表示。  <br/>2. **自监督语义分词机制**：结合音频子带特征与自监督方法构建功能解耦的潜在空间。  <br/>3. **动态训练策略**：通过自适应Dropout深度分层微调码本，增强编码嵌入空间的学习全面性。  <br/>4. **多通道PQMF编码**：在训练中引入多通道伪正交镜像滤波器，优化频谱特征提取。  <br/>5. **突破性压缩性能**：实现24kHz语音的170倍压缩（2.2kbps比特率）与近无损重建质量。  <br/>6. **实验验证优势**：在多个评估指标上超越现有基线模型，证明方法的有效性。|
|2509.16994v1|[Attentive AV-FusionNet: Audio-Visual Quality Prediction with Hybrid   Attention](http://arxiv.org/abs/2509.16994v1)|总结：  <br/>该论文提出一种结合学习型音频特征与手工视频特征的新型语音质量预测模型，通过注意力机制增强跨模态交互，引入模态相关性估计器实现自适应码率分配，在多样化内容上提升预测准确性和鲁棒性。<br/><br/>贡献点：  <br/>1. **混合特征融合**：整合学习到的Generative Machine Listener (GML)音频特征与手工设计的Video Multimethod Assessment Fusion (VMAF)视频特征，突破单一模态预测局限。  <br/>2. **上下文感知机制**：利用注意力机制捕捉音频-视频跨模态交互及模态内部关系，构建更精确的多模态质量表征。  <br/>3. **模态相关性建模**：提出模态相关性估计器，量化不同模态对内容质量的贡献，为自适应比特率分配提供依据。  <br/>4. **鲁棒性优化**：通过深度学习框架提升模型对多样化内容的适应能力，增强预测稳定性。  <br/>5. **实验验证**：在多类型内容上验证模型有效性，证明其相比传统方法在精度和鲁棒性上的显著提升。|
|2509.16195v1|[FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal   Distillation](http://arxiv.org/abs/2509.16195v1)|**贡献点：**  <br/>1. 提出FocalCodec-Stream，基于焦点调制的混合音频编码器，实现低比特率流式传输。  <br/>2. 首次将语音压缩至0.55-0.80 kbps，并具备80 ms理论延迟，突破传统非流式编码器的限制。  <br/>3. 融合多阶段因果蒸馏WavLM与轻量级优化模块，在延迟约束下显著提升重建质量。  <br/>4. 实验证明其在同等比特率下性能优于现有流式编码器，同时保留语义与声学信息。  <br/>5. 开源代码与预训练模型，推动语音压缩领域研究与实际应用。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出FocalCodec-Stream，基于焦点调制的混合编码器，实现低比特率流式传输，兼顾重建质量与效率，并开源代码促进应用。|
|2509.16193v1|[Are Multimodal Foundation Models All That Is Needed for Emofake   Detection?](http://arxiv.org/abs/2509.16193v1)|总结：  <br/>本研究提出SCAR框架，通过多模态基础模型（MFMs）的协同融合与交叉注意力机制，显著提升情感伪造检测性能，超越单模型与传统方法。<br/><br/>贡献点：  <br/>1. **提出MFMs优于AFMs的假设**：论证多模态预训练模型能更好捕捉情感表达的跨模态不一致性，提升伪造检测准确性。  <br/>2. **设计SCAR融合框架**：引入嵌套交叉注意力机制（分两阶段交互）与自注意力精炼模块，增强跨模态信息交换与噪声抑制。  <br/>3. **验证性能优势**：通过对比实验，证明MFMs在EFD任务中优于SOTA音频模型，且SCAR融合策略达到当前最优效果。|
|2509.16182v1|[Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are   Paralinguistic Pre-Trained Representations Sufficient?](http://arxiv.org/abs/2509.16182v1)|总结（100字以内）：  <br/>本研究指出现有跨语料库SER基准忽视PSP预训练模型的不足，验证了PSP模型在SER任务中的优势，提出TRILLsson表现最佳，并呼吁将PSP模型纳入SER评估，提升基准可信度和模型选择指导性。<br/><br/>贡献点：  <br/>1. 指出现有跨语料库SER基准测试未纳入PSP预训练模型，揭示其可靠性局限。  <br/>2. 提出PSP预训练模型在跨语料库SER中可能表现更优的假设，并系统验证。  <br/>3. 通过实验对比Paralinguistic、Monolingual、Multilingual及Speaker Recognition等PTM类型，证明TRILLsson在PSP领域表现突出。  <br/>4. 建议未来SER基准应纳入PSP专用模型，推动更科学的模型评估与任务适配性研究。|
|2509.16023v1|[Interpreting the Role of Visemes in Audio-Visual Speech Recognition](http://arxiv.org/abs/2509.16023v1)|总结：本研究通过可视化和探针方法分析AV-HuBERT模型中视觉模态的作用，揭示模态间协同效应，提出利用视觉信息提升AVSR性能的新策略。<br/><br/>贡献点：<br/>1. 首次应用t-SNE可视化AV-HuBERT模型中viseme的特征编码，发现视觉线索主导的自然聚类现象及音频的优化作用<br/>2. 通过模态探针实验，量化分析音频对视觉特征表示的增强效果，尤其针对视觉模糊或欠采样的viseme<br/>3. 揭示视觉模态与音频模态在AVSR中的互补与协同机制，为模型设计和多模态信息融合提供理论依据|
|2509.16010v1|[Fed-PISA: Federated Voice Cloning via Personalized Identity-Style   Adaptation](http://arxiv.org/abs/2509.16010v1)|**贡献点总结（100字以内）**  <br/>开发Fed-PISA框架，通过分离LoRA机制降低通信成本并保留音色，结合协同过滤优化模型异质性，有效提升语音克隆的风格表达与个性化效果。<br/><br/>**详细贡献点**  <br/>1. **提出Fed-PISA框架**：针对联邦学习在语音克隆（TTS）中的高通信成本和风格同质化问题，设计了一种隐私保护且协作的个性化语音合成方案。  <br/>2. **引入分离LoRA机制**：将说话者音色（ID-LoRA）保留在本地，仅传输轻量级风格参数（style-LoRA），显著减少参数交换量。  <br/>3. **设计协同过滤启发的聚合方法**：通过学习风格相似的客户端，动态生成定制化模型，增强系统对风格多样性与用户个性化需求的适应性。  <br/>4. **实验证明有效性**：在保持低通信成本的前提下，Fed-PISA在风格表达、自然度和说话者相似性等指标上优于现有联邦学习基线。|
|2509.15969v1|[VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency](http://arxiv.org/abs/2509.15969v1)|总结：VoXtream提出了一种自回归零样本流式TTS系统，通过创新架构实现超低延迟（102ms），并在小数据集上超越大模型基线，支持实时语音生成。<br/><br/>贡献点：<br/>1. **首次提出零样本流式TTS框架**：实现从首个词即开始语音生成，无需等待完整输入。<br/>2. **创新的动态前瞻机制**：无需延迟起始，提升实时性与交互体验。<br/>3. **三模态Transformer架构**： <br/>   - 增量音素Transformer处理输入<br/>   - 时间Transformer预测语义与持续时间<br/>   - 深度Transformer生成声学特征<br/>4. **行业领先的初始延迟**：GPU上实现102ms超低延迟，公开系统中最低。<br/>5. **小数据高性能**：仅需9k小时数据即可匹配或超越更大基线模型的多项指标。<br/>6. **全场景质量保障**：在输出端与全流式场景均保持竞争力的语音质量。|
|2509.15222v1|[Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition   and Fingering Annotation](http://arxiv.org/abs/2509.15222v1)|**贡献点：**  <br/>1. 提出集成式多模态数据采集工具包，解决钢琴表演数据获取的瓶颈问题。  <br/>2. 开发PiaRec GUI实现音频、视频、MIDI及元数据的同步采集。  <br/>3. 开发ASDF GUI实现视觉数据中演奏者指法的高效标注。  <br/>4. 通过系统整合，显著提升多模态钢琴表演数据集的构建效率。  <br/><br/>**总结：**  <br/>该研究提出集成工具包PiaRec和ASDF，高效解决多模态钢琴表演数据采集与标注难题，推动相关领域研究进展。|
|2509.15210v1|[Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR   Generation](http://arxiv.org/abs/2509.15210v1)|**贡献点：**  <br/>1. **提出MiNAF方法：** 引入显式几何特征（房间网格和距离分布）作为本地上下文的直接表示，弥补传统神经隐式方法对环境几何信息的不足。  <br/>2. **性能验证：** 通过与主流和最新基线方法的对比，证明MiNAF在多种评估指标下具有竞争力，并在训练样本有限时仍保持高鲁棒性。  <br/>3. **推动高保真声模拟：** 实现了对房间脉冲响应（RIR）更准确的预测，显著提升了声场模拟的真实感和质量。|
|2509.15140v1|[FCPE: A Fast Context-based Pitch Estimation Model](http://arxiv.org/abs/2509.15140v1)|总结：  <br/>提出高效抗噪的FCPE模型，结合Lynx-Net架构和深度可分离卷积，在保持低计算成本的同时达到高准确率和低延迟，代码开源。<br/><br/>贡献点：  <br/>1. **提出FCPE模型**：结合Lynx-Net架构与深度可分离卷积，有效捕捉梅尔频谱特征，解决噪声下音高估计性能退化问题。  <br/>2. **性能表现**：在MIR-1K数据集上实现96.79%的Raw Pitch Accuracy (RPA)，与现有SOTA方法性能相当。  <br/>3. **计算效率**：单张RTX 4090 GPU下的Real-Time Factor (RTF)为0.0062，显著优于传统算法。  <br/>4. **开源实现**：提供完整代码，便于复现与实际应用。|
|2509.15085v1|[Real-Time Streaming Mel Vocoding with Generative Flow Matching](http://arxiv.org/abs/2509.15085v1)|总结：  <br/>提出流式Mel vocoder MelFlow，结合生成流匹配和伪逆滤波器技术，实现16kHz语音低延迟生成（算法32ms，总48ms），并在消费级GPU上验证实时性，优于非流式基线模型HiFi-GAN的语音质量。<br/><br/>贡献点：  <br/>1. **提出新的流式生成框架**：基于生成流匹配（generative flow matching）和伪逆Mel滤波器银行，构建了MelFlow模型，首次实现流式Mel vocoding。  <br/>2. **低延迟音频生成**：达到算法延迟32ms、总延迟48ms，满足实时语音合成需求，算法效率显著优于传统方法。  <br/>3. **实证验证实时性**：在消费级笔记本GPU上实现实际流式处理，证明其在硬件资源受限下的可行性。  <br/>4. **语音质量提升**：在PESQ和SI-SDR指标上超越HiFi-GAN等非流式基线模型，展示模型在语音生成质量上的优势。|
|2509.15008v1|[Transfer Learning for Paediatric Sleep Apnoea Detection Using   Physiology-Guided Acoustic Models](http://arxiv.org/abs/2509.15008v1)|总结：  <br/>该论文提出一种迁移学习框架，利用成人睡眠数据预训练模型检测儿童OSA，创新性结合SpO2数据并系统评估三种训练策略，验证了其在家庭筛查中的有效性与临床价值。|
|2509.15001v1|[BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting   Speakers in Child-Centered Long-Form Recordings](http://arxiv.org/abs/2509.15001v1)|**总结（100字以内）:**  <br/>该论文提出BabyHuBERT，首个基于儿童长时录音的自监督语音模型，有效解决儿童与成人语音差异问题，在多语言场景和少数语言中表现优异，并开源模型与代码，为儿童语音研究提供基础框架。<br/><br/>**贡献点分点列出:**  <br/>1. **首个儿童语音自监督模型**：BabyHuBERT是首个专门训练于13,000小时多语言儿童长时录音的自监督表示模型，填补了该领域空白。  <br/>2. **跨语言适应性**：模型覆盖40种语言，支持多语言儿童语音研究，提升对低资源语言的处理能力。  <br/>3. **核心任务性能突破**：在说话人分割任务中，显著优于传统模型（如HuBERT和W2V2-LL4300），F1分数提升13.2-15.9个百分点。  <br/>4. **开源促进研究**：提供代码和模型，为儿童语音分析及相关下游任务（如语言理解、语音识别）提供可复用的基础框架。|
|2509.14959v1|[Discrete optimal transport is a strong audio adversarial attack](http://arxiv.org/abs/2509.14959v1)|贡献点总结（100字内）：  <br/>提出离散最优传输（DOT）作为高效黑盒对抗攻击方法，通过frame-level WavLM嵌入与熵最优传输实现分布对齐，结合神经声码器生成对抗样本。在ASVspoof数据集上验证了DOT的跨数据集迁移能力与稳定性，优于传统攻击方法，并揭示了声码器重叠对攻击效果的关键影响。|
|2509.14946v3|[SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding](http://arxiv.org/abs/2509.14946v3)|**贡献点总结（100字以内）:**  <br/>提出首个自动化生成大规模旁语言数据集的方法，并构建SynParaSpeech数据集，包含自然对话与精确时间戳，提升语音生成真实性及理解准确性，数据集开放共享。<br/><br/>**分点贡献:**  <br/>1. **首个自动化框架**：开发了首个用于生成大规模旁语言数据的自动化框架，突破依赖专有数据集的限制。  <br/>2. **高质量数据集构建**：创建SynParaSpeech数据集，涵盖6类旁语言现象，总时长118.75小时，并确保语音完整性和精确时间戳。  <br/>3. **现实相关性提升**：数据源自自然对话，增强实际应用场景的适配性，解决现有公开资源缺乏真实语境的不足。  <br/>4. **应用价值拓展**：通过自然旁语言合成改进语音生成效果，并通过优化旁语言事件检测提升语音理解能力。  <br/>5. **开源共享**：开放数据集及音频样本，促进研究社区的协作与验证（GitHub链接提供）。|
|2509.14946v1|[SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding](http://arxiv.org/abs/2509.14946v1)|**贡献点：**<br/>1. 提出首个自动化生成副语言数据的框架，解决依赖专有数据集的限制。  <br/>2. 构建 SynParaSpeech 数据集，包含 6 类副语言声音（如笑声、叹气）和 118.75 小时高精度时间戳的自然对话数据。  <br/>3. 数据集显著提升语音生成的自然性与语音理解的副语言事件检测能力。  <br/>4. 公开发布数据集及音频资源，推动语音领域研究与应用。  <br/><br/>**总结（100字以内）：**  <br/>本文提出首个自动化副语言数据生成框架，构建大规模公开数据集 SynParaSpeech，并通过其提升语音生成自然性和理解准确性，推动语音技术发展。|
|2509.14944v1|[Estimating Respiratory Effort from Nocturnal Breathing Sounds for   Obstructive Sleep Apnoea Screening](http://arxiv.org/abs/2509.14944v1)|总结：  <br/>本文提出基于夜间音频直接估计呼吸努力的新方法，通过潜在空间融合框架提升OSA检测性能，仅需智能手机音频即可实现无传感器、可扩展的长期监测。<br/><br/>**贡献点：**  <br/>1. **首次实现音频直接呼吸努力估计**：无需接触式传感器，通过夜间音频提取生理信息，解决传统方法的舒适性与可扩展性问题。  <br/>2. **潜在空间融合框架**：创新性地将呼吸努力嵌入与声学特征整合，提升OSA检测的敏感性和AUC指标。  <br/>3. **居家环境数据验证**：在157晚的真实家庭记录数据上测试，证明方法适用于实际场景。  <br/>4. **低AHI阈值下性能优化**：在低阻塞性睡眠呼吸暂停-低通气指数（AHI）阈值下表现优于纯音频基线。  <br/>5. **无传感器可行性**：仅依赖智能手机音频，实现低成本、可规模化、长期的OSA监测。|
|2509.14934v1|[Mitigating data replication in text-to-audio generative diffusion models   through anti-memorization guidance](http://arxiv.org/abs/2509.14934v1)|总结（100字以内）：  <br/>该研究提出AMG方法，通过三种反记忆策略减少文本到音频扩散模型的数据复制，实验证明有效抑制记忆，同时保持生成质量。<br/><br/>贡献点分点列出：<br/>1. **提出反记忆策略框架**：首次将反记忆化策略引入文本到音频扩散模型，通过修改采样过程抑制训练数据记忆，解决生成模型中普遍存在的数据复制问题。  <br/>2. **设计三类指导机制**：探索三种具体的反记忆化引导方法，在降低数据复制的同时，保持生成音频的保真度和语义对齐，优化生成质量与避免记忆的平衡。  <br/>3. **基于开源模型验证**：选用Stable Audio Open作为基线模型，利用其全开源架构和训练数据，确保方法的可复现性和实验的严谨性。  <br/>4. **实验证明有效性**：通过全面的实验分析，验证AMG在抑制模型记忆化方面显著有效，且不影响音频生成的保真度和语言语义一致性。|
|2509.14912v1|[Back to Ear: Perceptually Driven High Fidelity Music Reconstruction](http://arxiv.org/abs/2509.14912v1)|总结（100字以内）:  <br/>本文提出εar-VAE模型，通过引入听觉感知滤波器、双相位损失函数及改进的频谱监督方式，解决现有VAE在音频生成中相位精度和空间表示的不足，显著提升高频率谐波与立体声特性重建效果。<br/><br/>贡献点:  <br/>1. **K-加权感知滤波器**：在损失计算前应用，使训练目标与人耳听觉感知保持一致，提升相位准确性。  <br/>2. **双相位损失函数**：  <br/>   - 提出相关性损失，增强立体声一致性；  <br/>   - 设计基于瞬时频率与群延时的相位损失，提升相位精度。  <br/>3. **改进的频谱监督范式**：  <br/>   - 幅度由Mid/Side/Left/Right四通道联合监督；  <br/>   - 相位仅通过Left/Right通道监督，优化空间特征重建。|
|2509.14893v1|[Temporally Heterogeneous Graph Contrastive Learning for Multimodal   Acoustic event Classification](http://arxiv.org/abs/2509.14893v1)|**贡献点：**<br/>1. 提出THGCL框架，解决多模态声学事件分类中的时空对齐与跨模态噪声问题。<br/>2. 构建事件级时间图，以音视频片段为节点、时序关系为边，明确区分模态内与模态间时序依赖。<br/>3. 引入高斯过程（Gaussian Processes）建模模态内平滑性，Hawkes过程建模模态间衰减特性。<br/>4. 结合对比学习捕捉多模态细粒度关联，提升特征融合效果。<br/>5. 在AudioSet数据集上实现SOTA性能，验证方法有效性。<br/><br/>**总结：**  <br/>论文提出THGCL框架，通过时间图建模和对比学习，有效解决多模态声学分类中的时空对齐与噪声问题，取得最佳性能。|
|2509.14891v1|[Music4All A+A: A Multimodal Dataset for Music Information Retrieval   Tasks](http://arxiv.org/abs/2509.14891v1)|总结：  <br/>提出Music4All Artist and Album多模态数据集，支持艺术家/专辑层级的音乐信息检索任务，包含元数据、图像、文本等多模态信息，并通过跨领域实验揭示图像在类型分类中的重要性，推动多模态音乐推荐研究。<br/><br/>贡献点：  <br/>1. **提出新数据集**：构建Music4All A+A数据集，首次覆盖音乐艺术家和专辑层级，突破传统以单曲为单位的多模态数据局限。  <br/>2. **多模态资源整合**：整合元数据、图像表示、文本描述及轨道级交互数据，增强跨模态分析的灵活性。  <br/>3. **层级化分类支持**：为艺术家和专辑提供粒度更细的类别标签，适配多层级音乐信息检索任务需求。  <br/>4. **跨领域对比实验**：通过与电影领域的类型分类对比，验证图像模态在音乐分类中的有效性及模型泛化挑战。  <br/>5. **开源开放共享**：提供完整代码与数据集，促进研究复现与扩展，采用CC BY-NC-SA 4.0开源协议。|
|2509.14789v1|[Acoustic Simulation Framework for Multi-channel Replay Speech Detection](http://arxiv.org/abs/2509.14789v1)|总结：本研究提出多通道回声语音攻击的声学模拟框架，通过环境建模与噪声评估提升检测器泛化能力。<br/><br/>贡献点：  <br/>1. **提出多通道仿真框架**：首次构建基于公开资源的多通道回声语音攻击模拟系统，兼容真实与伪造语音场景。  <br/>2. **环境建模创新**：融入真实麦克风/扬声器脉冲响应、房间声学及噪声条件，提升模拟的真实性与多样性。  <br/>3. **方向性数据应用**：利用测量扬声器方向性信息，增强仿真中声源与接收端的空间特性还原度。  <br/>4. **新型欺骗场景定义**：区分"回声语音"与"无回声语音"的欺骗设置，量化不同环境因素对检测性能的影响。  <br/>5. **验证合成数据有效性**：基于M-ALRAD模型实验证明，合成数据可显著提升检测器对未见过环境的泛化能力。|
|2509.14785v1|[Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for   Multi-Source Conditions](http://arxiv.org/abs/2509.14785v1)|总结：提出Spatial-CLAP模型结合内容感知空间编码器与空间对比学习策略，有效处理多源音频-文本嵌入问题，并验证其在复杂场景下的优势，建立新范式。<br/><br/>贡献点：<br/>1. **引入内容感知空间编码器**：首次将音频内容与空间信息显式结合，实现音频源位置与语义的联合建模。<br/>2. **提出空间对比学习（SCL）**：设计针对多源条件的训练策略，通过强化源-位置对应关系提升嵌入鲁棒性。<br/>3. **验证多源场景有效性**：在多源混合音频（包括未见过的三源混合）上的实验表明，SCL显著优于传统单源训练范式。|
|2509.14784v1|[MELA-TTS: Joint transformer-diffusion model with representation   alignment for speech synthesis](http://arxiv.org/abs/2509.14784v1)|总结（100字以内）:  <br/>MELA-TTS提出联合变压器-扩散框架，实现端到端语音合成，无需分词和多阶段处理。引入表示对齐模块提升跨模态一致性，实验表明其在多种指标上达到SOTA，具有强大的零样本语音克隆能力。  <br/><br/>贡献点：  <br/>1. **首次联合Transformer与扩散模型**：构建端到端TTS框架，直接生成连续mel频谱图，摆脱传统分词及多阶段流程。  <br/>2. **表示对齐模块**：通过训练时对齐解码器输出与预训练ASR的语义嵌入，解决连续特征建模难题，增强文本-语音跨模态一致性。  <br/>3. **高效训练机制**：加速训练收敛，提升模型泛化能力。  <br/>4. **多场景性能优势**：在离线和流式合成模式下均表现优异，且具备强鲁棒性的零样本语音克隆能力。  <br/>5. **新基准设定**：为连续特征生成TTS方法树立新标杆，提供替代离散token范式的有效方案。|
|2509.14764v1|[Efficient Solutions for Mitigating Initialization Bias in Unsupervised   Self-Adaptive Auditory Attention Decoding](http://arxiv.org/abs/2509.14764v1)|**贡献点：**  <br/>1. 提出三种计算高效、无监督的听觉注意力解码（AAD）方法，克服了现有方法对真实标签的依赖。  <br/>2. 有效缓解初始化偏差问题，在性能上接近传统无偏方法，但显著降低计算复杂度（与数据量无关）。  <br/>3. 实现无需用户特定校准的通用性，提升多说话人环境下的实际应用可行性。  <br/>4. 开源代码，便于复现和推广，推动神经引导听力设备的研究与开发。  <br/><br/>**总结（100字以内）：**  <br/>本文提出三种高效无监督AAD方法，解决初始化偏差并降低计算成本，无需用户校准，助力神经引导听力设备应用，代码已开源。|
|2509.14737v1|[Pushing the Limits of End-to-End Diarization](http://arxiv.org/abs/2509.14737v1)|总结：  <br/>本研究提出EEND-TA模型，在多个语音数据集上达到新的说话人分割基准，尤其在DIHARD III中实现14.49%的DER，通过8人模拟混合物预训练提升模型容量和泛化性，同时保持推理效率。<br/><br/>贡献点：  <br/>1. **提出统一非自回归模型**：首次构建EEND-TA单模型架构，实现端到端说话人分割，显著提升DER性能（如DIHARD III达14.49%）。  <br/>2. **创新预训练方法**：通过8人模拟混合物预训练，增强对多样化说话人配置的表征能力，优化模型泛化性。  <br/>3. **多数据集验证**：在AliMeeting-far/near、AMI-Mix/SDM、DIHARD III、MagicData RAMC等公开数据集上刷新基准，证明方法普适性。  <br/>4. **效率与性能平衡**：在保持高效推理速度的同时，超越多数现有说话人分割方案，展现模型的实用价值。|
|2509.14684v1|[DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware   Text-to-Speech Synthesis](http://arxiv.org/abs/2509.14684v1)|总结（100字以内）:  <br/>DAIEN-TTS提出一种零样本TTS框架，通过解耦语音-环境分离模块与双掩码策略，实现对音色和环境音的独立控制，并引入双无类引导和SNR自适应策略，显著提升环境感知语音合成的自然度与环境保真度。<br/><br/>贡献点分点列出:  <br/>1. **零样本环境感知TTS框架**：首次构建支持环境场景感知的零样本TTS系统，无需环境数据训练即可合成个性化环境语音。  <br/>2. **解耦音频填充技术**：引入语音-环境分离模块（SES），将环境音与语音分离为独立的mel-spectrogram，实现语音与环境音的并行生成。  <br/>3. **双掩码与文本条件控制**：设计长度可变的随机span掩码，结合文本嵌入作为条件，同时恢复语音与环境音的缺失部分。  <br/>4. **双无类引导策略（DCFG）**：通过独立指导语音和环境组件生成，提升对说话人音色和环境音的可控性。  <br/>5. **SNR自适应对齐方法**：引入信号-噪声比（SNR）调整策略，使合成语音与环境提示在声学特征上更匹配。  <br/>6. **高保真环境合成能力**：实验验证框架在自然度、说话人相似性及环境保真度方面均优于现有方法。|
|2509.14677v1|[SpeechMLC: Speech Multi-label Classification](http://arxiv.org/abs/2509.14677v1)|总结：  <br/>提出多标签说话风格分类框架，整合交叉注意力机制与数据增强技术，解决数据不平衡问题，并分析人类标注一致性对模型的影响，提升通用人机交互应用的性能。<br/><br/>贡献点：  <br/>1. **多标签统一框架**：首次构建可同时检测多种说话风格的分类框架，突破传统单标签检测的局限。  <br/>2. **交叉注意力机制**：在Transformer解码器中引入跨注意力，有效提取多目标标签的显著语音特征。  <br/>3. **数据增强方法**：基于语音生成模型设计数据增强策略，缓解多标签数据集中的类别不平衡问题。  <br/>4. **人类感知分析**：定量评估人类标注一致性对分类准确率的影响，为模型优化提供心理学依据。  <br/>5. **全面验证实验**：在seen和unseen语料库上进行多维度客观评估，验证模型的泛化能力与实用性。|
|2509.14675v1|[How Does Instrumental Music Help SingFake Detection?](http://arxiv.org/abs/2509.14675v1)|**贡献点总结：**  <br/>本研究揭示了乐器伴奏在SingFake检测中主要作为数据增强而非语音特征，微调使模型更依赖浅层说话人特征而忽视内容与语义信息，为设计更可解释和稳健的检测系统提供新视角。<br/><br/>**分点贡献：**  <br/>1. **明确伴奏作用机制**：首次揭示乐器伴奏在SingFake检测中主要起数据增强作用，而非提供节奏、和声等内在语音特征。  <br/>2. **多维度模型分析**：从行为效应（如不同模型结构、伴奏类型、频率子带）和表征效应（如微调对编码器能力的影响）双视角研究模型运作机制。  <br/>3. **微调效果新发现**：发现微调会降低模型对内容、副语言及语义信息的敏感度，增强其对浅层说话人特征的依赖。  <br/>4. **指导模型优化**：提供了对语音与乐器线索利用方式的深入理解，为改进模型的可解释性和检测鲁棒性提供理论依据。|
|2509.14659v1|[Aligning Audio Captions with Human Preferences](http://arxiv.org/abs/2509.14659v1)|**贡献点总结：**  <br/>1. 提出基于RLHF的偏好对齐音频字幕框架，减少对成对标注数据的依赖。  <br/>2. 构建CLAP驱动的奖励模型，利用人类反馈捕捉细微偏好。  <br/>3. 无需真实字幕标注即可微调基线模型，提升灵活性。  <br/>4. 通过多数据集评估验证方法有效性，尤其在基线模型失效的场景表现更优。  <br/>5. 实现与监督方法相当的性能，证明其在实际场景中的可扩展性与实用性。  <br/><br/>**总结（100字内）：**  <br/>本研究提出基于RLHF的音频字幕框架，结合CLAP奖励模型捕捉人类偏好，无需真实标注即可优化基线模型。实验验证其在复杂场景下优于传统方法，且性能与监督方法相当，为语音字幕生成提供了更高效、更贴近需求的解决方案。|
|2509.14650v1|[Enhancing Situational Awareness in Wearable Audio Devices Using a   Lightweight Sound Event Localization and Detection System](http://arxiv.org/abs/2509.14650v1)|总结（100字以内）:  <br/>本研究提出结合ASC和SELD的环境智能框架，通过动态调整SEL模型敏感度提升听觉设备的环境感知能力，增强安全性与上下文意识。  <br/><br/>贡献点:  <br/>1. 提出首个将环境语境感知（ASC）与声音定位检测（SELD）结合的智能框架，解决ANC设备环境感知不足问题。  <br/>2. 引入轻量级ASC模型，实现高效环境分类以支持实时动态SEL调整。  <br/>3. 开发基于场景预测的动态敏感度调节机制，提升关键环境声音的检测与定位准确性。  <br/>4. 通过模拟数据验证框架有效性，证明其在ANC场景下的空间智能显著优于传统方法。  <br/>5. 推动智能听觉设备发展，为平衡舒适性与环境感知提供技术路径，提升安全性和上下文感知体验。|
|2509.14632v1|[Mitigating Intra-Speaker Variability in Diarization with   Style-Controllable Speech Augmentation](http://arxiv.org/abs/2509.14632v1)|总结：  <br/>提出风格可控的语音生成模型，解决说话人分割系统中因情绪、健康等因素导致的说话人身份混淆问题，在模拟情感数据集和AMI数据集上分别降低错误率49%和35%。<br/><br/>贡献点：  <br/>1. 提出风格可控的语音生成模型，有效应对说话人内在变化（如情绪、健康、语速等）带来的分割挑战。  <br/>2. 通过生成多样化风格样本（语音与风格多样性）增强数据表现力，保留说话人身份特征。  <br/>3. 创新性融合原始与生成音频的说话人嵌入，提升分割系统的鲁棒性与聚类能力。  <br/>4. 在模拟情感数据集和AMI数据集上验证方法有效性，实现显著误差率降低（49%和35%）。|
|2509.14579v1|[Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech   Synthesis](http://arxiv.org/abs/2509.14579v1)|总结：  <br/>该论文提出Cross-Lingual F5-TTS框架，通过消除对参考转录本的依赖，实现跨语言语音克隆，解决了当前流匹配TTS模型在无转录本场景下的局限性。<br/><br/>贡献点：  <br/>1. **提出无需参考转录本的跨语言语音克隆框架**：解决当前流匹配TTS模型在跨语言场景中依赖参考转录本的限制。  <br/>2. **基于强制对齐获取词边界**：通过预处理音频提示获取词边界信息，无需依赖转录本进行训练。  <br/>3. **多粒度说话速率预测器设计**：针对持续时间建模问题，训练不同语言粒度的预测器以自适应确定语音持续时间。  <br/>4. **验证有效性与性能**：实验表明该方法在保留F5-TTS高质量生成能力的同时，成功实现跨语言语音克隆。|
|2509.14479v1|[A long-form single-speaker real-time MRI speech dataset and benchmark](http://arxiv.org/abs/2509.14479v1)|总结：  <br/>本研究发布了一个包含实时MRI视频和同步音频的单说话人语音数据集，提供多类型衍生数据及基准测试结果，推动语音与发音合成研究。<br/><br/>贡献点：  <br/>1. **发布首个长时单说话人MRI语音数据集**：提供约1小时的美国英语母语者实时MRI视频与同步音频，填补了公开单人数据集的空白。  <br/>2. **提供多模态衍生数据**：包含语音区域裁剪视频、句子级分割、恢复降噪音频及兴趣区域时间序列，服务于多种下游任务。  <br/>3. **建立基准测试**：在发音合成与音素识别任务中提供基线性能，为后续研究提供参考对比标准。|
|2509.14161v1|[CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](http://arxiv.org/abs/2509.14161v1)|总结：  <br/>CS-FLEURS是一个面向代码切换语音识别和翻译的多语言数据集，包含测试集和训练集，覆盖113种语言对，支持高/低资源语言研究。<br/><br/>贡献点：  <br/>1. **首个多语言代码切换数据集**：覆盖52种语言，包含113种代码切换语言对，突破高资源语言限制。  <br/>2. **多样化测试集设计**：  <br/>   - 提供4类测试集（14、16、60、45个语言对），涵盖真实语音、生成式文本到语音、低资源语言及拼接式语音。  <br/>   - 多种生成方式（synthetic TTS, generative TTS, concatenative TTS）模拟真实场景。  <br/>3. **训练集支持**：包含128小时生成式文本到语音数据，针对16个X-英语语言对，促进模型训练与泛化能力提升。  <br/>4. **推动领域研究**：通过开放数据集，为代码切换语音识别与翻译的算法创新和资源扩展提供基准与动力。|
|2509.14136v1|[SV-Mixer: Replacing the Transformer Encoder with Lightweight MLPs for   Self-Supervised Model Compression in Speaker Verification](http://arxiv.org/abs/2509.14136v1)|**总结**（100字以内）：  <br/>SV-Mixer首次提出全MLP架构的SSL学生模型，通过多尺度、局部-全局和分组通道混合模块，在保持教师模型性能的同时显著降低参数与计算量，实现高效设备端部署。<br/><br/>**贡献点分点**：  <br/>1. **全MLP架构创新**：首次设计无自注意力机制的全MLP学生编码器，突破传统Transformer架构在设备端部署的限制。  <br/>2. **三模块混合设计**：提出三种轻量模块（多尺度混合、局部-全局混合、分组通道混合），分别解决时序、上下文和频谱特征的学习问题。  <br/>3. **高效蒸馏效果**：在75%压缩率下接近教师模型性能，参数与GMACs减少超50%，实现高准确率与低计算需求的平衡，推动实时设备部署。|
|2509.14069v1|[Lightweight Implicit Neural Network for Binaural Audio Synthesis](http://arxiv.org/abs/2509.14069v1)|总结：提出LINN框架，通过两阶段处理和隐式神经网络IBC模块，在保持高保真质量的同时显著提升计算效率，参数减少72.7%，为边缘设备的双耳音频应用提供新方案。<br/><br/>贡献点：<br/>1. 提出轻量级双耳音频合成框架LINN，解决高保真与计算效率的矛盾<br/>2. 创新性采用时间域变形生成初始估计，结合隐式神经网络IBC进行修正<br/>3. IBC模块直接预测幅度/相位修正，构建高度紧凑的模型架构<br/>4. 实现参数量减少72.7%和MACs显著下降，优于现有最高效方法<br/>5. 在边缘设备场景下保持与最优基线模型相当的感知质量|
|2509.14053v1|[Network representations reveal structured uncertainty in music](http://arxiv.org/abs/2509.14053v1)|总结：  <br/>本研究通过构建和比较八种音乐网络模型，揭示了音乐结构如何引导注意力与感知，支持预测处理理论，并为音乐认知研究提供网络科学视角。<br/><br/>贡献点：  <br/>1. **音乐网络建模**：提出将音乐视为网络结构，探索人类如何编码和处理听觉信息。  <br/>2. **特征选择评估**：系统评估不同特征组合（音高、八度、时长、音程）对网络结构的影响。  <br/>3. **多维度分析方法**：通过拓扑度量、熵分析和认知表示对比，量化模型的结构性与感知效率。  <br/>4. **认知效率发现**：发现简单特征模型更贴合人类感知，复杂模型引入认知低效，支持模块化认知架构。  <br/>5. **不确定性组织机制**：揭示音乐网络通过集中不确定性于特定节点，形成局部熵梯度，驱动紧张-释放的动态体验。  <br/>6. **理论关联**：将研究结果与预测处理和自由能最小化等认知理论连接，提供跨学科支持。  <br/>7. **研究新方向**：提出从网络科学视角研究音乐结构在不同文化、历史和流派中的演化路径。|
|2509.14052v1|[AnyAccomp: Generalizable Accompaniment Generation via Quantized Melodic   Bottleneck](http://arxiv.org/abs/2509.14052v1)|总结（100字以内）:  <br/>提出AnyAccomp框架，通过解耦伴奏生成与源分离伪影，利用量化旋律瓶颈和流匹配模型提升泛化能力，尤其在独奏器乐生成上取得突破，推动音乐创作工具应用。<br/><br/>贡献点分点列出：  <br/>1. **解决Train-Test不匹配问题**：提出AnyAccomp框架，首次将伴奏生成与源分离伪影解耦，避免模型对分离残留的过拟合。  <br/>2. **引入量化旋律瓶颈（Quantized Melodic Bottleneck）**：结合音色图（chromagram）与VQ-VAE，提取离散、音色不变的核心旋律表示，增强旋律分离的鲁棒性。  <br/>3. **基于流匹配模型的伴奏生成**：通过生成模型在提取的鲁棒代码（robust codes）条件下生成伴奏，提升生成质量与多样性。  <br/>4. **显著提升泛化性能**：在清洁人声和独奏乐器数据集上超越基线模型，尤其在生成独奏乐器（如纯音乐）时实现突破。  <br/>5. **推动音乐共创工具发展**：通过解决伴奏生成在乐器任务上的缺陷，为更通用、可交互的音乐创作工具奠定基础。|
|2509.14049v1|[Comprehensive Evaluation of CNN-Based Audio Tagging Models on   Resource-Constrained Devices](http://arxiv.org/abs/2509.14049v1)|总结：  <br/>本文评估多种CNN架构在Raspberry Pi上的部署效果，转换模型至ONNX格式提升便携性，通过长期测试验证性能稳定性，为边缘计算的音频分类提供实用指导。<br/><br/>贡献点：  <br/>1. **系统评估多样CNN架构**：全面对比了PANNs框架的1D/2D模型、ConvNeXt、MobileNetV3以及CNN9/CNN13等最新音频分类模型在资源受限设备上的表现。  <br/>2. **跨平台部署优化**：将所有模型转换为ONNX格式，提升模型在不同硬件平台上的兼容性与部署效率。  <br/>3. **长期稳定性验证**：引入连续24小时推理测试，分析模型在长时间运行中的延迟稳定性和热管理表现，突破传统单次评估局限。  <br/>4. **实用部署建议**：实验表明通过合理选择与优化模型，可在边缘计算场景中实现高效、稳定且可持续的音频分类性能。|
|2509.14003v1|[RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing](http://arxiv.org/abs/2509.14003v1)|总结（100字以内）:  <br/>提出基于修正流匹配的端到端音频编辑框架，构建多事件重叠音频数据集，无需辅助字幕或掩码即可实现语义对齐与高质量编辑，突破传统方法的限制。  <br/><br/>贡献点：  <br/>1. **提出新型扩散框架**：开发基于修正流匹配的端到端高效音频编辑模型，无需额外优化或成本高昂的训练策略。  <br/>2. **构建专用数据集**：创建包含重叠多事件音频的基准数据集，支持复杂场景下的模型训练与性能评估。  <br/>3. **无需辅助信息**：实现文本引导的语义对齐，无需依赖全字幕或人工掩码，提升实用性。  <br/>4. **保持编辑质量**：在多项指标上验证模型的编辑效果，达到与现有方法相当的高质量输出。|
|2509.13989v3|[Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in   Instruction-Guided Expressive Text-To-Speech Systems](http://arxiv.org/abs/2509.13989v3)|总结：  <br/>本研究提出E-VOC数据集，系统分析ITTS指令与感知间的对齐问题，发现gpt-4o-mini-tts在音高维度表现最佳，但多数系统存在指令遵循偏差和细粒度控制不足，为领域发展提供关键洞见。<br/><br/>贡献点：  <br/>1. **首次构建E-VOC语料库**：通过大规模人类评估，系统性揭示ITTS指令-感知差异，填补研究空白。  <br/>2. **双维度感知分析**：量化研究程度副词（degree adverbs）和情感强度（graded emotion intensity）对语音生成的影响。  <br/>3. **模型性能对比**：验证gpt-4o-mini-tts在指令与生成语音对齐度上优于其他系统，作为可靠基准。  <br/>4. **发现指令偏差现象**：五个ITTS系统普遍存在对"儿童/老年"等非成人语音指令的忽视，强调语音生成的局限性。  <br/>5. **提出细粒度控制挑战**：指出ITTS在解读细微属性指令（如年龄、强调）方面仍有显著提升空间。|
|2509.13989v2|[Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in   Instruction-Guided Expressive Text-To-Speech Systems](http://arxiv.org/abs/2509.13989v2)|总结（100字以内）:  <br/>该研究提出E-VOC数据集，分析ITTS在表达维度的感知差异，发现gpt-4o-mini-tts表现最佳，但多数系统存在语音年龄偏差及细粒度控制不足问题，揭示了指令与生成语音之间的对齐挑战。<br/><br/>贡献点分点列出：  <br/>1. **提出感知分析框架**：系统研究ITTS在程度副词和情感强度两个表达维度上的可控性，量化用户指令与语音生成的对齐程度。  <br/>2. **构建大规模评估数据集**：创建E-VOC语料库，涵盖语音年龄和词级强调等属性的人类评分，为研究提供基准资源。  <br/>3. **揭示模型表现差异**：发现gpt-4o-mini-tts在指令与生成语音的声学对齐上表现最优，凸显其可靠性。  <br/>4. **指出系统泛化问题**：五种ITTS系统普遍生成成年语音，而非按指令生成儿童或老年声音，反映模型对语音属性的适应不足。  <br/>5. **强调细粒度控制瓶颈**：证明现有系统在解析细微属性指令（如程度差异）时存在显著困难，需进一步优化。|
|2509.13989v1|[Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in   Instruction-Guided Expressive Text-To-Speech Systems](http://arxiv.org/abs/2509.13989v1)|总结（100字以内）:  <br/>该论文提出E-VOC语料库，通过大规模人类评估揭示ITTS指令与感知之间的差距，发现现有模型在语音控制上存在偏差和挑战，为改进ITTS系统性能提供依据。<br/><br/>贡献点：  <br/>1. **提出感知分析框架**：首次系统分析ITTS可控性在副词程度和情感强度等表达维度上的用户感知差异。  <br/>2. **构建E-VOC数据集**：创建包含大规模人类评价的语料库，填补ITTS领域对语音属性（如年龄、强调）系统研究的空白。  <br/>3. **揭示模型性能问题**：发现主流ITTS系统普遍无法准确响应关于儿童或老年语音的指令，且对细微属性变化的控制能力不足。  <br/>4. **量化指令-感知偏差**：通过实验验证gpt-4o-mini-tts在多维度发音一致性上表现最优，但其他模型存在显著差距。|
|2509.13878v1|[Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake   Detection](http://arxiv.org/abs/2509.13878v1)|总结（100字以内）：  <br/>本研究提出混合LoRA专家框架，通过动态路由机制提升语音深度伪造检测的泛化能力，显著降低领域外误判率，验证了其在对抗新型攻击中的有效性。<br/><br/>贡献点：  <br/>1. **提出Mixture-of-LoRA-experts框架**：首次将低秩适配器（LoRA）与混合专家（MoE）结合，用于解决语音深度伪造检测中的泛化不足问题。  <br/>2. **动态路由机制设计**：开发选择性激活专家的路由策略，使模型能自适应不同深度伪造方法，增强可扩展性。  <br/>3. **提升领域外检测性能**：实验表明，该方法在领域外场景中将平均EER从8.55%降至6.08%，大幅优于传统微调。  <br/>4. **验证通用性与有效性**：在多种深度伪造攻击数据上测试，证明模型具备跨攻击类型、跨数据集的广泛适用性。|
|2509.13853v2|[Noise Supervised Contrastive Learning and Feature-Perturbed for   Anomalous Sound Detection](http://arxiv.org/abs/2509.13853v2)|**总结（100字以内）**  <br/>提出单阶段监督对比学习（OS-SCL）方法与TFgram时频特征，显著降低设备间误报，提升无监督异常声检测性能，在DCASE 2020任务2中取得优异结果，并开源代码便于复现。<br/><br/>**贡献点分点列出：**  <br/>1. **提出OS-SCL方法**：通过在嵌入空间扰动特征和单阶段带噪声监督对比学习，有效解决跨设备同一类型样本的高频误报问题。  <br/>2. **创新TFgram时频特征**：直接从原始音频提取，捕捉关键异常信息，显著提升检测性能（AUC 95.71%等）。  <br/>3. **实验验证性能优势**：在DCASE 2020任务2中使用Log-Mel（94.64% AUC）和TFgram（95.71% AUC）均表现优异，验证方法有效性。  <br/>4. **开源实现**：提供完整代码库，促进方法复现与应用（链接：[www.github.com/huangswt/OS-SCL](www.github.com/huangswt/OS-SCL)）。|
|2509.13853v1|[Noise Supervised Contrastive Learning and Feature-Perturbed for   Anomalous Sound Detection](http://arxiv.org/abs/2509.13853v1)|总结：  <br/>本研究提出OS-SCL方法与TFgram特征，解决无监督异常声音检测中的设备差异误报问题，并在DCASE 2020数据集上取得优异性能。<br/><br/>贡献点：  <br/>1. 提出单阶段监督对比学习框架OS-SCL，通过特征扰动和噪声监督机制缓解设备间同类型样本的误报问题；  <br/>2. 设计时频特征TFgram，直接从原始音频中提取，有效捕捉异常声音的关键信息；  <br/>3. 在DCASE 2020 Task 2上验证方法有效性，使用Log-Mel和TFgram分别实现94.64%和95.71%的AUC性能；  <br/>4. 开源完整实现代码，便于复现与应用。|
|2509.13741v1|[Self-Guided Target Sound Extraction and Classification Through Universal   Sound Separation Model and Multiple Clues](http://arxiv.org/abs/2509.13741v1)|总结：  <br/>该论文提出了一种多阶段自引导框架，通过集成声源分离、分类与提取模块实现自主目标识别，利用闭环迭代机制提升性能，并在DCASE 2025 Task4官方数据集上取得最优结果。<br/><br/>贡献点：  <br/>1. **框架创新**：设计了首个结合USS（通用声源分离）、SC（单标签分类）和TSE（目标声提取）的多阶段自引导系统，实现端到端处理。  <br/>2. **自主性提升**：通过系统内部生成目标信息，无需外部指导即可完成声源分离与标签预测。  <br/>3. **闭环优化机制**：引入分离波形与分类结果的循环反馈流程，实现分离与分类的迭代精细化。  <br/>4. **性能突破**：在官方测试集上，CA-SDRi提升11.00 dB，标签预测准确率达55.8%，超越ResUNetK基线。  <br/>5. **竞赛成果**：框架在DCASE 2025 Task4中取得第一名，验证了其有效性与先进性。|
|2509.13670v1|[A High-Quality and Low-Complexity Streamable Neural Speech Codec with   Knowledge Distillation](http://arxiv.org/abs/2509.13670v1)|总结：  <br/>提出StreamCodec2，通过全因果架构和减少卷积通道实现低延迟、低复杂度语音编码，在保持高质量的同时引入知识蒸馏技术提升模型表现。<br/><br/>贡献点：  <br/>1. **提升效率与质量平衡**：基于StreamCodec的改进，采用全因果架构与降低卷积通道数，实现低延迟（20ms）和低计算复杂度（910 MFLOPs）。  <br/>2. **知识蒸馏补偿策略**：引入非因果高复杂度教师编解码器，通过知识蒸馏技术缓解因果化模型和剪枝带来的音质下降问题。  <br/>3. **轻量化模型设计**：显著降低模型参数量（5.4 M），推动语音编码在实时通信和高效压缩等场景中的实际应用。|
|2509.13667v1|[A Distilled Low-Latency Neural Vocoder with Explicit Amplitude and Phase   Prediction](http://arxiv.org/abs/2509.13667v1)|总结：  <br/>提出DLL-APNet，结合因果卷积与知识蒸馏，实现低延迟高质量语音生成，资源消耗少，性能接近非因果模型。  <br/><br/>贡献点：  <br/>1. **首次明确关注延迟问题**：指出主流声码器忽视延迟这一实时应用关键因素，强调其对用户体验的影响。  <br/>2. **创新架构设计**：通过显式预测幅度与相位谱，并结合逆短时傅里叶变换（iSTFT）重建语音，提出低延迟生成方案。  <br/>3. **因果卷积应用**：采用因果卷积约束信息利用范围，有效降低延迟并保证实时性。  <br/>4. **知识蒸馏策略**：设计非因果教师模型指导因果学生模型，缓解因果约束导致的语音质量下降问题。  <br/>5. **性能验证**：实验表明DLL-APNet在质量上超越其他因果声码器，资源效率更高，且与非因果主流模型质量相当。|
|2509.13658v1|[Assessing Data Replication in Symbolic Music via Adapted Structural   Similarity Index Measure](http://arxiv.org/abs/2509.13658v1)|总结：  <br/>本文提出SSIMuse，首次将图像领域的结构相似性度量（SSIM）应用于符号音乐，提出两种变体以评估作曲和动态表现中的数据复制问题，并验证其检测精确性，强调其伦理与社会意义。<br/><br/>贡献点：  <br/>1. **首次跨领域应用**：将SSIM图像相似性度量方法引入符号音乐领域，填补了对复杂音乐结构（如节奏、音色纹理）评估的空白。  <br/>2. **双变体设计**：开发SSIMuse-B（二进制钢琴卷）与SSIMuse-V（速度基钢琴卷），分别针对音乐创作与动态表现的复制检测需求。  <br/>3. **实验验证有效性**：在合成数据集上证明SSIMuse可可靠检测至少1小节的精确复制，提升音乐生成模型的可监督性。  <br/>4. **伦理与社会讨论**：揭示AI生成音乐复制数据的潜在风险，引发对音乐生成领域伦理、法律及经济影响的广泛关注。  <br/>5. **开源促进研究**：提供代码实现，推动学术界对音乐生成复制检测技术的进一步探索与验证。|
|2509.13581v1|[Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse   Sensors](http://arxiv.org/abs/2509.13581v1)|**贡献点：**  <br/>1. **首次提出针对高性能光学鼠标传感器的侧信道攻击**（Mic-E-Mouse），揭示其可被用于窃听的潜在漏洞。  <br/>2. **发现语音信号与表面振动的关联**：通过音频信号引发的细微振动可被鼠标光学传感器捕获，赋予侧信道攻击可行性。  <br/>3. **无需系统权限即可实现攻击**：用户空间软件可收集并广播敏感信息，突破传统侧信道攻击需系统级权限的限制。  <br/>4. **创新性数据处理技术**：提出结合维纳滤波、重采样校正和编码器-only频谱神经滤波的端到端过滤流程，显著提升信号质量。  <br/>5. **验证攻击鲁棒性**：在多种场景（音量、DPI、表面材质等）下评估，成功实现语音重建（SNR提升19 dB）与语音识别（42%-61%准确率）。  <br/>6. **代码与数据集公开**：提供完整工具链与实验数据，推动该领域研究和防御技术开发。  <br/><br/>**总结（100字以内）：**  <br/>该论文提出首个利用光学鼠标传感器窃听的侧信道攻击方法，通过用户空间软件实现无权限数据收集，并创新过滤技术提升语音识别效果，推动硬件安全研究。|
|2509.13548v3|[Mixture-of-Experts Framework for Field-of-View Enhanced Signal-Dependent   Binauralization of Moving Talkers](http://arxiv.org/abs/2509.13548v3)|总结：  <br/>提出基于隐式定位的信号依赖框架，实现动态双耳音频渲染与移动声源增强，支持AR/VR中的语音聚焦与降噪应用，无需方向估计或阵列几何限制，提升空间音频系统的灵活性与实时性。<br/><br/>贡献点：  <br/>1. **新型混合专家框架**：首次将混合专家（MoE）模型应用于双耳信号匹配的视场增强任务，实现动态场景适应。  <br/>2. **隐式定位与在线组合**：通过隐式定位机制动态组合多个双耳滤波器，避免传统显式方向估计的复杂性。  <br/>3. **支持移动声源实时处理**：适用于连续运动声源，实现用户可控制的声音方向增强与抑制，保持自然双耳线索。  <br/>4. **阵列几何无关性**：方法不依赖特定麦克风阵列配置，兼容下一代消费级音频设备的多样化场景。|
|2509.13548v2|[Mixture-of-Experts Framework for Field-of-View Enhanced Signal-Dependent   Binauralization of Moving Talkers](http://arxiv.org/abs/2509.13548v2)|**贡献点**：  <br/>1. 提出首个基于mixture of experts框架的field-of-view增强方法，用于双耳信号匹配。  <br/>2. 实现动态空间音频渲染，支持连续说话者运动下的实时方向控制（强调/抑制特定声源）。  <br/>3. 无需显式方向到达估计或依赖Ambisonics域，采用信号依赖的隐式定位策略。  <br/>4. 适用于AR/VR中的语音聚焦、降噪及世界锁定音频等关键应用。  <br/>5. 方法与阵列几何无关，提供灵活的空间音频捕获与个性化播放方案。  <br/><br/>**总结**：  <br/>该研究提出一种无需显式DOA估计的双耳信号增强框架，支持动态空间音频渲染及AR/VR应用场景，具有阵列无关的灵活性。|
|2509.13548v1|[Field of View Enhanced Signal Dependent Binauralization with Mixture of   Experts Framework for Continuous Source Motion](http://arxiv.org/abs/2509.13548v1)|**贡献点总结（100字以内）：**  <br/>提出基于隐式定位的信号依赖框架，实现动态空间音频渲染，支持连续说话者运动下的声音方向控制，兼容AR/VR应用，独立于阵列几何结构，提供灵活灵活的解决方案。  <br/><br/>**分点贡献：**  <br/>1. **提出新的场-视增强框架**：基于混合专家模型（MOE），改进双耳信号匹配技术，解决传统方法的不足。  <br/>2. **动态空间音频渲染**：适应连续说话者运动，实现声音方向的实时可调（强调/压制）。  <br/>3. **隐式定位技术**：无需显式方向估计，直接通过信号特性进行在线组合。  <br/>4. **支持关键应用场景**：包括语音聚焦、降噪和世界锁定音频，增强沉浸式体验。  <br/>5. **独立于阵列几何结构**：方法具有普适性，可适配不同硬件配置。  <br/>6. **实时处理能力**：能够动态跟踪和增强移动声源，满足低延迟要求。  <br/>7. **保持天然双耳线索**：保留人耳感知的立体声信息，在提升灵活性的同时不破坏听觉自然性。  <br/>8. **简化计算流程**：避免Ambisonics域的复杂处理，降低计算成本。  <br/>9. **泛化性强**：适用于下一代消费设备的空间音频捕获与个性化播放需求。  <br/>10. **创新性结合**：将信号依赖性与隐式定位融合，在线优化多路双耳滤波器协作。|
|2509.13442v1|[Enhancing Speaker-Independent Dysarthric Speech Severity Classification   with DSSCNet and Cross-Corpus Adaptation](http://arxiv.org/abs/2509.13442v1)|**贡献点总结**（100字以内）：  <br/>提出DSSCNet架构（融合卷积、SE和残差网络），提升失语症语音严重程度分类的判别能力；设计跨语料库微调框架，增强说话人无关场景下的泛化性能；在TORGO和UA-Speech数据集上取得SOTA结果，准确率较现有方法显著提升。  <br/><br/>**分点贡献**：  <br/>1. **创新网络架构**：提出DSSCNet，融合卷积神经网络（CNN）、Squeeze-Excitation（SE）模块和残差网络，增强对失语症语音特征的提取能力。  <br/>2. **SE模块优化**：利用SE模块选择性聚焦关键特征，减少信息损失，显著提升模型性能。  <br/>3. **跨语料库微调框架**：改进基于检测的迁移学习方法，设计适用于说话人无关（SID）场景的跨语料库微调框架。  <br/>4. **实验验证与性能提升**：在TORGO和UA-Speech数据集上，采用OSPS和LOSO协议验证，在SID场景下准确率分别达75.80%（TORGO）和79.44%（UA-Speech），优于现有SOTA方法。|
|2509.13395v1|[TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech   Recognition Abilities of Large Multimodal Models](http://arxiv.org/abs/2509.13395v1)|总结（100字以内）:  <br/>本文提出TICL方法，通过语义上下文提升多模态模型的语音识别性能，无需微调。在带口音英语、多语言及儿童语音等任务中实现最高84.7%的相对WER下降，并通过消融实验验证其鲁棒性与高效性。<br/><br/>贡献点：  <br/>1. **提出TICL方法**：基于文本嵌入与KNN算法，构建简单可扩展的SICL流水线，无需微调即可提升语音识别能力。  <br/>2. **跨任务有效性验证**：在复杂场景（如口音英语、多语言、儿童语音）中超越零样本表现，证明方法的泛化性。  <br/>3. **显著性能提升**：实现最高84.7%的相对WER降低，凸显语义上下文对语音识别的关键作用。  <br/>4. **消融研究与效率证明**：通过系统消融实验验证方法的鲁棒性及计算效率，为后续研究提供参考。|
|2509.13390v1|[A Domain Knowledge Informed Approach for Anomaly Detection of Electric   Vehicle Interior Sounds](http://arxiv.org/abs/2509.13390v1)|总结：  <br/>提出基于结构化扰动生成代理异常的无监督模型选择方法，构建并公开高保真电动车辆座舱声音数据集，验证了新方法在异常检测任务中的有效性。<br/><br/>贡献点：  <br/>1. **领域知识驱动的模型选择**：首次将结构化扰动生成的代理异常（proxy-anomalies）引入无监督模型选择，通过人工设计故障特征替代真实故障数据，解决验证集缺乏真实异常的问题。  <br/>2. **高保真数据集构建**：创建包含五种典型故障（Imbalance, Modulation, Whine, Wind, Pulse Width Modulation）的电动车辆座舱声音数据集，经专家评审和声学合成验证，为研究提供基准。  <br/>3. **性能提升验证**：在无监督设置下，通过代理异常验证集选择最优模型，实验结果显著优于传统基于重建误差的模型选择方法，证明方法的有效性。|
|2509.13285v1|[Contrastive timbre representations for musical instrument and   synthesizer retrieval](http://arxiv.org/abs/2509.13285v1)|**贡献点总结：**  <br/>1. 提出对比学习框架，支持单/多乐器音频检索。  <br/>2. 引入生成真实正负对的音频增强技术。  <br/>3. 实验验证方法在多乐器混合输入中优于现有方案，准确率达81.7%（top-1）和95.7%（top-5）。  <br/><br/>（98字）|
|2509.13215v1|[Importance-Weighted Domain Adaptation for Sound Source Tracking](http://arxiv.org/abs/2509.13215v1)|总结：  <br/>提出一种针对声源跟踪的新型无监督域适应方法，解决变长序列与方向多样性差异问题，提升真实环境下的声源定位性能。<br/><br/>贡献点：  <br/>1. 首次将无监督域适应（UDA）应用于动态声源跟踪（SST），弥补现有方法在静态SSL的局限性；  <br/>2. 引入RNN最终隐藏状态作为固定维特征表示，解决变长输入序列的特征维度不匹配问题；  <br/>3. 设计重要性加权对抗训练机制，通过优先对齐真实域相似的合成样本缓解方向多样性差异；  <br/>4. 实验验证该方法能有效适应真实环境，显著提升合成数据预训模型的SST性能。|
|2509.13148v2|[Can Large Audio Language Models Understand Audio Well? Speech, Scene and   Events Understanding Benchmark for LALMs](http://arxiv.org/abs/2509.13148v2)|总结：  <br/>本文提出首个兼顾语音与非语音能量差异的音频理解基准SSEU-Bench，并通过引入Chain-of-Thought方法提升模型在联合理解任务中的表现，验证了其有效性。<br/><br/>贡献点：  <br/>1. **首个全面基准**：提出SSEU-Bench，首次系统考虑音频中语音与非语音成分的能量差异，填补现有基准在真实场景应用中的空白。  <br/>2. **独立与联合任务支持**：构建支持语音、场景、事件独立理解及联合理解的双模式测试框架，更贴近实际交互需求。  <br/>3. **揭示模型局限性**：验证部分LALMs在联合理解任务中表现不佳，明确指出现有模型的性能瓶颈。  <br/>4. **提出改进方法**：引入Chain-of-Thought机制，通过分步推理显著提升模型对复杂音频任务的处理能力。|
|2509.13148v1|[Can Large Audio Language Models Understand Audio Well? Speech, Scene and   Events Understanding Benchmark for LALMs](http://arxiv.org/abs/2509.13148v1)|总结：提出首个考虑音量差异和多任务联合理解的音频理解基准SSEU-Bench，并引入Chain-of-Thought方法提升模型性能。<br/><br/>贡献点：<br/>1. **提出首个融合音量差异与多任务联合理解的音频基准SSEU-Bench**，覆盖语音、场景与事件的独立与联合理解场景。<br/>2. **揭示现有LALMs在联合理解任务中的性能不足**，通过实验验证其在复杂任务中存在短板。<br/>3. **引入Chain-of-Thought推理框架**，通过分解复杂任务提升模型对多模态音频信息的联合理解能力。|
|2509.13093v3|[GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR](http://arxiv.org/abs/2509.13093v3)|总结：  <br/>本文提出GLAD Mixture-of-Experts方法，通过融合全局说话人信息与局部声学特征提升多说话人语音识别性能，首次将MoE策略应用于端到端MTASR，并在高重叠场景下取得优于现有方法的实验结果。<br/><br/>贡献点：  <br/>1. **提出GLAD MoE架构**：首次将Mixture-of-Experts（MoE）策略引入端到端多说话人语音识别（MTASR），结合全局-局部信息动态融合机制。  <br/>2. **全局-局部协同路由**：通过同时利用全局上下文和细粒度局部声学特征，实现说话人特定的专家路由策略，提升模型对重叠语音的处理能力。  <br/>3. **高重叠场景优化**：在LibriSpeechMix数据集上验证了GLAD对高重叠多说话人场景的鲁棒性，显著优于现有MTASR方法。  <br/>4. **开源实现与资源**：提供公开代码和训练数据集，便于复现与进一步研究。|
|2509.13093v2|[GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR](http://arxiv.org/abs/2509.13093v2)|**贡献点总结（100字以内）**:  <br/>提出首个将MoE与全局-本地融合策略结合的end-to-end MTASR模型GLAD，通过动态融合说话人信息和细粒度声学特征实现专家选择，显著提升高重叠场景下的多说话人语音识别性能。<br/><br/>**分点贡献列表**:  <br/>1. **方法创新**：提出Global-Local Aware Dynamic Mixture-of-Experts (GLAD)框架，首次将Mixture-of-Experts (MoE)应用于端到端多说话人语音识别（MTASR），并引入全局-本地信息动态融合机制。  <br/>2. **说话人感知融合**：设计基于全局上下文和局部声学特征的动态路由策略，实现说话人特定的专家选择，增强模型对重叠语音的处理能力。  <br/>3. **实验验证**：在LibriSpeechMix数据集上验证GLAD性能，结果显示其在挑战性多说话人场景（尤其是高重叠条件）下优于现有方法。  <br/>4. **开源贡献**：开源代码及训练数据集，便于社区复现与扩展研究（URL见原文）。|
|2509.13093v1|[GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR](http://arxiv.org/abs/2509.13093v1)|总结：  <br/>本文提出GLAD MoE模型，通过全局-局部信息融合提升多说话人语音识别性能，尤其在高重叠场景，并首次将MoE应用于端到端MTASR。<br/><br/>贡献点：  <br/>1. 提出Global-Local Aware Dynamic (GLAD) Mixture-of-Experts框架，结合全局上下文与局部声学特征进行多说话人语音识别。  <br/>2. 引入动态融合机制，通过全局和局部信息的协同作用优化专家选择策略。  <br/>3. 实现基于说话人特异性的路由（routing），提升模型对重叠语音的处理能力。  <br/>4. 在LibriSpeechMix数据集上验证了GLAD在高重叠多说话人场景中的有效性，超越现有方法。  <br/>5. 首次将Mixture-of-Experts技术应用于端到端多说话人语音识别任务，并提出全局-局部融合策略。|
|2509.13085v1|[Token-based Attractors and Cross-attention in Spoof Diarization](http://arxiv.org/abs/2509.13085v1)|**贡献点：**  <br/>1. **提出可学习的token机制**：引入learnable tokens，分别表征真实和欺骗语音的声学特征，填补了传统方法缺乏显式参考点的不足。  <br/>2. **设计交互式表示提取方法**：通过token与帧级嵌入的交互，提取更具判别性的跨模态表征，提升真实与生成语音的分离能力。  <br/>3. **验证方法有效性**：在PartialSpoof数据集上的大量实验表明，该方法在真实检测和欺骗技术分类任务中显著优于现有方法。  <br/><br/>**总结：**  <br/>本研究通过引入可学习token和交互机制，改进了语音欺骗检测的模型结构，有效提升了真实性识别与分类性能。|
|2509.13068v2|[MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement](http://arxiv.org/abs/2509.13068v2)|总结：  <br/>提出多尺度残差编码器，实现低比特率高保真语音重建及TTS中SOTA性能与说话人相似度，支持语音转换中的音色与韵律独立操控。<br/><br/>贡献点：  <br/>1. **多尺度残差编码结构**：首次将语音分解为语义、音色、韵律和残差四类信息流，实现高保真重建与信息分离。  <br/>2. **轻量级TTS系统**：基于该编码器构建的两阶段语言模型在数据和计算需求较低的情况下，达到SOTA WER和更优说话人相似度。  <br/>3. **语音转换能力**：编码器设计支持独立调控说话人音色与韵律，提升语音转换任务的灵活性与效果。  <br/>4. **开源实现**：提供完整推理代码、预训练模型及音频样本，促进研究复现与应用。|
|2509.13068v1|[MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement](http://arxiv.org/abs/2509.13068v1)|**贡献点：**  <br/>1. **多尺度残差编码器设计**：提出低比特率、多尺度的残差音频编码器，将语音分解为语义、音色、韵律和残差四个独立流，实现信息解耦。  <br/>2. **轻量高效TTS模型**：构建两阶段语言模型，基于该编码器，在轻量设计和低数据需求下达到SOTA WER及更高说话人相似度。  <br/>3. **语音转换应用**：编码器结构在语音转换任务中表现优异，支持对音色和韵律的独立操控。  <br/><br/>**总结（100字内）：**  <br/>该论文提出一种多尺度残差音频编码器，实现语音语义、音色、韵律的解耦，并应用于轻量TTS合成，显著提升性能，同时支持语音转换中的音色与韵律独立操控。|
|2509.12845v1|[Improving Anomalous Sound Detection with Attribute-aware Representation   from Domain-adaptive Pre-training](http://arxiv.org/abs/2509.12845v1)|总结：  <br/>本文提出通过聚合层次聚类和领域自适应预训练模型生成伪标签，并结合监督微调提升分类性能，在DCASE 2025数据集上显著优于现有方法。<br/><br/>贡献点：  <br/>1. **伪标签生成方法**：首次采用基于聚合层次聚类的策略，利用领域自适应预训练模型提取的特征表示为ASD任务分配伪属性标签，解决标注数据缺失问题。  <br/>2. **模型优化框架**：提出结合领域自适应预训练与监督微调的端到端框架，显著提升机器属性分类性能，达到新SOTA。  <br/>3. **实验验证**：在DCASE 2025挑战数据集上验证方法有效性，超越之前团队在该挑战中的最优系统。|
|2509.12831v1|[A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip   Sync Synthesis](http://arxiv.org/abs/2509.12831v1)|总结（100字以内）:  <br/>本文提出一种模块化流水线，基于Transformer的潜伏扩散模型实现高保真零样本语音克隆，并采用轻量级GAN架构完成实时唇部同步，显著降低对大规模数据和计算资源的依赖，适用于复杂环境并支持未来多模态扩展。<br/><br/>贡献点分点：  <br/>1. **模块化语音克隆系统**：提出Tortoise文本到语音系统，结合Transformer和潜伏扩散模型，实现零样本语音克隆（仅需少量训练样本）。  <br/>2. **轻量级实时唇同步**：设计轻量级生成对抗网络（GAN），在噪声或低资源场景中实现高效、鲁棒的实时唇部同步。  <br/>3. **降低预训练依赖**：减少对大规模预训练数据的需求，提升模型在非受限场景下的适用性与泛化能力。  <br/>4. **扩展性架构**：模块化结构支持未来多模态及文本引导的语音调制技术扩展，增强系统灵活性与实用性。|
|2509.12786v1|[Beyond Bars: Distribution of Edit Operations in Historical Prints](http://arxiv.org/abs/2509.12786v1)|**贡献点：**  <br/>1. 提出了一种高效减少音乐学语料数字化时间的方法，通过采样乐谱小节而非完整编码。  <br/>2. 针对样本选择的代表性问题，系统评估了三种不同采样方法的优劣。  <br/>3. 以贝多芬《Bagatelles Op.33》为案例研究，验证采样方法在揭示差异性中的有效性。  <br/>4. 推动音乐学研究向大规模统计分析发展，提升研究结果的可信度。  <br/>5. 为理解19世纪编辑实践和历史音乐作品的学术编辑提供了新视角。  <br/><br/>**总结：**  <br/>本文提出基于小节采样的高效语料研究方法，评估三种策略并以贝多芬作品为例验证其有效性，推动音乐学分析与编辑实践的结合。|
|2509.12712v1|[Timbre-Adaptive Transcription: A Lightweight Architecture with   Associative Memory for Dynamic Instrument Separation](http://arxiv.org/abs/2509.12712v1)|总结（100字以内）:  <br/>提出轻量级音色无关主干网络与认知启发的记忆机制，实现高效多音色转录与分离，减少训练数据需求并提升泛化能力，在公共基准上表现优于现有方法。<br/><br/>贡献点分点列出：<br/>1. **轻量化音色无关主干网络**：仅需现有模型一半参数即实现SOTA性能，解决模型复杂度与泛化能力的矛盾。<br/>2. **认知启发的记忆机制**：通过注意机制动态编码新音色，模仿人类听觉认知实现未见过音色的适应性分离。<br/>3. **合成数据集生成方法**：提供低成本高精度的多音色生成方案，显著降低训练数据需求（仅需12.5分钟）。<br/>4. **端到端分离模块验证**：分离模块在音色区分任务中表现突出，证实模型在实际应用中的有效性。<br/>5. **跨领域方法论创新**：推动音色感知分离研究向生物启发架构方向发展，提供可复用的高效框架。|
|2509.12668v1|[Investigating the Potential of Multi-Stage Score Fusion in   Spoofing-Aware Speaker Verification](http://arxiv.org/abs/2509.12668v1)|**贡献点：**<br/>1. 提出模块化欺骗感知说话人验证（SASV）框架，整合ASV与反制措施（CM）子系统。  <br/>2. 设计多阶段融合策略，替代传统单阶段分数级融合方法。  <br/>3. 采用ECAPA-TDNN（ASV）与AASIST（CM）子系统构建基础模块。  <br/>4. 引入SVM和逻辑回归分类器优化SASV框架的决策过程。  <br/>5. 在第二阶段结合子系统输出与原始分数，动态修正融合分类器。  <br/>6. 集成RawGAT生成的辅助分数，进一步提升框架鲁棒性。  <br/>7. 在SASV2022挑战数据集上实现1.30% EER，较基线提升24%。  <br/><br/>**总结（100字以内）：**  <br/>本研究构建了多阶段模块化SASV框架，整合ECAPA-TDNN、AASIST与RawGAT子系统，通过动态修正分类器与辅助分数增强鲁棒性，在SASV2022数据集上取得1.30% EER，较基线提升24%。|
|2509.12667v1|[Osu2MIR: Beat Tracking Dataset Derived From Osu! Data](http://arxiv.org/abs/2509.12667v1)|总结: 该研究创新性地利用Osu!游戏数据作为音乐信息检索的注释源，开发了提取框架并验证了其可靠性，为MIR社区提供了可扩展、多样化的数据资源。<br/><br/>贡献点：<br/>1. 提出Osu!游戏数据作为音乐节奏和弱拍注释的新来源，挖掘社区创作的非主流音乐类型数据<br/>2. 开发完整的注释提取流水线，实现beatmap到结构化注释的自动转换<br/>3. 建立注释质量评估标准：单时间点/间距≥5秒的beatmap注释更可靠，需人工校验的阈值明确<br/>4. 验证Osu!注释在多歌曲场景下的高一致性，证明其在MIR研究中的有效性<br/>5. 公布高质量数据子集osu2beat2025及开源工具链，为研究提供可复用的资源|
|2509.12275v3|[Omni-CLST: Error-aware Curriculum Learning with guided Selective   chain-of-Thought for audio question answering](http://arxiv.org/abs/2509.12275v3)|总结：提出Omni-CLST框架，通过错误感知课程学习和指导性思维dropout机制，有效提升多模态音频-语言理解的通用性，取得MMAU-mini和MMAR数据集的SOTA结果。<br/><br/>贡献点：<br/>1. 提出Omni-CLST框架：首个结合错误感知课程学习与选择性链式思维指导的AQA任务框架<br/>2. 创新性策略：采用双重关键方法（难度排序的课程策略+动态聚焦的思维dropout机制）提升模型训练效率<br/>3. 实验突破：在MMAU-mini上达到73.80%性能，在MMAR上取得64.30%的SOTA结果，验证框架的有效性|
|2509.12275v2|[Omni-CLST: Error-aware Curriculum Learning with guided Selective   chain-of-Thought for audio question answering](http://arxiv.org/abs/2509.12275v2)|**贡献点分点总结：**  <br/>1. **提出Omni-CLST框架**：结合误差感知课程学习与引导式精选链式思维（Selective Chain-of-Thought），为音频问答任务提供新的训练范式。  <br/>2. **设计双策略提升效率**：  <br/>   - 通过错误感知课程安排按难度组织样本；  <br/>   - 引入引导式思维丢弃机制，聚焦于挑战性推理案例。  <br/>3. **突破数据利用瓶颈**：无需依赖新构建数据集，直接高效利用现有高质量音频问答数据。  <br/>4. **实验验证有效性**：在MMAU-mini和MMAR数据集上达到SOTA性能（73.80%和64.30%），证明其在多模态音频-语言理解中的泛化能力。  <br/><br/>**总结（100字以内）：**  <br/>本文提出Omni-CLST框架，通过误差感知课程学习与引导式链式思维优化音频问答训练，高效复用现有高质量数据，并在多个基准测试中取得SOTA性能。|
|2509.12275v1|[Omni-CLST: Error-aware Curriculum Learning with guided Selective   chain-of-Thought for audio questuin answering](http://arxiv.org/abs/2509.12275v1)|总结：  <br/>提出Omni-CLST框架，通过错误感知课程学习和引导性思维丢弃机制提升音频问答性能，结合GRPO训练实现高质量数据高效利用，在两个数据集上取得SOTA结果。<br/><br/>贡献点：  <br/>1. **提出Omni-CLST框架**：首次将错误感知课程学习与引导性链式思维推理结合，用于音频问答任务。  <br/>2. **设计双策略**：  <br/>   - 引入错误感知课程，按样本难度动态组织训练数据；  <br/>   - 创新引导性思维丢弃机制，强化对挑战性案例的推理聚焦。  <br/>3. **实验验证优势**：在MMAU-mini和MMAR数据集上分别达到73.80%和64.30%的准确率，刷新MMAR的SOTA记录。|
|2509.12003v1|[Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and   Fusion of SSL-Based Countermeasures](http://arxiv.org/abs/2509.12003v1)|**贡献点：**<br/>1. **层贡献分析**：系统性评估六种预训练SSL模型在四种测试语料库的表现，揭示不同编码器的层特征对检测任务的差异化贡献。<br/>2. **轻量化策略**：提出基于单层选择的池头方法，相较多头因子化注意池化（MHFA）减少系统参数达80%，同时保持高性能。<br/>3. **预训练策略影响**：实验证明SSL编码器的预训练策略直接影响其对领域外（OOD）攻击的泛化能力。<br/>4. **多模型融合**：设计得分级融合框架，通过整合多个编码器信息提升对OOD条件的鲁棒性。<br/><br/>**总结（100字以内）：**  <br/>本文通过分析SSL模型层贡献、优化池头策略、揭示预训练策略影响，以及设计多模型得分融合，显著提升音频深度伪造检测在领域外条件下的泛化能力，同时降低参数量。|
|2509.11973v1|[MusicSwarm: Biologically Inspired Intelligence for Music Composition](http://arxiv.org/abs/2509.11973v1)|总结（100字以内）:  <br/>该研究提出MusicSwarm框架，通过去中心化蜂群协作生成高质量音乐，无需参数更新。创新性地结合stigmergic信号与动态共识机制，在符号、音频和图论分析中展现卓越表现，并揭示小世界网络架构对音乐结构形成的作用，为跨领域协作创造性任务提供新思路。<br/><br/>贡献点:  <br/>1. **去中心化音乐生成**：首次展示基于冻结基础模型的蜂群系统（无参数更新）可协同创作连贯、长篇音乐，突破传统需持续训练的范式。  <br/>2. **多维度分析验证**：通过符号、音频、图论分析证明该系统在质量、多样性、结构性及创造性指标上优于集中式多智能体系统。  <br/>3. **动态角色收敛机制**：发现智能体动态趋向互补角色的稳定配置，揭示音乐创作中分工协作的自组织特性。  <br/>4. **小世界网络架构**：利用自相似性网络揭示音乐结构中的高效长程连接与专业化桥接模式，解释局部创新如何形成全局音乐形式。  <br/>5. **跨领域可迁移性**：提出基于交互规则、共享内存和动态共识的通用框架，为协作写作、设计及科学发现等任务提供高效解决方案。|
|2509.11957v1|[EEND-SAA: Enrollment-Less Main Speaker Voice Activity Detection Using   Self-Attention Attractors](http://arxiv.org/abs/2509.11957v1)|总结：  <br/>提出EEND-SAA框架，无需注册信息即可在开放场景中实时检测主说话人，通过语句连续性和音量优化实现SOTA性能，尤其在多说话人重叠和噪声环境下表现突出。<br/><br/>贡献点：  <br/>1. **无需注册信息的主说话人检测**：突破传统TS-VAD依赖短时注册语音的限制，适用于未知主说话人的开放域场景（如会议、客服电话）。  <br/>2. **流式兼容性**：采用因果掩码（causal masking）实现实时处理，支持连续语音流的在线检测。  <br/>3. **基于语音连续性与音量的主说话人定义**：通过语句稳定性（连续性）和清晰度（音量）动态判定主说话人，而非依赖预先知识。  <br/>4. **双自注意力吸引子架构**：在EEND模型中引入两个自注意力吸引子（self-attention attractors），增强对主说话人特征的建模能力。  <br/>5. **实验验证SOTA性能**：在LibriSpeech多说话人混合数据集上，将主说话人DER降低至3.61%（对比6.63%），F1提升至0.9818，验证了方法在复杂场景下的有效性。|
|2509.11824v1|[Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case   Study with Suno and Udio](http://arxiv.org/abs/2509.11824v1)|**贡献点：**  <br/>1. **大规模数据收集**：构建了涵盖2024年5月至10月Suno和Udio用户生成音乐的数据集，揭示AI音乐创作的当前应用趋势及文化现象。  <br/>2. **多维分析方法**：结合文本嵌入模型、降维与聚类技术，系统分析提示内容、标签及歌词，实现跨语言和跨文化视角的深度挖掘。  <br/>3. **交互式可视化工具**：开发自动注释的交互式图表，直观呈现用户生成音乐的歌词主题、语言偏好及模型引导策略（如元标签使用）。  <br/>4. **开放研究支持**：共享代码和资源，推动音乐学对AI生成音乐文化实践的进一步研究，促进学术研究的可复现性与跨领域合作。  <br/><br/>**总结：**  <br/>本研究通过大规模数据与先进分析方法，揭示了AI音乐平台用户创作的歌词主题、语言偏好及模型引导策略，推动了音乐学对AI生成音乐文化实践的研究，并开放了相关资源。|
|2509.11717v3|[Neural Audio Codecs for Prompt-Driven Universal Sound Separation](http://arxiv.org/abs/2509.11717v3)|**贡献点：**  <br/>1. 提出 CodecSep，首个基于神经音频编解码器（NAC）的**端侧通用文本驱动声学分离模型**。  <br/>2. 首创将 DAC 压缩与**CLAP 派生 FiLM 参数调控的 Transformer 掩码器**结合，提升分离精度与效率。  <br/>3. 在六项开放域基准测试中，**分离保真度（SI-SDR）超越 AudioSep**，同时保持**感知质量（ViSQOL）竞争力**并**匹配或超越固定类基线模型**（如 TDANet、CodecFormer、SDCodec）。  <br/>4. 实现**极低计算开销**（仅需 1.35~GMACs，约为传统谱域模型的 54 倍更低），适应边缘设备部署需求。  <br/>5. 保持**完全比特流兼容性**，支持端到端的高效音频处理。  <br/><br/>**总结（100字以内）：**  <br/>提出 CodecSep，首个NAC-based文本驱动声学分离模型，结合DAC与FiLM调控的Transformer，实现高效计算与卓越性能，适用于边缘设备。|
|2509.11717v2|[Neural Audio Codecs for Prompt-Driven Universal Source Separation](http://arxiv.org/abs/2509.11717v2)|**贡献点总结**  <br/>1. 提出首个基于神经音频编解码器（NAC）的文本驱动源分离模型CodecSep，支持边缘设备上的通用分离任务。  <br/>2. 结合DAC压缩与Transformer掩码器，利用CLAP衍生的FiLM参数实现动态调控。  <br/>3. 在开放域基准测试中，分离保真度（SI-SDR）超越AudioSep，感知质量（ViSQOL）与固定茎基线（TDANet、CodecFormer、SDCodec）表现相当或更优。  <br/>4. 计算效率显著提升（1.35 GMACs，约比AudioSep低54倍），并保持位流兼容性，适合实际部署。  <br/><br/>**摘要总结（100字内）**：  <br/>CodecSep是首个支持边缘部署的文本驱动通用源分离模型，结合DAC压缩与FiLM调制Transformer掩码器，实现高效分离并在多基准测试中超越AudioSep，计算量仅为传统方法的1/54。|
|2509.11717v1|[Neural Audio Codecs for Prompt-Driven Universal Source Separation](http://arxiv.org/abs/2509.11717v1)|**贡献点：**  <br/>1. **提出首个NAC-based通用文本驱动分离模型**：首次将神经音频编解码器（NAC）技术应用于设备端通用、文本驱动的音频源分离任务，突破传统频谱域模型的计算瓶颈。  <br/>2. **创新模型结构**：结合DAC压缩与CLAP生成的FiLM参数调制Transformer掩码器，实现高效且灵活的分离机制。  <br/>3. **性能优势**：在分离保真度（SI-SDR）上超越AudioSep，同时保持与感知质量（ViSQOL）及固定茎基线（如TDANet、CodecFormer、SDCodec）竞争力。  <br/>4. **高效部署能力**：仅需1.35 GMACs计算量（比AudioSep少54倍），且完全兼容比特流传输，支持边缘设备实时应用。  <br/><br/>**总结（100字以内）：**  <br/>CodecSep是首个NAC-based通用文本驱动音频分离模型，通过DAC压缩与FiLM调制Transformer掩码器，实现高保真分离及低计算量（1.35 GMACs），显著优于AudioSep并在固定茎基线中表现优异，适合边缘设备部署。|
|2509.11709v1|[Room acoustics affect communicative success in hybrid meeting spaces: a   pilot study](http://arxiv.org/abs/2509.11709v1)|**贡献点总结（100字以内）**  <br/>该研究首次关注混合会议中室内外声学设计的关联性，通过前后对比实验验证了空间干预对提升语音通信质量的潜在影响，为混合会议环境优化提供实证依据，并通过跨领域背景解释增强结果可理解性。<br/><br/>**分点贡献**  <br/>1. **问题聚焦**：首次系统性探讨混合会议场景中声学设计对通信效果的影响，揭示传统关注互联网连接而忽视声学环境的现状。  <br/>2. **实证方法**：采用前后对比实验（两组人员、两次录音）验证声学干预效果，为同类研究提供可借鉴的实证框架。  <br/>3. **实际应用价值**：尽管样本量有限，研究结果明确表明声学改进可提升混合会议的沟通效率，为会议空间设计提供实践指导。  <br/>4. **跨领域桥梁**：向语音通信领域读者普及声学背景知识，促进跨学科对混合会议语音质量问题的协同研究。|
|2509.11606v2|[Scaling to Multimodal and Multichannel Heart Sound Classification:   Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals](http://arxiv.org/abs/2509.11606v2)|总结：  <br/>提出结合信号处理与扩散模型的增强数据集方法，显著提升Transformer模型在多模态心音分类中的性能，验证其在心血管疾病检测中的有效性。<br/><br/>贡献点：  <br/>1. **融合传统信号处理与扩散模型**：结合WaveGrad和DiffWave等去噪扩散模型，与传统信号处理技术结合，构建多模态心音数据增强方案。  <br/>2. **增强数据集支持的微调框架**：利用增强数据集对Wav2Vec 2.0模型进行微调，提升其在单通道、同步多模态及多通道心音信号中的分类能力。  <br/>3. **多场景实验验证效果**：在CinC 2016单通道数据集、同步PCG-ECG数据集及穿戴式胸带多通道数据集上均取得SOTA性能（准确率超90%，MCC超0.8），证实方法的泛化性。  <br/>4. **推动CVD检测研究**：通过数据增强与模型优化，克服多模态多通道数据稀缺问题，为心血管疾病早期筛查提供新思路。|
|2509.11606v1|[Scaling to Multimodal and Multichannel Heart Sound Classification:   Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals](http://arxiv.org/abs/2509.11606v1)|**分点贡献：**<br/>1. **跨模态数据增强方法创新**：首次融合传统信号处理技术与去噪扩散模型（WaveGrad/DiffWave），解决同步PCG-ECG信号及多通道PCG数据稀缺问题，构建融合增强数据集。<br/>2. **模型优化与迁移应用**：基于Wav2Vec 2.0框架设计分类器，利用增强数据集实现对单通道、多通道及同步多模态信号的联合分类，提升诊断性能。<br/>3. **多场景验证与性能突破**：在CinC 2016、训练-a数据集及穿戴式背心数据集中验证方法，达到当前最优指标（如95.12%准确率、95.12% MCC），凸显Transformer模型的优越性。<br/>4. **临床场景适配性探索**：提出针对实际医疗场景的解决方案，通过多模态和多通道心音分类推动CVD早筛技术发展，为可穿戴设备数据应用提供新范式。<br/><br/>**总结（100字以内）**  <br/>本研究结合传统信号处理与扩散模型生成增强数据集，用于优化Wav2Vec 2.0分类器，实现多模态/多通道心音分类，达到SOTA性能，为CVD早筛提供可扩展的解决方案。|
|2509.11474v1|[Acoustic Overspecification in Electronic Dance Music Taxonomy](http://arxiv.org/abs/2509.11474v1)|**分点贡献：**<br/>1. 提出无监督学习框架，利用新型tempogram特征挖掘EDM的自然声学结构，摆脱商业标签依赖。  <br/>2. 引入多标准特征选择策略，结合声学特征与预训练模型（MERT/CLAP）验证结果可靠性。  <br/>3. 揭示当前EDM分类体系存在显著过度细分问题（35类→19-23类），证实其实际声学本质被错误定义。  <br/><br/>**总结（100字以内）：**  <br/>提出无监督方法结合tempogram特征与多标准特征选择，验证EDM分类的自然声学结构，并揭示现有分类的过度细分问题。|
|2509.11425v2|[FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs](http://arxiv.org/abs/2509.11425v2)|总结（100字以内）:  <br/>提出FuseCodec方法，统一声学、语义和上下文表示，引入三种互补技术提升对齐与监督，应用于零样本语音合成并实现LibriSpeech任务的SOTA性能。<br/><br/>贡献点分点列出：  <br/>1. **方法创新**：提出FuseCodec框架，首次通过强跨模态对齐与全局监督，统一声学、语义和上下文表示，解决传统编码器忽视语义信息的问题。  <br/>2. **三技术融合**：  <br/>   - (i) **潜在表示融合**：直接整合语义与上下文特征至编码器潜在空间，实现鲁棒统一表征。  <br/>   - (ii) **全局监督机制**：利用全局池化和广播的语义-上下文表示监督离散标记，增强时序一致性与跨模态对齐。  <br/>   - (iii) **局部时间对齐**：在局部窗口内动态匹配上下文与语音标记，实现细粒度的标记级监督。  <br/>3. **应用扩展**：推出FuseCodec-TTS，验证方法在零样本语音合成任务中的可行性。  <br/>4. **性能突破**：在LibriSpeech数据集上取得SOTA表现，超越EnCodec、SpeechTokenizer和DAC，在转录精度、感知质量、可懂度及说话人相似度等指标均领先。|
|2509.11425v1|[FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs](http://arxiv.org/abs/2509.11425v1)|**贡献点总结：**  <br/>1. **提出 FuseCodec**：首次将声学、语义和上下文表示统一，通过强跨模态对齐和全局监督解决传统编码器忽视语义与上下文的问题。  <br/>2. **提出三类互补技术**：  <br/>   - (i) **潜在空间融合**：直接集成语义与上下文特征至编码器潜在空间，实现统一表示学习。  <br/>   - (ii) **全局语义-上下文监督**：利用全局池化后广播的语义与上下文表示增强离散标记的时序一致性与跨模态对齐。  <br/>   - (iii) **局部时间对齐监督**：在局部窗口内动态匹配上下文与语音标记，提供细粒度的标记级监督。  <br/>3. **扩展至零样本语音合成**：开发 FuseCodec-TTS，验证方法在零样本任务中的通用性。  <br/>4. **SOTA性能表现**：在 LibriSpeech 上超越 EnCodec、SpeechTokenizer 和 DAC，提升转录准确率、感知质量、可理解性和说话人相似性。  <br/>5. **开源与可复现性**：提供代码与预训练模型，便于社区验证与应用。  <br/><br/>**（100字以内摘要）**  <br/>本文提出 FuseCodec，融合声学、语义与上下文表示，通过三种互补技术提升跨模态对齐与监督，应用于零样本语音合成，并在 LibriSpeech 上取得 SOTA 性能，开源代码促进研究。|
|2509.11241v1|[Revisiting Meter Tracking in Carnatic Music using Deep Learning   Approaches](http://arxiv.org/abs/2509.11241v1)|**贡献点总结：**  <br/>1. 填补了SOTA深度学习模型在Carnatic音乐节奏分析中的研究空白。  <br/>2. 评估了TCN（轻量架构）与Beat This!（Transformer模型）在Carnatic音乐中的表现。  <br/>3. 提出音乐感知驱动的迁移学习策略（微调与参数优化）。  <br/>4. 证明深度学习模型可通过适应策略有效应用于非主流音乐传统，推动Meter Tracking系统更广泛适用。  <br/><br/>**摘要总结（100字以内）：**  <br/>本研究评估TCN和Beat This!在Carnatic音乐中的节拍跟踪效果，通过迁移学习策略验证SOTA模型的适应性，证明其可匹敌传统DBN方法，为构建包容性更强的跨文化音乐信息检索系统提供新路径。|
|2509.11183v1|[WeaveMuse: An Open Agentic System for Multimodal Music Understanding and   Generation](http://arxiv.org/abs/2509.11183v1)|总结：  <br/>该论文提出WeaveMuse多智能体系统，支持音乐理解、符号创作和音频合成，具备跨模态交互能力，可灵活部署并适应不同硬件需求，通过约束模式和参数高效方法增强可控性与模型自适应性，推动MIR工具的开放性和普及化。<br/><br/>贡献点：  <br/>1. 提出WeaveMuse多智能体系统，整合音乐理解、符号创作与音频合成功能。  <br/>2. 设计可扩展架构，区分专家代理（需求推导与输出验证）和管理代理（工具调度与交互控制）。  <br/>3. 支持本地部署（量化、推理策略）和云端部署（HFApi），兼顾硬件兼容性与社区开放性。  <br/>4. 引入约束模式、结构化解码及参数高效适配器，提升模型对MIR任务的可控性与适应性。  <br/>5. 实现跨模态交互（文本、符号、视觉、音频）及分析-合成-渲染循环，解决多格式协同约束。  <br/>6. 通过开放模型支持、灵活内存管理与可复现部署路径，推动MIR工具的民主化与普及化。|
|2509.11168v1|[An Entropy-Guided Curriculum Learning Strategy for Data-Efficient   Acoustic Scene Classification under Domain Shift](http://arxiv.org/abs/2509.11168v1)|总结：  <br/>提出基于熵引导的课程学习策略，有效缓解ASC跨设备泛化问题，无需增加模型复杂度，适用于有限标记数据场景，具有通用性和高效性。<br/><br/>贡献点：  <br/>1. **解决跨设备泛化挑战**：针对声学场景分类中设备域偏移问题，设计数据效率高的训练策略。  <br/>2. **熵引导课程学习框架**：首次将Shannon熵作为域不变性度量，通过不确定性排序构建渐进式学习流程。  <br/>3. **无需额外架构改造**：方法兼容现有ASC模型，不增加推理成本或训练复杂度。  <br/>4. **验证有效性与普适性**：在DCASE 2024基准测试中证明策略对有限标记数据的适应性，展示广泛适用性。|
|2509.11128v1|[ENJ: Optimizing Noise with Genetic Algorithms to Jailbreak LSMs](http://arxiv.org/abs/2509.11128v1)|总结：  <br/>本文提出ENJ方法，利用遗传算法将环境噪声转化为可优化的攻击载体，显著提升语音模型攻击有效性与隐蔽性，揭示噪声在语音安全中的双重作用，为复杂声学环境下的模型防御提供新思路。<br/><br/>贡献点：  <br/>1. 提出Evolutionary Noise Jailbreak (ENJ)框架，通过遗传算法将环境噪声转化为主动攻击载体，突破传统方法在攻击有效性与隐蔽性间的平衡难题。  <br/>2. 引入种群初始化、交叉融合与概率突变等操作，实现对融合恶意指令与背景噪声的音频样本的迭代优化。  <br/>3. 攻击生成的音频样本在人类听觉上表现为无害噪声，却能诱导模型解析并执行有害指令，体现高隐蔽性。  <br/>4. 在主流语音模型上验证ENJ的攻击有效性显著优于现有基线方法，证明其普适性与威胁性。  <br/>5. 揭示噪声在语音安全中的双重角色（干扰与攻击载体），为模型防御机制设计提供关键理论洞见。|
|2509.11084v1|[Length-Aware Rotary Position Embedding for Text-Speech Alignment](http://arxiv.org/abs/2509.11084v1)|总结：提出LARoPE方法，通过长度归一化改进RoPE，显著提升TTS对齐精度与生成质量，克服时长变化影响，实现zero-shot场景下的SOTA表现。<br/><br/>贡献点：<br/>1. 提出长度感知RoPE（LARoPE）方法，通过长度归一化索引计算相对位置距离，替代传统绝对索引机制<br/>2. 实现更快的损失收敛速度与更精确的文本-语音对齐效果<br/>3. 在15-30秒超长语音生成中保持性能稳定，解决RoPE的时长相关退化问题<br/>4. 在标准zero-shot TTS基准测试中取得最先进词错误率（WER）结果|
|2509.10951v1|[Local Density-Based Anomaly Score Normalization for Domain   Generalization](http://arxiv.org/abs/2509.10951v1)|总结（100字以内）:  <br/>提出一种基于局部密度的异常分数归一化方法，有效缓解多领域声学环境下的域不匹配问题，验证其在多种嵌入式异常检测系统中的优越性能，优于现有归一化方案。<br/><br/>贡献点分列:  <br/>1. **提出新型归一化方案**：设计基于局部密度的异常分数归一化方法，直接解决源域与目标域之间的分布不匹配问题，避免单一决策阈值在跨领域场景中的性能衰减。  <br/>2. **通用性验证**：通过多数据集实验验证该方法对不同类型的嵌入式异常检测系统（如基于距离的模型）均有效，证明其跨领域的适应能力。  <br/>3. **性能提升**：对比实验表明，该方案在异常检测准确率上优于现有归一化方法，显著提高在目标域中的检测效果。  <br/>4. **简化实现**：强调方法的简洁性，在保持高效的同时降低计算复杂度，便于实际部署和应用。|
|2509.09746v2|[Deep Learning for Tuberculosis Screening in a High-burden Setting using   Cough Analysis and Speech Foundation Models](http://arxiv.org/abs/2509.09746v2)|总结：本研究提出基于咳嗽声的AI模型，利用大规模真实临床数据提升结核病筛查准确率，达到WHO基准，验证了其在低资源环境中的实用性与鲁棒性。<br/><br/>贡献点：<br/>1. 构建首个大规模真实世界结核病咳嗽数据集（512名参与者，含非结核病症状患者）<br/>2. 首次将预训练语音基础模型应用于结核病筛查任务<br/>3. 提出多模态融合方法（结合音频与临床特征）显著提升诊断性能<br/>4. 验证模型在真实环境下的鲁棒性（抗噪、抗设备差异、抗时间影响）<br/>5. 达成WHO结核病筛查技术基准（敏感度90.3%、特异度73.1%）<br/>6. 显示AI辅助筛查在资源有限地区的可行性与临床价值|
|2509.09716v2|[VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](http://arxiv.org/abs/2509.09716v2)|**贡献点分点总结：**  <br/>1. **提出新任务**：定义Voice Style Adaptation（VSA）任务，探索语音语言模型通过自然语言指令调整语音风格（如音色、语调、角色扮演等）的能力。  <br/>2. **构建双语基准**：推出VStyle数据集，覆盖中英文，包含四大类语音生成任务（音色属性、自然指令、角色扮演、隐式共情），填补该领域的评测空白。  <br/>3. **设计评估框架**：提出LALM as a Judge框架，从文本忠实度、风格一致性、自然度等维度分阶段评估模型输出，提升评估的客观性和可复现性。  <br/>4. **实验验证局限性**：通过对比商业系统和开源SLMs，揭示当前模型在可控风格适应上的显著不足，突出该任务的创新性与挑战性。  <br/>5. **开源数据与工具**：公开数据集、代码及评估工具，为社区提供研究基础，推动以用户为中心的语音交互技术发展。  <br/><br/>**总结（100字以内）：**  <br/>本研究提出Voice Style Adaptation任务，构建双语VStyle基准与LALM Judge评估框架，揭示现有SLMs在可控风格适应上的局限，旨在推动更自然的人机语音交互技术发展。|
|2509.07376v2|[Progressive Facial Granularity Aggregation with Bilateral   Attribute-based Enhancement for Face-to-Speech Synthesis](http://arxiv.org/abs/2509.07376v2)|总结：该论文提出通过细粒度面部属性建模和多视角训练策略，提升FTV合成的语音与面部一致性及稳定性，克服现有方法对视觉信息丢失和训练效率低的不足。<br/><br/>贡献点：  <br/>1. **细粒度面部属性建模**：将面部图像分解为非重叠区域，构建多粒度表征以保留性别、种族等关键视觉信息。  <br/>2. **多任务学习优化**：同步优化视觉与声学领域的说话人属性，增强跨模态对齐的鲁棒性。  <br/>3. **多视角训练策略**：通过不同角度和光照条件下的面部图像配对同一语音，提升模型泛化能力。  <br/>4. **提升合成稳定性**：实验证明显著改善语音生成的稳定性与语音-面部一致性。|
|2509.05983v2|[TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching   Vietnamese-English Speech Recognition](http://arxiv.org/abs/2509.05983v2)|贡献点总结：<br/>1. 提出Two-Stage Phoneme-Centric (TSPC)架构，通过扩展越南语音素集作为中间表示解决代码切换中的音系变化问题；<br/>2. 实验验证TSPC在越南语-英语混合语言ASR中优于PhoWhisper-base等基线模型，词错误率降低至20.8%；<br/>3. 实现资源优化，减少训练数据需求；<br/>4. 设计支持音素适应和语言转换机制，提升复杂代码切换场景的识别性能。|
|2509.05849v2|[From perception to production: how acoustic invariance facilitates   articulatory learning in a self-supervised vocal imitation model](http://arxiv.org/abs/2509.05849v2)|总结（100字以内）: <br/>该研究提出基于自监督学习的语音到发音映射模型，验证了wav2vec 2.0中间层特征在发音学习中的优越性，揭示了自监督学习如何平衡语音区分性和说话人不变性，为儿童语音习得的发育理论提供了计算证据。<br/><br/>贡献点：<br/>1. 提出首个结合自监督学习的语音到发音映射框架，包含特征提取器、逆模型和合成器三部分模块化设计<br/>2. 发现wav2vec 2.0模型的中间层特征相比传统MFCC，在发音轨迹学习和说话人不变性方面表现更优<br/>3. 通过实验验证自监督学习能实现与人类发音模式相匹配的语义表征，提升发音动作的可区分性<br/>4. 为语言发育理论提供计算证据，证明感知语音类别学习可引导发音产出能力的发展<br/>5. 建立语音感知与发音生成的关联性，揭示婴儿在无明确指导下解决复杂语音映射问题的潜在机制|
|2509.03913v2|[SwinSRGAN: Swin Transformer-based Generative Adversarial Network for   High-Fidelity Speech Super-Resolution](http://arxiv.org/abs/2509.03913v2)|**总结（100字以内）:**  <br/>提出SwinSRGAN，基于MDCT幅度的端到端语音超分辨率框架，结合Swin Transformer和混合对抗训练，引入稀疏感知正则化，实现多采样率实时处理，显著提升性能并具备强泛化能力。<br/><br/>**分点贡献:**  <br/>1. **提出SwinSRGAN框架**：采用端到端设计，基于MDCT幅度进行语音超分辨率，避免传统两阶段mel-vocoder管道的表示不匹配问题。  <br/>2. **创新网络结构**：基于Swin Transformer构建U-Net，有效捕捉长程频谱-时序依赖关系，提升建模能力。  <br/>3. **混合对抗训练方案**：结合时间域MPD/MSD判别器与多频段MDCT判别器，专攻高频带内容，增强对抗效果。  <br/>4. **稀疏感知正则化**：引入arcsinh压缩MDCT的稀疏感知正则化，优化瞬态成分保留，减少过度平滑问题。  <br/>5. **多采样率实时处理**：支持单次通过中将不同采样率信号统一上采样至48kHz，实现端到端实时性。  <br/>6. **性能提升验证**：在标准基准测试中，显著降低客观误差并提高ABX偏好评分，验证模型有效性。  <br/>7. **强泛化能力**：在HiFi-TTS零样本测试中无需微调即优于NVSR和mdctGAN，证明跨数据集适应性。|
|2509.00683v2|[PicoAudio2: Temporal Controllable Text-to-Audio Generation with Natural   Language Description](http://arxiv.org/abs/2509.00683v2)|总结：该研究提出PicoAudio2框架，通过结合真实与模拟数据及细粒度文本输入，提升TTA生成的时序控制与音质，支持开放文本查询。<br/><br/>贡献点：<br/>1. **解决数据限制**：引入真实音频-文本数据集的时间戳标注，结合模拟数据训练，突破合成数据主导的音频质量瓶颈。<br/>2. **增强架构设计**：提出融合时间戳矩阵与自由文本输入的混合架构，实现细粒度时序控制与粗粒度文本语义的协同建模。<br/>3. **提升开放性**：支持基于开放自由文本的音频生成，克服传统模型依赖闭合词汇表的局限。<br/>4. **实验验证优势**：在真实数据集上验证PicoAudio2在时序可控性和音频质量上的显著提升。|
|2509.00230v2|[Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0,   XLS-R, and Whisper for Speaker Identification Tasks](http://arxiv.org/abs/2509.00230v2)|总结：本文比较了Wav2Vec 2.0、XLS-R和Whisper在说话人识别任务中的表现，揭示了不同模型对说话人特征的捕捉特性及其最优层数。<br/><br/>贡献点：<br/>1. 首次系统评估三类先进语音编码器（Wav2Vec 2.0/XLS-R/Whisper）在说话人识别任务中的性能差异。<br/>2. 提出通过SVCCA、k-means和t-SNE分析模型层间表示，揭示各模型说话人特征提取的层次差异。<br/>3. 发现Wav2Vec 2.0与XLS-R在早期层有效捕捉说话人特征，微调显著提升其稳定性与识别性能。<br/>4. 证实Whisper的说话人信息主要存在于深层，其性能优势伴随模型深度增加。<br/>5. 为说话人识别任务提供各模型的最优Transformer层数配置方案。|
|2509.00221v2|[Speech Foundation Models Generalize to Time Series Tasks from Wearable   Sensor Data](http://arxiv.org/abs/2509.00221v2)|总结（100字以内）：  <br/>本研究展示语音基础模型可迁移到可穿戴传感器时序任务，通过简单探针方法提升数据稀缺场景的性能，首次证明语音模型的卷积编码器对传感器应用的有效性，推动跨模态时间序列模型的统一发展。<br/><br/>贡献点：  <br/>1. **跨模态表示能力**：提出语音基础模型（如HuBERT、wav2vec 2.0）的表示可泛化至传感器时序任务，突破语音领域限制。  <br/>2. **性能优势**：在情绪分类、心律检测、活动识别等任务中，基于语音模型的探针超越模态专用自监督模型。  <br/>3. **卷积编码器关键性**：发现语音模型的卷积特征编码器对传感器数据处理尤为重要，揭示其跨领域有效性。  <br/>4. **数据稀缺场景优化**：通过简单探针方法提升低资源时间序列任务的性能，降低模型训练成本。  <br/>5. **统一模型方向**：为开发融合语音与传感器模态的通用时间序列模型提供新思路，促进多模态研究。|